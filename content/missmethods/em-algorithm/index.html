---
title: Expectation Maximisation Algorithm
summary: 
tags:
- Expectation Maximisation Algorithm
- Likelihood Based Methods Ignorable
date: "2016-04-27T00:00:00Z"

# Optional external URL for project (replaces project detail page).
external_link: ""

categories: ["rubric"]
bibliography: [emalgorithm.bib]

reading_time: false  # Show estimated reading time?
share: false  # Show social sharing links?
profile: false  # Show author profile?
commentable: false  # Allow visitors to comment? Supported by the Page, Post, and Docs content types.
editable: true  # Allow visitors to edit the page? Supported by the Page, Post, and Docs content types.

image:
  caption: Photo by rawpixel on Unsplash
  focal_point: Smart
  preview_only: true

links:
#- icon: 
#  icon_pack: 
#  name: 
#  url: 
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""
url_poster: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example
---



<p>Patterns of incomplete data in practice often do not have the forms that allow explicit <em>Maximum Likelihood</em>(ML) estimates to be calculated. Suppose we have a model for the complete data <span class="math inline">\(Y\)</span>, with density <span class="math inline">\(f(Y\mid \theta)\)</span>, indexed by the set of unknown parameters <span class="math inline">\(\theta\)</span>. Writing <span class="math inline">\(Y=(Y_0,Y_1)\)</span> in terms of the observed <span class="math inline">\(Y_0\)</span> and missing <span class="math inline">\(Y_1\)</span> components, and assuming that the missingness mechanism is <em>Missing At Random</em>(MAR), we want to maximise the likelihood</p>
<p>\[
L\left(\theta \mid Y_0 \right) = \int f\left(Y_0, Y_1 \mid \theta \right)dY_1
\]</p>
<p>with respect to <span class="math inline">\(\theta\)</span>. When the likelihood is differentiable and unimodal, ML estimates can be found by solving the likelihood equation</p>
<p>\[
D_l\left(\theta \mid Y_0 \right) \equiv \frac{\partial ln L\left(\theta \mid Y_0 \right)}{\partial \theta} = 0,
\]</p>
<p>while, if a closed-form solution cannot be found, iterative methods can be applied. One of these methods is the popular <em>Expectation Maximisation</em>(EM) algorithm (<span class="citation">Dempster, Laird, and Rubin (1977)</span>).</p>
<p>The EM algorithm is a general iterative method for ML estimation in incomplete data problems. The basic idea behind it is based on a sequence of steps:</p>
<ul>
<li><p>Replace missing values by estimated values</p></li>
<li><p>Estimate the parameters</p></li>
<li><p>Re-estimate the missing values assuming the new parameter estimates are correct</p></li>
<li><p>Re-estimate parameters</p></li>
</ul>
<p>The procedure is then iterated until apparent convergence. Each iteration of EM consists of an <em>expectation step</em> (E step) and a <em>maximisation step</em> (M step) which ensure that, under general conditions, each iteration increases the loglikelihood <span class="math inline">\(l(\theta \mid Y_0)\)</span>. In addition, if the loglikelihood is bounded, the sequence <span class="math inline">\(\{l(\theta_t \mid Y_0), t=(0,1,\ldots)\}\)</span> converges to a stationary value of <span class="math inline">\(l(\theta \mid Y_0)\)</span>.</p>
<div id="the-e-step-and-the-m-step" class="section level2">
<h2>The E step and the M step</h2>
<p>The M step simply consists of performing ML estimation of <span class="math inline">\(\theta\)</span> as if there were no missing data, that is, after they had been filled in. The E step finds the conditional expectation of the missing values given the observed data and current estimated parameters. In practice, EM does not necessarily substitute the missing values themselves but its key idea is that they are generally not <span class="math inline">\(Y_0\)</span> but the functions of <span class="math inline">\(Y_0\)</span> appearing in the complete data loglikelihood <span class="math inline">\(l(\theta \mid Y)\)</span>. Specifically, let <span class="math inline">\(\theta_t\)</span> be the current estimate of <span class="math inline">\(\theta\)</span>, then the E step finds the expected complete data loglikelihood if <span class="math inline">\(\theta\)</span> were <span class="math inline">\(\theta_t\)</span>:</p>
<p>\[
Q\left(\theta \mid \theta_t \right) = \int l\left(\theta \mid Y \right)f\left(Y_0 \mid Y_1 , \theta = \theta_t \right)dY_0.
\]</p>
<p>The M step determines <span class="math inline">\(\theta_{t+1}\)</span> by maximising this expected complete data loglikelihood:</p>
<p>\[
Q\left(\theta_{t+1} \mid \theta_t \right) \geq Q\left(\theta \mid \theta_t \right),
\]</p>
<p>for all <span class="math inline">\(\theta\)</span>.</p>
<div id="univariate-normal-data-example" class="section level3">
<h3>Univariate Normal Data Example</h3>
<p>Suppose <span class="math inline">\(y_i\)</span> form a an iid sample from a Normal distribution with population mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, for <span class="math inline">\(i=1,\ldots,n_{cc}\)</span> observed units and <span class="math inline">\(i=n_{cc}+1,\ldots,n\)</span> missing units. Under the assumption that the missingness mechanism is ignorable, the expectation of each missing <span class="math inline">\(y_i\)</span> given <span class="math inline">\(Y_{obs}\)</span> and <span class="math inline">\(\theta=(\mu,\sigma^2)\)</span> is <span class="math inline">\(\mu\)</span>. Since the loglikelihood based on all <span class="math inline">\(y_i\)</span> is linear in the sufficient statistics <span class="math inline">\(\sum_{i=1}^n y_i\)</span> and <span class="math inline">\(\sum_{i=1}^n y^2_i\)</span>, the E step of the algorithm calculates</p>
<p>\[
E\left(\sum_{i=1}^{n}y_i \mid \theta_t, Y_0 \right) = \sum_{i=1}^{n_{cc}}y_i + (n-n_{cc})\mu_t
\]</p>
<p>and</p>
<p>\[
E\left(\sum_{i=1}^{n}y^2_i \mid \theta_t, Y_0 \right) = \sum_{i=1}^{n_{cc}}y^2_i + (n-n_{cc})\left(\mu^2_t + \sigma^2_t \right)
\]</p>
<p>for current estimates <span class="math inline">\(\theta_t=(\mu_t,\sigma_t)\)</span> of the parameters. Note that simply substituting <span class="math inline">\(\mu_t\)</span> for the missing values <span class="math inline">\(y_{n_{cc}+1},\ldots,y_n\)</span> is not correct since the term <span class="math inline">\((n-n_{cc})(\sigma_t^2)\)</span> is omitted. Without missing data, the ML estimate of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are <span class="math inline">\(\frac{\sum_{i=1}^ny_i}{n}\)</span> and <span class="math inline">\(\frac{\sum_{i=1}^ny^2_i}{n}-\left(\frac{\sum_{i=1}^ny_i}{n}\right)^2\)</span>, respectively. The M step uses the same expressions based on the current expectations of the sufficient statistics calculated in the E step. Thus, the M step calculates</p>
<p>\[
\mu_{t+1} = \frac{E\left(\sum_{i=1}^n y_i \mid \theta_t, Y_0 \right)}{n}
\]</p>
<p>and</p>
<p>\[
\sigma^2_{t+1} = \frac{E\left(\sum_{i=1}^n y^2_i \mid \theta_t, Y_0 \right)}{n} - \mu^2_{t+1}.
\]</p>
<p>Setting <span class="math inline">\(\mu_t=\mu_{t+1}=\hat{\mu}\)</span> and <span class="math inline">\(\sigma_t=\sigma_{t+1}=\hat{\sigma}\)</span> in these equations shows that a fixed point of these iterations is <span class="math inline">\(\hat{\mu}=\frac{\sum_{i=1}^{n_{cc}}y_i}{n_{cc}}\)</span> and <span class="math inline">\(\hat{\sigma}^2=\frac{\sum_{i=1}^{n_{cc}}y^2_i}{n_{cc}} - \hat{\mu}^2\)</span>, which are the ML estimates of the parameters from <span class="math inline">\(Y_0\)</span> assuming MAR and distinctness of the parameters.</p>
</div>
</div>
<div id="extensions-of-em" class="section level2">
<h2>Extensions of EM</h2>
<p>There are a variety of applications where the M step does not have a simple computational form. In such cases, one way to avoid an iterative M step is to increase the Q function, rather than maximising it at each iteration, which corresponds to a <em>Generalised Expectation Maximisation</em>(GEM) algorithm. GEM inceases the likelihood at each iteration but appropriate convergence is not guaranteed without further specification of the process of increasing the Q function. One specific case of GEM is the <em>Expectation Conditional Maximisation</em>(ECM) algorithm (<span class="citation">Meng and Rubin (1993)</span>), which replaces the M step with a sequence of <span class="math inline">\(S\)</span> conditional maximisation (CM) steps, each of which maximises the Q function over <span class="math inline">\(\theta\)</span> but with some vector function of <span class="math inline">\(\theta\)</span>, say <span class="math inline">\(g_s(\theta)\)</span>, fixed at its previous values for <span class="math inline">\(s=1,\ldots,S\)</span>. Very briefly, assume that we have a parameter <span class="math inline">\(\theta\)</span> that can be partitioned into subvectors <span class="math inline">\(\theta=(\theta_1,\ldots,\theta_S)\)</span>, then we can take the <span class="math inline">\(s\)</span>-th of the CM steps to be maximisation with respect to <span class="math inline">\(\theta_s\)</span> with all other parameters held fixed. Alternatively, it may be useful to take the <span class="math inline">\(s\)</span>-th of the CM steps to be simultaneous maximisation over all of the subvectors expect <span class="math inline">\(\theta_s\)</span>, which is fixed. Because the ECM increases Q, it belongs to the class of GEM algorithms and therefore monotonically increases the likelihood of <span class="math inline">\(\theta\)</span>. When the set of functions <span class="math inline">\(g\)</span> is “space filling” in the sense that it allows unconstrained maximisation over <span class="math inline">\(\theta\)</span> in its parameter space, ECM converges to a stationary point under the same conditions ensuring convergence of EM.</p>
<p>The <em>Expectation Conditional Maximisation Either</em>(ECME) algorithm (<span class="citation">Liu and Rubin (1994)</span>) is another version of GEM, which replaces some of the CM steps of ECM, maximising the constrained expected complete data loglikelihood function, with steps that maximise the correspondingly constrained actual likelihood function. The algorithm has stable monotone convergence and basic simplicity implementation relative to competing faster converging methods, and can have faster convergence rate than EM or ECM, measured using either the number of iterations or actual computer time. The The <em>Alternative Expectation Conditional Maximisation</em>(AECM) algorithm (<span class="citation">Meng and Van Dyk (1997)</span>) builds on the ECME idea by maximising functions other than Q or L in particular CM steps, corresponding to varying definitions of what constitutes missing data. An iteration of AECM consists of cycles, each consisting of an E step with a particular definition of complete and missing data, followed by CM steps, which can result in enhanced computational efficiency.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-dempster1977maximum">
<p>Dempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the Em Algorithm.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 39 (1): 1–22.</p>
</div>
<div id="ref-liu1994ecme">
<p>Liu, Chuanhai, and Donald B Rubin. 1994. “The Ecme Algorithm: A Simple Extension of Em and Ecm with Faster Monotone Convergence.” <em>Biometrika</em> 81 (4): 633–48.</p>
</div>
<div id="ref-meng1993maximum">
<p>Meng, Xiao-Li, and Donald B Rubin. 1993. “Maximum Likelihood Estimation via the Ecm Algorithm: A General Framework.” <em>Biometrika</em> 80 (2): 267–78.</p>
</div>
<div id="ref-meng1997algorithm">
<p>Meng, Xiao-Li, and David Van Dyk. 1997. “The Em Algorithm—an Old Folk-Song Sung to a Fast New Tune.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 59 (3): 511–67.</p>
</div>
</div>
</div>
