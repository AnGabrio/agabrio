---
title: Augmented Inverse Probability Weighting
summary: 
tags:
- Weighting Methods
- Semiparametric Methods
- Weighting Adjustments
- Inverse Probability Weighting
- Augmented Inverse Probability Weighting
date: "2016-04-27T00:00:00Z"

# Optional external URL for project (replaces project detail page).
external_link: ""

categories: ["rubric"]
bibliography: [aipw.bib]

reading_time: false  # Show estimated reading time?
share: false  # Show social sharing links?
profile: false  # Show author profile?
commentable: false  # Allow visitors to comment? Supported by the Page, Post, and Docs content types.
editable: true  # Allow visitors to edit the page? Supported by the Page, Post, and Docs content types.

image:
  caption: Photo by rawpixel on Unsplash
  focal_point: Smart
  preview_only: true

links:
#- icon: 
#  icon_pack: 
#  name: 
#  url: 
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""
url_poster: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example
---



<p>A general problem associated with the implementatio of <em>Inverse Probability Weighting</em> (IPW) methods is that information in some available data is ignored by focussing only on the complete cases (<span class="citation">Schafer and Graham (2002)</span>). This has provided room to extend these methods to make a more efficient use of the available information through the incorporation of an “augmentation” term, which lead to the development of the so called <em>Augmented Inverse Probability Weighting</em> (AIPW) methods. These approaches extend IPW methods by creating predictions from a model to recover the information in the incomplete units and applying IPW to the residuals from the model (<span class="citation">Little and Rubin (2019)</span>).</p>
<p>Considering the IPW <em>Generalised Estimating Equation</em> (GEE)</p>
<p>\[
\sum_{i=1}^{n_r} = w_i(\hat{\alpha})D_i(x_i,\beta)(y_i-g(x_i,\beta))=0,
\]</p>
<p>where <span class="math inline">\(w_i(\hat{\alpha})=\frac{1}{p(x_i,z_i \mid \hat{\alpha})}\)</span>, with <span class="math inline">\(p(x_i,z_i \mid \hat{\alpha})\)</span> an estimate of the probability of being a complete unit estimated for example using logistic regressions of the missingness indicator <span class="math inline">\(m_i\)</span> on the vectors of the covariate and auxiliary variables <span class="math inline">\(x_i\)</span> and <span class="math inline">\(z_i\)</span>, respectively. A problem of this IPW estimator is that it has poor small sample properties when the propensity score gets close to zero or one for some observations, which will lead to high variance in the estimator. AIPW methods can provide estimators of <span class="math inline">\(\beta\)</span> which are more efficient than their nonaugmented IPW versions. In general, AIPW estimating functions provide a method for constructing estimators of <span class="math inline">\(\beta\)</span> based on two terms:</p>
<ol style="list-style-type: decimal">
<li><p>The usual IPW term <span class="math inline">\(p(x_i,z_i \mid \hat{\alpha})\)</span></p></li>
<li><p>An augmentation term <span class="math inline">\(g^\star(x_i,\beta)\)</span></p></li>
</ol>
<p>The basis for the first term is a complete data unbiased estimating function for <span class="math inline">\(\beta\)</span>, whereas the basis for the second term is some function of the observed data chosen so it has conditional mean of zero given the complete data (<span class="citation">Molenberghs et al. (2014)</span>).</p>
<div id="doubly-robust-estimators" class="section level2">
<h2>Doubly Robust Estimators</h2>
<p>An important class of AIPW methods is known as <em>doubly robust</em> estimators, which have desirable robustness properties (<span class="citation">Robins, Rotnitzky, and Laan (2000)</span>,<span class="citation">Robins and Rotnitzky (2001)</span>). The key feature of these estimators is that they relax the assumption that the model of the missingness probabilities is correctly specified, although requiring additional assumptions on the model for <span class="math inline">\(y_i \mid x_i\)</span>. For example, doubly robust estimators for a population mean parameter <span class="math inline">\(\mu\)</span> could be obtained as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Fit a logistic regression model for the probability of observing <span class="math inline">\(y_i\)</span> as a function of <span class="math inline">\(x_i\)</span> and <span class="math inline">\(z_i\)</span> to derive the individual weights <span class="math inline">\(w_i(\hat{\alpha})\)</span>.</p></li>
<li><p>Fit a generalized linear model for the outcome of responders in function of <span class="math inline">\(x_i\)</span> using weights <span class="math inline">\(w_i(\hat{\alpha})\)</span> and let <span class="math inline">\(g^\star(x_i,\beta)\)</span> denote the fitted values for subject <span class="math inline">\(i\)</span>.</p></li>
<li><p>Take the sample average of the fitted values <span class="math inline">\(g^\star(x_i,\beta)\)</span> of both respondents and nonrespondents as an estimate of the population mean <span class="math inline">\(\hat{\mu}\)</span></p></li>
</ol>
<p>Doubly robust estimators require the specification of two models: one for the missingness probability and another for the distribution of the incomplete data. When the augmentation term <span class="math inline">\(g^\star(x_i,\beta)\)</span> is selected and modelled correctly according to the distribution of the complete data, the resulting estimator of <span class="math inline">\(\beta\)</span> is consistent even if the model of missingness is misspecified. On the other hand, if the model of missingness is correctly specified, the augmentation term no longer needs to be correctly specified to yield consistent estimators of <span class="math inline">\(\beta\)</span> (<span class="citation">Scharfstein, Daniels, and Robins (2003)</span>,<span class="citation">Bang and Robins (2005)</span>). Doubly robust estimators therefore allow to obtain an unbiased estimating function for <span class="math inline">\(\beta\)</span> if either the model for the incomplete data or the model for the missingness mechanism has been correctly specified.</p>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>Suppose the full data consists of a single outcome variable <span class="math inline">\(y\)</span> and an additional variable <span class="math inline">\(z\)</span> and that the objective is to estimate the population outcome mean <span class="math inline">\(\mu=\text{E}[y]\)</span>. When <span class="math inline">\(y\)</span> is partially observed (while <span class="math inline">\(Z\)</span> is always fully observed), individuals may fall into one of two missingness patterns <span class="math inline">\(r=(r_{y},r_{z})\)</span>, namely <span class="math inline">\(r=(1,1)\)</span> if both variables are observed or <span class="math inline">\(r=(1,0)\)</span> if <span class="math inline">\(y\)</span> is missing. Let <span class="math inline">\(c=1\)</span> if <span class="math inline">\(r=(1,1)\)</span> and <span class="math inline">\(c=0\)</span> otherwise, so that the observed data can be summarised as <span class="math inline">\((c,cy,z)\)</span>. Assuming that missingness only depends on <span class="math inline">\(z\)</span>, that is</p>
<p>\[
p(c=1 \mid y,z)=p(c=1 \mid z)=\pi(z),
\]</p>
<p>then the missing data mechanism is <em>Missing At Random</em> (MAR). Under these conditions, consider the consistent IPW complete case estimating equation</p>
<p>\[
\sum_{i=1}^n\frac{c_i}{\pi(z_i \mid \hat{\alpha})}(y_i-\mu)=0,
\]</p>
<p>which can be used to weight the contribution of each complete case by the inverse of <span class="math inline">\(\pi(z_i \mid \hat{\alpha})\)</span>, typically estimated via logistic regressions. A general problem of this type of estimators is that they discard all the available data among the non-completers and are therefore inefficient. However, it is possible to augment the simple IPW complete case estimating equation to improve efficiency. The optimal estimator for <span class="math inline">\(\mu\)</span> within this class is the solution to the estimating equation</p>
<p>\[
\sum_{i=1}^n \left(\frac{c_i}{\pi(z_i \mid \hat{\alpha})}(y_i-\mu) - \frac{c_i-\pi(z_i \mid \hat{\alpha})}{\pi(z_i \mid \hat{\alpha})}\text{E}[(y_i-\mu)\mid z_i] \right),
\]</p>
<p>which leads to the estimator</p>
<p>\[
\mu_{aipw}=\frac{1}{n}\sum_{i=1}^n \left(\frac{c_iy_i}{\pi(z_i\mid \hat{\alpha})} - \frac{c_i - \pi(z_i\mid \hat{\alpha})}{\pi(z_i\mid \hat{\alpha})} \text{E}[y_i \mid z_i] \right).
\]</p>
<p>The conditional expectation <span class="math inline">\(\text{E}[y_i \mid z_i]\)</span> is not known and must be estimated from the data. Under a <em>Missing At Random</em> (MAR) assumption we have that <span class="math inline">\(\text{E}[y \mid z]=\text{E}[y \mid z, c=1]\)</span>, that is the conditional expecation of <span class="math inline">\(y\)</span> given <span class="math inline">\(z\)</span> is the same as that among the completers. Thus, we can specify a model <span class="math inline">\(m(z,\xi)\)</span> for <span class="math inline">\(\text{E}[y \mid z]\)</span>, indexed by the parameter <span class="math inline">\(\xi\)</span>, that can be estimated from the completers. If <span class="math inline">\(y\)</span> is continuous, a simple choice is to estimate <span class="math inline">\(\hat{\xi}\)</span> by OLS from the completers. The AIPW estimator for <span class="math inline">\(\mu\)</span> then becomes</p>
<p>\[
\mu_{aipw}=\frac{1}{n}\sum_{i=1}^n \left(\frac{c_iy_i}{\pi(z_i\mid \hat{\alpha})} - \frac{c_i - \pi(z_i\mid \hat{\alpha})}{\pi(z_i\mid \hat{\alpha})} m(z_i\mid \hat{\xi}) \right).
\]</p>
<p>It can be shown that this estimator is more efficient that the simple IPW complete case estimator for <span class="math inline">\(\mu\)</span> and that it has a double robustness property. This ensures that <span class="math inline">\(\mu_{aipw}\)</span> is a consitent estimator of <span class="math inline">\(\mu\)</span> if <strong>either</strong></p>
<ul>
<li><p>the model <span class="math inline">\(\pi(z\mid\alpha)\)</span> is correctly specified, <strong>or</strong></p></li>
<li><p>the model <span class="math inline">\(m(z\mid \xi)\)</span> is correctly specified.</p></li>
</ul>
<p>To see a derivation of the double robustness property I put here a link to some nice <a href="https://www4.stat.ncsu.edu/~davidian/st790/notes/chap5.pdf">paper</a>.</p>
</div>
<div id="conlcusions" class="section level2">
<h2>Conlcusions</h2>
<p>As all weighting methods, such as IPW, AIPW methods are <em>semiparametric</em> methods that aim to achieve robustness and good performance over more general classes of population distributions. However, semiparametric estimators can be less efficient and less powerful than <em>Maximum Likelihood</em> or <em>Bayesian</em> estimators under a well specified parametric model. With missing data, <span class="citation">Rubin (1976)</span> results show that likelihood-based methods perform uniformly well over any <em>Missing At Random</em> (MAR) missingness distribution, and the user does not need to specify that distribution. However, semiparametric methods that relax assumptions about the data must in turn assume a specific form for the distribution of missingness. It has been argued that, for these semiparametric methods to gain a substantial advantage over well-specified likelihood methods, the parametric model has to be grossly misspecified (<span class="citation">Meng (2000)</span>).</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-bang2005doubly">
<p>Bang, Heejung, and James M Robins. 2005. “Doubly Robust Estimation in Missing Data and Causal Inference Models.” <em>Biometrics</em> 61 (4): 962–73.</p>
</div>
<div id="ref-little2019statistical">
<p>Little, Roderick JA, and Donald B Rubin. 2019. <em>Statistical Analysis with Missing Data</em>. Vol. 793. John Wiley &amp; Sons.</p>
</div>
<div id="ref-meng2000missing">
<p>Meng, Xiao-Li. 2000. “Missing Data: Dial M for???” <em>Journal of the American Statistical Association</em> 95 (452): 1325–30.</p>
</div>
<div id="ref-molenberghs2014handbook">
<p>Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. <em>Handbook of Missing Data Methodology</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-robins2001comment">
<p>Robins, James M, and Andrea Rotnitzky. 2001. “Comment on the Bickel and Kwon Article,‘Inference for Semiparametric Models: Some Questions and an Answer’.” <em>Statistica Sinica</em> 11 (4): 920–36.</p>
</div>
<div id="ref-robins2000profile">
<p>Robins, James M, Andrea Rotnitzky, and Mark van der Laan. 2000. “On Profile Likelihood: Comment.” <em>Journal of the American Statistical Association</em> 95 (450): 477–82.</p>
</div>
<div id="ref-rubin1976inference">
<p>Rubin, Donald B. 1976. “Inference and Missing Data.” <em>Biometrika</em> 63 (3): 581–92.</p>
</div>
<div id="ref-schafer2002missing">
<p>Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” <em>Psychological Methods</em> 7 (2): 147.</p>
</div>
<div id="ref-scharfstein2003incorporating">
<p>Scharfstein, Daniel O, Michael J Daniels, and James M Robins. 2003. “Incorporating Prior Beliefs About Selection Bias into the Analysis of Randomized Trials with Missing Outcomes.” <em>Biostatistics</em> 4 (4): 495–512.</p>
</div>
</div>
</div>
