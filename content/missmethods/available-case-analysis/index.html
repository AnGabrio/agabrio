---
title: Available Case Analysis
summary: Available-case analysis also arises when a researcher simply excludes a variable or set of variables from the analysis because of their missing-data rates
tags:
- Delete Case Methods
- Available Case Analysis
- Listwise Deletion
- Complete Case Analysis
date: "2016-04-27T00:00:00Z"
weight: 2

# Optional external URL for project (replaces project detail page).
external_link: ""

categories: ["rubric"]
bibliography: [aca.bib]

reading_time: false  # Show estimated reading time?
share: false  # Show social sharing links?
profile: false  # Show author profile?
commentable: false  # Allow visitors to comment? Supported by the Page, Post, and Docs content types.
editable: true  # Allow visitors to edit the page? Supported by the Page, Post, and Docs content types.

image:
  caption: Photo by rawpixel on Unsplash
  focal_point: Smart
  preview_only: true

links:
#- icon: 
#  icon_pack: 
#  name: 
#  url: 
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""
url_poster: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>Complete case analysis (CCA) can be particularly inefficient for data sets with a large number of variables which are partially observed. An alternative approach that can be used to conduct univariate analyses in known as <em>Available Case Analysis</em> (ACA), which uses all the available cases, separately for each variable under examination, to estimate the quantities of interest.</p>
<p>The main drawback of ACA is that the sample used to perform the analysis varies from variable to variable according to the patterns of missing data, which generates problems of comparability across variables if the missingness mechanism is not <em>missing completely at random</em> (MCAR), i.e. the missing data probabilities depend on the variables under study. While estimates of means and variances can be easily computed, measures of covariation need to be adjusted. In particular, for estimating sample covariances, this approach is known as <em>pairwise deletion</em> or <em>pairwise inclusion</em></p>
<div id="pairwise-measures-of-covariation" class="section level2">
<h2>Pairwise measures of covariation</h2>
<p>One possible approach to estimate pairwise measures of covariation for <span class="math inline">\(y_j\)</span> and <span class="math inline">\(y_k\)</span> is to use only those units <span class="math inline">\(i=1,\ldots,n_{ac}\)</span> for which both variables are observed (<span class="citation">Little and Rubin (2019)</span>). For example, one can compute pairwise sample covariances as:</p>
<p>\[
s^{ac}_{jk} = \frac{\sum_{i \in I_{ac}}(y_{ij}-\bar{y}_{j}^{ac})(y_{ik}-\bar{y}_{k}^{ac})}{(n_{ac}-1)},
\]</p>
<p>where <span class="math inline">\(I_{ac}\)</span> is the set of <span class="math inline">\(n_{ac}\)</span> with both <span class="math inline">\(y_j\)</span> and <span class="math inline">\(y_k\)</span> observed, while the sample means <span class="math inline">\(\bar{y}^{ac}_{j}\)</span> and <span class="math inline">\(\bar{y}^{ac}_{k}\)</span> are calculated over this set of units. We can also estimate the sample correlation</p>
<p>\[
r^{\star}_{jk} = \frac{s^{ac}_{jk}}{\sqrt{s^2_{j}s^{2}_{k}}},
\]</p>
<p>where <span class="math inline">\(s^2_{j}\)</span> and <span class="math inline">\(s^2_{k}\)</span> are the sample variances computed over the sets of observed units <span class="math inline">\(I_{j}\)</span> and <span class="math inline">\(I_{k}\)</span>, respectively. A problem of this type of correlation estimate is that it can lie outside the range <span class="math inline">\((-1,1)\)</span>, which is typically addressed by computing <em>pairwise correlations</em> (<span class="citation">Wilks (1932)</span>), where variances are estimated from the set of units with both variables observed <span class="math inline">\(I_{jk}\)</span>, i.e. </p>
<p>\[
r^{ac}_{jk} = \frac{s^{ac}_{jk}}{\sqrt{s^{2,ac}_{j}s^{2,ac}_{k}}}.
\]</p>
<p>In addition, we could also replace the sample means <span class="math inline">\(\bar{y}^{ac}_{j}\)</span> and <span class="math inline">\(\bar{y}^{ac}_{k}\)</span>, evaluated on the common set of units <span class="math inline">\(I_{jk}\)</span>, with <span class="math inline">\(\bar{y}_{j}\)</span> and <span class="math inline">\(\bar{y}_{k}\)</span>, which are evaluated on the sets of units <span class="math inline">\(I_{j}\)</span> and <span class="math inline">\(I_{k}\)</span>, respectively. This leads to the following estimates for the sample covariances (<span class="citation">Matthai (1951)</span>):</p>
<p>\[
s^{\star}_{jk} = \frac{\sum_{i \in I_{ac}}(y_{ij}-\bar{y}_{j})(y_{ik}-\bar{y}_{k})}{(n_{ac}-1)},
\]</p>
<p>Pairwise AC estimates aim at recovering information from partially-observed units that are lost by CCA. However, when considered together, the estimates suffer from inconsistencies that undermine the validity of these methods. For example, pairwise correlation matrices may be not positive definite. Because parameters are estimated from different sets of units, different approaches can be used to obtain estimate of the measures of uncertainty (<span class="citation">Schafer and Graham (2002)</span>).</p>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>AC estimates allow to make use of all the available evidence in the data and may be more efficient that CCA when the missingness mechanism is MCAR and correlations are modest (<span class="citation">Kim and Curry (1977)</span>). However, when correlations are more substantial, ACA may become even less efficient than CCA (<span class="citation">Haitovsky (1968)</span>, <span class="citation">Azen and Van Guilder (1981)</span>).</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references hanging-indent">
<div id="ref-azen1981conclusions">
<p>Azen, S, and M Van Guilder. 1981. “Conclusions Regarding Algorithms for Handling Incomplete Data.” <em>1981 Proceedings of the Statistical Computing Section</em>, 53–56.</p>
</div>
<div id="ref-haitovsky1968missing">
<p>Haitovsky, Yoel. 1968. “Missing Data in Regression Analysis.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 30 (1): 67–82.</p>
</div>
<div id="ref-kim1977treatment">
<p>Kim, Jae-On, and James Curry. 1977. “The Treatment of Missing Data in Multivariate Analysis.” <em>Sociological Methods &amp; Research</em> 6 (2): 215–40.</p>
</div>
<div id="ref-little2019statistical">
<p>Little, Roderick JA, and Donald B Rubin. 2019. <em>Statistical Analysis with Missing Data</em>. Vol. 793. John Wiley &amp; Sons.</p>
</div>
<div id="ref-matthai1951estimation">
<p>Matthai, Abraham. 1951. “Estimation of Parameters from Incomplete Data with Application to Design of Sample Surveys.” <em>Sankhyā: The Indian Journal of Statistics</em>, 145–52.</p>
</div>
<div id="ref-schafer2002missing">
<p>Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” <em>Psychological Methods</em> 7 (2): 147.</p>
</div>
<div id="ref-wilks1932moments">
<p>Wilks, Samuel S. 1932. “Moments and Distributions of Estimates of Population Parameters from Fragmentary Samples.” <em>The Annals of Mathematical Statistics</em> 3 (3): 163–95.</p>
</div>
</div>
</div>
