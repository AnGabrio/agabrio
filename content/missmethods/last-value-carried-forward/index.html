---
title: Implicit Single Imputation 
summary: 
tags:
- Explicit Single Imputation
- Implicit Single Imputation
- Single Imputation
- Hot Deck Methods
- Predictive Mean Matching
- Last Value Carried Forward
date: "2016-04-27T00:00:00Z"

# Optional external URL for project (replaces project detail page).
external_link: ""

categories: ["rubric"]
bibliography: [lvcf.bib]

reading_time: false  # Show estimated reading time?
share: false  # Show social sharing links?
profile: false  # Show author profile?
commentable: false  # Allow visitors to comment? Supported by the Page, Post, and Docs content types.
editable: true  # Allow visitors to edit the page? Supported by the Page, Post, and Docs content types.

image:
  caption: Photo by rawpixel on Unsplash
  focal_point: Smart
  preview_only: true

links:
#- icon: 
#  icon_pack: 
#  name: 
#  url: 
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""
url_poster: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example
---



<p>All case deletion methods, such as <em>Complete Case Analysis</em>(CCA) or <em>Available Case Analysis</em>(ACA) make no use of units with partially observed data, when estimating the marginal distribution of the variables under study or the covariation between variables. Clearly, this is inefficient and a tempting alternative would be to <em>impute</em> or “fill in” the unobserved data with some plausible values. When a single value is used to replace each missing data, we talk about <em>Single Imputation</em>(SI) methods and, according to the precedure used to generate these imputations, different SI methods can be used. In general, the idea of imputing the missing values is really appealing as it allows to recover the full sample on which standard complete data methods can be applied to derive the estimates of interest.</p>
<p>However, it is important to be aware of the potential problems of imputing missing data without a clear understanding about the process underlying the values we want to impute, which is the key factor to determine whether the selected approach would be plausible in the context considered. Indeed, imputation should be conceptualised as draws from a predictive distribution of the missing values and require methods for creating a predictive distribution for the imputation based on the observed data. According to <span class="citation">Little and Rubin (2019)</span>, these predictive distributions can be created using</p>
<ol style="list-style-type: decimal">
<li><p><em>Explicit modelling</em>, when the distribution is based on formal statistical models which make the underlying assumptions explicit.</p></li>
<li><p><em>Implicit modelling</em>, when the distribution is based on an algorithm which implicitly relies on some underlying model assumptions.</p></li>
</ol>
<p>In this part, we focus on some of the most popular <em>Implicit Single Imputation</em> methods. These include: <em>Hot Deck Imputation</em>(SI-HD), where missing values are imputed using observed values from similar responding units in the sample; <em>Substitution</em>(SI-S), where nonresponding units are replaced with alternative units not yet selected into the sample; <em>Cold Deck Imputation</em>(SI-CD), where missing values are replaced with a constant value from an external source; <em>Composite Methods</em>, which combine procedures from the previous approaches. We will specifically focus on SI-HD methods, which are the most popular among these.</p>
<div id="hot-deck-imputation" class="section level2">
<h2>Hot Deck Imputation</h2>
<p>SI-HD procedures refer to the deck of match <a href="https://en.wikipedia.org/wiki/Punched_card#Hollerith&#39;s_early_punched_card_formats">Hollerith cards</a> for the donors available for a nonrespondent. Suppose that a sample of <span class="math inline">\(n\)</span> out of <span class="math inline">\(N\)</span> units is selected and that <span class="math inline">\(n_{cc}\)</span> out of <span class="math inline">\(n\)</span> are recorded. Given an equal probability sampling scheme, the mean of <span class="math inline">\(y\)</span> can be estimated from the filled-in data as the mean of the responding and the imputed units</p>
<p>\[
\bar{y}_{HD}=\frac{(n_{cc}\bar{y}_{cc}+(n-n_{cc})\bar{y}^{\star})}{n},
\]</p>
<p>where <span class="math inline">\(\bar{y}_{cc}\)</span> is the mean of the responding units, and <span class="math inline">\(\bar{y}^\star=\sum_{i=1}^{n_{cc}}\frac{H_iy_i}{n-n_{cc}}\)</span>. <span class="math inline">\(H_i\)</span> is the number of times <span class="math inline">\(y_i\)</span> is used as substitute for a missing value of <span class="math inline">\(y\)</span>, with <span class="math inline">\(\sum_{i=1}^{n_{cc}}H_i=n-n_{cc}\)</span> being the number of missing units. The proprties of <span class="math inline">\(bar{y}_{HD}\)</span> depend on the procedure used to generate the numbers <span class="math inline">\(H_i\)</span> and in general the mean and sampling variance of this estimator can be written as</p>
<p>\[
E[\bar{y}_{HD}]=E[E[\bar{y}_{HD}\mid y_{obs}]] ;;; \text{and} ;;; Var(\bar{y}_{HD})=Var(E[\bar{y}_{HD} \mid y_{obs}]) + E[Var(\bar{y}_{HD} \mid y_{obs})],
\]</p>
<p>where the inner expectations and variances are taken over the distribution of <span class="math inline">\(H_i\)</span> given the observed data <span class="math inline">\(y_{obs}\)</span>, and the outer expectations and variances are taken over the model distribution of <span class="math inline">\(y\)</span>. The term <span class="math inline">\(E[Var(\bar{y}_{HD} \mid y_{obs})]\)</span> represents the additional sampling variance from the stochastic imputation procedure. Examples of these procedures include <em>predictive mean matching</em> or PMM(<span class="citation">Little and Rubin (2019)</span>) and <em>last value carried forward</em> or LVCF(<span class="citation">Little and Rubin (2019)</span>).</p>
<div id="predictive-mean-matching" class="section level3">
<h3>Predictive Mean Matching</h3>
<p>A general approach to hot-deck imputation is to define a metric <span class="math inline">\(d(i,j)\)</span> measuring the distance between units based on observed variables <span class="math inline">\(x_{i1},\ldots,x_{iJ}\)</span> and then choose the imputed values that come from responding units close to the unit with the missing value, i.e. we choose the imputed value for <span class="math inline">\(y_i\)</span> from a <em>donor pool</em> of units <span class="math inline">\(j\)</span> that are such that <span class="math inline">\(y_j,x_1,\ldots,x_J\)</span> are observed and <span class="math inline">\(d(i,j)\)</span> is less than some value <span class="math inline">\(d_0\)</span>. Varying the value for <span class="math inline">\(d_0\)</span> can control the number of available donors <span class="math inline">\(j\)</span>. When the choice of the metric has the form</p>
<p>\[
d(i,j)=(\hat{y}(x_i)-\hat{y}(x_j))^2,
\]</p>
<p>where <span class="math inline">\(\hat{y}(x_i)\)</span> is the predicted value of <span class="math inline">\(y\)</span> from the regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span> from the complete units, then the procedure is known as PMM. A powerful aspect of this metric is that it weights predictors according to their ability to predict the missing variable, which allows to have some protection against misspecification of the regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span>, even though better approaches are available when good matches to donor units cannot be found or the sample size is small.</p>
</div>
<div id="last-value-carried-forward" class="section level3">
<h3>Last Value Carried Forward</h3>
<p>Longitudinal data are often subject to attrition when units leave the study prematurely. Let <span class="math inline">\(y_i=(y_{i1},\ldots,y_{iJ})\)</span> be a <span class="math inline">\((J\times1)\)</span> vector of partially-observed outcomes for subject <span class="math inline">\(i\)</span>, and denote with <span class="math inline">\(y_{i,obs}\)</span> and <span class="math inline">\(y_{i,mis}\)</span> the observed and missing components of <span class="math inline">\(y_i\)</span>, i.e. <span class="math inline">\(y=(y_{i,obs},y_{i,mis})\)</span>. Define the indicator variable <span class="math inline">\(m_i\)</span> taking value 0 for complete units and <span class="math inline">\(j\)</span> if subject <span class="math inline">\(i\)</span> drops out between <span class="math inline">\(j-1\)</span> and <span class="math inline">\(j\)</span> time points. LVCF, also called <em>last observation carried forward</em>(<span class="citation">Pocock (2013)</span>), imputes all missing values for individual <span class="math inline">\(i\)</span> (for whom <span class="math inline">\(m_i=j\)</span>) using the last recorded value for that unit, that is</p>
<p>\[
\hat{y}_{it}=y_{i,j-1},
\]</p>
<p>where <span class="math inline">\(t=j,\ldots,J\)</span>. Although simple, this approach makes the often unrealistic assumption that the value of the outcome remains unchanged after dropout.</p>
</div>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>According to <span class="citation">Little and Rubin (2019)</span>, imputation should generally be</p>
<ol style="list-style-type: decimal">
<li><p><strong>Conditional</strong> on observed variables, to reduce bias, improve precision and preserve association between variables.</p></li>
<li><p><strong>Multivariate</strong>, to preserve association between missing variables.</p></li>
<li><p><strong>Draws</strong> from the predictive distributions rather than means, to provide valid estimates of a wide range of estimands.</p></li>
</ol>
<p>Nevertheless, a main problem of SI methods is that inferences based on the imputed data do not account for imputation uncertainty and standard errors are therefore systematically underestimated, p-values of tests are too significant and confidence intervals are too narrow.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-little2019statistical">
<p>Little, Roderick JA, and Donald B Rubin. 2019. <em>Statistical Analysis with Missing Data</em>. Vol. 793. John Wiley &amp; Sons.</p>
</div>
<div id="ref-pocock2013clinical">
<p>Pocock, Stuart J. 2013. <em>Clinical Trials: A Practical Approach</em>. John Wiley &amp; Sons.</p>
</div>
</div>
</div>
