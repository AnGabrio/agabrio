---
title: "Item Response Theory Models (Stan)"
author: "Andrea Gabrio"
date: '2020-03-21'
slug: irt-stan
categories:
- item response theory
- Stan
- R
tags:
- tutorials
- Stan
- item response theory
subtitle: ''
summary: ''
lastmod: '2020-03-21T11:54:30+01:00'
featured: no
draft: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
bibliography: citations_stan14.bib
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. <code>BUGS</code> (Bayesian inference Using <em>Gibbs Sampling</em>) is an algorithm and supporting language (resembling <code>R</code>) dedicated to performing the Gibbs sampling implementation of <em>Markov Chain Monte Carlo</em> (MCMC) method. Dialects of the <code>BUGS</code> language are implemented within three main projects:</p>
<ol style="list-style-type: decimal">
<li><p><strong>OpenBUGS</strong> - written in component pascal.</p></li>
<li><p><strong>JAGS</strong> - (Just Another Gibbs Sampler) - written in <code>C++</code>.</p></li>
<li><p><strong>Stan</strong> - a dedicated Bayesian modelling framework written in <code>C++</code> and implementing <em>Hamiltonian</em> MCMC samplers.</p></li>
</ol>
<p>Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of <code>R</code>, and thus, they are best accessed from within <code>R</code> itself. As such there are multiple packages devoted to interfacing with the various software implementations:</p>
<ul>
<li><p><em>R2OpenBUGS</em> - interfaces with <code>OpenBUGS</code></p></li>
<li><p><em>R2jags</em> - interfaces with <code>JAGS</code></p></li>
<li><p><em>rstan</em> - interfaces with <code>Stan</code></p></li>
</ul>
<p>This tutorial will demonstrate how to fit models in <code>Stan</code> (<span class="citation"><a href="#ref-gelman2015stan" role="doc-biblioref">Gelman, Lee, and Guo</a> (<a href="#ref-gelman2015stan" role="doc-biblioref">2015</a>)</span>) using the package <code>rstan</code> (<span class="citation"><a href="#ref-rstanpackage" role="doc-biblioref">Stan Development Team</a> (<a href="#ref-rstanpackage" role="doc-biblioref">2018</a>)</span>) as interface, which also requires to load some other packages.</p>
<div id="overview" class="section level1">
<h1>Overview</h1>
<p>Item response theory (IRT) is a paradigm for investigating the relationship between an individual’s response to a single test item and their performance on an overall measure of the ability or trait that item was intended to measure. Many models exist in the IRT field for evaulating how well an item captures an underlying latent trait, but some of the most popular IRT models are <em>logistic IRT models</em> for dichotmous responses. In particular, the main types of models are:</p>
<p>1 <strong>1 parameter logistic model</strong></p>
<p>2 <strong>2 parameter logistic model</strong></p>
<p>3 <strong>3 parameter logistic model</strong></p>
<p>Throughout this tutorial, I assume that the reader has some basic understanding of IRT model and working knowledge of a software implementation of the <code>Stan</code> language. However, if this is not the case, excellent sources for learning IRT are <span class="citation"><a href="#ref-baker2004item" role="doc-biblioref">Baker and Kim</a> (<a href="#ref-baker2004item" role="doc-biblioref">2004</a>)</span>, who provide a mathematically detailed introduction to IRT, and <span class="citation"><a href="#ref-hambleton1991fundamentals" role="doc-biblioref">Hambleton, Swaminathan, and Rogers</a> (<a href="#ref-hambleton1991fundamentals" role="doc-biblioref">1991</a>)</span>, who give an intuitive introduction to the topic. For an in-depth description of how to implement different types of IRT models in <code>Stan</code>, I also refer to this very nice review of <span class="citation"><a href="#ref-luo2018using" role="doc-biblioref">Luo and Jiao</a> (<a href="#ref-luo2018using" role="doc-biblioref">2018</a>)</span> and this other online <a href="https://quantdev.ssri.psu.edu/sites/qdev/files/IRT_tutorial_FA17.html#3_2-parameter_logistic_(2pl)_irt_model">tutorial</a>.</p>
<p>At the core of all the IRT models presented in this tutorial is the <em>Item Response Function</em> (IRF). The IRF estimates the probability of getting item <span class="math inline">\(j\)</span> “correct” as a function of item characteristics and the <span class="math inline">\(i\)</span>-th individual’s latent trait/ability level (<span class="math inline">\(\theta_i\)</span>). These item response functions are defined by a logistic curve (i.e. an <span class="math inline">\(S\)</span>-shape from <span class="math inline">\(0-1\)</span>).</p>
</div>
<div id="parameter-logistic-model-1plm" class="section level1">
<h1>1 parameter logistic model (1PLM)</h1>
<p>The 1PLM is used for data collected on <span class="math inline">\(n\)</span> individuals who have each given responses on <span class="math inline">\(p\)</span>
different items. The items have binary outcomes, i.e. the items are scored as <span class="math inline">\(1\)</span> if correct and <span class="math inline">\(0\)</span> if not. The <span class="math inline">\(i\)</span>-th individual in the sample is assumed to have a latent ability <span class="math inline">\(\theta_i\)</span>, and the <span class="math inline">\(i\)</span>-th individual’s response on the <span class="math inline">\(j\)</span>-th item is a random variable <span class="math inline">\(Y_{ij}\)</span> with a Bernoulli distribution. The probability that the <span class="math inline">\(i\)</span>-th individual correctly answers the <span class="math inline">\(j\)</span>-th item (i.e. the probability that <span class="math inline">\(Y_{ij} = 1\)</span>) is assumed to have the following IRF form:</p>
<p><span class="math display">\[p_{ij} = P(Y_{ij}=1 \mid \theta_i,\delta_j)=\frac{1}{1+\text{exp}(\theta_i-\delta_j)},\]</span></p>
<p>where <span class="math inline">\(\delta_j\)</span> is the <em>difficulty parameter</em> for item <span class="math inline">\(j\)</span> of the test, and is assumed to be normally distributed according to some mean <span class="math inline">\(\mu_{\delta}\)</span> and standard deviation <span class="math inline">\(\sigma_{\delta}\)</span> which must be specified by the analyst. Each <em>latent ability parameter</em> <span class="math inline">\(\theta_i\)</span> is also assumed to be distributed according to a standard normal distribution.</p>
<div id="load-the-data" class="section level2">
<h2>Load the data</h2>
<p>I read in the data from the file <code>wideformat.csv</code>, which contains (simulated) data from <span class="math inline">\(n=1000\)</span> individuals taking a <span class="math inline">\(5\)</span>-item test. Items are coded <span class="math inline">\(1\)</span> for correct and <span class="math inline">\(0\)</span> for incorrect responses. When we get descriptives of the data, we see that the items differ in terms of the proportion of people who answered correctly, so we expect that we have some differences in item difficulty here.</p>
<pre class="r"><code>&gt; data_dicho&lt;-read.csv(&quot;wideformat.csv&quot;, sep = &quot;,&quot;)
&gt; head(data_dicho)
       ID gender age Item.1 Item.2 Item.3 Item.4 Item.5
1 person1   Male  40      0      0      0      0      0
2 person2 Female  27      0      0      0      0      0
3 person3   Male  13      0      0      0      0      0
4 person4 Female  17      0      0      0      0      1
5 person5 Female  30      0      0      0      0      1
6 person6 Female  46      0      0      0      0      1
&gt; 
&gt; #check proportion of correct responses by item
&gt; apply(data_dicho[,4:8], 2, sum)/nrow(data_dicho)
Item.1 Item.2 Item.3 Item.4 Item.5 
 0.924  0.709  0.553  0.763  0.870 
&gt; 
&gt; #summarise the data
&gt; library(psych)
&gt; describe(data_dicho)
        vars    n   mean     sd median trimmed    mad min  max range  skew
ID*        1 1000 500.50 288.82  500.5  500.50 370.65   1 1000   999  0.00
gender*    2 1000   1.50   0.50    2.0    1.51   0.00   1    2     1 -0.02
age        3 1000  25.37  14.43   25.0   25.36  17.79   1   50    49  0.01
Item.1     4 1000   0.92   0.27    1.0    1.00   0.00   0    1     1 -3.20
Item.2     5 1000   0.71   0.45    1.0    0.76   0.00   0    1     1 -0.92
Item.3     6 1000   0.55   0.50    1.0    0.57   0.00   0    1     1 -0.21
Item.4     7 1000   0.76   0.43    1.0    0.83   0.00   0    1     1 -1.24
Item.5     8 1000   0.87   0.34    1.0    0.96   0.00   0    1     1 -2.20
        kurtosis   se
ID*        -1.20 9.13
gender*    -2.00 0.02
age        -1.21 0.46
Item.1      8.22 0.01
Item.2     -1.16 0.01
Item.3     -1.96 0.02
Item.4     -0.48 0.01
Item.5      2.83 0.01</code></pre>
</div>
<div id="fit-the-model" class="section level2">
<h2>Fit the model</h2>
<p>We fit the 1PLM to the data. First, I rename and preprocess the data to be passed to <code>Stan</code>.</p>
<pre class="r"><code>&gt; Y&lt;-data_dicho[,4:8]
&gt; n&lt;-nrow(Y)
&gt; p&lt;-ncol(Y)
&gt; data_list&lt;-list(Y=Y,n=n,p=p)</code></pre>
<p>Then I specify the model using the following <code>Stan</code> code.</p>
<pre class="r"><code>&gt; model1&lt;-&quot;
+ data {
+ int&lt;lower=0&gt; n;
+ int&lt;lower=0&gt; p;
+ int&lt;lower=0,upper=1&gt; Y[n,p];
+ }
+ parameters {
+ vector[n] theta;
+ vector[p] delta;
+ real mu_delta;
+ real&lt;lower=0&gt; sigma_delta;
+ }
+ transformed parameters{
+ vector&lt;lower=0,upper=1&gt;[p] prob[n];
+  for(i in 1:n){
+   for (j in 1:p){
+    prob[i,j] = inv_logit(theta[i] - delta[j]);
+   }
+  }
+ }
+ model {
+ theta ~ normal(0,1);
+ delta ~ normal(mu_delta,sigma_delta);
+ mu_delta ~ normal(0,5);
+ sigma_delta ~ cauchy(0,5);
+  for(i in 1:n){
+   for (j in 1:p){
+    Y[i,j] ~ bernoulli(prob[i,j]);
+   }
+  }
+ }
+ generated quantities {
+ vector[p] loglik_y[n];
+ vector[p] Y_rep[n];
+  for (i in 1: n){
+   for (j in 1: p){
+     loglik_y[i,j] = bernoulli_lpmf(Y[i,j] | prob[i,j]);
+     Y_rep[i,j] = bernoulli_rng(prob[i,j]); 
+   }
+  }
+ }
+ &quot;
&gt; ## write the model to a text file
&gt; writeLines(model1, con = &quot;model1PLM.stan&quot;)</code></pre>
<p>Next, I define the nodes (parameters and derivatives) to monitor and the chain parameters.</p>
<pre class="r"><code>&gt; params &lt;- c(&quot;delta&quot;, &quot;theta&quot;, &quot;prob&quot;,&quot;loglik_y&quot;,&quot;Y_rep&quot;)
&gt; nChains = 2
&gt; burnInSteps = 500
&gt; thinSteps = 1
&gt; numSavedSteps = 2500  #across all chains
&gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&gt; nIter
[1] 1750</code></pre>
<p>Start the <code>Stan</code> model (check the model, load data into the model, specify the number of chains and compile the model). Run the <code>Stan</code> code via the <code>rstan</code> interface and the <code>stan</code> function.</p>
<pre class="r"><code>&gt; library(rstan)
&gt; set.seed(3456)
&gt; model1_stan&lt;- stan(data = data_list, file = &quot;model1PLM.stan&quot;, 
+                    chains = nChains, pars = params, iter = nIter, 
+                    warmup = burnInSteps, thin = thinSteps)

SAMPLING FOR MODEL &#39;model1PLM&#39; NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.001 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 1750 [  0%]  (Warmup)
Chain 1: Iteration:  175 / 1750 [ 10%]  (Warmup)
Chain 1: Iteration:  350 / 1750 [ 20%]  (Warmup)
Chain 1: Iteration:  501 / 1750 [ 28%]  (Sampling)
Chain 1: Iteration:  675 / 1750 [ 38%]  (Sampling)
Chain 1: Iteration:  850 / 1750 [ 48%]  (Sampling)
Chain 1: Iteration: 1025 / 1750 [ 58%]  (Sampling)
Chain 1: Iteration: 1200 / 1750 [ 68%]  (Sampling)
Chain 1: Iteration: 1375 / 1750 [ 78%]  (Sampling)
Chain 1: Iteration: 1550 / 1750 [ 88%]  (Sampling)
Chain 1: Iteration: 1725 / 1750 [ 98%]  (Sampling)
Chain 1: Iteration: 1750 / 1750 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 11.824 seconds (Warm-up)
Chain 1:                17.505 seconds (Sampling)
Chain 1:                29.329 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL &#39;model1PLM&#39; NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 1750 [  0%]  (Warmup)
Chain 2: Iteration:  175 / 1750 [ 10%]  (Warmup)
Chain 2: Iteration:  350 / 1750 [ 20%]  (Warmup)
Chain 2: Iteration:  501 / 1750 [ 28%]  (Sampling)
Chain 2: Iteration:  675 / 1750 [ 38%]  (Sampling)
Chain 2: Iteration:  850 / 1750 [ 48%]  (Sampling)
Chain 2: Iteration: 1025 / 1750 [ 58%]  (Sampling)
Chain 2: Iteration: 1200 / 1750 [ 68%]  (Sampling)
Chain 2: Iteration: 1375 / 1750 [ 78%]  (Sampling)
Chain 2: Iteration: 1550 / 1750 [ 88%]  (Sampling)
Chain 2: Iteration: 1725 / 1750 [ 98%]  (Sampling)
Chain 2: Iteration: 1750 / 1750 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 11.168 seconds (Warm-up)
Chain 2:                16.923 seconds (Sampling)
Chain 2:                28.091 seconds (Total)
Chain 2: </code></pre>
</div>
<div id="plot-the-item-characteristic-curves" class="section level2">
<h2>Plot the item characteristic curves</h2>
<p><em>Item characteristic curves</em> (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the <span class="math inline">\(x\)</span>-axis (higher values represent hight ability). Probability of a “correct” answer (<span class="math inline">\(Y_{ij}=1\)</span>) to an item is plotted on the <span class="math inline">\(y\)</span>-axis.</p>
<pre class="r"><code>&gt; #extract parameters
&gt; model1_stan_par&lt;-extract(model1_stan)
&gt; 
&gt; #see average value of item difficulty
&gt; diff&lt;-model1_stan_par$delta
&gt; apply(diff,2,mean)
[1] -2.8564001 -1.0625142 -0.2600015 -1.3879639 -2.2119688
&gt; 
&gt; #plot icc for each individual with respect to each of the 5 items
&gt; theta&lt;-apply(model1_stan_par$theta, 2, mean)
&gt; prob&lt;-apply(model1_stan_par$prob,c(2,3),mean)
&gt; plot(theta,prob[,1], type = &quot;n&quot;, ylab = &quot;probability of correct response&quot;, xlab=&quot;ability&quot;,
+      xlim = c(-2.5,1), ylim = c(0,1))
&gt; lines(theta,prob[,1],col=&quot;red&quot;)
&gt; lines(theta,prob[,2],col=&quot;blue&quot;)
&gt; lines(theta,prob[,3],col=&quot;orange&quot;)
&gt; lines(theta,prob[,4],col=&quot;green&quot;)
&gt; lines(theta,prob[,5],col=&quot;black&quot;)
&gt; legend(&quot;bottomright&quot;,legend = c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;), lty = c(1), col=c(&quot;red&quot;,&quot;blue&quot;,&quot;orange&quot;,&quot;green&quot;,&quot;black&quot;), bty = &quot;n&quot;, cex = 0.5)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/icc1-1.png" width="672" /></p>
<p>We see that item <span class="math inline">\(3\)</span> is the most difficult item (it’s curve is farthest to the right), and item <span class="math inline">\(1\)</span> is the easiest (it’s curve is farthest to the left). The same conclusions can be drawn by checking the difficulty estimates above.</p>
</div>
<div id="plot-the-item-information-curves" class="section level2">
<h2>Plot the item information curves</h2>
<p><em>Item information curves</em> (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the <span class="math inline">\(1\)</span>st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.</p>
<pre class="r"><code>&gt; #plot iic for each individual with respect to each of the 5 items
&gt; neg_prob&lt;-1-prob
&gt; information&lt;-prob*neg_prob
&gt; plot(theta,information[,1], type = &quot;n&quot;, ylab = &quot;information&quot;, xlab=&quot;ability&quot;,
+      xlim = c(-2.5,1), ylim = c(0,0.3))
&gt; lines(theta,information[,1],col=&quot;red&quot;)
&gt; lines(theta,information[,2],col=&quot;blue&quot;)
&gt; lines(theta,information[,3],col=&quot;orange&quot;)
&gt; lines(theta,information[,4],col=&quot;green&quot;)
&gt; lines(theta,information[,5],col=&quot;black&quot;)
&gt; legend(&quot;bottomleft&quot;,legend = c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;), lty = c(1), col=c(&quot;red&quot;,&quot;blue&quot;,&quot;orange&quot;,&quot;green&quot;,&quot;black&quot;), bty = &quot;n&quot;, cex = 0.5)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/iic1-1.png" width="672" /></p>
<p>Similar to the ICCs, we see that item <span class="math inline">\(3\)</span> provides the most information about high ability levels (the peak of its IIC is farthest to the right) and item <span class="math inline">\(1\)</span> and <span class="math inline">\(5\)</span> provides the most information about lower ability levels (the peak of its IIC is farthest to the left). We have seen that all ICCs and IICs for the items have the same shape in the 1PL model (i.e. all items are equally good at providing information about the latent trait). In the 2PL and 3PL models, we will see that this does not have to be the case.</p>
<p>Next, we plot the information curve for the whole test. This is simply the sum of the individual IICs above. Ideally, we want a test which provides fairly good covereage of a wide range of latent ability levels. Otherwise, the test is only good at identifying a limited range of ability levels.</p>
<pre class="r"><code>&gt; #plot iic for each individual with respect to whole test
&gt; information_test&lt;-apply(information,1,sum)
&gt; plot(theta,information_test, type = &quot;n&quot;, ylab = &quot;information (test)&quot;, xlab=&quot;ability&quot;,
+      xlim = c(-2.5,1), ylim = c(0,1.5))
&gt; lines(theta,information_test,col=&quot;black&quot;,lty=2)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/iic1tot-1.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; summary(information_test)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.5536  0.5699  0.7665  0.7756  0.9305  1.0791 </code></pre>
<p>We see that this test provides the most information about low ability levels (the peak is around ability level <span class="math inline">\(-1.5\)</span>), and less information about very high ability levels.</p>
</div>
<div id="assess-fit" class="section level2">
<h2>Assess fit</h2>
<p>We perform posterior predictive checks to test whether individual items fit the 1PLM by comparing quantities computed from the predictions of the model with those from the observed data. If these match reasonably well, then there is indication that the model has a good fit.</p>
<pre class="r"><code>&gt; library(bayesplot)
&gt; library(ggplot2)
&gt; Y.rep&lt;-model1_stan_par$Y_rep
&gt; 
&gt; #Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars
&gt; ppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(&quot;Item 1&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m1-1.png" width="672" /></p>
<pre class="r"><code>&gt; ppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(&quot;Item 2&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m1-2.png" width="672" /></p>
<pre class="r"><code>&gt; ppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(&quot;Item 3&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m1-3.png" width="672" /></p>
<pre class="r"><code>&gt; ppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(&quot;Item 4&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m1-4.png" width="672" /></p>
<pre class="r"><code>&gt; ppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(&quot;Item 5&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m1-5.png" width="672" /></p>
</div>
<div id="plot-ability-scores" class="section level2">
<h2>Plot ability scores</h2>
<p>We can conclude by summarising and plotting the latent ability scores of the participants</p>
<pre class="r"><code>&gt; #summary stats for theta across both iterations and individuals
&gt; summary(theta)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-2.05247 -0.48303  0.07869  0.00209  0.69533  0.73909 
&gt; 
&gt; #histogram and kernel density plot of theta averaged across iterations
&gt; dens.theta&lt;-density(theta, bw=0.3)
&gt; hist(theta, breaks = 5, prob = T)
&gt; lines(dens.theta, lwd=2, col=&quot;red&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/theta1-1.png" width="672" /></p>
<p>We see that the mean of ability scores is around <span class="math inline">\(0\)</span>, and the standard deviation about <span class="math inline">\(1\)</span> (these are estimated ability scores are standardised).</p>
</div>
</div>
<div id="parameter-logistic-model-2plm" class="section level1">
<h1>2 parameter logistic model (2PLM)</h1>
<p>In the 2PLM, the probability that the <span class="math inline">\(i\)</span>-th individual correctly answers the <span class="math inline">\(j\)</span>-th item (i.e. the probability that <span class="math inline">\(Y_{ij} = 1\)</span>) is assumed to have the following IRF form:</p>
<p><span class="math display">\[p_{ij} = P(Y_{ij}=1 \mid \theta_i,\delta_j,\alpha_j)=\frac{1}{1+\text{exp}(\alpha_j(\theta_i-\delta_j))},\]</span></p>
<p>where <span class="math inline">\(\alpha_j\)</span> is the <em>discrimination parameter</em> for item <span class="math inline">\(j\)</span> of the test, and is assumed to be positive and lognormally distributed according to some mean <span class="math inline">\(\mu_{\alpha}\)</span> and standard deviation <span class="math inline">\(\sigma_{\alpha}\)</span> which must be specified by the analyst. The item discriminability <span class="math inline">\(\alpha_j\)</span> indicates how well an item is able to discriminate between persons with different ability levels. Item discriminability is reflected in the steepness of the slope of the ICC.</p>
<div id="fit-the-model-1" class="section level2">
<h2>Fit the model</h2>
<p>We fit the 2PLM to the data using the following <code>Stan</code> code.</p>
<pre class="r"><code>&gt; model2&lt;-&quot;
+ data {
+ int&lt;lower=0&gt; n;
+ int&lt;lower=0&gt; p;
+ int&lt;lower=0,upper=1&gt; Y[n,p];
+ }
+ parameters {
+ vector[n] theta;
+ vector&lt;lower=0&gt; [p] alpha;
+ vector[p] delta;
+ real mu_delta;
+ real&lt;lower=0&gt; sigma_alpha;
+ real&lt;lower=0&gt; sigma_delta;
+ }
+ transformed parameters{
+ vector&lt;lower=0,upper=1&gt;[p] prob[n];
+  for(i in 1:n){
+   for (j in 1:p){
+    prob[i,j] = inv_logit(alpha[j]*(theta[i] - delta[j]));
+   }
+  }
+ }
+ model {
+ theta ~ normal(0,1);
+ delta ~ normal(mu_delta,sigma_delta);
+ mu_delta ~ normal(0,5);
+ sigma_delta ~ cauchy(0,5);
+ alpha ~ lognormal(0,sigma_alpha);
+ sigma_alpha ~ cauchy(0,5);
+  for(i in 1:n){
+   for (j in 1:p){
+    Y[i,j] ~ bernoulli(prob[i,j]);
+   }
+  }
+ }
+ generated quantities {
+ vector[p] loglik_y[n];
+ vector[p] Y_rep[n];
+  for (i in 1: n){
+   for (j in 1: p){
+     loglik_y[i,j] = bernoulli_lpmf(Y[i,j] | prob[i,j]);
+     Y_rep[i,j] = bernoulli_rng(prob[i,j]); 
+   }
+  }
+ }
+ &quot;
&gt; ## write the model to a text file
&gt; writeLines(model2, con = &quot;model2PLM.stan&quot;)</code></pre>
<p>Next, I define the nodes (parameters and derivatives) to monitor and the chain parameters.</p>
<pre class="r"><code>&gt; params &lt;- c(&quot;delta&quot;, &quot;alpha&quot;,&quot;theta&quot;, &quot;prob&quot;,&quot;loglik_y&quot;,&quot;Y_rep&quot;)
&gt; nChains = 2
&gt; burnInSteps = 500
&gt; thinSteps = 1
&gt; numSavedSteps = 2500  #across all chains
&gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&gt; nIter
[1] 1750</code></pre>
<p>Start the <code>Stan</code> model (check the model, load data into the model, specify the number of chains and compile the model). Run the <code>Stan</code> code via the <code>rstan</code> interface and the <code>stan</code> function.</p>
<pre class="r"><code>&gt; set.seed(3456)
&gt; model2_stan&lt;- stan(data = data_list, file = &quot;model2PLM.stan&quot;, 
+                    chains = nChains, pars = params, iter = nIter, 
+                    warmup = burnInSteps, thin = thinSteps)

SAMPLING FOR MODEL &#39;model2PLM&#39; NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.001 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 1750 [  0%]  (Warmup)
Chain 1: Iteration:  175 / 1750 [ 10%]  (Warmup)
Chain 1: Iteration:  350 / 1750 [ 20%]  (Warmup)
Chain 1: Iteration:  501 / 1750 [ 28%]  (Sampling)
Chain 1: Iteration:  675 / 1750 [ 38%]  (Sampling)
Chain 1: Iteration:  850 / 1750 [ 48%]  (Sampling)
Chain 1: Iteration: 1025 / 1750 [ 58%]  (Sampling)
Chain 1: Iteration: 1200 / 1750 [ 68%]  (Sampling)
Chain 1: Iteration: 1375 / 1750 [ 78%]  (Sampling)
Chain 1: Iteration: 1550 / 1750 [ 88%]  (Sampling)
Chain 1: Iteration: 1725 / 1750 [ 98%]  (Sampling)
Chain 1: Iteration: 1750 / 1750 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 78.795 seconds (Warm-up)
Chain 1:                41.905 seconds (Sampling)
Chain 1:                120.7 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL &#39;model2PLM&#39; NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.001 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 1750 [  0%]  (Warmup)
Chain 2: Iteration:  175 / 1750 [ 10%]  (Warmup)
Chain 2: Iteration:  350 / 1750 [ 20%]  (Warmup)
Chain 2: Iteration:  501 / 1750 [ 28%]  (Sampling)
Chain 2: Iteration:  675 / 1750 [ 38%]  (Sampling)
Chain 2: Iteration:  850 / 1750 [ 48%]  (Sampling)
Chain 2: Iteration: 1025 / 1750 [ 58%]  (Sampling)
Chain 2: Iteration: 1200 / 1750 [ 68%]  (Sampling)
Chain 2: Iteration: 1375 / 1750 [ 78%]  (Sampling)
Chain 2: Iteration: 1550 / 1750 [ 88%]  (Sampling)
Chain 2: Iteration: 1725 / 1750 [ 98%]  (Sampling)
Chain 2: Iteration: 1750 / 1750 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 32.74 seconds (Warm-up)
Chain 2:                47.985 seconds (Sampling)
Chain 2:                80.725 seconds (Total)
Chain 2: </code></pre>
</div>
<div id="plot-the-item-characteristic-curves-1" class="section level2">
<h2>Plot the item characteristic curves</h2>
<p><em>Item characteristic curves</em> (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the <span class="math inline">\(x\)</span>-axis (higher values represent hight ability). Probability of a “correct” answer (<span class="math inline">\(Y_{ij}=1\)</span>) to an item is plotted on the <span class="math inline">\(y\)</span>-axis.</p>
<pre class="r"><code>&gt; #extract parameters
&gt; model2_stan_par&lt;-extract(model2_stan)
&gt; 
&gt; discr&lt;-model2_stan_par$alpha
&gt; diff&lt;-model2_stan_par$delta
&gt; #see average value of item difficulty
&gt; apply(diff,2,mean)
[1] -3.2361513 -1.3702513 -0.3117896 -1.8137234 -2.7622803
&gt; #see average value of item discriminability
&gt; apply(discr,2,mean)
[1] 0.9062298 0.7615282 0.8492812 0.7549198 0.7976353
&gt; 
&gt; #plot icc for each individual with respect to each of the 5 items
&gt; theta&lt;-apply(model2_stan_par$theta, 2, mean)
&gt; prob&lt;-apply(model2_stan_par$prob,c(2,3),mean)
&gt; plot(theta,prob[,1], type = &quot;n&quot;, ylab = &quot;probability of correct response&quot;, xlab=&quot;ability&quot;,xlim = c(-2.5,1), ylim = c(0,1))
&gt; lines(theta,prob[,1],col=&quot;red&quot;)
&gt; lines(theta,prob[,2],col=&quot;blue&quot;)
&gt; lines(theta,prob[,3],col=&quot;orange&quot;)
&gt; lines(theta,prob[,4],col=&quot;green&quot;)
&gt; lines(theta,prob[,5],col=&quot;black&quot;)
&gt; legend(&quot;bottomright&quot;,legend = c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;), lty = c(1), col=c(&quot;red&quot;,&quot;blue&quot;,&quot;orange&quot;,&quot;green&quot;,&quot;black&quot;), bty = &quot;n&quot;, cex = 0.5)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/icc2-1.png" width="672" /></p>
<p>Unlike the ICCs for the 1PLM, the ICCs for the 2PLM do not all have the same shape. Item curves which are more “spread out” indicate lower discriminability (i.e. that individuals of a range of ability levels have some probability of getting the item correct). Compare this to an item with high discriminability (steep slope): for this item, we have a better estimate of the individual’s latent ability based on whether they got the question right or wrong. Because of the differing slopes, the rank-order of item difficulty changes across different latent ability levels. We can see that item <span class="math inline">\(3\)</span> is still the most difficult item (i.e. lowest probability of getting correct for most latent trait values, up until about <span class="math inline">\(\theta=0.2\)</span>). Items <span class="math inline">\(1\)</span> and <span class="math inline">\(5\)</span> are the easiest.</p>
</div>
<div id="plot-the-item-information-curves-1" class="section level2">
<h2>Plot the item information curves</h2>
<p><em>Item information curves</em> (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the <span class="math inline">\(1\)</span>st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.</p>
<pre class="r"><code>&gt; #plot iic for each individual with respect to each of the 5 items
&gt; neg_prob&lt;-1-prob
&gt; information&lt;-prob*neg_prob
&gt; information2&lt;-information*(apply(discr,2,mean))^2
&gt; plot(theta,information2[,1], type = &quot;n&quot;, ylab = &quot;information&quot;, xlab=&quot;ability&quot;,
+      xlim = c(-2.5,1), ylim = c(0,0.3))
&gt; lines(theta,information2[,1],col=&quot;red&quot;)
&gt; lines(theta,information2[,2],col=&quot;blue&quot;)
&gt; lines(theta,information2[,3],col=&quot;orange&quot;)
&gt; lines(theta,information2[,4],col=&quot;green&quot;)
&gt; lines(theta,information2[,5],col=&quot;black&quot;)
&gt; legend(&quot;bottomleft&quot;,legend = c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;), lty = c(1), col=c(&quot;red&quot;,&quot;blue&quot;,&quot;orange&quot;,&quot;green&quot;,&quot;black&quot;), bty = &quot;n&quot;, cex = 0.5)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/iic2-1.png" width="672" /></p>
<p>The item IICs demonstrate that some items provide more information about latent ability for different ability levels. The higher the item discriminability estimate, the more information an item provides about ability levels around the point where there is a <span class="math inline">\(50\%\)</span> chance of getting the item right (i.e. the steepest point in the ICC slope). For example, item <span class="math inline">\(3\)</span> (orange) clearly provides the most information at high ability levels, around <span class="math inline">\(\theta=-0.5\)</span>, but almost no information about low ability levels (<span class="math inline">\(&lt; -1\)</span>) because the item is already too hard for those participants. In contrast, item <span class="math inline">\(1\)</span> (red), which has low discriminability, doesn’t give very much information overall, but covers a wide range of ability levels.</p>
<p>Next, we plot the item information curve for the whole test. This is the sum of all the item IICs above.</p>
<pre class="r"><code>&gt; #plot iic for each individual with respect to whole test
&gt; information_test&lt;-apply(information2,1,sum)
&gt; plot(theta,information_test, type = &quot;n&quot;, ylab = &quot;information (test)&quot;, xlab=&quot;ability&quot;,
+      xlim = c(-2.5,1), ylim = c(0,1.5))
&gt; lines(theta,information_test,col=&quot;black&quot;,lty=2)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/iic2tot-1.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; summary(information_test)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.3541  0.4493  0.5176  0.5283  0.5869  0.8999 </code></pre>
<p>The IIC for the whole test shows that the test provides the most information for slightly-lower-than average ability levels (about <span class="math inline">\(\theta=-1\)</span>), but does not provide much information about extremely high ability levels.</p>
</div>
<div id="assess-fit-1" class="section level2">
<h2>Assess fit</h2>
<p>Next, we check how well the 2PLM fits the data.</p>
<pre class="r"><code>&gt; Y.rep&lt;-model2_stan_par$Y_rep
&gt; 
&gt; #Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars
&gt; ppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(&quot;Item 1&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m2-1.png" width="672" /></p>
<pre class="r"><code>&gt; ppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(&quot;Item 2&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m2-2.png" width="672" /></p>
<pre class="r"><code>&gt; ppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(&quot;Item 3&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m2-3.png" width="672" /></p>
<pre class="r"><code>&gt; ppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(&quot;Item 4&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m2-4.png" width="672" /></p>
<pre class="r"><code>&gt; ppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(&quot;Item 5&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m2-5.png" width="672" /></p>
<p>We can also compare the fit of the 1PLM and 2PLM using relative measures of fit or <em>information criteria</em>. These are computed based on the deviance and a penalty for model complexity called the effective number of parameters <span class="math inline">\(p\)</span>. Here we consider two Bayesian measures known as the <em>Widely Applicable</em> (WAIC) and <em>Leave One Out</em> (LOOIC) Information Criterion, which can be easily obtained through the functions <code>waic</code> and <code>loo</code> in the package <code>loo</code>.</p>
<pre class="r"><code>&gt; library(loo)
&gt; #extract log-likelihood
&gt; loglik_m1&lt;-model1_stan_par$loglik_y
&gt; loglik_m2&lt;-model2_stan_par$loglik_y
&gt; 
&gt; #waic
&gt; waic_m1&lt;-waic(loglik_m1)
&gt; waic_m2&lt;-waic(loglik_m2)
&gt; 
&gt; #looic
&gt; looic_m1&lt;-loo(loglik_m1)
&gt; looic_m2&lt;-loo(loglik_m2)
&gt; 
&gt; #compare
&gt; table_waic&lt;-rbind(waic_m1$estimates[2:3,1],waic_m2$estimates[2:3,1])
&gt; table_looic&lt;-rbind(looic_m1$estimates[2:3,1],looic_m2$estimates[2:3,1])
&gt; rownames(table_waic)&lt;-rownames(table_looic)&lt;-c(&quot;1PLM&quot;,&quot;2PLM&quot;)
&gt; knitr::kable(cbind(table_waic,table_looic), &quot;pandoc&quot;, align = &quot;c&quot;)</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">p_waic</th>
<th align="center">waic</th>
<th align="center">p_loo</th>
<th align="center">looic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1PLM</td>
<td align="center">1.525284</td>
<td align="center">6.519258</td>
<td align="center">1.866221</td>
<td align="center">7.201132</td>
</tr>
<tr class="even">
<td>2PLM</td>
<td align="center">1.422705</td>
<td align="center">6.455726</td>
<td align="center">1.702409</td>
<td align="center">7.015135</td>
</tr>
</tbody>
</table>
<p>Both criteria suggest that the 2PLM has a slightly better fit to the data.</p>
</div>
<div id="plot-ability-scores-1" class="section level2">
<h2>Plot ability scores</h2>
<p>Plot the density curve of the estimated ability scores</p>
<pre class="r"><code>&gt; #summary stats for theta across both iterations and individuals
&gt; summary(theta)
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
-1.945004 -0.437572  0.064892 -0.000004  0.633274  0.673980 
&gt; 
&gt; #histogram and kernel density plot of theta averaged across iterations
&gt; dens.theta&lt;-density(theta, bw=0.3)
&gt; hist(theta, breaks = 5, prob = T)
&gt; lines(dens.theta, lwd=2, col=&quot;red&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/theta2-1.png" width="672" /></p>
<p>We see that the mean of ability scores is around <span class="math inline">\(0\)</span>, and the standard deviation about <span class="math inline">\(1\)</span> (these are estimated ability scores are standardised).</p>
</div>
</div>
<div id="parameter-logistic-model-3plm" class="section level1">
<h1>3 parameter logistic model (3PLM)</h1>
<p>In the 3PLM, the probability that the <span class="math inline">\(i\)</span>-th individual correctly answers the <span class="math inline">\(j\)</span>-th item (i.e. the probability that <span class="math inline">\(Y_{ij} = 1\)</span>) is assumed to have the following IRF form:</p>
<p><span class="math display">\[p_{ij} = P(Y_{ij}=1 \mid \theta_i,\delta_j,\alpha_j,\eta_j)=\eta_j + (1-\eta_j) \frac{1}{1+\text{exp}(\alpha_j(\theta_i-\delta_j))},\]</span></p>
<p>where <span class="math inline">\(\eta_j\)</span> is the <em>guessing parameter</em>. Under this model, individuals with zero ability have a nonzero chance of endorsing any item, just by guessing randomly. The guessing parameter is reflected in the <span class="math inline">\(y\)</span>-intercept (i.e. probability) of the ICC. The parameter is normally distributed according to some mean <span class="math inline">\(\mu_{\eta}\)</span> and standard deviation <span class="math inline">\(\sigma_{\eta}\)</span> which must be specified by the analyst.</p>
<div id="fit-the-model-2" class="section level2">
<h2>Fit the model</h2>
<p>We fit the 3PLM to the data using the following <code>Stan</code> code.</p>
<pre class="r"><code>&gt; model3&lt;-&quot;
+ data {
+ int&lt;lower=0&gt; n;
+ int&lt;lower=0&gt; p;
+ int&lt;lower=0,upper=1&gt; Y[n,p];
+ }
+ parameters {
+ vector[n] theta;
+ vector&lt;lower=0&gt; [p] alpha;
+ vector[p] delta;
+ vector&lt;lower=0,upper=1&gt;[p] eta; //item pseudo-guessing
+ real mu_delta;
+ real&lt;lower=0&gt; sigma_alpha;
+ real&lt;lower=0&gt; sigma_delta;
+ }
+ transformed parameters{
+ vector&lt;lower=0,upper=1&gt;[p] prob_star[n];
+ vector&lt;lower=0,upper=1&gt;[p] prob[n];
+  for(i in 1:n){
+   for (j in 1:p){
+    prob_star[i,j] = inv_logit(alpha[j]*(theta[i] - delta[j]));
+    prob[i, j] = eta[j] + (1-eta[j])*prob_star[i,j]; 
+   }
+  }
+ }
+ model {
+ theta ~ normal(0,1);
+ delta ~ normal(mu_delta,sigma_delta);
+ mu_delta ~ normal(0,5);
+ sigma_delta ~ cauchy(0,5);
+ alpha ~ lognormal(0,sigma_alpha);
+ sigma_alpha ~ cauchy(0,5);
+ eta ~ beta(5,23);
+  for(i in 1:n){
+   for (j in 1:p){
+    Y[i,j] ~ bernoulli(prob[i,j]);
+   }
+  }
+ }
+ generated quantities {
+ vector[p] loglik_y[n];
+ vector[p] Y_rep[n];
+  for (i in 1: n){
+   for (j in 1: p){
+     loglik_y[i,j] = bernoulli_lpmf(Y[i,j] | prob[i,j]);
+     Y_rep[i,j] = bernoulli_rng(prob[i,j]); 
+   }
+  }
+ }
+ &quot;
&gt; ## write the model to a text file
&gt; writeLines(model3, con = &quot;model3PLM.stan&quot;)</code></pre>
<p>Next, I define the nodes (parameters and derivatives) to monitor and the chain parameters.</p>
<pre class="r"><code>&gt; params &lt;- c(&quot;delta&quot;, &quot;alpha&quot;, &quot;eta&quot;,&quot;theta&quot;, &quot;prob&quot;,&quot;loglik_y&quot;,&quot;Y_rep&quot;)
&gt; nChains = 2
&gt; burnInSteps = 500
&gt; thinSteps = 1
&gt; numSavedSteps = 2500  #across all chains
&gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&gt; nIter
[1] 1750</code></pre>
<p>Start the <code>Stan</code> model (check the model, load data into the model, specify the number of chains and compile the model). Run the <code>Stan</code> code via the <code>rstan</code> interface and the <code>stan</code> function.</p>
<pre class="r"><code>&gt; set.seed(3456)
&gt; model3_stan&lt;- stan(data = data_list, file = &quot;model3PLM.stan&quot;, 
+                    chains = nChains, pars = params, iter = nIter, 
+                    warmup = burnInSteps, thin = thinSteps)

SAMPLING FOR MODEL &#39;model3PLM&#39; NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.002 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 20 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 1750 [  0%]  (Warmup)
Chain 1: Iteration:  175 / 1750 [ 10%]  (Warmup)
Chain 1: Iteration:  350 / 1750 [ 20%]  (Warmup)
Chain 1: Iteration:  501 / 1750 [ 28%]  (Sampling)
Chain 1: Iteration:  675 / 1750 [ 38%]  (Sampling)
Chain 1: Iteration:  850 / 1750 [ 48%]  (Sampling)
Chain 1: Iteration: 1025 / 1750 [ 58%]  (Sampling)
Chain 1: Iteration: 1200 / 1750 [ 68%]  (Sampling)
Chain 1: Iteration: 1375 / 1750 [ 78%]  (Sampling)
Chain 1: Iteration: 1550 / 1750 [ 88%]  (Sampling)
Chain 1: Iteration: 1725 / 1750 [ 98%]  (Sampling)
Chain 1: Iteration: 1750 / 1750 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 30.502 seconds (Warm-up)
Chain 1:                52.971 seconds (Sampling)
Chain 1:                83.473 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL &#39;model3PLM&#39; NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.001 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 1750 [  0%]  (Warmup)
Chain 2: Iteration:  175 / 1750 [ 10%]  (Warmup)
Chain 2: Iteration:  350 / 1750 [ 20%]  (Warmup)
Chain 2: Iteration:  501 / 1750 [ 28%]  (Sampling)
Chain 2: Iteration:  675 / 1750 [ 38%]  (Sampling)
Chain 2: Iteration:  850 / 1750 [ 48%]  (Sampling)
Chain 2: Iteration: 1025 / 1750 [ 58%]  (Sampling)
Chain 2: Iteration: 1200 / 1750 [ 68%]  (Sampling)
Chain 2: Iteration: 1375 / 1750 [ 78%]  (Sampling)
Chain 2: Iteration: 1550 / 1750 [ 88%]  (Sampling)
Chain 2: Iteration: 1725 / 1750 [ 98%]  (Sampling)
Chain 2: Iteration: 1750 / 1750 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 35.361 seconds (Warm-up)
Chain 2:                53.403 seconds (Sampling)
Chain 2:                88.764 seconds (Total)
Chain 2: </code></pre>
</div>
<div id="plot-the-item-characteristic-curves-2" class="section level2">
<h2>Plot the item characteristic curves</h2>
<p><em>Item characteristic curves</em> (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the <span class="math inline">\(x\)</span>-axis (higher values represent hight ability). Probability of a “correct” answer (<span class="math inline">\(Y_{ij}=1\)</span>) to an item is plotted on the <span class="math inline">\(y\)</span>-axis.</p>
<pre class="r"><code>&gt; #extract parameters
&gt; model3_stan_par&lt;-extract(model3_stan)
&gt; 
&gt; discr&lt;-model3_stan_par$alpha
&gt; diff&lt;-model3_stan_par$delta
&gt; gues&lt;-model3_stan_par$eta
&gt; #see average value of item difficulty
&gt; apply(diff,2,mean)
[1] -2.7984897 -0.7753931  0.1717751 -1.1931997 -2.1962556
&gt; #see average value of item discriminability
&gt; apply(discr,2,mean)
[1] 0.9541800 0.9031630 1.0175164 0.8777688 0.8861185
&gt; #see average value of item guessing
&gt; apply(gues,2,mean)
[1] 0.1892703 0.1891533 0.1632674 0.1933265 0.1934880
&gt; 
&gt; #plot icc for each individual with respect to each of the 5 items
&gt; theta&lt;-apply(model3_stan_par$theta, 2, mean)
&gt; prob&lt;-apply(model3_stan_par$prob,c(2,3),mean)
&gt; plot(theta,prob[,1], type = &quot;n&quot;, ylab = &quot;probability of correct response&quot;, xlab=&quot;ability&quot;,
+      xlim = c(-2.5,1), ylim = c(0,1))
&gt; lines(theta,prob[,1],col=&quot;red&quot;)
&gt; lines(theta,prob[,2],col=&quot;blue&quot;)
&gt; lines(theta,prob[,3],col=&quot;orange&quot;)
&gt; lines(theta,prob[,4],col=&quot;green&quot;)
&gt; lines(theta,prob[,5],col=&quot;black&quot;)
&gt; legend(&quot;bottomright&quot;,legend = c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;), lty = c(1), col=c(&quot;red&quot;,&quot;blue&quot;,&quot;orange&quot;,&quot;green&quot;,&quot;black&quot;), bty = &quot;n&quot;, cex = 0.5)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/icc3-1.png" width="672" /></p>
<p>The slopes of the ICCs look very similar to those of the 2PLM. We can see that all items have <span class="math inline">\(y\)</span>-intercepts greater than zero, so that even at very low ability levels, there is some chance of getting these items correct (via guessing).</p>
</div>
<div id="plot-the-item-information-curves-2" class="section level2">
<h2>Plot the item information curves</h2>
<p><em>Item information curves</em> (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the <span class="math inline">\(1\)</span>st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.</p>
<p>Here I plot the IICs using points, rather than lines, to better display the patterns of the individuals, which vary substantially according to whether the item was correctly chosen due to chance or not.</p>
<pre class="r"><code>&gt; #plot iic for each individual with respect to each of the 5 items
&gt; neg_prob&lt;-1-prob
&gt; information.p1&lt;-neg_prob/prob
&gt; information.p2&lt;-(prob-apply(gues,2,mean))^2/(1-apply(gues,2,mean))^2
&gt; information3&lt;-(apply(discr,2,mean))^2*(information.p2)*(information.p1)
&gt; plot(theta,information3[,1], type = &quot;n&quot;, ylab = &quot;information&quot;, xlab=&quot;ability&quot;,
+      xlim = c(-2.5,1), ylim = c(0,0.7))
&gt; points(theta,information3[,1],col=&quot;red&quot;)
&gt; points(theta,information3[,2],col=&quot;blue&quot;)
&gt; points(theta,information3[,3],col=&quot;orange&quot;)
&gt; points(theta,information3[,4],col=&quot;green&quot;)
&gt; points(theta,information3[,5],col=&quot;black&quot;)
&gt; legend(&quot;bottomleft&quot;,legend = c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;), lty = c(1), col=c(&quot;red&quot;,&quot;blue&quot;,&quot;orange&quot;,&quot;green&quot;,&quot;black&quot;), bty = &quot;n&quot;, cex = 0.5)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/iic3-1.png" width="672" /></p>
<p>Next, we plot the item information curve for the whole test. This is the sum of all the item IICs above.</p>
<pre class="r"><code>&gt; #plot iic for each individual with respect to whole test
&gt; information_test&lt;-apply(information3,1,sum)
&gt; plot(theta,information_test, type = &quot;n&quot;, ylab = &quot;information (test)&quot;, xlab=&quot;ability&quot;,
+      xlim = c(-2.5,1), ylim = c(0,1.5))
&gt; points(theta,information_test,col=&quot;black&quot;,lty=2)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/iic3tot-1.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; summary(information_test)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.4119  0.4919  0.5320  0.5470  0.5817  0.7770 </code></pre>
</div>
<div id="assess-fit-2" class="section level2">
<h2>Assess fit</h2>
<p>Next, we check how well the 3PLM fits the data.</p>
<pre class="r"><code>&gt; Y.rep&lt;-model3_stan_par$Y_rep
&gt; 
&gt; #Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars
&gt; ppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(&quot;Item 1&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m3-1.png" width="672" /></p>
<pre class="r"><code>&gt; ppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(&quot;Item 2&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m3-2.png" width="672" /></p>
<pre class="r"><code>&gt; ppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(&quot;Item 3&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m3-3.png" width="672" /></p>
<pre class="r"><code>&gt; ppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(&quot;Item 4&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m3-4.png" width="672" /></p>
<pre class="r"><code>&gt; ppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(&quot;Item 5&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/diag_m3-5.png" width="672" /></p>
<p>We can also compare the fit of the 1PLM, 2PLM and 3PLM using relative measures of fit or <em>information criteria</em>. These are computed based on the deviance and a penalty for model complexity called the effective number of parameters <span class="math inline">\(p\)</span>. Here we consider two Bayesian measures known as the <em>Widely Applicable</em> (WAIC) and <em>Leave One Out</em> (LOOIC) Information Criterion, which can be easily obtained through the functions <code>waic</code> and <code>loo</code> in the package <code>loo</code>.</p>
<pre class="r"><code>&gt; #extract log-likelihood
&gt; loglik_m3&lt;-model3_stan_par$loglik_y
&gt; 
&gt; #waic
&gt; waic_m3&lt;-waic(loglik_m3)
&gt; 
&gt; #looic
&gt; looic_m3&lt;-loo(loglik_m3)
&gt; 
&gt; #compare
&gt; table_waic&lt;-rbind(waic_m1$estimates[2:3,1],waic_m2$estimates[2:3,1],waic_m3$estimates[2:3,1])
&gt; table_looic&lt;-rbind(looic_m1$estimates[2:3,1],looic_m2$estimates[2:3,1],looic_m3$estimates[2:3,1])
&gt; rownames(table_waic)&lt;-rownames(table_looic)&lt;-c(&quot;1PLM&quot;,&quot;2PLM&quot;,&quot;3PLM&quot;)
&gt; knitr::kable(cbind(table_waic,table_looic), &quot;pandoc&quot;, align = &quot;c&quot;)</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">p_waic</th>
<th align="center">waic</th>
<th align="center">p_loo</th>
<th align="center">looic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1PLM</td>
<td align="center">1.525284</td>
<td align="center">6.519258</td>
<td align="center">1.866221</td>
<td align="center">7.201132</td>
</tr>
<tr class="even">
<td>2PLM</td>
<td align="center">1.422705</td>
<td align="center">6.455726</td>
<td align="center">1.702409</td>
<td align="center">7.015135</td>
</tr>
<tr class="odd">
<td>3PLM</td>
<td align="center">1.404502</td>
<td align="center">6.428445</td>
<td align="center">1.695632</td>
<td align="center">7.010705</td>
</tr>
</tbody>
</table>
<p>Both criteria suggest that both 1PLM and 2PLM have a better fit to the data than 3PLM.</p>
</div>
<div id="plot-ability-scores-2" class="section level2">
<h2>Plot ability scores</h2>
<p>Plot the density curve of the estimated ability scores</p>
<pre class="r"><code>&gt; #summary stats for theta across both iterations and individuals
&gt; summary(theta)
      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
-1.8520796 -0.4391371  0.0657133 -0.0007343  0.6520041  0.7074601 
&gt; 
&gt; #histogram and kernel density plot of theta averaged across iterations
&gt; dens.theta&lt;-density(theta, bw=0.3)
&gt; hist(theta, breaks = 5, prob = T)
&gt; lines(dens.theta, lwd=2, col=&quot;red&quot;)</code></pre>
<p><img src="/tutorial/irt-stan/2020-02-01-irt-stan_files/figure-html/theta3-1.png" width="672" /></p>
<p>We see that the mean of ability scores is around <span class="math inline">\(0\)</span>, and the standard deviation about <span class="math inline">\(1\)</span> (these are estimated ability scores are standardised).</p>
</div>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<p>The use of <code>Stan</code> software to estimate IRT models allows the user to alter existing
code to fit new variations of current models that cannot be fit in existing software packages. For example, longitudinal or multilevel data can easily be accommodated by small changes to existing <code>Stan</code> code. The <code>Stan</code> software takes care of the “grunt work” involved in estimating model parameters by constructing an MCMC algorithm to sample from the posterior distribution. Using <code>Stan</code> frees the user to experiment with different models that may be more appropriate for specialised data than the models that can currently be fit in other software packages. Of course, more complicated models involve more parameters than simpler models, and the analyst must specify prior distributions for these new parameters. This is a small price to pay, however, for the flexibility that the Bayesian framework and <code>Stan</code> software provide.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-baker2004item" class="csl-entry">
Baker, Frank B, and Seock-Ho Kim. 2004. <em>Item Response Theory: Parameter Estimation Techniques</em>. CRC Press.
</div>
<div id="ref-gelman2015stan" class="csl-entry">
Gelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. <span>“Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.”</span> <em>Journal of Educational and Behavioral Statistics</em> 40 (5): 530–43.
</div>
<div id="ref-hambleton1991fundamentals" class="csl-entry">
Hambleton, Ronald K, Hariharan Swaminathan, and H Jane Rogers. 1991. <em>Fundamentals of Item Response Theory</em>. Sage.
</div>
<div id="ref-luo2018using" class="csl-entry">
Luo, Yong, and Hong Jiao. 2018. <span>“Using the Stan Program for Bayesian Item Response Theory.”</span> <em>Educational and Psychological Measurement</em> 78 (3): 384–408.
</div>
<div id="ref-rstanpackage" class="csl-entry">
Stan Development Team. 2018. <span>“<span>RStan</span>: The <span>R</span> Interface to <span>Stan</span>.”</span> <a href="http://mc-stan.org/">http://mc-stan.org/</a>.
</div>
</div>
</div>
