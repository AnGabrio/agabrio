---
title: "Super basic introduction to JAGS"
author: "Andrea Gabrio"
date: 2018-07-23T21:13:14-05:00
categories: ["R"]
tags: ["linear regression", "JAGS", "introduction"]
authors: ["Andrea Gabrio"]
bibliography: [citations_jags.bib]
draft: no
slug: super-basic-introduction-to-jags
---



<p>The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using <code>JAGS</code> via <code>R</code>.</p>
<p>Prerequisites:</p>
<ul>
<li>The latest version of <code>R</code>, which can be downloaded and installed for Windows, Mac or Linux OS from the <a href="https://www.r-project.org/%7D">CRAN</a> website</li>
<li>I also <strong>strongly</strong> recommend to download and install <a href="https://www.rstudio.com/">Rstudio</a>, an integrated development environment which provides an “user-friendly” interaction with <code>R</code> (e.g. many drop-down menus, tabs, customisation options)</li>
</ul>
<div id="preliminaries" class="section level1">
<h1>Preliminaries</h1>
<div id="what-is-jags" class="section level2">
<h2>What is JAGS?</h2>
<p><code>JAGS</code> or <strong>J</strong>ust <strong>A</strong>nother <strong>G</strong>ibbs <strong>S</strong>ampler is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (<span class="citation">Plummer (2004)</span>). <code>JAGS</code> is a free software based on the <strong>B</strong>ayesian inference <strong>U</strong>sing <strong>G</strong>ibbs <strong>S</strong>ampling (informally <code>BUGS</code>) language at the base of <code>WinBUGS/OpenBUGS</code> but, unlike these programs, it is written in <code>C++</code> and is platform independent. The latest version of <code>JAGS</code> can be dowloaded from Martyn Plummer’s <a href="https://sourceforge.net/projects/mcmc-jags/files/JAGS/">repository</a> and is available for different OS. There are different <code>R</code> packages which function as frontends for <code>JAGS</code>. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the <code>R2jags</code> package (<span class="citation">Su et al. (2015)</span>) and show how to fit <code>JAGS</code> models using this package.</p>
</div>
<div id="installing-jags-and-r2jags" class="section level2">
<h2>Installing JAGS and R2jags</h2>
<p>Install the latest version of <code>JAGS</code> for your OS. Next, install the package <code>R2jags</code> from within <code>R</code> or <code>Rstudio</code>, via the package installer or by typing in the command line</p>
<pre class="r"><code>&gt; install.packages(&quot;R2jags&quot;, dependencies = TRUE)</code></pre>
<p>The <code>dependencies = TRUE</code> option will automatically install all the packages on which the functions in the <code>R2jags</code> package rely.</p>
</div>
</div>
<div id="basic-model" class="section level1">
<h1>Basic model</h1>
<div id="simulate-data" class="section level2">
<h2>Simulate data</h2>
<p>For an example dataset, I simulate my own data in <code>R</code>. I create a continuous outcome variable <span class="math inline">\(y\)</span> as a function of one predictor <span class="math inline">\(x\)</span> and a disturbance term <span class="math inline">\(\epsilon\)</span>. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span> coefficients, i.e.</p>
<p><span class="math display">\[y = \beta_0 + \beta_1 x + \epsilon  \]</span></p>
<p>The <code>R</code> commands which I use to simulate the data are the following:</p>
<pre class="r"><code>&gt; n.sim=100; set.seed(123)
&gt; x=rnorm(n.sim, mean = 5, sd = 2)
&gt; epsilon=rnorm(n.sim, mean = 0, sd = 1)
&gt; beta0=1.5
&gt; beta1=1.2
&gt; y=beta0 + beta1 * x + epsilon</code></pre>
<p>Then, I define all the data for <code>JAGS</code> in a list object</p>
<pre class="r"><code>&gt; datalist=list(&quot;y&quot;,&quot;x&quot;,&quot;n.sim&quot;)</code></pre>
</div>
<div id="model-file" class="section level2">
<h2>Model file</h2>
<p>Now, I write the model for <code>JAGS</code> and save it as a text file named <code>"basic.mod.txt"</code> in the current working directory</p>
<pre class="r"><code>&gt; basic.mod= &quot;
+ model {
+ #model
+  for(i in 1:n.sim){
+   y[i] ~ dnorm(mu[i], tau)
+   mu[i] = beta0 + beta1 * x[i]
+  }
+ #priors
+ beta0 ~ dnorm(0, 0.01)
+ beta1 ~ dnorm(0, 0.01)
+ tau ~ dgamma(0.01,0.01)
+ }
+ &quot;</code></pre>
<p>The part of the model inside the for loop denotes the likelihood, which is evaluated for each individual in the sample using a Normal distribution parameterised by some mean <code>mu</code> and precision <code>tau</code> (where, precision = 1/variance). The covariate <code>x</code> is included at the mean level using a linear regression, which is indexed by the intercept <code>beta0</code> and slope <code>beta1</code> terms. The second part defines the prior distributions for all parameters of the model, namely the regression coefficients and the precision. Weakly informative priors are used since I assume that I do not have any prior knowledge about these parameters.</p>
<p>To write and save the model as the text file “basic.mod.txt” in the current working directory, I use the <code>writeLines</code> function</p>
<pre class="r"><code>&gt; writeLines(basic.mod, &quot;basic.mod.txt&quot;)</code></pre>
</div>
<div id="pre-processing" class="section level2">
<h2>Pre-processing</h2>
<p>Define the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in <code>JAGS</code></p>
<pre class="r"><code>&gt; params=c(&quot;beta0&quot;,&quot;beta1&quot;)
&gt; inits=function(){list(&quot;beta0&quot;=rnorm(1), &quot;beta1&quot;=rnorm(1))}</code></pre>
<p>The function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters. However, for more complex models, this may not be immediate and a lot of trial and error may be required. However, <code>JAGS</code> can automatically select the initial values for all parameters in an efficient way even for relatively complex models. This can be achieved by setting <code>inits=NULL</code>, which is then passed to the <code>jags</code> function in <code>R2jags</code>.</p>
<p>Before using <code>R2jags</code> for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable</p>
<pre class="r"><code>&gt; library(R2jags)
&gt; set.seed(123)</code></pre>
</div>
<div id="fit-the-model" class="section level2">
<h2>Fit the model</h2>
<p>Now, we can fit the model in <code>JAGS</code> using the <code>jags</code> function in the <code>R2jags</code> package and save it in the object <code>basic.mod</code></p>
<pre class="r"><code>&gt; basic.mod=jags(data = datalist, inits = inits,
+   parameters.to.save = params, n.chains = 2, n.iter = 9000, 
+   n.burnin = 1000, model.file = &quot;basic.mod.txt&quot;)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 100
   Unobserved stochastic nodes: 3
   Total graph size: 406

Initializing model</code></pre>
<p>While the model is running, the function prints out some information related to the Bayesian graph (corresponding to the specification used for the model) underneath <code>JAGS</code>, such as number of observed and unobserved nodes and graph size.</p>
</div>
<div id="post-processing" class="section level2">
<h2>Post-processing</h2>
<p>Once the model has finished running, a summary of the posteiror estimates and convergence diagnostics for all parameters specified can be seen by typing <code>print(basic.mod)</code> or, alternatively,</p>
<pre class="r"><code>&gt; print(basic.mod$BUGSoutput$summary)</code></pre>
<pre><code>          mean    sd   2.5%   25%   50%   75% 97.5% Rhat n.eff
beta0      1.5 0.293   0.95   1.3   1.5   1.7   2.1    1  1600
beta1      1.2 0.053   1.07   1.1   1.2   1.2   1.3    1   920
deviance 278.9 2.508 276.04 277.1 278.2 279.9 285.5    1  2000</code></pre>
<p>The posterior distribution of each parameter is summarised in terms of:</p>
<ul>
<li>The mean, sd and some percentiles</li>
<li>Potential scale reduction factor <code>Rhat</code> and effective sample size <code>n.eff</code> (<span class="citation">Gelman (2013)</span>). The first is a measure to assess issues in convergence of the MCMC algorithm (typically a value below <span class="math inline">\(1.05\)</span> for all parameters is considered ok). The second is a measure which assesses the adequacy of the posterior sample (typically values close to the total number of iterations are desirable for all parameters).</li>
</ul>
<p>The deviance is a goodness of fit statistic and is used in the construction of the “Deviance Information Criterion” or DIC (<span class="citation">Spiegelhalter et al. (2014)</span>), which is a <em>relative</em> measure of model comparison. The DIC of the model can be accessed by typing</p>
<pre class="r"><code>&gt; basic.mod$BUGSoutput$DIC
[1] 282</code></pre>
</div>
<div id="diagnostics" class="section level2">
<h2>Diagnostics</h2>
<p>More diagnostics are available when we convert the model output into an MCMC object using the command</p>
<pre class="r"><code>&gt; basic.mod.mcmc=as.mcmc(basic.mod)</code></pre>
<p>Different packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the <code>mcmcplots</code> package (<span class="citation">Curtis (2015)</span>) to obtain graphical diagnostics and results.</p>
<pre class="r"><code>&gt; install.packages(&quot;mcmcplots&quot;)
&gt; library(mcmcplots)</code></pre>
<p>For example, density and trace plots can be obtained by typing</p>
<pre class="r"><code>&gt; denplot(basic.mod.mcmc, parms = c(&quot;beta0&quot;,&quot;beta1&quot;))</code></pre>
<p><img src="/JAGS/basic-introduction/2019-06-29-super-basic-introduction-to-jags_files/figure-html/diagnostic3-1.png" width="672" /></p>
<pre class="r"><code>&gt; traplot(basic.mod.mcmc, parms = c(&quot;beta0&quot;,&quot;beta1&quot;))</code></pre>
<p><img src="/JAGS/basic-introduction/2019-06-29-super-basic-introduction-to-jags_files/figure-html/diagnostic3-2.png" width="672" /></p>
<p>Both types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains).</p>
</div>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<p>This tutorial was simply a brief introduction on how simple linear regression models can be fitted using the Bayesian software <code>JAGS</code> via the <code>R2jags</code> package. Although this may seem a complex procedure compared with simply fitting a linear model under the frequentist framework, however, the real advantages of Bayesian methods become evident when the complexity of the analysis is increased (which is often the case in real applications). Indeed, the flexibility in Bayesian modelling allows to account for increasingly complex models in a relatively easy way. In addition, Bayesian methods are ideal when the interest is in taking into account the potential impact that different sources of uncertainty may have on the final results, as they allow the natural propagation of uncertainty throughout each quantity in the model.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-curtis2015mcmcplots">
<p>Curtis, SM. 2015. “Mcmcplots: Create Plots from Mcmc Output.” <em>R Package Version 0.4</em> 2.</p>
</div>
<div id="ref-gelman2013bayesian">
<p>Gelman, Andrew. 2013. <em>Bayesian Data Analysis</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-plummer2004jags">
<p>Plummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”</p>
</div>
<div id="ref-spiegelhalter2014deviance">
<p>Spiegelhalter, David J, Nicola G Best, Bradley P Carlin, and Angelika Van Der Linde. 2014. “The Deviance Information Criterion: 12 Years on.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 76 (3): 485–93.</p>
</div>
<div id="ref-su2015package">
<p>Su, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” <em>R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags</em>.</p>
</div>
</div>
</div>
