---
title: "Generalised Linear Mixed Models - JAGS"
author: "Andrea Gabrio"
date: 2020-02-15T21:13:14-05:00
categories: ["R", "JAGS", "generalised linear mixed models"]
tags: ["tutorials", "JAGS", "generalised linear mixed models"]
authors: ["Andrea Gabrio"]
bibliography: [citations_jags16.bib]
draft: no
slug: glmm-jags
---



<p>This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. <code>BUGS</code> (Bayesian inference Using <em>Gibbs Sampling</em>) is an algorithm and supporting language (resembling <code>R</code>) dedicated to performing the Gibbs sampling implementation of <em>Markov Chain Monte Carlo</em> (MCMC) method. Dialects of the <code>BUGS</code> language are implemented within three main projects:</p>
<ol style="list-style-type: decimal">
<li><p><strong>OpenBUGS</strong> - written in component pascal.</p></li>
<li><p><strong>JAGS</strong> - (Just Another Gibbs Sampler) - written in <code>C++</code>.</p></li>
<li><p><strong>STAN</strong> - a dedicated Bayesian modelling framework written in <code>C++</code> and implementing <em>Hamiltonian</em> MCMC samplers.</p></li>
</ol>
<p>Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of <code>R</code>, and thus, they are best accessed from within <code>R</code> itself. As such there are multiple packages devoted to interfacing with the various software implementations:</p>
<ul>
<li><p><em>R2OpenBUGS</em> - interfaces with <code>OpenBUGS</code></p></li>
<li><p><em>R2jags</em> - interfaces with <code>JAGS</code></p></li>
<li><p><em>rstan</em> - interfaces with <code>STAN</code></p></li>
</ul>
<p>This tutorial will demonstrate how to fit models in <code>JAGS</code> (<span class="citation">Plummer (2004)</span>) using the package <code>R2jags</code> (<span class="citation">Su et al. (2015)</span>) as interface, which also requires to load some other packages.</p>
<div id="binomial" class="section level1">
<h1>Binomial</h1>
<div id="data-generation" class="section level2">
<h2>Data generation</h2>
<p>Goodness of fit tests are concerned with comparing the observed frequencies with those expected on the basis of a specific null hypothesis. So lets now fabricate a motivating scenario and some data. We will create a scenario that involves items classified into one of three groups (A, B and C). The number of items in each classification group are then tallied up. Out of a total of $4$7 items, <span class="math inline">\(15\)</span> where of type A, <span class="math inline">\(9\)</span> where of type B and <span class="math inline">\(23\)</span> where of type C. We could evaluate a parity (a <span class="math inline">\(1:1:1\)</span> ratio from these data. In a frequentist context, this might involve testing a null hypothesis that the observed data could have come from a population with a <span class="math inline">\(1:1\)</span> item ratio. In this case the probability would be the probability of obtaining the observed ratio of frequencies when the null hypothesis is true. In a Bayesian context, there are numerous ways that we could tackle these data. We would be evaluating the evidence for the null hypothesis (<span class="math inline">\(1:1:1\)</span> item ratio) given the observed by estimating the degree of freedom from a chi-square distribution. Alternatively, we could estimate the value of the three population fractions which are expected to be 1/3, 1/3, 1/3 when <span class="math inline">\(1:1:1\)</span>. We will explore this option first and then explore the chi-square approach second. To extend the example, lets also explore a <span class="math inline">\(1:1:2\)</span> ratio.</p>
<p>We start by generating the observed data:</p>
<pre class="r"><code>&gt;  #the observed frequences of A and B
&gt; obs &lt;- c(15, 9, 23)
&gt; obs
[1] 15  9 23</code></pre>
</div>
<div id="model-fitting" class="section level2">
<h2>Model fitting</h2>
<pre class="r"><code>&gt; modelString=&quot;
+  model {
+    #Likelihood
+   for (i in 1:nGroups) {
+     obs[i] ~ dbin(p[i],n[i])
+     p[i] ~ dbeta(a[i],b[i])
+     a[i] ~ dgamma(1,0.01)
+     b[i] ~ dgamma(1,0.01)
+   }
+ }
+ &quot;
&gt; 
&gt; writeLines(modelString, con=&#39;modelbin.txt&#39;)
&gt; 
&gt; #The observed item frequencies
&gt; obs &lt;- c(15, 9, 23)
&gt; data.list &lt;- list(obs = obs, n = c(47, 47, 47), nGroups = 3)
&gt; data.list
$obs
[1] 15  9 23

$n
[1] 47 47 47

$nGroups
[1] 3
&gt; 
&gt; params &lt;- c(&#39;p&#39;)
&gt; nChains = 2
&gt; burnInSteps = 5000
&gt; thinSteps = 1
&gt; numSavedSteps = 20000
&gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&gt; 
&gt; library(R2jags)
&gt; data.r2jags &lt;- jags(data=data.list,model.file=&#39;modelbin.txt&#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 3
   Unobserved stochastic nodes: 9
   Total graph size: 18

Initializing model
&gt; 
&gt; print(data.r2jags)
Inference for Bugs model at &quot;modelbin.txt&quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff
p[1]       0.325   0.065  0.204  0.279  0.324  0.369  0.457 1.007   270
p[2]       0.205   0.057  0.106  0.164  0.201  0.243  0.324 1.001 10000
p[3]       0.492   0.072  0.355  0.443  0.492  0.541  0.632 1.001  4800
deviance  15.241   2.364 12.544 13.511 14.660 16.334 21.177 1.001 10000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 2.8 and DIC = 18.0
DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
<p><strong>Conclusions</strong>: Initially, we should focus our attention on the Rhat and n.eff columns. These are the scale reduction and number of effective samples respectively and they provide an indication of the degree of mixing or coverage of the samples. Ideally, the n.eff values should be approximately equal to the number of saved samples (in this case <span class="math inline">\(4701\)</span>), and the Rhat values should be approximately <span class="math inline">\(1\)</span> (complete convergence). Whilst the actual values are likely to differ substantially from run to run (due to the stochastic nature of the way the chains traverse the posterior distribution), on this occasion, the n.eff of the first two probability parameters (p[1] and p[2]) are substantially lower than <span class="math inline">\(4700\)</span>. Hence, the samples of these parameters may not accurately reflect the posterior distribution. We might consider altering one or more of the chain behavioural paramters (such as the thinning rate), alter the model definition (or priors) itself.</p>
</div>
<div id="model-evaluation" class="section level2">
<h2>Model evaluation</h2>
<pre class="r"><code>&gt; library(mcmcplots)
&gt; denplot(data.r2jags, parms = c(&quot;p&quot;))</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag-1.png" width="672" /></p>
<pre class="r"><code>&gt; traplot(data.r2jags, parms = c(&quot;p&quot;))</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag-2.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; autocorr.diag(as.mcmc(data.r2jags))
          deviance       p[1]        p[2]         p[3]
Lag 0   1.00000000 1.00000000  1.00000000  1.000000000
Lag 1   0.68267965 0.80481535  0.81718010  0.824484449
Lag 5   0.19023252 0.36717636  0.40855991  0.438047294
Lag 10  0.01283481 0.13233461  0.18566394  0.210279554
Lag 50 -0.02733898 0.01296104 -0.00839694 -0.009310405</code></pre>
</div>
</div>
<div id="chi-square" class="section level1">
<h1>Chi-square</h1>
<div id="model-fitting-1" class="section level2">
<h2>Model fitting</h2>
<pre class="r"><code>&gt; modelString=&quot;
+ data {
+  for (i in 1:n){
+       resid[i] &lt;- pow(obs[i]-exp[i],2)/exp[i]
+     }
+     chisq &lt;- sum(resid)
+  }
+  model {
+    #Likelihood
+    chisq  ~ dchisqr(k)
+    #Priors
+    k ~ dunif(0.01,100)
+ }
+ &quot;
&gt; 
&gt; writeLines(modelString, con=&#39;modelchi.txt&#39;)
&gt; 
&gt; #The observed item frequencies
&gt; obs &lt;- c(15, 9, 23)
&gt; #The expected item frequencies (for a 1:1:1 ratio)
&gt; exp &lt;- rep(sum(obs) * 1/3, 3)
&gt; data.list &lt;- list(obs = obs, exp = exp, n = 3)
&gt; data.list
$obs
[1] 15  9 23

$exp
[1] 15.66667 15.66667 15.66667

$n
[1] 3
&gt; 
&gt; #The expected item frequencies (for a 1:1:2 ratio)
&gt; exp &lt;- sum(obs) * c(1/4, 1/4, 2/4)
&gt; data.list1 &lt;- list(obs = obs, exp = exp, n = 3)
&gt; data.list1
$obs
[1] 15  9 23

$exp
[1] 11.75 11.75 23.50

$n
[1] 3
&gt; 
&gt; params &lt;- c(&quot;chisq&quot;, &quot;resid&quot;, &quot;k&quot;)
&gt; nChains = 2
&gt; burnInSteps = 5000
&gt; thinSteps = 1
&gt; numSavedSteps = 20000
&gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&gt; 
&gt; data.r2jags &lt;- jags(data=data.list,model.file=&#39;modelchi.txt&#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling data graph
   Resolving undeclared variables
   Allocating nodes
   Initializing
   Reading data back into data table
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 1
   Unobserved stochastic nodes: 1
   Total graph size: 14

Initializing model
&gt; 
&gt; print(data.r2jags)
Inference for Bugs model at &quot;modelchi.txt&quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect  2.5%   25%   50%    75%  97.5%  Rhat n.eff
chisq      6.298   0.000 6.298 6.298 6.298  6.298  6.298 1.000     1
k          8.279   3.541 2.356 5.724 7.935 10.459 16.061 1.001  9200
resid[1]   0.028   0.000 0.028 0.028 0.028  0.028  0.028 1.000     1
resid[2]   2.837   0.000 2.837 2.837 2.837  2.837  2.837 1.000     1
resid[3]   3.433   0.000 3.433 3.433 3.433  3.433  3.433 1.000     1
deviance   5.306   1.360 4.346 4.439 4.783  5.609  9.146 1.001 10000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 0.9 and DIC = 6.2
DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
<p><strong>Conclusions</strong>: The median degrees of freedom (<span class="math inline">\(k\)</span>) was <span class="math inline">\(8.00\)</span> with a <span class="math inline">\(95\)</span>% spread of <span class="math inline">\(2.31-16.16\)</span>. This interval does not include the value of <span class="math inline">\(2\)</span> (expected value of the chi2 distribution for this hypothesis). Hence there is evidence that the population ratio deviates from a 1:1:1 ratio.</p>
<pre class="r"><code>&gt; data.r2jags1 &lt;- jags(data=data.list1,model.file=&#39;modelchi.txt&#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling data graph
   Resolving undeclared variables
   Allocating nodes
   Initializing
   Reading data back into data table
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 1
   Unobserved stochastic nodes: 1
   Total graph size: 14

Initializing model
&gt; 
&gt; print(data.r2jags1)
Inference for Bugs model at &quot;modelchi.txt&quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect  2.5%   25%   50%   75% 97.5%  Rhat n.eff
chisq      1.553   0.000 1.553 1.553 1.553 1.553 1.553 1.000     1
k          3.358   1.850 0.601 1.967 3.090 4.473 7.698 1.001 10000
resid[1]   0.899   0.000 0.899 0.899 0.899 0.899 0.899 1.000     1
resid[2]   0.644   0.000 0.644 0.644 0.644 0.644 0.644 1.000     1
resid[3]   0.011   0.000 0.011 0.011 0.011 0.011 0.011 1.000     1
deviance   3.815   1.321 2.870 2.966 3.318 4.129 7.674 1.002  2300

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 0.9 and DIC = 4.7
DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
<p><strong>Conclusions</strong>: The median degrees of freedom (<span class="math inline">\(k\)</span>) was <span class="math inline">\(3.31\)</span> with a <span class="math inline">\(95\)</span>% spread of <span class="math inline">\(0.57-7.61\)</span>. This interval comfortably includes the value of <span class="math inline">\(2\)</span> (expected value of the chi2 distribution for this hypothesis). Hence there is no evidence that the population ratio deviates from a 1:1:2 ratio.</p>
</div>
<div id="model-evaluation-1" class="section level2">
<h2>Model evaluation</h2>
<pre class="r"><code>&gt; denplot(data.r2jags, parms = c(&quot;k&quot;))</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag_chi-1.png" width="672" /></p>
<pre class="r"><code>&gt; traplot(data.r2jags, parms = c(&quot;k&quot;))</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag_chi-2.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; autocorr.diag(as.mcmc(data.r2jags))
       chisq     deviance            k resid[1] resid[2] resid[3]
Lag 0    NaN  1.000000000  1.000000000      NaN      NaN   1.0000
Lag 1    NaN  0.442254918  0.261323204      NaN      NaN   0.9998
Lag 5    NaN  0.021391203  0.010672399      NaN      NaN   0.9990
Lag 10   NaN -0.007353145 -0.015931057      NaN      NaN   0.9980
Lag 50   NaN  0.007103731 -0.001695536      NaN      NaN   0.9900</code></pre>
</div>
<div id="exploration-of-the-trends" class="section level2">
<h2>Exploration of the trends</h2>
<p>There are a number of avenues we could take in order to explore the data and models further. One thing we could do is calculate the probability that <span class="math inline">\(k\)</span> is greater than <span class="math inline">\(2\)</span> (the expected value) for each hypothesis. This can be done either by modifying the <code>JAGS</code> code to include a derivative that uses the step function, or we can derive it within <code>R</code> from the <span class="math inline">\(k\)</span> samples. Lets explore the latter.</p>
<pre class="r"><code>&gt; k &lt;- data.r2jags$BUGSoutput$sims.matrix[, &quot;k&quot;]
&gt; pr &lt;- sum(k &gt; 2)/length(k)
&gt; pr
[1] 0.9823
&gt; 
&gt; k &lt;- data.r2jags1$BUGSoutput$sims.matrix[, &quot;k&quot;]
&gt; pr1 &lt;- sum(k &gt; 2)/length(k)
&gt; pr1
[1] 0.7419</code></pre>
<p><strong>Conclusions</strong>: the probability that the expected value exceeds <span class="math inline">\(2\)</span> for the 1:1:1 hypothesis is <span class="math inline">\(0.982\)</span> (<span class="math inline">\(98.2\)</span>%). There is an <span class="math inline">\(98.2\)</span>% likelihood that the population is not 1:1:1.
We could also compare the two alternative hypotheses. The 1:1:2 hypothesis has lower DIC and is therefore considered a better fit (<span class="math inline">\(4.7\)</span> vs <span class="math inline">\(6.4\)</span>). This is a difference in DIC of around <span class="math inline">\(1.7\)</span> units. So the data have higher support for a 1:1:2 population ratio than a 1:1:1 ratio.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-plummer2004jags">
<p>Plummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”</p>
</div>
<div id="ref-su2015package">
<p>Su, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” <em>R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags</em>.</p>
</div>
</div>
</div>
