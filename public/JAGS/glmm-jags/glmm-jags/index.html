<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.1">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Andrea Gabrio">

  
  
  
    
  
  <meta name="description" content="This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:
OpenBUGS - written in component pascal.
JAGS - (Just Another Gibbs Sampler) - written in C&#43;&#43;.">

  
  <link rel="alternate" hreflang="en-us" href="/jags/glmm-jags/glmm-jags/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:450,700|Oswald+Sans:600,700|Roboto+Mono:550,700">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.548c5488bf2acb3767aba941aacbd58f.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academic.4fe06d02a41da8ea4cf58ceef0b5213f.css">
  

  
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/jags/glmm-jags/glmm-jags/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Andrea Gabrio">
  <meta property="og:url" content="/jags/glmm-jags/glmm-jags/">
  <meta property="og:title" content="Generalised Linear Mixed Models - JAGS | Andrea Gabrio">
  <meta property="og:description" content="This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:
OpenBUGS - written in component pascal.
JAGS - (Just Another Gibbs Sampler) - written in C&#43;&#43;."><meta property="og:image" content="/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2020-02-15T21:13:14-05:00">
  
  <meta property="article:modified_time" content="2020-02-15T21:13:14-05:00">
  

  


  





  <title>Generalised Linear Mixed Models - JAGS | Andrea Gabrio</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Andrea Gabrio</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/post/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/research/"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/publication/"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/talk/"><span>Talks</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Software</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/missingHE/"><span>missingHE</span></a>
            </li>
            
          </ul>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Tutorials</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/JAGS/"><span>JAGS</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/OpenBUGS/"><span>OpenBUGS</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/STAN/"><span>STAN</span></a>
            </li>
            
          </ul>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/missingdata/"><span>Missing Data</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Generalised Linear Mixed Models - JAGS</h1>

  

  
    



<meta content="2020-02-15 21:13:14 -0500 -0500" itemprop="datePublished">
<meta content="2020-02-15 21:13:14 -0500 -0500" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a>Andrea Gabrio</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Feb 15, 2020</time>
  </span>
  

  

  

  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    <a href="/categories/r/">R</a>, <a href="/categories/jags/">JAGS</a>, <a href="/categories/generalised-linear-mixed-models/">generalised linear mixed models</a></span>
  

  
    

  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      


<p>This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. <code>BUGS</code> (Bayesian inference Using <em>Gibbs Sampling</em>) is an algorithm and supporting language (resembling <code>R</code>) dedicated to performing the Gibbs sampling implementation of <em>Markov Chain Monte Carlo</em> (MCMC) method. Dialects of the <code>BUGS</code> language are implemented within three main projects:</p>
<ol style="list-style-type: decimal">
<li><p><strong>OpenBUGS</strong> - written in component pascal.</p></li>
<li><p><strong>JAGS</strong> - (Just Another Gibbs Sampler) - written in <code>C++</code>.</p></li>
<li><p><strong>STAN</strong> - a dedicated Bayesian modelling framework written in <code>C++</code> and implementing <em>Hamiltonian</em> MCMC samplers.</p></li>
</ol>
<p>Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of <code>R</code>, and thus, they are best accessed from within <code>R</code> itself. As such there are multiple packages devoted to interfacing with the various software implementations:</p>
<ul>
<li><p><em>R2OpenBUGS</em> - interfaces with <code>OpenBUGS</code></p></li>
<li><p><em>R2jags</em> - interfaces with <code>JAGS</code></p></li>
<li><p><em>rstan</em> - interfaces with <code>STAN</code></p></li>
</ul>
<p>This tutorial will demonstrate how to fit models in <code>JAGS</code> (<span class="citation">Plummer (2004)</span>) using the package <code>R2jags</code> (<span class="citation">Su et al. (2015)</span>) as interface, which also requires to load some other packages.</p>
<div id="overview" class="section level1">
<h1>Overview</h1>
<p>In some respects, <em>Generalized Linear Mixed effects Models</em> (GLMM) are a hierarchical extension of <em>Generalized linear models</em> (GLM) in a similar manner that Linear Mixed effects Models (LMM) are a hierarchical extension of Linear Models (LM). However, whilst the Gaussian (normal) distribution facilitates a relatively straight way of generating the marginal likelihood of the observed response by integrating likelihoods across all possible (and unobserved) levels of a random effect to yield parameter estimates, the same cannot be said for other distributions. Consequently various approximations have been developed to estimate the fixed and random parameters for GLMM’s:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Penalized quasi-likelihood</strong> (PQL). This method approximates a quasi-likelihood by iterative fitting of (re)weighted linear mixed effects models based on the fit of GLM fit. Specifically, it estimates the fixed effects parameters by fitting a GLM that incorporates a correlation (variance-covariance) structure resulting from a LMM and then refits a LMM to re-estimate the variance-covariance structure by using the variance structure from the previous GLM. The cycle continues to iterate until either the fit improvement is below a threshold or a defined number of iterations has occurred. Whilst this is a relatively simple approach, that enables us to leverage methodologies for accommodating heterogeneity and spatial/temporal autocorrelation, it is known to perform poorly (estimates biased towards large variance) for Poisson distributions when the expected value is less than <span class="math inline">\(5\)</span> and for binary data when the expected number of successes or failures are less than <span class="math inline">\(5\)</span>. Moreover, as it approximates quasi-likelihood rather than likelihood, likelihood based inference and information criterion methods (such as likelihood ratio tests and AIC) are not appropriate with this approach. Instead, Wald tests are required for inference.</p></li>
<li><p><strong>Laplace approximation</strong>. This approach utilises a second-order Taylor series expansion to approximate (a mathematical technique for approximating the properties of a function around a point by taking multiple derivatives of the function and summing them together) the likelihood function. If we assume that the likelihood function is approximately normal and thus a quadratic function on a log scale, we can use second-order Taylor series expansion to approximate this likelihood. Whilst this approach is considered to be more accurate than PQL, it is considerably slower and unable to accommodate alternative variance and correlation structures.</p></li>
<li><p><strong>Gauss-Hermite quadrature</strong> (GHQ). This approach approximates the marginal likelihood by approximating the value of integrals at specific points (quadratures). This technique can be further adapted by allowing the number of quadratures and their weights to be optimized via a set of rules.</p></li>
<li><p><strong>Markov-chain Monte-Carlo</strong> (MCMC). This takes a bruit force approach by recreating the likelihood by traversing the likelihood function with sequential sampling proportional to the likelihood. Although this approach is very robust (when the posteriors have converged), they are computationally very intense. Interestingly, some (including Andrew Gelman) argue that PQL, Laplace and GHQ do not yield estimates. Rather they are only approximations of estimates. By contrast, as MCMC methods are able to integrate over all levels by bruit force, the resulting parameters are indeed true estimates.</p></li>
</ol>
<p>We will focus on the last approach which is the more general among the ones considered here and which is based on a Bayesian approach, which can be very flexible and accurate, yet very slow and complex.</p>
</div>
<div id="hierarchical-poisson-regression" class="section level1">
<h1>Hierarchical Poisson regression</h1>
<p>The model I will be developing is a Bayesian hierarchical Poisson regression model which I borrow from a very interesting work about modelling match results in soccer, available both as a <a href="http://www.sumsar.net/papers/baath_2015_modeling_match_resluts_in_soccer.pdf">technical report</a> and as a series of <a href="http://www.sumsar.net/blog/2013/07/modeling-match-results-in-la-liga-part-one/">online posts</a>. The objective of the analysis was to model the match results from the last five seasons of <em>La Liga</em>, the premium Spanish football (soccer) league. In total there were <span class="math inline">\(1900\)</span> rows in the dataset each with information regarding which was the home and away team, what these teams scored and what season it was. The goal outcomes of the teams are assumed to be distributed according to a Poisson distribution, while also taking into account.</p>
</div>
<div id="loading-the-data" class="section level1">
<h1>Loading the data</h1>
<p>I start by loading libraries, reading in the data and preprocessing it for <code>JAGS</code>. The last <span class="math inline">\(50\)</span> matches have unknown outcomes and I create a new data frame <code>d</code> holding only matches with known outcomes. I will come back to the unknown outcomes later when it is time to use the model for prediction. I also load a <code>R</code> function called <code>plotPost</code> which was previously coded in order to facilitate the plotting of the posterior results of the model. All information about the model structure, data and functions can be found on the webpage of the original post of the author or in his technical report (<span class="citation">Bååth (2015)</span>).</p>
<pre class="r"><code>&gt; library(R2jags)
&gt; library(coda)
&gt; library(mcmcplots)
&gt; library(stringr)
&gt; library(plyr)
&gt; library(xtable)
&gt; library(ggplot2)
&gt; source(&quot;plotPost.R&quot;)
&gt; set.seed(12345)  # for reproducibility
&gt; 
&gt; load(&quot;laliga.RData&quot;)
&gt; 
&gt; # -1 = Away win, 0 = Draw, 1 = Home win
&gt; laliga$MatchResult &lt;- sign(laliga$HomeGoals - laliga$AwayGoals)
&gt; 
&gt; # Creating a data frame d with only the complete match results
&gt; d &lt;- na.omit(laliga)
&gt; teams &lt;- unique(c(d$HomeTeam, d$AwayTeam))
&gt; seasons &lt;- unique(d$Season)
&gt; 
&gt; # A list for JAGS with the data from d where the strings are coded as
&gt; # integers
&gt; data_list &lt;- list(HomeGoals = d$HomeGoals, AwayGoals = d$AwayGoals, HomeTeam = as.numeric(factor(d$HomeTeam,
+     levels = teams)), AwayTeam = as.numeric(factor(d$AwayTeam, levels = teams)),
+     Season = as.numeric(factor(d$Season, levels = seasons)), n_teams = length(teams),
+     n_games = nrow(d), n_seasons = length(seasons))
&gt; 
&gt; # Convenience function to generate the type of column names Jags outputs.
&gt; col_name &lt;- function(name, ...) {
+     paste0(name, &quot;[&quot;, paste(..., sep = &quot;,&quot;), &quot;]&quot;)
+ }
&gt; data_list$n_seasons&lt;-NULL
&gt; data_list$Season&lt;-NULL</code></pre>
</div>
<div id="modeling-match-results" class="section level1">
<h1>Modeling Match Results</h1>
<div id="data-check" class="section level2">
<h2>Data check</h2>
<p>ow are the number of goals for each team in a football match distributed? Well, let’s start by assuming that all football matches are roughly equally long, that both teams have many chances at making a goal and that each team have the same probability of making a goal each goal chance. Given these assumptions the distribution of the number of goals for each team should be well captured by a Poisson distribution. A quick and dirty comparison between the actual distribution of the number of scored goals and a Poisson distribution having the same mean number of scored goals support this notion.</p>
<pre class="r"><code>&gt; par(mfcol = c(2, 1), mar = rep(2.2, 4))
&gt; hist(c(d$AwayGoals, d$HomeGoals), xlim = c(-0.5, 8), breaks = -1:9 + 0.5, main = &quot;Distribution of the number of goals\nscored by a team in a match.&quot;)
&gt; mean_goals &lt;- mean(c(d$AwayGoals, d$HomeGoals))
&gt; hist(rpois(9999, mean_goals), xlim = c(-0.5, 8), breaks = -1:9 + 0.5, main = &quot;Random draw from a Poisson distribution with\nthe same mean as the distribution above.&quot;)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/data_check-1.png" width="672" /></p>
</div>
<div id="model-fitting" class="section level2">
<h2>Model fitting</h2>
<p>All teams aren’t equally good and it will be assumed that all teams have a latent skill variable and the skill of a team minus the skill of the opposing team defines the predicted outcome of a game. As the number of goals are assumed to be Poisson distributed it is natural that the skills of the teams are on the log scale of the mean of the distribution. The distribution of the number of goals for team <span class="math inline">\(i\)</span> when facing team <span class="math inline">\(j\)</span> is then</p>
<p><span class="math display">\[ \text{Goals} \sim \text{Pois}(\lambda)\]</span></p>
<p>where <span class="math inline">\(\lambda=\text{baseline} + \text{skill}_i - \text{skill}_j\)</span>. Baseline is the log average number of goals when both teams are equally good. The goal outcome of a match between home team <span class="math inline">\(i\)</span> and away team <span class="math inline">\(j\)</span> is modeled as:</p>
<p><span class="math display">\[ \text{HomeGoals}_{ij} \sim \text{Pois}(\lambda_{\text{home},ij}),\]</span></p>
<p><span class="math display">\[ \text{AwayGoals}_{ij} \sim \text{Pois}(\lambda_{\text{away},ij}),\]</span></p>
<p>where</p>
<p><span class="math display">\[ \log(\lambda_{\text{home},ij}) = \text{baseline} + \text{skill}_i - \text{skill}_j, \]</span></p>
<p><span class="math display">\[ \log(\lambda_{\text{away},ij}) = \text{baseline} + \text{skill}_j - \text{skill}_i. \]</span></p>
<p>Add some priors to that and you’ve got a Bayesian model going! I set the prior distributions over the baseline to:</p>
<p><span class="math display">\[ \text{baseline} \sim N(0, 4^2),\]</span></p>
<p>and the skill of all <span class="math inline">\(n\)</span> teams using a hierarchical approach to :</p>
<p><span class="math display">\[ \text{skill}_{1,\ldots,n} \sim N(\mu_{\text{teams}}, \sigma^2_{\text{teams}}),\]</span></p>
<p>so that teams are assumed to have similar but not identical mean and variance parameters for thier skill parameters. These priors are made vague. For example, the prior on the baseline have a SD of <code>4</code> but since this is on the log scale of the mean number of goals it corresponds to one SD from the mean <code>0</code> covering the range of <code>[0.02,54.6]</code> goals. Turning this into a <code>JAGS</code> model requires some minor adjustments. The model have to loop over all the match results, which adds some for-loops. <code>JAGS</code> parameterises the normal distribution with precision (the reciprocal of the variance) instead of variance so the hyperpriors have to be converted. Finally I have to “anchor” the skill of one team to a constant otherwise the mean skill can drift away freely (<em>sum to zero constraint</em>) and the model cannot be identified. Doing these adjustments results in the following model description:</p>
<pre class="r"><code>&gt; m1_string &lt;- &quot;model {
+ for(i in 1:n_games) {
+   HomeGoals[i] ~ dpois(lambda_home[HomeTeam[i],AwayTeam[i]])
+   AwayGoals[i] ~ dpois(lambda_away[HomeTeam[i],AwayTeam[i]])
+ }
+ 
+ for(home_i in 1:n_teams) {
+   for(away_i in 1:n_teams) {
+     lambda_home[home_i, away_i] &lt;- exp(baseline + skill[home_i] - skill[away_i])
+     lambda_away[home_i, away_i] &lt;- exp(baseline + skill[away_i] - skill[home_i])
+   }
+ }
+ 
+ skill[1] &lt;- 0
+ for(j in 2:n_teams) {
+   skill[j] ~ dnorm(group_skill, group_tau)
+ }  
+ 
+ group_skill ~ dnorm(0, 0.0625)
+ group_tau &lt;- 1 / pow(group_sigma, 2)
+ group_sigma ~ dunif(0, 3)
+ baseline ~ dnorm(0, 0.0625)
+ }
+ &quot;
&gt; 
&gt; ## write the model to a text file
&gt; writeLines(m1_string, con = &quot;model1.txt&quot;)</code></pre>
<p>Next, we define the nodes (parameters and derivatives) to monitor and the chain parameters.</p>
<pre class="r"><code>&gt; params &lt;- c(&quot;baseline&quot;, &quot;skill&quot;, &quot;group_skill&quot;, &quot;group_sigma&quot;)
&gt; nChains = 2
&gt; burnInSteps = 3000
&gt; thinSteps = 1
&gt; numSavedSteps = 15000  #across all chains
&gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&gt; nIter
[1] 10500</code></pre>
<p>Start the <code>JAGS</code> model (check the model, load data into the model, specify the number of chains and compile the model). Run the <code>JAGS</code> code via the <code>R2jags</code> interface and the <code>jags</code> function. Note that the first time jags is run after the <code>R2jags</code> package is loaded, it is often necessary to run any kind of randomisation function just to initiate the .Random.seed variable.</p>
<pre class="r"><code>&gt; m1.r2jags &lt;- jags(data = data_list, inits = NULL, parameters.to.save = params,
+     model.file = &quot;model1.txt&quot;, n.chains = nChains, n.iter = nIter,
+     n.burnin = burnInSteps, n.thin = thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 3700
   Unobserved stochastic nodes: 31
   Total graph size: 9151

Initializing model
&gt; 
&gt; print(m1.r2jags)
Inference for Bugs model at &quot;model1.txt&quot;, fit using jags,
 2 chains, each with 10500 iterations (first 3000 discarded)
 n.sims = 15000 iterations saved
              mu.vect sd.vect      2.5%       25%       50%       75%     97.5%
baseline        0.281   0.014     0.253     0.271     0.281     0.291     0.309
group_sigma     0.225   0.034     0.169     0.201     0.222     0.246     0.302
group_skill     0.016   0.062    -0.104    -0.026     0.016     0.057     0.136
skill[1]        0.000   0.000     0.000     0.000     0.000     0.000     0.000
skill[2]        0.185   0.061     0.064     0.145     0.185     0.226     0.307
skill[3]        0.017   0.069    -0.117    -0.030     0.017     0.064     0.154
skill[4]       -0.013   0.061    -0.132    -0.054    -0.013     0.028     0.109
skill[5]       -0.180   0.098    -0.376    -0.247    -0.179    -0.113     0.008
skill[6]       -0.048   0.063    -0.170    -0.090    -0.049    -0.007     0.081
skill[7]       -0.013   0.058    -0.128    -0.051    -0.013     0.025     0.103
skill[8]        0.199   0.060     0.084     0.159     0.199     0.240     0.317
skill[9]       -0.077   0.063    -0.200    -0.119    -0.076    -0.034     0.046
skill[10]      -0.110   0.062    -0.230    -0.152    -0.110    -0.068     0.011
skill[11]       0.698   0.057     0.588     0.658     0.696     0.736     0.811
skill[12]       0.133   0.060     0.017     0.092     0.133     0.174     0.249
skill[13]       0.017   0.059    -0.096    -0.023     0.016     0.057     0.132
skill[14]       0.038   0.061    -0.078    -0.003     0.037     0.080     0.160
skill[15]      -0.008   0.060    -0.124    -0.048    -0.009     0.033     0.108
skill[16]      -0.117   0.099    -0.305    -0.186    -0.118    -0.051     0.080
skill[17]       0.606   0.058     0.495     0.566     0.606     0.646     0.720
skill[18]      -0.071   0.070    -0.205    -0.118    -0.072    -0.026     0.071
skill[19]      -0.115   0.069    -0.247    -0.162    -0.115    -0.068     0.022
skill[20]       0.075   0.064    -0.045     0.032     0.074     0.117     0.204
skill[21]      -0.104   0.065    -0.231    -0.148    -0.105    -0.060     0.027
skill[22]      -0.212   0.099    -0.403    -0.281    -0.213    -0.146    -0.017
skill[23]      -0.161   0.101    -0.360    -0.230    -0.159    -0.094     0.036
skill[24]      -0.118   0.101    -0.319    -0.186    -0.118    -0.050     0.085
skill[25]       0.009   0.071    -0.131    -0.037     0.010     0.057     0.147
skill[26]       0.058   0.069    -0.079     0.011     0.058     0.104     0.195
skill[27]      -0.061   0.080    -0.218    -0.115    -0.060    -0.005     0.088
skill[28]      -0.118   0.079    -0.272    -0.170    -0.119    -0.065     0.037
skill[29]      -0.059   0.105    -0.260    -0.130    -0.062     0.011     0.155
deviance    10912.856   7.406 10900.319 10907.610 10912.214 10917.514 10928.852
             Rhat n.eff
baseline    1.001 15000
group_sigma 1.002  1500
group_skill 1.002  1600
skill[1]    1.000     1
skill[2]    1.005   410
skill[3]    1.009   180
skill[4]    1.007   670
skill[5]    1.002  2400
skill[6]    1.001  4400
skill[7]    1.002  2400
skill[8]    1.001 11000
skill[9]    1.001 15000
skill[10]   1.009   190
skill[11]   1.001  4700
skill[12]   1.005   340
skill[13]   1.001 14000
skill[14]   1.006   310
skill[15]   1.002  2600
skill[16]   1.003 12000
skill[17]   1.001 15000
skill[18]   1.002  2700
skill[19]   1.003   880
skill[20]   1.002  1800
skill[21]   1.002  1000
skill[22]   1.001  2800
skill[23]   1.001 15000
skill[24]   1.005   380
skill[25]   1.001 15000
skill[26]   1.002  2300
skill[27]   1.001 15000
skill[28]   1.001 15000
skill[29]   1.001 13000
deviance    1.001  3900

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 27.4 and DIC = 10940.3
DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
</div>
<div id="mcmc-diagnostics" class="section level2">
<h2>MCMC diagnostics</h2>
<p>Using the generated MCMC samples I can now look at the credible skill values of any team. Let’s look at the trace plot and the distribution of the skill parameters for FC Sevilla and FC Valencia.</p>
<pre class="r"><code>&gt; team_par&lt;-c(which(teams == c(&quot;FC Sevilla&quot;)), which(teams == &quot;FC Valencia&quot;))
&gt; denplot(m1.r2jags, parms = team_par, style = &quot;plain&quot;, main = c(&quot;Sevilla&quot;,&quot;Valenica&quot;))</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag-1.png" width="672" /></p>
<pre class="r"><code>&gt; traplot(m1.r2jags, parms = team_par, style = &quot;plain&quot;, main = c(&quot;Sevilla&quot;,&quot;Valenica&quot;))</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag-2.png" width="672" /></p>
</div>
<div id="model-validation" class="section level2">
<h2>Model validation</h2>
<p>Seems like Sevilla and Valencia have similar skill with Valencia being slightly better. Using the MCMC samples it is not only possible to look at the distribution of parameter values but it is also straight forward to simulate matches between teams and look at the credible distribution of number of goals scored and the probability of a win for the home team, a win for the away team or a draw. The following functions simulates matches with one team as home team and one team as away team and plots the predicted result together with the actual outcomes of any matches in the <code>laliga</code> data set.</p>
<pre class="r"><code>&gt; # Plots histograms over home_goals, away_goals, the difference in goals
&gt; # and a barplot over match results.
&gt; plot_goals &lt;- function(home_goals, away_goals) {
+     n_matches &lt;- length(home_goals)
+     goal_diff &lt;- home_goals - away_goals
+     match_result &lt;- ifelse(goal_diff &lt; 0, &quot;away_win&quot;, ifelse(goal_diff &gt; 0,
+         &quot;home_win&quot;, &quot;equal&quot;))
+     hist(home_goals, xlim = c(-0.5, 10), breaks = (0:100) - 0.5)
+     hist(away_goals, xlim = c(-0.5, 10), breaks = (0:100) - 0.5)
+     hist(goal_diff, xlim = c(-6, 6), breaks = (-100:100) - 0.5)
+     barplot(table(match_result)/n_matches, ylim = c(0, 1))
+ }
&gt; 
&gt; 
&gt; plot_pred_comp1 &lt;- function(home_team, away_team, ms) {
+     # Simulates and plots game goals scores using the MCMC samples from the m1
+     # model.
+     par(mar=c(1,1,1,1))
+     par(mfrow = c(2, 4))
+     baseline &lt;- ms[, &quot;baseline&quot;]
+     home_skill &lt;- ms[, which(teams == home_team)]
+     away_skill &lt;- ms[, which(teams == away_team)]
+     home_goals &lt;- rpois(nrow(ms), exp(baseline + home_skill - away_skill))
+     away_goals &lt;- rpois(nrow(ms), exp(baseline + away_skill - home_skill))
+     plot_goals(home_goals, away_goals)
+     # Plots the actual distribution of goals between the two teams
+     home_goals &lt;- d$HomeGoals[d$HomeTeam == home_team &amp; d$AwayTeam == away_team]
+     away_goals &lt;- d$AwayGoals[d$HomeTeam == home_team &amp; d$AwayTeam == away_team]
+     plot_goals(home_goals, away_goals)
+ }</code></pre>
<p>Let’s look at Valencia (home team) vs. Sevilla (away team). The graph below shows the simulation on the first row and the historical data on the second row.</p>
<pre class="r"><code>&gt; ms1&lt;-as.matrix(m1.r2jags$BUGSoutput$sims.matrix)
&gt; plot_pred_comp1(&quot;FC Valencia&quot;, &quot;FC Sevilla&quot;, ms1)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_val2-1.png" width="672" /></p>
<p>Here we discover a problem with the current model. While the simulated data looks the same, except that the home team and the away team swapped places, the historical data now shows that Sevilla often wins against Valencia when being the home team. Our model doesn’t predict this because it doesn’t considers the advantage of being the home team.</p>
</div>
</div>
<div id="accounting-for-home-advantage" class="section level1">
<h1>Accounting for home advantage</h1>
<p>The only change to the model needed to account for the home advantage is to split the baseline into two components, a home baseline and an away baseline. The following <code>JAGS</code> model implements this change by splitting <code>baseline</code> into <code>home_baseline</code> and <code>away_baseline</code>.</p>
<div id="model-fitting-1" class="section level2">
<h2>Model fitting</h2>
<pre class="r"><code>&gt; # model 2
&gt; m2_string &lt;- &quot;model {
+ for(i in 1:n_games) {
+   HomeGoals[i] ~ dpois(lambda_home[HomeTeam[i],AwayTeam[i]])
+   AwayGoals[i] ~ dpois(lambda_away[HomeTeam[i],AwayTeam[i]])
+ }
+ 
+ for(home_i in 1:n_teams) {
+   for(away_i in 1:n_teams) {
+     lambda_home[home_i, away_i] &lt;- exp( home_baseline + skill[home_i] - skill[away_i])
+     lambda_away[home_i, away_i] &lt;- exp( away_baseline + skill[away_i] - skill[home_i])
+   }
+ }
+ 
+ skill[1] &lt;- 0 
+ for(j in 2:n_teams) {
+   skill[j] ~ dnorm(group_skill, group_tau)
+ }
+ 
+ group_skill ~ dnorm(0, 0.0625)
+ group_tau &lt;- 1/pow(group_sigma, 2)
+ group_sigma ~ dunif(0, 3)
+ 
+ home_baseline ~ dnorm(0, 0.0625)
+ away_baseline ~ dnorm(0, 0.0625)
+ }
+ &quot;
&gt; 
&gt; ## write the model to a text file
&gt; writeLines(m2_string, con = &quot;model2.txt&quot;)</code></pre>
<p>And now re-fit the model</p>
<pre class="r"><code>&gt; params &lt;- c(&quot;home_baseline&quot;, &quot;away_baseline&quot;, &quot;skill&quot;, &quot;group_sigma&quot;, &quot;group_skill&quot;)
&gt; nChains = 2
&gt; burnInSteps = 3000
&gt; thinSteps = 1
&gt; numSavedSteps = 15000  #across all chains
&gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&gt; 
&gt; m2.r2jags &lt;- jags(data = data_list, inits = NULL, parameters.to.save = params,
+     model.file = &quot;model2.txt&quot;, n.chains = nChains, n.iter = nIter,
+     n.burnin = burnInSteps, n.thin = thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 3700
   Unobserved stochastic nodes: 32
   Total graph size: 10863

Initializing model
&gt; 
&gt; print(m2.r2jags)
Inference for Bugs model at &quot;model2.txt&quot;, fit using jags,
 2 chains, each with 10500 iterations (first 3000 discarded)
 n.sims = 15000 iterations saved
                mu.vect sd.vect      2.5%       25%       50%       75%
away_baseline     0.081   0.022     0.038     0.067     0.082     0.096
group_sigma       0.226   0.035     0.169     0.201     0.221     0.246
group_skill       0.019   0.061    -0.102    -0.022     0.019     0.060
home_baseline     0.449   0.019     0.413     0.436     0.449     0.462
skill[1]          0.000   0.000     0.000     0.000     0.000     0.000
skill[2]          0.187   0.061     0.066     0.147     0.187     0.228
skill[3]          0.017   0.068    -0.120    -0.030     0.017     0.063
skill[4]         -0.011   0.058    -0.126    -0.050    -0.011     0.028
skill[5]         -0.179   0.100    -0.375    -0.246    -0.178    -0.111
skill[6]         -0.044   0.062    -0.167    -0.087    -0.045    -0.002
skill[7]         -0.009   0.061    -0.127    -0.050    -0.009     0.033
skill[8]          0.199   0.059     0.081     0.159     0.201     0.240
skill[9]         -0.078   0.063    -0.205    -0.121    -0.077    -0.035
skill[10]        -0.108   0.064    -0.234    -0.151    -0.108    -0.066
skill[11]         0.699   0.057     0.583     0.661     0.699     0.737
skill[12]         0.132   0.059     0.019     0.092     0.132     0.173
skill[13]         0.025   0.061    -0.097    -0.016     0.025     0.065
skill[14]         0.041   0.059    -0.074     0.000     0.041     0.081
skill[15]        -0.006   0.060    -0.125    -0.046    -0.005     0.035
skill[16]        -0.115   0.099    -0.311    -0.181    -0.116    -0.049
skill[17]         0.611   0.059     0.494     0.571     0.612     0.653
skill[18]        -0.068   0.070    -0.204    -0.115    -0.069    -0.020
skill[19]        -0.114   0.070    -0.252    -0.162    -0.113    -0.065
skill[20]         0.077   0.062    -0.047     0.035     0.077     0.118
skill[21]        -0.102   0.064    -0.228    -0.145    -0.101    -0.058
skill[22]        -0.202   0.098    -0.399    -0.266    -0.201    -0.138
skill[23]        -0.167   0.098    -0.361    -0.233    -0.167    -0.102
skill[24]        -0.115   0.099    -0.306    -0.183    -0.116    -0.049
skill[25]         0.010   0.071    -0.132    -0.035     0.010     0.057
skill[26]         0.061   0.069    -0.075     0.014     0.062     0.109
skill[27]        -0.059   0.078    -0.211    -0.112    -0.060    -0.007
skill[28]        -0.113   0.082    -0.274    -0.167    -0.113    -0.058
skill[29]        -0.051   0.105    -0.265    -0.121    -0.050     0.021
deviance      10742.730   7.596 10729.548 10737.288 10742.120 10747.534
                  97.5%  Rhat n.eff
away_baseline     0.126 1.002  1600
group_sigma       0.305 1.001 15000
group_skill       0.138 1.006   330
home_baseline     0.486 1.001 15000
skill[1]          0.000 1.000     1
skill[2]          0.306 1.005   380
skill[3]          0.149 1.006   310
skill[4]          0.102 1.002  1900
skill[5]          0.013 1.002  1100
skill[6]          0.076 1.005   400
skill[7]          0.110 1.004   440
skill[8]          0.312 1.008   240
skill[9]          0.045 1.003   620
skill[10]         0.014 1.005   330
skill[11]         0.811 1.004   580
skill[12]         0.245 1.008   200
skill[13]         0.144 1.003   840
skill[14]         0.154 1.003   700
skill[15]         0.111 1.008   210
skill[16]         0.078 1.006   300
skill[17]         0.722 1.005   420
skill[18]         0.070 1.005   360
skill[19]         0.019 1.007   290
skill[20]         0.200 1.006   310
skill[21]         0.025 1.010   170
skill[22]        -0.009 1.002  1600
skill[23]         0.028 1.003   900
skill[24]         0.078 1.006   330
skill[25]         0.151 1.007   240
skill[26]         0.196 1.003   770
skill[27]         0.097 1.001  3900
skill[28]         0.051 1.002  1800
skill[29]         0.153 1.003   620
deviance      10759.025 1.004   460

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 28.8 and DIC = 10771.5
DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
</div>
<div id="mcmc-diagnostics-1" class="section level2">
<h2>MCMC diagnostics</h2>
<p>Looking at the trace plots and distributions of <code>home_baseline</code> and <code>away_baseline</code> shows that there is a considerable home advantage.</p>
<pre class="r"><code>&gt; team_par&lt;-c(&quot;home_baseline&quot;, &quot;away_baseline&quot;)
&gt; denplot(m2.r2jags, parms = team_par, style = &quot;plain&quot;, main = c(&quot;home_baseline&quot;,&quot;away_baseline&quot;))</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag_m2-1.png" width="672" /></p>
<pre class="r"><code>&gt; traplot(m2.r2jags, parms = team_par, style = &quot;plain&quot;, main = c(&quot;home_baseline&quot;,&quot;away_baseline&quot;))</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag_m2-2.png" width="672" /></p>
</div>
<div id="model-validation-1" class="section level2">
<h2>Model validation</h2>
<p>Looking at the difference between <code>exp(home_baseline)</code> and <code>exp(away_baseline)</code> shows that the home advantage is realised as roughly <span class="math inline">\(0.5\)</span> more goals for the home team.</p>
<pre class="r"><code>&gt; ms2&lt;-as.matrix(m2.r2jags$BUGSoutput$sims.matrix)
&gt; plotPost(exp(ms2[, &quot;home_baseline&quot;]) - exp(ms2[, &quot;away_baseline&quot;]), compVal = 0,
+     xlab = &quot;Home advantage in number of goals&quot;)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag_post-1.png" width="672" /></p>
<pre><code>                                       mean    median      mode hdiMass
Home advantage in number of goals 0.4822046 0.4822645 0.4831489    0.95
                                     hdiLow   hdiHigh compVal pcGTcompVal
Home advantage in number of goals 0.4096975 0.5549439       0           1
                                  ROPElow ROPEhigh pcInROPE
Home advantage in number of goals      NA       NA       NA</code></pre>
<p>Comparing the DIC of the of the two models also indicates that the new model is better.</p>
<pre class="r"><code>&gt; dic_m1&lt;-m1.r2jags$BUGSoutput$DIC
&gt; dic_m2&lt;-m2.r2jags$BUGSoutput$DIC
&gt; diff_dic&lt;-dic_m1 - dic_m2
&gt; diff_dic
[1] 168.7556</code></pre>
<p>Finally we’ll look at the simulated results for Valencia (home team) vs Sevilla (away team) using the estimates from the new model with the first row of the graph showing the predicted outcome and the second row showing the actual data.</p>
<pre class="r"><code>&gt; plot_pred_comp2 &lt;- function(home_team, away_team, ms) {
+     par(mar=c(1,1,1,1))
+     par(mfrow = c(2, 4))
+     home_baseline &lt;- ms[, &quot;home_baseline&quot;]
+     away_baseline &lt;- ms[, &quot;away_baseline&quot;]
+     home_skill &lt;- ms[, col_name(&quot;skill&quot;, which(teams == home_team))]
+     away_skill &lt;- ms[, col_name(&quot;skill&quot;, which(teams == away_team))]
+     home_goals &lt;- rpois(nrow(ms), exp(home_baseline + home_skill - away_skill))
+     away_goals &lt;- rpois(nrow(ms), exp(away_baseline + away_skill - home_skill))
+     plot_goals(home_goals, away_goals)
+     home_goals &lt;- d$HomeGoals[d$HomeTeam == home_team &amp; d$AwayTeam == away_team]
+     away_goals &lt;- d$AwayGoals[d$HomeTeam == home_team &amp; d$AwayTeam == away_team]
+     plot_goals(home_goals, away_goals)
+ }
&gt; 
&gt; plot_pred_comp2(&quot;FC Valencia&quot;, &quot;FC Sevilla&quot;, ms2)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag_post3-1.png" width="672" /></p>
<p>And similarly Sevilla (home team) vs Valencia (away team).</p>
<pre class="r"><code>&gt; plot_pred_comp2(&quot;FC Sevilla&quot;, &quot;FC Valencia&quot;, ms2)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag_post4-1.png" width="672" /></p>
<p>Now the results are closer to the historical data as both Sevilla and Valencia are more likely to win when playing as the home team. At this point in the modeling process I decided to try to split the skill parameter into two components, offence skill and defense skill, thinking that some teams might be good at scoring goals but at the same time be bad at keeping the opponent from scoring. This didn’t seem to result in any better fit however, perhaps because the offensive and defensive skill of a team tend to be highly related. There is however one more thing I would like to change with the model.</p>
</div>
</div>
<div id="allowing-for-skill-variation-over-the-season" class="section level1">
<h1>Allowing for skill variation over the season</h1>
<p>The data set <code>laliga</code> contains data from five different seasons and an assumption of the current model is that a team has the same skill during all seasons. This is probably not a realistic assumption, teams probably differ in their year-to-year performance. And what more, some teams do not even participate in all seasons in the <code>laliga</code> data set, as a result of dropping out of the first division, as the following diagram shows:</p>
<div id="data-check-1" class="section level2">
<h2>Data check</h2>
<pre class="r"><code>&gt; qplot(Season, HomeTeam, data = d, ylab = &quot;Team&quot;, xlab = &quot;Particicipation by Season&quot;) + theme_classic()</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/data_check_m3-1.png" width="672" /></p>
<p>The second iteration of the model was therefore modified to include the year-to-year variability in team skill. This was done by allowing each team to have one skill parameter per season but to connect the skill parameters by using a team’s skill parameter for season <span class="math inline">\(t\)</span> in the prior distribution for that team’s skill parameter for season <span class="math inline">\(t+1\)</span> so that</p>
<p><span class="math display">\[ \text{skill}_{t+1} \sim N(\text{skill}_t,\sigma^2_{\text{season}})\]</span></p>
<p>for all different <span class="math inline">\(t\)</span>, except the first season which is given an vague prior. Here <span class="math inline">\(\sigma^2_{\text{season}}\)</span> is a parameter estimated using the whole data set. The home and away baselines are given the same kind of priors and below is the resulting <code>JAGS</code> model.</p>
<pre class="r"><code>&gt; # model 3
&gt; m3_string &lt;- &quot;model {
+ for(i in 1:n_games) {
+   HomeGoals[i] ~ dpois(lambda_home[Season[i], HomeTeam[i],AwayTeam[i]])
+   AwayGoals[i] ~ dpois(lambda_away[Season[i], HomeTeam[i],AwayTeam[i]])
+ }
+ 
+ for(season_i in 1:n_seasons) {
+   for(home_i in 1:n_teams) {
+     for(away_i in 1:n_teams) {
+       lambda_home[season_i, home_i, away_i] &lt;- exp( home_baseline[season_i] + skill[season_i, home_i] - skill[season_i, away_i])
+       lambda_away[season_i, home_i, away_i] &lt;- exp( away_baseline[season_i] + skill[season_i, away_i] - skill[season_i, home_i])
+     }
+   }
+ }
+ 
+ skill[1, 1] &lt;- 0 
+ for(j in 2:n_teams) {
+   skill[1, j] ~ dnorm(group_skill, group_tau)
+ }
+ 
+ group_skill ~ dnorm(0, 0.0625)
+ group_tau &lt;- 1/pow(group_sigma, 2)
+ group_sigma ~ dunif(0, 3)
+ 
+ home_baseline[1] ~ dnorm(0, 0.0625)
+ away_baseline[1] ~ dnorm(0, 0.0625)
+ 
+ for(season_i in 2:n_seasons) {
+   skill[season_i, 1] &lt;- 0 
+   for(j in 2:n_teams) {
+     skill[season_i, j] ~ dnorm(skill[season_i - 1, j], season_tau)
+   }
+   home_baseline[season_i] ~ dnorm(home_baseline[season_i - 1], season_tau)
+   away_baseline[season_i] ~ dnorm(away_baseline[season_i - 1], season_tau)
+ }
+ 
+ season_tau &lt;- 1/pow(season_sigma, 2) 
+ season_sigma ~ dunif(0, 3) 
+ }
+ &quot;
&gt; 
&gt; ## write the model to a text file
&gt; writeLines(m3_string, con = &quot;model3.txt&quot;)</code></pre>
<p>And now re-fit the model. These changes to the model unfortunately introduces quit a lot of autocorrelation when running the MCMC sampler. Also, re-define the data list to include information for the season parameters.</p>
<pre class="r"><code>&gt; data_list_m3 &lt;- list(HomeGoals = d$HomeGoals, AwayGoals = d$AwayGoals, HomeTeam = as.numeric(factor(d$HomeTeam,
+     levels = teams)), AwayTeam = as.numeric(factor(d$AwayTeam, levels = teams)),
+     Season = as.numeric(factor(d$Season, levels = seasons)), n_teams = length(teams),
+     n_games = nrow(d), n_seasons = length(seasons))
&gt; params &lt;- c(&quot;home_baseline&quot;, &quot;away_baseline&quot;, &quot;skill&quot;, &quot;season_sigma&quot;, &quot;group_sigma&quot;, &quot;group_skill&quot;)
&gt; nChains = 2
&gt; burnInSteps = 3000
&gt; thinSteps = 1
&gt; numSavedSteps = 15000  #across all chains
&gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&gt; 
&gt; m3.r2jags &lt;- jags(data = data_list_m3, inits = NULL, parameters.to.save = params,
+     model.file = &quot;model3.txt&quot;, n.chains = nChains, n.iter = nIter,
+     n.burnin = burnInSteps, n.thin = thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 3700
   Unobserved stochastic nodes: 153
   Total graph size: 26525

Initializing model
&gt; 
&gt; print(m3.r2jags)
Inference for Bugs model at &quot;model3.txt&quot;, fit using jags,
 2 chains, each with 10500 iterations (first 3000 discarded)
 n.sims = 15000 iterations saved
                   mu.vect sd.vect      2.5%       25%       50%       75%
away_baseline[1]     0.105   0.038     0.030     0.079     0.103     0.130
away_baseline[2]     0.078   0.030     0.018     0.059     0.079     0.099
away_baseline[3]     0.067   0.031     0.005     0.047     0.069     0.089
away_baseline[4]     0.064   0.032    -0.001     0.043     0.065     0.087
away_baseline[5]     0.079   0.035     0.011     0.056     0.080     0.101
group_sigma          0.217   0.035     0.158     0.193     0.214     0.238
group_skill          0.018   0.060    -0.090    -0.023     0.015     0.057
home_baseline[1]     0.447   0.029     0.392     0.428     0.446     0.466
home_baseline[2]     0.437   0.026     0.383     0.421     0.438     0.454
home_baseline[3]     0.443   0.026     0.389     0.427     0.443     0.459
home_baseline[4]     0.452   0.027     0.400     0.434     0.451     0.469
home_baseline[5]     0.454   0.031     0.394     0.434     0.452     0.474
season_sigma         0.033   0.019     0.001     0.016     0.032     0.048
skill[1,1]           0.000   0.000     0.000     0.000     0.000     0.000
skill[2,1]           0.000   0.000     0.000     0.000     0.000     0.000
skill[3,1]           0.000   0.000     0.000     0.000     0.000     0.000
skill[4,1]           0.000   0.000     0.000     0.000     0.000     0.000
skill[5,1]           0.000   0.000     0.000     0.000     0.000     0.000
skill[1,2]           0.178   0.067     0.047     0.136     0.175     0.218
skill[2,2]           0.169   0.065     0.039     0.129     0.167     0.207
skill[3,2]           0.181   0.064     0.061     0.139     0.177     0.219
skill[4,2]           0.195   0.065     0.076     0.148     0.189     0.235
skill[5,2]           0.216   0.075     0.090     0.161     0.209     0.265
skill[1,3]           0.015   0.074    -0.126    -0.033     0.013     0.060
skill[2,3]           0.017   0.075    -0.127    -0.030     0.014     0.063
skill[3,3]           0.020   0.074    -0.121    -0.027     0.017     0.066
skill[4,3]           0.022   0.071    -0.111    -0.024     0.018     0.066
skill[5,3]           0.027   0.075    -0.111    -0.021     0.023     0.074
skill[1,4]          -0.008   0.066    -0.122    -0.053    -0.012     0.035
skill[2,4]          -0.011   0.063    -0.118    -0.053    -0.015     0.031
skill[3,4]          -0.010   0.063    -0.120    -0.052    -0.015     0.031
skill[4,4]          -0.020   0.065    -0.136    -0.064    -0.024     0.022
skill[5,4]          -0.022   0.069    -0.151    -0.069    -0.025     0.023
skill[1,5]          -0.174   0.098    -0.370    -0.238    -0.173    -0.107
skill[2,5]          -0.174   0.104    -0.384    -0.242    -0.174    -0.104
skill[3,5]          -0.174   0.111    -0.394    -0.247    -0.174    -0.102
skill[4,5]          -0.174   0.117    -0.408    -0.251    -0.174    -0.098
skill[5,5]          -0.174   0.123    -0.420    -0.254    -0.175    -0.095
skill[1,6]          -0.034   0.074    -0.161    -0.085    -0.039     0.017
skill[2,6]          -0.048   0.068    -0.167    -0.096    -0.052    -0.004
skill[3,6]          -0.058   0.067    -0.176    -0.106    -0.060    -0.015
skill[4,6]          -0.065   0.072    -0.193    -0.117    -0.067    -0.019
skill[5,6]          -0.071   0.075    -0.207    -0.125    -0.072    -0.025
skill[1,7]          -0.006   0.069    -0.124    -0.054    -0.013     0.038
skill[2,7]          -0.014   0.065    -0.129    -0.058    -0.021     0.027
skill[3,7]          -0.009   0.064    -0.120    -0.054    -0.016     0.031
skill[4,7]          -0.006   0.066    -0.117    -0.052    -0.013     0.035
skill[5,7]          -0.001   0.071    -0.122    -0.051    -0.009     0.044
skill[1,8]           0.200   0.067     0.069     0.155     0.198     0.242
skill[2,8]           0.206   0.063     0.086     0.162     0.203     0.246
skill[3,8]           0.209   0.063     0.090     0.164     0.206     0.250
skill[4,8]           0.204   0.064     0.082     0.158     0.202     0.244
skill[5,8]           0.195   0.069     0.059     0.151     0.194     0.239
skill[1,9]          -0.055   0.073    -0.193    -0.102    -0.057    -0.005
skill[2,9]          -0.074   0.068    -0.203    -0.119    -0.077    -0.026
skill[3,9]          -0.087   0.068    -0.219    -0.133    -0.088    -0.040
skill[4,9]          -0.105   0.074    -0.254    -0.155    -0.101    -0.057
skill[5,9]          -0.105   0.083    -0.279    -0.159    -0.100    -0.049
skill[1,10]         -0.121   0.072    -0.246    -0.175    -0.125    -0.072
skill[2,10]         -0.109   0.070    -0.224    -0.163    -0.113    -0.061
skill[3,10]         -0.102   0.071    -0.219    -0.156    -0.106    -0.051
skill[4,10]         -0.111   0.073    -0.234    -0.168    -0.116    -0.060
skill[5,10]         -0.111   0.083    -0.259    -0.173    -0.116    -0.055
skill[1,11]          0.676   0.072     0.526     0.629     0.680     0.726
skill[2,11]          0.696   0.063     0.571     0.654     0.699     0.738
skill[3,11]          0.712   0.060     0.600     0.672     0.712     0.750
skill[4,11]          0.728   0.061     0.617     0.688     0.725     0.761
skill[5,11]          0.727   0.064     0.606     0.686     0.725     0.763
skill[1,12]          0.146   0.064     0.025     0.106     0.144     0.186
skill[2,12]          0.143   0.060     0.028     0.105     0.141     0.180
skill[3,12]          0.131   0.058     0.019     0.094     0.130     0.168
skill[4,12]          0.127   0.059     0.010     0.089     0.126     0.164
skill[5,12]          0.127   0.064     0.002     0.086     0.126     0.166
skill[1,13]          0.031   0.065    -0.087    -0.014     0.028     0.071
skill[2,13]          0.035   0.062    -0.077    -0.008     0.032     0.073
skill[3,13]          0.023   0.061    -0.091    -0.019     0.020     0.062
skill[4,13]          0.017   0.062    -0.101    -0.026     0.015     0.057
skill[5,13]          0.015   0.067    -0.117    -0.030     0.014     0.057
skill[1,14]          0.028   0.064    -0.095    -0.015     0.025     0.069
skill[2,14]          0.027   0.061    -0.091    -0.014     0.024     0.065
skill[3,14]          0.030   0.059    -0.085    -0.010     0.027     0.067
skill[4,14]          0.046   0.062    -0.067     0.004     0.040     0.085
skill[5,14]          0.055   0.068    -0.064     0.008     0.049     0.099
skill[1,15]          0.015   0.065    -0.106    -0.029     0.009     0.055
skill[2,15]          0.019   0.063    -0.095    -0.024     0.012     0.058
skill[3,15]         -0.003   0.061    -0.115    -0.043    -0.005     0.034
skill[4,15]         -0.014   0.062    -0.132    -0.055    -0.015     0.024
skill[5,15]         -0.037   0.071    -0.182    -0.082    -0.034     0.009
skill[1,16]         -0.120   0.096    -0.304    -0.187    -0.122    -0.055
skill[2,16]         -0.121   0.103    -0.319    -0.192    -0.122    -0.051
skill[3,16]         -0.121   0.109    -0.329    -0.195    -0.121    -0.047
skill[4,16]         -0.121   0.116    -0.342    -0.197    -0.122    -0.044
skill[5,16]         -0.120   0.121    -0.357    -0.201    -0.121    -0.041
skill[1,17]          0.543   0.083     0.372     0.491     0.544     0.600
skill[2,17]          0.588   0.067     0.470     0.541     0.586     0.631
skill[3,17]          0.621   0.068     0.495     0.575     0.617     0.666
skill[4,17]          0.646   0.075     0.501     0.593     0.644     0.697
skill[5,17]          0.640   0.077     0.497     0.586     0.637     0.694
skill[1,18]         -0.068   0.075    -0.209    -0.122    -0.069    -0.021
skill[2,18]         -0.075   0.074    -0.219    -0.126    -0.075    -0.029
skill[3,18]         -0.068   0.078    -0.218    -0.122    -0.067    -0.019
skill[4,18]         -0.061   0.081    -0.217    -0.117    -0.060    -0.011
skill[5,18]         -0.054   0.081    -0.208    -0.110    -0.054    -0.002
skill[1,19]         -0.099   0.066    -0.220    -0.144    -0.101    -0.057
skill[2,19]         -0.106   0.065    -0.227    -0.151    -0.108    -0.064
skill[3,19]         -0.122   0.070    -0.260    -0.170    -0.121    -0.074
skill[4,19]         -0.122   0.080    -0.290    -0.174    -0.119    -0.069
skill[5,19]         -0.122   0.089    -0.312    -0.176    -0.118    -0.065
skill[1,20]          0.086   0.068    -0.034     0.038     0.083     0.130
skill[2,20]          0.083   0.065    -0.031     0.036     0.079     0.126
skill[3,20]          0.082   0.065    -0.033     0.034     0.078     0.125
skill[4,20]          0.065   0.070    -0.068     0.017     0.063     0.110
skill[5,20]          0.065   0.079    -0.094     0.014     0.064     0.115
skill[1,21]         -0.097   0.081    -0.245    -0.154    -0.103    -0.042
skill[2,21]         -0.101   0.073    -0.234    -0.152    -0.104    -0.048
skill[3,21]         -0.100   0.071    -0.230    -0.149    -0.103    -0.050
skill[4,21]         -0.107   0.070    -0.237    -0.155    -0.110    -0.059
skill[5,21]         -0.108   0.074    -0.245    -0.158    -0.114    -0.058
skill[1,22]         -0.197   0.106    -0.391    -0.269    -0.198    -0.126
skill[2,22]         -0.204   0.100    -0.389    -0.272    -0.205    -0.138
skill[3,22]         -0.204   0.107    -0.401    -0.278    -0.204    -0.134
skill[4,22]         -0.205   0.114    -0.418    -0.282    -0.205    -0.131
skill[5,22]         -0.205   0.120    -0.432    -0.287    -0.206    -0.127
skill[1,23]         -0.158   0.099    -0.343    -0.228    -0.160    -0.092
skill[2,23]         -0.164   0.094    -0.340    -0.232    -0.165    -0.102
skill[3,23]         -0.163   0.102    -0.357    -0.235    -0.165    -0.097
skill[4,23]         -0.164   0.108    -0.373    -0.239    -0.165    -0.094
skill[5,23]         -0.164   0.114    -0.386    -0.241    -0.165    -0.092
skill[1,24]         -0.102   0.105    -0.310    -0.171    -0.105    -0.033
skill[2,24]         -0.107   0.102    -0.306    -0.174    -0.108    -0.041
skill[3,24]         -0.112   0.097    -0.301    -0.177    -0.112    -0.049
skill[4,24]         -0.112   0.104    -0.317    -0.181    -0.113    -0.044
skill[5,24]         -0.112   0.111    -0.329    -0.185    -0.112    -0.041
skill[1,25]          0.009   0.086    -0.149    -0.046     0.007     0.066
skill[2,25]          0.009   0.081    -0.145    -0.044     0.006     0.061
skill[3,25]          0.008   0.074    -0.140    -0.042     0.006     0.055
skill[4,25]          0.013   0.074    -0.139    -0.038     0.011     0.060
skill[5,25]          0.003   0.077    -0.143    -0.047     0.001     0.053
skill[1,26]          0.048   0.088    -0.117    -0.019     0.046     0.108
skill[2,26]          0.049   0.083    -0.104    -0.013     0.047     0.104
skill[3,26]          0.049   0.076    -0.083    -0.009     0.047     0.099
skill[4,26]          0.068   0.075    -0.063     0.014     0.066     0.118
skill[5,26]          0.091   0.082    -0.057     0.034     0.091     0.145
skill[1,27]         -0.051   0.093    -0.236    -0.108    -0.049     0.000
skill[2,27]         -0.054   0.088    -0.231    -0.108    -0.050    -0.003
skill[3,27]         -0.055   0.084    -0.224    -0.108    -0.052    -0.006
skill[4,27]         -0.057   0.077    -0.213    -0.107    -0.054    -0.012
skill[5,27]         -0.053   0.079    -0.211    -0.104    -0.051    -0.007
skill[1,28]         -0.110   0.099    -0.292    -0.188    -0.112    -0.039
skill[2,28]         -0.113   0.094    -0.288    -0.188    -0.115    -0.046
skill[3,28]         -0.117   0.088    -0.282    -0.186    -0.120    -0.052
skill[4,28]         -0.121   0.081    -0.274    -0.186    -0.123    -0.063
skill[5,28]         -0.124   0.082    -0.278    -0.188    -0.127    -0.066
skill[1,29]         -0.057   0.129    -0.272    -0.154    -0.066     0.028
skill[2,29]         -0.059   0.127    -0.270    -0.155    -0.067     0.022
skill[3,29]         -0.061   0.124    -0.268    -0.154    -0.068     0.019
skill[4,29]         -0.062   0.121    -0.268    -0.152    -0.069     0.014
skill[5,29]         -0.063   0.117    -0.267    -0.149    -0.068     0.010
deviance         10731.636  11.981 10708.084 10723.239 10732.002 10740.556
                     97.5%  Rhat n.eff
away_baseline[1]     0.185 1.056    34
away_baseline[2]     0.135 1.004   500
away_baseline[3]     0.123 1.001  7800
away_baseline[4]     0.121 1.001  3300
away_baseline[5]     0.148 1.006   280
group_sigma          0.297 1.008   210
group_skill          0.146 1.015  5000
home_baseline[1]     0.508 1.018   260
home_baseline[2]     0.487 1.005 15000
home_baseline[3]     0.493 1.004 15000
home_baseline[4]     0.506 1.007   390
home_baseline[5]     0.516 1.011   260
season_sigma         0.069 1.323    11
skill[1,1]           0.000 1.000     1
skill[2,1]           0.000 1.000     1
skill[3,1]           0.000 1.000     1
skill[4,1]           0.000 1.000     1
skill[5,1]           0.000 1.000     1
skill[1,2]           0.323 1.003  5900
skill[2,2]           0.315 1.003 15000
skill[3,2]           0.323 1.008   880
skill[4,2]           0.338 1.012   290
skill[5,2]           0.377 1.016   130
skill[1,3]           0.170 1.017   110
skill[2,3]           0.174 1.014   150
skill[3,3]           0.175 1.013   180
skill[4,3]           0.173 1.011   250
skill[5,3]           0.185 1.004   470
skill[1,4]           0.130 1.005   510
skill[2,4]           0.121 1.004   680
skill[3,4]           0.123 1.006   380
skill[4,4]           0.117 1.003   820
skill[5,4]           0.121 1.002  1200
skill[1,5]           0.015 1.002  1400
skill[2,5]           0.033 1.002  1500
skill[3,5]           0.046 1.002  1800
skill[4,5]           0.060 1.002  2000
skill[5,5]           0.074 1.002  1900
skill[1,6]           0.116 1.008   620
skill[2,6]           0.092 1.010 15000
skill[3,6]           0.084 1.012  3500
skill[4,6]           0.085 1.011  1200
skill[5,6]           0.086 1.012   710
skill[1,7]           0.145 1.005 11000
skill[2,7]           0.133 1.011 15000
skill[3,7]           0.134 1.015 15000
skill[4,7]           0.142 1.012 15000
skill[5,7]           0.154 1.007 15000
skill[1,8]           0.337 1.001 15000
skill[2,8]           0.339 1.003 15000
skill[3,8]           0.341 1.001  6700
skill[4,8]           0.340 1.003 15000
skill[5,8]           0.338 1.003  1400
skill[1,9]           0.090 1.019   140
skill[2,9]           0.059 1.010   520
skill[3,9]           0.046 1.007 13000
skill[4,9]           0.035 1.004   790
skill[5,9]           0.047 1.003   790
skill[1,10]          0.027 1.011   150
skill[2,10]          0.032 1.016   110
skill[3,10]          0.040 1.021    98
skill[4,10]          0.037 1.011   170
skill[5,10]          0.057 1.008   200
skill[1,11]          0.815 1.035    59
skill[2,11]          0.825 1.019   110
skill[3,11]          0.839 1.006   320
skill[4,11]          0.860 1.002  1100
skill[5,11]          0.866 1.002  1000
skill[1,12]          0.279 1.002  1900
skill[2,12]          0.267 1.002  1300
skill[3,12]          0.247 1.009   300
skill[4,12]          0.244 1.012   190
skill[5,12]          0.255 1.006   280
skill[1,13]          0.168 1.002  4300
skill[2,13]          0.167 1.004 15000
skill[3,13]          0.153 1.009   350
skill[4,13]          0.148 1.014   170
skill[5,13]          0.152 1.012   210
skill[1,14]          0.159 1.007   240
skill[2,14]          0.153 1.007   240
skill[3,14]          0.156 1.006   410
skill[4,14]          0.176 1.001 15000
skill[5,14]          0.202 1.002  1400
skill[1,15]          0.155 1.001  4000
skill[2,15]          0.154 1.001  4000
skill[3,15]          0.122 1.008   210
skill[4,15]          0.113 1.013   130
skill[5,15]          0.100 1.030    58
skill[1,16]          0.071 1.004   970
skill[2,16]          0.084 1.002  1200
skill[3,16]          0.095 1.002  1000
skill[4,16]          0.108 1.002   970
skill[5,16]          0.121 1.002  1000
skill[1,17]          0.704 1.009   190
skill[2,17]          0.721 1.002 15000
skill[3,17]          0.759 1.023   100
skill[4,17]          0.800 1.049    48
skill[5,17]          0.796 1.039    58
skill[1,18]          0.086 1.011   160
skill[2,18]          0.077 1.008   220
skill[3,18]          0.089 1.010   170
skill[4,18]          0.099 1.011   160
skill[5,18]          0.108 1.014   120
skill[1,19]          0.039 1.013   130
skill[2,19]          0.028 1.016   100
skill[3,19]          0.017 1.028    62
skill[4,19]          0.036 1.020    83
skill[5,19]          0.054 1.017    98
skill[1,20]          0.229 1.006   280
skill[2,20]          0.221 1.006   300
skill[3,20]          0.220 1.004   510
skill[4,20]          0.207 1.001 15000
skill[5,20]          0.223 1.001 15000
skill[1,21]          0.063 1.006   320
skill[2,21]          0.043 1.005   390
skill[3,21]          0.042 1.007   240
skill[4,21]          0.037 1.007   250
skill[5,21]          0.044 1.009   190
skill[1,22]          0.014 1.011   150
skill[2,22]         -0.001 1.012   160
skill[3,22]          0.010 1.010   180
skill[4,22]          0.026 1.008   220
skill[5,22]          0.038 1.006   270
skill[1,23]          0.043 1.003   710
skill[2,23]          0.026 1.002  1200
skill[3,23]          0.042 1.002  1900
skill[4,23]          0.054 1.001  2600
skill[5,23]          0.070 1.002  2300
skill[1,24]          0.109 1.001 15000
skill[2,24]          0.096 1.001  5000
skill[3,24]          0.085 1.001  2700
skill[4,24]          0.098 1.001  3600
skill[5,24]          0.109 1.002  3300
skill[1,25]          0.186 1.021   480
skill[2,25]          0.174 1.030   410
skill[3,25]          0.163 1.041   410
skill[4,25]          0.169 1.041   230
skill[5,25]          0.162 1.023   310
skill[1,26]          0.228 1.010   240
skill[2,26]          0.220 1.015   170
skill[3,26]          0.203 1.026   120
skill[4,26]          0.224 1.052    49
skill[5,26]          0.257 1.078    31
skill[1,27]          0.143 1.001  3900
skill[2,27]          0.129 1.001  9900
skill[3,27]          0.118 1.001 15000
skill[4,27]          0.103 1.002  1600
skill[5,27]          0.112 1.001  3800
skill[1,28]          0.087 1.003   720
skill[2,28]          0.068 1.004   470
skill[3,28]          0.052 1.006   310
skill[4,28]          0.035 1.009   190
skill[5,28]          0.035 1.012   140
skill[1,29]          0.235 1.058   160
skill[2,29]          0.230 1.061   160
skill[3,29]          0.224 1.068   150
skill[4,29]          0.215 1.076   130
skill[5,29]          0.204 1.085   110
deviance         10752.921 1.060    31

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 69.4 and DIC = 10801.0
DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
</div>
<div id="mcmc-diagnostics-2" class="section level2">
<h2>MCMC diagnostics</h2>
<p>The following graph shows the trace plot and distribution of the <code>season_sigma</code> parameter.</p>
<pre class="r"><code>&gt; denplot(m3.r2jags, parms = &quot;season_sigma&quot;, style = &quot;plain&quot;)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag_m3-1.png" width="672" /></p>
<pre class="r"><code>&gt; traplot(m3.r2jags, parms = &quot;season_sigma&quot;, style = &quot;plain&quot;)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/mcmc_diag_m3-2.png" width="672" /></p>
<p>Calculating and comparing the DIC of this model with the former model show no substantial difference.</p>
<pre class="r"><code>&gt; dic_m2&lt;-m2.r2jags$BUGSoutput$DIC
&gt; dic_m3&lt;-m3.r2jags$BUGSoutput$DIC
&gt; diff_dic&lt;-dic_m2 - dic_m3
&gt; diff_dic
[1] -29.50679</code></pre>
<p>However, I believe the assumptions of the current model (m3) are more reasonable so I’ll stick with this model.</p>
</div>
</div>
<div id="ranking-the-teams-of-la-liga" class="section level1">
<h1>Ranking the teams of La Liga</h1>
<p>We’ll start by ranking the teams of La Liga using the estimated skill parameters from the 2012/2013 season. The values of the skill parameters are difficult to interpret as they are relative to the skill of the team that had its skill parameter “anchored” at zero. To put them on a more interpretable scale I’ll first zero center the skill parameters by subtracting the mean skill of all teams, I then add the home baseline and exponentiate the resulting values. These rescaled skill parameters are now on the scale of expected number of goals when playing home team. Below is a caterpillar plot of the median of the rescaled skill parameters together with the <span class="math inline">\(68\)</span>% and <span class="math inline">\(95\)</span>% credible intervals. The plot is ordered according to the median skill and thus also gives the ranking of the teams.</p>
<pre class="r"><code>&gt; # The ranking of the teams for the 2012/13 season.
&gt; ms3&lt;-m3.r2jags$BUGSoutput$sims.matrix
&gt; team_skill &lt;- ms3[, str_detect(string = colnames(ms3), &quot;skill\\[5,&quot;)]
&gt; team_skill &lt;- (team_skill - rowMeans(team_skill)) + ms3[, &quot;home_baseline[5]&quot;]
&gt; team_skill &lt;- exp(team_skill)
&gt; colnames(team_skill) &lt;- teams
&gt; team_skill &lt;- team_skill[, order(colMeans(team_skill), decreasing = T)]
&gt; par(mar = c(2, 0.7, 0.7, 0.7), xaxs = &quot;i&quot;)
&gt; caterplot(team_skill, labels.loc = &quot;above&quot;, val.lim = c(0.7, 3.8)) + theme_classic()</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/cat_plot-1.png" width="672" /></p>
<pre><code>NULL</code></pre>
<p>Two teams are clearly ahead of the rest, FC Barcelona and Real Madrid CF. Let’s look at the credible difference between the two teams.</p>
<pre class="r"><code>&gt; plotPost(team_skill[, &quot;FC Barcelona&quot;] - team_skill[, &quot;Real Madrid CF&quot;], compVal = 0,
+     xlab = &quot;← Real Madrid     vs     Barcelona →&quot;)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/cat_plot2-1.png" width="672" /></p>
<pre><code>                                                      mean    median      mode
&lt;U+2190&gt; Real Madrid     vs     Barcelona &lt;U+2192&gt; 0.26639 0.2657129 0.2950102
                                                   hdiMass     hdiLow  hdiHigh
&lt;U+2190&gt; Real Madrid     vs     Barcelona &lt;U+2192&gt;    0.95 -0.1863186 0.785901
                                                   compVal pcGTcompVal ROPElow
&lt;U+2190&gt; Real Madrid     vs     Barcelona &lt;U+2192&gt;       0   0.8660667      NA
                                                   ROPEhigh pcInROPE
&lt;U+2190&gt; Real Madrid     vs     Barcelona &lt;U+2192&gt;       NA       NA</code></pre>
<p>FC Barcelona is the better team with a probability of <span class="math inline">\(82\)</span>%</p>
</div>
<div id="predicting-the-end-game-of-la-liga-20122013" class="section level1">
<h1>Predicting the End Game of La Liga 2012/2013</h1>
<p>In the laliga data set the results of the <span class="math inline">\(50\)</span> last games of the 2012/2013 season was missing. Using our model we can now both predict and simulate the outcomes of these <span class="math inline">\(50\)</span> games. The R code below calculates a number of measures for each game (both the games with known and unknown outcomes):</p>
<ul>
<li><p>The mode of the simulated number of goals, that is, the most likely number of scored goals. If we were asked to bet on the number of goals in a game this is what we would use.</p></li>
<li><p>The mean of the simulated number of goals, this is our best guess of the average number of goals in a game.</p></li>
<li><p>The most likely match result for each game.</p></li>
<li><p>A random sample from the distributions of credible home scores, away scores and match results. This is how La Liga actually could have played out in an alternative reality.</p></li>
</ul>
<pre class="r"><code>&gt; n &lt;- nrow(ms3)
&gt; m3_pred &lt;- sapply(1:nrow(laliga), function(i) {
+   home_team &lt;- which(teams == laliga$HomeTeam[i])
+   away_team &lt;- which(teams == laliga$AwayTeam[i])
+   season &lt;- which(seasons == laliga$Season[i])
+   home_skill &lt;- ms3[, col_name(&quot;skill&quot;, season, home_team)]
+   away_skill &lt;- ms3[, col_name(&quot;skill&quot;, season, away_team)]
+   home_baseline &lt;- ms3[, col_name(&quot;home_baseline&quot;, season)]
+   away_baseline &lt;- ms3[, col_name(&quot;away_baseline&quot;, season)]
+ 
+   home_goals &lt;- rpois(n, exp(home_baseline + home_skill - away_skill))
+   away_goals &lt;- rpois(n, exp(away_baseline + away_skill - home_skill))
+   home_goals_table &lt;- table(home_goals)
+   away_goals_table &lt;- table(away_goals)
+   match_results &lt;- sign(home_goals - away_goals)
+   match_results_table &lt;- table(match_results)
+ 
+   mode_home_goal &lt;- as.numeric(names(home_goals_table)[ which.max(home_goals_table)])
+   mode_away_goal &lt;- as.numeric(names(away_goals_table)[ which.max(away_goals_table)])
+   match_result &lt;-  as.numeric(names(match_results_table)[which.max(match_results_table)])
+   rand_i &lt;- sample(seq_along(home_goals), 1)
+ 
+   c(mode_home_goal = mode_home_goal, mode_away_goal = mode_away_goal, match_result = match_result,
+     mean_home_goal = mean(home_goals), mean_away_goal = mean(away_goals),
+     rand_home_goal = home_goals[rand_i], rand_away_goal = away_goals[rand_i],
+     rand_match_result = match_results[rand_i])
+ })
&gt; m3_pred &lt;- t(m3_pred)</code></pre>
<p>First let’s compare the distribution of the number of goals in the data with the predicted mode, mean and randomised number of goals for all the games (focusing on the number of goals for the home team). First the actual distribution of the number of goals for the home teams.</p>
<pre class="r"><code>&gt; hist(laliga$HomeGoals, breaks = (-1:10) + 0.5, xlim = c(-0.5, 10), main = &quot;Distribution of the number of goals\nscored by a home team in a match.&quot;, xlab = &quot;&quot;)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/pred1_m3-1.png" width="672" /></p>
<p>This next plot shows the distribution of the modes from the predicted distribution of home goals from each game. That is, what is the most probable outcome, for the home team, in each game.</p>
<pre class="r"><code>&gt; hist(m3_pred[, &quot;mode_home_goal&quot;], breaks = (-1:10) + 0.5, xlim = c(-0.5, 10),
+     main = &quot;Distribution of predicted most\nprobable scoreby a home team in\na match.&quot;, xlab = &quot;&quot;)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/pred2_m3-1.png" width="672" /></p>
<p>For almost all games the single most likely number of goals is one. Actually, if you know nothing about a La Liga game betting on one goal for the home team is <span class="math inline">\(78\)</span>% of the times the best bet. Lest instead look at the distribution of the predicted mean number of home goals in each game.</p>
<pre class="r"><code>&gt; hist(m3_pred[, &quot;mean_home_goal&quot;], breaks = (-1:10) + 0.5, xlim = c(-0.5, 10),
+     main = &quot;Distribution of predicted mean \n score by a home team in a match.&quot;, xlab = &quot;&quot;)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/pred3_m3-1.png" width="672" /></p>
<p>For most games the expected number of goals are <span class="math inline">\(2\)</span>. That is, even if your safest bet is one goal you would expect to see around two goals. The distribution of the mode and the mean number of goals doesn’t look remotely like the actual number of goals. This was not to be expected, we would however expect the distribution of randomized goals (where for each match the number of goals has been randomly drawn from that match’s predicted home goal distribution) to look similar to the actual number of home goals. Looking at the histogram below, this seems to be the case.</p>
<pre class="r"><code>&gt; hist(m3_pred[, &quot;rand_home_goal&quot;], breaks = (-1:10) + 0.5, xlim = c(-0.5, 10),
+     main = &quot;Distribution of randomly draw \n score by a home team in a match.&quot;, xlab = &quot;&quot;)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/pred4_m3-1.png" width="672" /></p>
<p>We can also look at how well the model predicts the data. This should probably be done using cross validation, but as the number of effective parameters are much smaller than the number of data points a direct comparison should at least give an estimated prediction accuracy in the right ballpark.</p>
<pre class="r"><code>&gt; mean(laliga$HomeGoals == m3_pred[, &quot;mode_home_goal&quot;], na.rm = T)
[1] 0.3318919
&gt; 
&gt; mean((laliga$HomeGoals - m3_pred[, &quot;mean_home_goal&quot;])^2, na.rm = T)
[1] 1.457061</code></pre>
<p>So on average the model predicts the correct number of home goals <span class="math inline">\(34\)</span>% of the time and guesses the average number of goals with a mean squared error of <span class="math inline">\(1.45\)</span>. Now we’ll look at the actual and predicted match outcomes. The graph below shows the match outcomes in the data with <span class="math inline">\(1\)</span> being a home win, <span class="math inline">\(0\)</span> being a draw and <span class="math inline">\(-1\)</span> being a win for the away team.</p>
<pre class="r"><code>&gt; hist(laliga$MatchResult, breaks = (-2:1) + 0.5, main = &quot;Actual match results.&quot;, xlab = &quot;&quot;)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/pred6_m3-1.png" width="672" /></p>
<p>Now looking at the most probable outcomes of the matches according to the model.</p>
<pre class="r"><code>&gt; hist(m3_pred[, &quot;match_result&quot;], breaks = (-2:1) + 0.5, main = &quot;Predicted match results.&quot;, xlab = &quot;&quot;)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/pred7_m3-1.png" width="672" /></p>
<p>For almost all matches the safest bet is to bet on the home team. While draws are not uncommon it is never the safest bet. As in the case with the number of home goals, the randomized match outcomes have a distribution similar to the actual match outcomes:</p>
<pre class="r"><code>&gt; hist(m3_pred[, &quot;rand_match_result&quot;], breaks = (-2:1) + 0.5, main = &quot;Randomized match results.&quot;, xlab = &quot;&quot;)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/pred8_m3-1.png" width="672" /></p>
<pre class="r"><code>&gt; mean(laliga$MatchResult == m3_pred[, &quot;match_result&quot;], na.rm = T)
[1] 0.5637838</code></pre>
<p>The model predicts the correct match outcome <span class="math inline">\(56\)</span>% of the time. Pretty good! Now that we’ve checked that the model reasonably predicts the La Liga history let’s predict the La Liga endgame! The code below displays the predicted mode and mean number of goals for the endgame and the predicted winner of each game.</p>
<pre class="r"><code>&gt; laliga_forecast &lt;- laliga[is.na(laliga$HomeGoals), c(&quot;Season&quot;, &quot;Week&quot;, &quot;HomeTeam&quot;,
+     &quot;AwayTeam&quot;)]
&gt; m3_forecast &lt;- m3_pred[is.na(laliga$HomeGoals), ]
&gt; laliga_forecast$mean_home_goals &lt;- round(m3_forecast[, &quot;mean_home_goal&quot;], 1)
&gt; laliga_forecast$mean_away_goals &lt;- round(m3_forecast[, &quot;mean_away_goal&quot;], 1)
&gt; laliga_forecast$mode_home_goals &lt;- m3_forecast[, &quot;mode_home_goal&quot;]
&gt; laliga_forecast$mode_away_goals &lt;- m3_forecast[, &quot;mode_away_goal&quot;]
&gt; laliga_forecast$predicted_winner &lt;- ifelse(m3_forecast[, &quot;match_result&quot;] ==
+     1, laliga_forecast$HomeTeam, ifelse(m3_forecast[, &quot;match_result&quot;] == -1,
+     laliga_forecast$AwayTeam, &quot;Draw&quot;))
&gt; 
&gt; rownames(laliga_forecast) &lt;- NULL
&gt; print(xtable(laliga_forecast, align = &quot;cccccccccc&quot;))
% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Tue Feb 25 22:45:56 2020
\begin{table}[ht]
\centering
\begin{tabular}{cccccccccc}
  \hline
 &amp; Season &amp; Week &amp; HomeTeam &amp; AwayTeam &amp; mean\_home\_goals &amp; mean\_away\_goals &amp; mode\_home\_goals &amp; mode\_away\_goals &amp; predicted\_winner \\ 
  \hline
1 &amp; 2012/13 &amp;  34 &amp; Celta Vigo &amp; Athletic Club Bilbao &amp; 1.50 &amp; 1.20 &amp; 1.00 &amp; 1.00 &amp; Celta Vigo \\ 
  2 &amp; 2012/13 &amp;  34 &amp; Deportivo de La CoruÃ±a &amp; AtlÃ©tico Madrid &amp; 1.20 &amp; 1.40 &amp; 1.00 &amp; 1.00 &amp; AtlÃ©tico Madrid \\ 
  3 &amp; 2012/13 &amp;  34 &amp; FC Barcelona &amp; Betis Sevilla &amp; 3.20 &amp; 0.50 &amp; 3.00 &amp; 0.00 &amp; FC Barcelona \\ 
  4 &amp; 2012/13 &amp;  34 &amp; FC Sevilla &amp; Espanyol Barcelona &amp; 1.80 &amp; 1.00 &amp; 1.00 &amp; 0.00 &amp; FC Sevilla \\ 
  5 &amp; 2012/13 &amp;  34 &amp; FC Valencia &amp; CA Osasuna &amp; 2.00 &amp; 0.90 &amp; 2.00 &amp; 0.00 &amp; FC Valencia \\ 
  6 &amp; 2012/13 &amp;  34 &amp; Getafe CF &amp; Real Sociedad San Sebastian &amp; 1.50 &amp; 1.20 &amp; 1.00 &amp; 1.00 &amp; Getafe CF \\ 
  7 &amp; 2012/13 &amp;  34 &amp; Granada CF &amp; MÃ¡laga CF &amp; 1.30 &amp; 1.30 &amp; 1.00 &amp; 1.00 &amp; Granada CF \\ 
  8 &amp; 2012/13 &amp;  34 &amp; RCD Mallorca &amp; Levante U.D. &amp; 1.50 &amp; 1.10 &amp; 1.00 &amp; 1.00 &amp; RCD Mallorca \\ 
  9 &amp; 2012/13 &amp;  34 &amp; Real Madrid CF &amp; Real Valladolid &amp; 3.20 &amp; 0.50 &amp; 3.00 &amp; 0.00 &amp; Real Madrid CF \\ 
  10 &amp; 2012/13 &amp;  34 &amp; Real Zaragoza &amp; Rayo Vallecano &amp; 1.50 &amp; 1.10 &amp; 1.00 &amp; 1.00 &amp; Real Zaragoza \\ 
  11 &amp; 2012/13 &amp;  35 &amp; Athletic Club Bilbao &amp; RCD Mallorca &amp; 1.60 &amp; 1.00 &amp; 1.00 &amp; 1.00 &amp; Athletic Club Bilbao \\ 
  12 &amp; 2012/13 &amp;  35 &amp; AtlÃ©tico Madrid &amp; FC Barcelona &amp; 1.00 &amp; 1.80 &amp; 0.00 &amp; 1.00 &amp; FC Barcelona \\ 
  13 &amp; 2012/13 &amp;  35 &amp; Betis Sevilla &amp; Celta Vigo &amp; 1.70 &amp; 1.00 &amp; 1.00 &amp; 1.00 &amp; Betis Sevilla \\ 
  14 &amp; 2012/13 &amp;  35 &amp; CA Osasuna &amp; Getafe CF &amp; 1.50 &amp; 1.10 &amp; 1.00 &amp; 1.00 &amp; CA Osasuna \\ 
  15 &amp; 2012/13 &amp;  35 &amp; Espanyol Barcelona &amp; Real Madrid CF &amp; 0.80 &amp; 2.10 &amp; 0.00 &amp; 2.00 &amp; Real Madrid CF \\ 
  16 &amp; 2012/13 &amp;  35 &amp; Levante U.D. &amp; Real Zaragoza &amp; 1.80 &amp; 1.00 &amp; 1.00 &amp; 0.00 &amp; Levante U.D. \\ 
  17 &amp; 2012/13 &amp;  35 &amp; MÃ¡laga CF &amp; FC Sevilla &amp; 1.50 &amp; 1.20 &amp; 1.00 &amp; 1.00 &amp; MÃ¡laga CF \\ 
  18 &amp; 2012/13 &amp;  35 &amp; Rayo Vallecano &amp; FC Valencia &amp; 1.20 &amp; 1.40 &amp; 1.00 &amp; 1.00 &amp; FC Valencia \\ 
  19 &amp; 2012/13 &amp;  35 &amp; Real Sociedad San Sebastian &amp; Granada CF &amp; 2.00 &amp; 0.90 &amp; 2.00 &amp; 0.00 &amp; Real Sociedad San Sebastian \\ 
  20 &amp; 2012/13 &amp;  35 &amp; Real Valladolid &amp; Deportivo de La CoruÃ±a &amp; 1.60 &amp; 1.10 &amp; 1.00 &amp; 1.00 &amp; Real Valladolid \\ 
  21 &amp; 2012/13 &amp;  36 &amp; Celta Vigo &amp; AtlÃ©tico Madrid &amp; 1.20 &amp; 1.50 &amp; 1.00 &amp; 1.00 &amp; AtlÃ©tico Madrid \\ 
  22 &amp; 2012/13 &amp;  36 &amp; Deportivo de La CoruÃ±a &amp; Espanyol Barcelona &amp; 1.50 &amp; 1.20 &amp; 1.00 &amp; 1.00 &amp; Deportivo de La CoruÃ±a \\ 
  23 &amp; 2012/13 &amp;  36 &amp; FC Barcelona &amp; Real Valladolid &amp; 3.50 &amp; 0.50 &amp; 3.00 &amp; 0.00 &amp; FC Barcelona \\ 
  24 &amp; 2012/13 &amp;  36 &amp; FC Sevilla &amp; Real Sociedad San Sebastian &amp; 1.60 &amp; 1.00 &amp; 1.00 &amp; 1.00 &amp; FC Sevilla \\ 
  25 &amp; 2012/13 &amp;  36 &amp; Getafe CF &amp; FC Valencia &amp; 1.30 &amp; 1.30 &amp; 1.00 &amp; 1.00 &amp; Getafe CF \\ 
  26 &amp; 2012/13 &amp;  36 &amp; Granada CF &amp; CA Osasuna &amp; 1.40 &amp; 1.20 &amp; 1.00 &amp; 1.00 &amp; Granada CF \\ 
  27 &amp; 2012/13 &amp;  36 &amp; Levante U.D. &amp; Rayo Vallecano &amp; 1.70 &amp; 1.00 &amp; 1.00 &amp; 1.00 &amp; Levante U.D. \\ 
  28 &amp; 2012/13 &amp;  36 &amp; RCD Mallorca &amp; Betis Sevilla &amp; 1.50 &amp; 1.20 &amp; 1.00 &amp; 1.00 &amp; RCD Mallorca \\ 
  29 &amp; 2012/13 &amp;  36 &amp; Real Madrid CF &amp; MÃ¡laga CF &amp; 2.90 &amp; 0.60 &amp; 2.00 &amp; 0.00 &amp; Real Madrid CF \\ 
  30 &amp; 2012/13 &amp;  36 &amp; Real Zaragoza &amp; Athletic Club Bilbao &amp; 1.40 &amp; 1.20 &amp; 1.00 &amp; 1.00 &amp; Real Zaragoza \\ 
  31 &amp; 2012/13 &amp;  37 &amp; Athletic Club Bilbao &amp; Levante U.D. &amp; 1.60 &amp; 1.10 &amp; 1.00 &amp; 1.00 &amp; Athletic Club Bilbao \\ 
  32 &amp; 2012/13 &amp;  37 &amp; AtlÃ©tico Madrid &amp; RCD Mallorca &amp; 2.00 &amp; 0.80 &amp; 1.00 &amp; 0.00 &amp; AtlÃ©tico Madrid \\ 
  33 &amp; 2012/13 &amp;  37 &amp; Betis Sevilla &amp; Real Zaragoza &amp; 1.80 &amp; 1.00 &amp; 1.00 &amp; 0.00 &amp; Betis Sevilla \\ 
  34 &amp; 2012/13 &amp;  37 &amp; CA Osasuna &amp; FC Sevilla &amp; 1.40 &amp; 1.30 &amp; 1.00 &amp; 1.00 &amp; CA Osasuna \\ 
  35 &amp; 2012/13 &amp;  37 &amp; Espanyol Barcelona &amp; FC Barcelona &amp; 0.80 &amp; 2.30 &amp; 0.00 &amp; 2.00 &amp; FC Barcelona \\ 
  36 &amp; 2012/13 &amp;  37 &amp; FC Valencia &amp; Granada CF &amp; 2.20 &amp; 0.80 &amp; 2.00 &amp; 0.00 &amp; FC Valencia \\ 
  37 &amp; 2012/13 &amp;  37 &amp; Getafe CF &amp; Rayo Vallecano &amp; 1.70 &amp; 1.00 &amp; 1.00 &amp; 0.00 &amp; Getafe CF \\ 
  38 &amp; 2012/13 &amp;  37 &amp; MÃ¡laga CF &amp; Deportivo de La CoruÃ±a &amp; 1.80 &amp; 0.90 &amp; 1.00 &amp; 0.00 &amp; MÃ¡laga CF \\ 
  39 &amp; 2012/13 &amp;  37 &amp; Real Sociedad San Sebastian &amp; Real Madrid CF &amp; 0.90 &amp; 1.90 &amp; 0.00 &amp; 1.00 &amp; Real Madrid CF \\ 
  40 &amp; 2012/13 &amp;  37 &amp; Real Valladolid &amp; Celta Vigo &amp; 1.60 &amp; 1.10 &amp; 1.00 &amp; 1.00 &amp; Real Valladolid \\ 
  41 &amp; 2012/13 &amp;  38 &amp; Celta Vigo &amp; Espanyol Barcelona &amp; 1.50 &amp; 1.20 &amp; 1.00 &amp; 1.00 &amp; Celta Vigo \\ 
  42 &amp; 2012/13 &amp;  38 &amp; Deportivo de La CoruÃ±a &amp; Real Sociedad San Sebastian &amp; 1.30 &amp; 1.30 &amp; 1.00 &amp; 1.00 &amp; Deportivo de La CoruÃ±a \\ 
  43 &amp; 2012/13 &amp;  38 &amp; FC Barcelona &amp; MÃ¡laga CF &amp; 3.10 &amp; 0.60 &amp; 3.00 &amp; 0.00 &amp; FC Barcelona \\ 
  44 &amp; 2012/13 &amp;  38 &amp; FC Sevilla &amp; FC Valencia &amp; 1.50 &amp; 1.20 &amp; 1.00 &amp; 1.00 &amp; FC Sevilla \\ 
  45 &amp; 2012/13 &amp;  38 &amp; Granada CF &amp; Getafe CF &amp; 1.40 &amp; 1.20 &amp; 1.00 &amp; 1.00 &amp; Granada CF \\ 
  46 &amp; 2012/13 &amp;  38 &amp; Levante U.D. &amp; Betis Sevilla &amp; 1.50 &amp; 1.10 &amp; 1.00 &amp; 1.00 &amp; Levante U.D. \\ 
  47 &amp; 2012/13 &amp;  38 &amp; RCD Mallorca &amp; Real Valladolid &amp; 1.60 &amp; 1.10 &amp; 1.00 &amp; 1.00 &amp; RCD Mallorca \\ 
  48 &amp; 2012/13 &amp;  38 &amp; Rayo Vallecano &amp; Athletic Club Bilbao &amp; 1.50 &amp; 1.20 &amp; 1.00 &amp; 1.00 &amp; Rayo Vallecano \\ 
  49 &amp; 2012/13 &amp;  38 &amp; Real Madrid CF &amp; CA Osasuna &amp; 3.10 &amp; 0.60 &amp; 2.00 &amp; 0.00 &amp; Real Madrid CF \\ 
  50 &amp; 2012/13 &amp;  38 &amp; Real Zaragoza &amp; AtlÃ©tico Madrid &amp; 1.10 &amp; 1.50 &amp; 1.00 &amp; 1.00 &amp; AtlÃ©tico Madrid \\ 
   \hline
\end{tabular}
\end{table}</code></pre>
<p>While these predictions are good if you want to bet on the likely winner they do not reflect how the actual endgame will play out, e.g., there is not a single draw in the predicted_winner column. So at last let’s look at a possible version of the La Liga endgame by displaying the simulated match results calculated earlier.</p>
<pre class="r"><code>&gt; laliga_sim &lt;- laliga[is.na(laliga$HomeGoals), c(&quot;Season&quot;, &quot;Week&quot;, &quot;HomeTeam&quot;,
+     &quot;AwayTeam&quot;)]
&gt; laliga_sim$home_goals &lt;- m3_forecast[, &quot;rand_home_goal&quot;]
&gt; laliga_sim$away_goals &lt;- m3_forecast[, &quot;rand_away_goal&quot;]
&gt; laliga_sim$winner &lt;- ifelse(m3_forecast[, &quot;rand_match_result&quot;] == 1, laliga_forecast$HomeTeam,
+     ifelse(m3_forecast[, &quot;rand_match_result&quot;] == -1, laliga_forecast$AwayTeam,
+         &quot;Draw&quot;))
&gt; 
&gt; rownames(laliga_sim) &lt;- NULL
&gt; print(xtable(laliga_sim, align = &quot;cccccccc&quot;))
% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Tue Feb 25 22:45:56 2020
\begin{table}[ht]
\centering
\begin{tabular}{cccccccc}
  \hline
 &amp; Season &amp; Week &amp; HomeTeam &amp; AwayTeam &amp; home\_goals &amp; away\_goals &amp; winner \\ 
  \hline
1 &amp; 2012/13 &amp;  34 &amp; Celta Vigo &amp; Athletic Club Bilbao &amp; 0.00 &amp; 0.00 &amp; Draw \\ 
  2 &amp; 2012/13 &amp;  34 &amp; Deportivo de La CoruÃ±a &amp; AtlÃ©tico Madrid &amp; 1.00 &amp; 5.00 &amp; AtlÃ©tico Madrid \\ 
  3 &amp; 2012/13 &amp;  34 &amp; FC Barcelona &amp; Betis Sevilla &amp; 5.00 &amp; 0.00 &amp; FC Barcelona \\ 
  4 &amp; 2012/13 &amp;  34 &amp; FC Sevilla &amp; Espanyol Barcelona &amp; 1.00 &amp; 0.00 &amp; FC Sevilla \\ 
  5 &amp; 2012/13 &amp;  34 &amp; FC Valencia &amp; CA Osasuna &amp; 1.00 &amp; 2.00 &amp; CA Osasuna \\ 
  6 &amp; 2012/13 &amp;  34 &amp; Getafe CF &amp; Real Sociedad San Sebastian &amp; 1.00 &amp; 0.00 &amp; Getafe CF \\ 
  7 &amp; 2012/13 &amp;  34 &amp; Granada CF &amp; MÃ¡laga CF &amp; 0.00 &amp; 2.00 &amp; MÃ¡laga CF \\ 
  8 &amp; 2012/13 &amp;  34 &amp; RCD Mallorca &amp; Levante U.D. &amp; 0.00 &amp; 1.00 &amp; Levante U.D. \\ 
  9 &amp; 2012/13 &amp;  34 &amp; Real Madrid CF &amp; Real Valladolid &amp; 2.00 &amp; 1.00 &amp; Real Madrid CF \\ 
  10 &amp; 2012/13 &amp;  34 &amp; Real Zaragoza &amp; Rayo Vallecano &amp; 0.00 &amp; 2.00 &amp; Rayo Vallecano \\ 
  11 &amp; 2012/13 &amp;  35 &amp; Athletic Club Bilbao &amp; RCD Mallorca &amp; 2.00 &amp; 0.00 &amp; Athletic Club Bilbao \\ 
  12 &amp; 2012/13 &amp;  35 &amp; AtlÃ©tico Madrid &amp; FC Barcelona &amp; 1.00 &amp; 3.00 &amp; FC Barcelona \\ 
  13 &amp; 2012/13 &amp;  35 &amp; Betis Sevilla &amp; Celta Vigo &amp; 2.00 &amp; 1.00 &amp; Betis Sevilla \\ 
  14 &amp; 2012/13 &amp;  35 &amp; CA Osasuna &amp; Getafe CF &amp; 2.00 &amp; 1.00 &amp; CA Osasuna \\ 
  15 &amp; 2012/13 &amp;  35 &amp; Espanyol Barcelona &amp; Real Madrid CF &amp; 2.00 &amp; 3.00 &amp; Real Madrid CF \\ 
  16 &amp; 2012/13 &amp;  35 &amp; Levante U.D. &amp; Real Zaragoza &amp; 2.00 &amp; 0.00 &amp; Levante U.D. \\ 
  17 &amp; 2012/13 &amp;  35 &amp; MÃ¡laga CF &amp; FC Sevilla &amp; 0.00 &amp; 1.00 &amp; FC Sevilla \\ 
  18 &amp; 2012/13 &amp;  35 &amp; Rayo Vallecano &amp; FC Valencia &amp; 2.00 &amp; 4.00 &amp; FC Valencia \\ 
  19 &amp; 2012/13 &amp;  35 &amp; Real Sociedad San Sebastian &amp; Granada CF &amp; 0.00 &amp; 3.00 &amp; Granada CF \\ 
  20 &amp; 2012/13 &amp;  35 &amp; Real Valladolid &amp; Deportivo de La CoruÃ±a &amp; 3.00 &amp; 2.00 &amp; Real Valladolid \\ 
  21 &amp; 2012/13 &amp;  36 &amp; Celta Vigo &amp; AtlÃ©tico Madrid &amp; 1.00 &amp; 1.00 &amp; Draw \\ 
  22 &amp; 2012/13 &amp;  36 &amp; Deportivo de La CoruÃ±a &amp; Espanyol Barcelona &amp; 1.00 &amp; 0.00 &amp; Deportivo de La CoruÃ±a \\ 
  23 &amp; 2012/13 &amp;  36 &amp; FC Barcelona &amp; Real Valladolid &amp; 4.00 &amp; 1.00 &amp; FC Barcelona \\ 
  24 &amp; 2012/13 &amp;  36 &amp; FC Sevilla &amp; Real Sociedad San Sebastian &amp; 2.00 &amp; 0.00 &amp; FC Sevilla \\ 
  25 &amp; 2012/13 &amp;  36 &amp; Getafe CF &amp; FC Valencia &amp; 1.00 &amp; 0.00 &amp; Getafe CF \\ 
  26 &amp; 2012/13 &amp;  36 &amp; Granada CF &amp; CA Osasuna &amp; 3.00 &amp; 3.00 &amp; Draw \\ 
  27 &amp; 2012/13 &amp;  36 &amp; Levante U.D. &amp; Rayo Vallecano &amp; 3.00 &amp; 0.00 &amp; Levante U.D. \\ 
  28 &amp; 2012/13 &amp;  36 &amp; RCD Mallorca &amp; Betis Sevilla &amp; 3.00 &amp; 0.00 &amp; RCD Mallorca \\ 
  29 &amp; 2012/13 &amp;  36 &amp; Real Madrid CF &amp; MÃ¡laga CF &amp; 1.00 &amp; 1.00 &amp; Draw \\ 
  30 &amp; 2012/13 &amp;  36 &amp; Real Zaragoza &amp; Athletic Club Bilbao &amp; 1.00 &amp; 0.00 &amp; Real Zaragoza \\ 
  31 &amp; 2012/13 &amp;  37 &amp; Athletic Club Bilbao &amp; Levante U.D. &amp; 2.00 &amp; 0.00 &amp; Athletic Club Bilbao \\ 
  32 &amp; 2012/13 &amp;  37 &amp; AtlÃ©tico Madrid &amp; RCD Mallorca &amp; 1.00 &amp; 1.00 &amp; Draw \\ 
  33 &amp; 2012/13 &amp;  37 &amp; Betis Sevilla &amp; Real Zaragoza &amp; 0.00 &amp; 6.00 &amp; Real Zaragoza \\ 
  34 &amp; 2012/13 &amp;  37 &amp; CA Osasuna &amp; FC Sevilla &amp; 2.00 &amp; 0.00 &amp; CA Osasuna \\ 
  35 &amp; 2012/13 &amp;  37 &amp; Espanyol Barcelona &amp; FC Barcelona &amp; 1.00 &amp; 3.00 &amp; FC Barcelona \\ 
  36 &amp; 2012/13 &amp;  37 &amp; FC Valencia &amp; Granada CF &amp; 1.00 &amp; 0.00 &amp; FC Valencia \\ 
  37 &amp; 2012/13 &amp;  37 &amp; Getafe CF &amp; Rayo Vallecano &amp; 0.00 &amp; 1.00 &amp; Rayo Vallecano \\ 
  38 &amp; 2012/13 &amp;  37 &amp; MÃ¡laga CF &amp; Deportivo de La CoruÃ±a &amp; 0.00 &amp; 0.00 &amp; Draw \\ 
  39 &amp; 2012/13 &amp;  37 &amp; Real Sociedad San Sebastian &amp; Real Madrid CF &amp; 1.00 &amp; 4.00 &amp; Real Madrid CF \\ 
  40 &amp; 2012/13 &amp;  37 &amp; Real Valladolid &amp; Celta Vigo &amp; 2.00 &amp; 0.00 &amp; Real Valladolid \\ 
  41 &amp; 2012/13 &amp;  38 &amp; Celta Vigo &amp; Espanyol Barcelona &amp; 1.00 &amp; 1.00 &amp; Draw \\ 
  42 &amp; 2012/13 &amp;  38 &amp; Deportivo de La CoruÃ±a &amp; Real Sociedad San Sebastian &amp; 1.00 &amp; 3.00 &amp; Real Sociedad San Sebastian \\ 
  43 &amp; 2012/13 &amp;  38 &amp; FC Barcelona &amp; MÃ¡laga CF &amp; 2.00 &amp; 0.00 &amp; FC Barcelona \\ 
  44 &amp; 2012/13 &amp;  38 &amp; FC Sevilla &amp; FC Valencia &amp; 3.00 &amp; 1.00 &amp; FC Sevilla \\ 
  45 &amp; 2012/13 &amp;  38 &amp; Granada CF &amp; Getafe CF &amp; 0.00 &amp; 1.00 &amp; Getafe CF \\ 
  46 &amp; 2012/13 &amp;  38 &amp; Levante U.D. &amp; Betis Sevilla &amp; 1.00 &amp; 2.00 &amp; Betis Sevilla \\ 
  47 &amp; 2012/13 &amp;  38 &amp; RCD Mallorca &amp; Real Valladolid &amp; 1.00 &amp; 0.00 &amp; RCD Mallorca \\ 
  48 &amp; 2012/13 &amp;  38 &amp; Rayo Vallecano &amp; Athletic Club Bilbao &amp; 2.00 &amp; 0.00 &amp; Rayo Vallecano \\ 
  49 &amp; 2012/13 &amp;  38 &amp; Real Madrid CF &amp; CA Osasuna &amp; 3.00 &amp; 0.00 &amp; Real Madrid CF \\ 
  50 &amp; 2012/13 &amp;  38 &amp; Real Zaragoza &amp; AtlÃ©tico Madrid &amp; 0.00 &amp; 1.00 &amp; AtlÃ©tico Madrid \\ 
   \hline
\end{tabular}
\end{table}</code></pre>
<p>Now we see a number of games resulting in a draw. We also see that Malaga manages to beat Real Madrid in week <span class="math inline">\(36\)</span>, against all odds, even though playing as the away team.</p>
</div>
<div id="calculating-the-predicted-payout-for-sevilla-vs-valencia-2013-06-01" class="section level1">
<h1>Calculating the Predicted Payout for Sevilla vs Valencia, 2013-06-01</h1>
<p>At the time when this model was developed (2013-05-28) most of the matches in the 2012/2013 season had been played and Barcelona was already the winner (and the most skilled team as predicted by my model). There were however some matches left, for example, Sevilla (home team) vs Valencia (away team) at the <span class="math inline">\(1\)</span>st of June, 2013. One of the powers with using Bayesian modeling and MCMC sampling is that once you have the MCMC samples of the parameters it is straight forward to calculate any quantity resulting from these estimates while still retaining the uncertainty of the parameter estimates. So let’s look at the predicted distribution of the number of goals for the Sevilla vs Valencia game and see if I can use my model to make some money. I’ll start by using the MCMC samples to calculate the distribution of the number of goals for Sevilla and Valencia.</p>
<pre class="r"><code>&gt; n &lt;- nrow(ms3)
&gt; home_team &lt;- which(teams == &quot;FC Sevilla&quot;)
&gt; away_team &lt;- which(teams == &quot;FC Valencia&quot;)
&gt; season &lt;- which(seasons == &quot;2012/13&quot;)
&gt; home_skill &lt;- ms3[, col_name(&quot;skill&quot;, season, home_team)]
&gt; away_skill &lt;- ms3[, col_name(&quot;skill&quot;, season, away_team)]
&gt; home_baseline &lt;- ms3[, col_name(&quot;home_baseline&quot;, season)]
&gt; away_baseline &lt;- ms3[, col_name(&quot;away_baseline&quot;, season)]
&gt; 
&gt; home_goals &lt;- rpois(n, exp(home_baseline + home_skill - away_skill))
&gt; away_goals &lt;- rpois(n, exp(away_baseline + away_skill - home_skill))</code></pre>
<p>Looking at summary of these two distributions shows that it will be a close game but with a slight advantage for the home team Sevilla.</p>
<pre class="r"><code>&gt; par(mfrow = c(2, 2), mar = rep(2.2, 4))
&gt; plot_goals(home_goals, away_goals)</code></pre>
<p><img src="/JAGS/glmm-jags/2020-02-01-glmm-jags_files/figure-html/pp2-1.png" width="672" /></p>
<p>When developing the model (2013-05-28) the author got the specific payouts (that is, how much would I get back if my bet was successful) for betting on the outcome of this game on a betting site. Using my simulated distribution of the number of goals I can calculate the predicted payouts of my model.</p>
<pre class="r"><code>&gt; 1/c(Sevilla = mean(home_goals &gt; away_goals), Draw = mean(home_goals == away_goals),
+     Valencia = mean(home_goals &lt; away_goals))
 Sevilla     Draw Valencia 
2.281369 3.841229 3.318584 </code></pre>
<p>I should clearly bet on Sevilla as my model predicts a payout of <span class="math inline">\(2.24\)</span> (that is, a likely win for Sevilla) while betsson.com gives me the much higher payout of <span class="math inline">\(3.2\)</span>. It is also possible to bet on the final goal outcome so let’s calculate what payouts my model predicts for different goal outcomes.</p>
<pre class="r"><code>&gt; goals_payout &lt;- laply(0:6, function(home_goal) {
+     laply(0:6, function(away_goal) {
+         1/mean(home_goals == home_goal &amp; away_goals == away_goal)
+     })
+ })
&gt; 
&gt; colnames(goals_payout) &lt;- paste(&quot;Valencia&quot;, 0:6, sep = &quot; - &quot;)
&gt; rownames(goals_payout) &lt;- paste(&quot;Sevilla&quot;, 0:6, sep = &quot; - &quot;)
&gt; goals_payout &lt;- round(goals_payout, 1)
&gt; print(xtable(goals_payout, align = &quot;cccccccc&quot;))
% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Tue Feb 25 22:45:56 2020
\begin{table}[ht]
\centering
\begin{tabular}{cccccccc}
  \hline
 &amp; Valencia - 0 &amp; Valencia - 1 &amp; Valencia - 2 &amp; Valencia - 3 &amp; Valencia - 4 &amp; Valencia - 5 &amp; Valencia - 6 \\ 
  \hline
Sevilla - 0 &amp; 13.80 &amp; 11.90 &amp; 21.50 &amp; 47.30 &amp; 161.30 &amp; 714.30 &amp; 2500.00 \\ 
  Sevilla - 1 &amp; 9.50 &amp; 8.00 &amp; 13.40 &amp; 36.90 &amp; 122.00 &amp; 441.20 &amp; 1666.70 \\ 
  Sevilla - 2 &amp; 12.80 &amp; 11.50 &amp; 19.40 &amp; 56.40 &amp; 176.50 &amp; 625.00 &amp; 1875.00 \\ 
  Sevilla - 3 &amp; 26.50 &amp; 22.90 &amp; 39.80 &amp; 97.40 &amp; 500.00 &amp; 1363.60 &amp; Inf \\ 
  Sevilla - 4 &amp; 81.50 &amp; 58.80 &amp; 101.40 &amp; 306.10 &amp; 1071.40 &amp; 3000.00 &amp; Inf \\ 
  Sevilla - 5 &amp; 211.30 &amp; 223.90 &amp; 319.10 &amp; 1000.00 &amp; 2500.00 &amp; 15000.00 &amp; Inf \\ 
  Sevilla - 6 &amp; 750.00 &amp; 483.90 &amp; 1500.00 &amp; 3000.00 &amp; 7500.00 &amp; Inf &amp; Inf \\ 
   \hline
\end{tabular}
\end{table}</code></pre>
<p>The most likely result is 1 - 1 with a predicted payout of <span class="math inline">\(8.4\)</span> and betsson.com agrees with this also offering their lowest payout for this bet, <span class="math inline">\(5.3\)</span>. Not good enough! Looking at the payouts at bettson.com I can see that Sevilla - Valencia: 2 - 0 gives me a payout of <span class="math inline">\(16.0\)</span>, that’s much better than my predicted payout of <span class="math inline">\(13.1\)</span>. I’ll go for that!</p>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<p>I believe the model has a lot things going for it. It is conceptually quite simple and easy to understand, implement and extend. It captures the patterns in and distribution of the data well. It allows me to easily calculate the probability of any outcome, from a game with whichever teams from any La Liga season. Want to calculate the probability that RCD Mallorca (home team) vs Malaga CF (away team) in the Season 2009/2010 would result in a draw? Easy! What’s the probability of the total number of goals in Granada CF vs Athletic Club Bilbao being a prime number? No problemo! What if Real Madrid from 2008/2009 met Barcelona from 2012/2013 in 2010/2011 and both teams had the home advantage? Well, that’s possible. There are also a couple of things that could be improved (many which are not too hard to address). Currently there is assumed to be no dependency between the goal distributions of the home and away teams, but this might not be realistic. Maybe if one team have scored more goals the other team “looses steam” (a negative correlation between the teams’ scores) or instead maybe the other team tries harder (a positive correlation). Such dependencies could maybe be added to the model using copulas. * One of the advantages of Bayesian statistics is the possibility to used informative priors. As I have no knowledge of football I’ve been using vague priors but with the help of a more knowledgeable football fan the model could be given more informative priors. Also, the predictive performance of the model has not been as thoroughly examined and this could be remedied with a healthy dose of cross validation.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-baaaath2015modeling">
<p>Bååth, Rasmus. 2015. “Modeling Match Results in Soccer Using a Hierarchical Bayesian Poisson Model.” Technical Report LUCS minor 18, Lund University Cognitive Science, Lund, Sweden.</p>
</div>
<div id="ref-plummer2004jags">
<p>Plummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”</p>
</div>
<div id="ref-su2015package">
<p>Su, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” <em>R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags</em>.</p>
</div>
</div>
</div>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/tutorials/">tutorials</a>
  
  <a class="badge badge-light" href="/tags/jags/">JAGS</a>
  
  <a class="badge badge-light" href="/tags/generalised-linear-mixed-models/">generalised linear mixed models</a>
  
</div>



    
      








  
  
    
  
  





  
  
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="/authors/andrea-gabrio/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/jags/glm2-jags/glm2-jags/">Generalised Linear Models part II - JAGS</a></li>
          
          <li><a href="/jags/glm-jags/glm-jags/">Generalised Linear Models - JAGS</a></li>
          
          <li><a href="/jags/gof-tests-jags/gof-tests-jags/">Goodness of fit tests - JAGS</a></li>
          
          <li><a href="/jags/partly-nested-anova-jags/block-anova-jags/">Partly Nested Anova - JAGS</a></li>
          
          <li><a href="/jags/block-anova-jags/block-anova-jags/">Randomised Complete Block Anova - JAGS</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.9ef1b53ee2bde6c7f33b150c6ba4d452.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">

  <p class="powered-by">
    &#169; Andrea Gabrio &middot;
    <code>2020</code> 
    &middot; Based on the 
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
