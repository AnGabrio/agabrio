<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.1">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Andrea Gabrio">

  
  
  
    
  
  <meta name="description" content="This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:
OpenBUGS - written in component pascal.
JAGS - (Just Another Gibbs Sampler) - written in C&#43;&#43;.">

  
  <link rel="alternate" hreflang="en-us" href="/jags/ancova-jags/ancova-jags/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:450,700|Oswald+Sans:600,700|Roboto+Mono:550,700">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.79d76725ea956cf98045e5d9e289f2aa.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academic.d8c4ad17876b79f73e24a31116b20b9c.css">
  

  
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/jags/ancova-jags/ancova-jags/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Andrea Gabrio">
  <meta property="og:url" content="/jags/ancova-jags/ancova-jags/">
  <meta property="og:title" content="Analysis of Covariance - JAGS | Andrea Gabrio">
  <meta property="og:description" content="This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:
OpenBUGS - written in component pascal.
JAGS - (Just Another Gibbs Sampler) - written in C&#43;&#43;."><meta property="og:image" content="/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2020-02-05T21:13:14-05:00">
  
  <meta property="article:modified_time" content="2020-02-05T21:13:14-05:00">
  

  


  





  <title>Analysis of Covariance - JAGS | Andrea Gabrio</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Andrea Gabrio</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/post/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/research/"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/publication/"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/talk/"><span>Talks</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Software</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/missingHE/"><span>missingHE</span></a>
            </li>
            
          </ul>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Tutorials</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/JAGS/"><span>JAGS</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/OpenBUGS/"><span>OpenBUGS</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/STAN/"><span>STAN</span></a>
            </li>
            
          </ul>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/rubric/"><span>Missing Data</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Analysis of Covariance - JAGS</h1>

  

  
    



<meta content="2020-02-05 21:13:14 -0500 -0500" itemprop="datePublished">
<meta content="2020-02-05 21:13:14 -0500 -0500" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a>Andrea Gabrio</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Feb 5, 2020</time>
  </span>
  

  

  

  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    <a href="/categories/r/">R</a>, <a href="/categories/ancova/">ancova</a>, <a href="/categories/jags/">JAGS</a>, <a href="/categories/factor-analysis/">factor analysis</a></span>
  

  
    

  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. <code>BUGS</code> (Bayesian inference Using <em>Gibbs Sampling</em>) is an algorithm and supporting language (resembling <code>R</code>) dedicated to performing the Gibbs sampling implementation of <em>Markov Chain Monte Carlo</em> (MCMC) method. Dialects of the <code>BUGS</code> language are implemented within three main projects:</p>
<ol style="list-style-type: decimal">
<li><p><strong>OpenBUGS</strong> - written in component pascal.</p></li>
<li><p><strong>JAGS</strong> - (Just Another Gibbs Sampler) - written in <code>C++</code>.</p></li>
<li><p><strong>STAN</strong> - a dedicated Bayesian modelling framework written in <code>C++</code> and implementing <em>Hamiltonian</em> MCMC samplers.</p></li>
</ol>
<p>Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of <code>R</code>, and thus, they are best accessed from within <code>R</code> itself. As such there are multiple packages devoted to interfacing with the various software implementations:</p>
<ul>
<li><p><em>R2OpenBUGS</em> - interfaces with <code>OpenBUGS</code></p></li>
<li><p><em>R2jags</em> - interfaces with <code>JAGS</code></p></li>
<li><p><em>rstan</em> - interfaces with <code>STAN</code></p></li>
</ul>
<p>This tutorial will demonstrate how to fit models in <code>JAGS</code> (<span class="citation">Plummer (2004)</span>) using the package <code>R2jags</code> (<span class="citation">Su et al. (2015)</span>) as interface, which also requires to load some other packages.</p>
<div id="overview" class="section level1">
<h1>Overview</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Previous tutorials have concentrated on designs for either continuous (Regression) or categorical (ANOVA) predictor variables. <em>Analysis of covariance</em> (ANCOVA) models are essentially ANOVA models that incorporate one or more continuous and categorical variables (covariates). Although the relationship between a response variable and a covariate may itself be of substantial clinical interest, typically covariate(s) are incorporated to reduce the amount of unexplained variability in the model and thereby increase the power of any treatment effects.</p>
<p>In ANCOVA, a reduction in unexplained variability is achieved by adjusting the response (to each treatment) according to slight differences in the covariate means as well as accounting for any underlying trends between the response and covariate(s). To do so, the extent to which the within treatment group small differences in covariate means between groups and treatment groups are essentially compared via differences in their <span class="math inline">\(y\)</span>-intercepts. The total variation is thereafter partitioned into explained (using the deviations between the overall trend and trends approximated for each of the treatment groups) and unexplained components (using the deviations between the observations and the approximated within group trends). In this way, ANCOVA can be visualized as a regular ANOVA in which the group and overall means are replaced by group and overall trendlines. Importantly, it should be apparent that ANCOVA is only appropriate when each of the within group trends have the same slope and are thus parallel to one another and the overall trend. Furthermore, ANCOVA is not appropriate when the resulting adjustments must be extrapolated from a linear relationship outside the measured range of the covariate.</p>
<p>As an example, an experiment might be set up to investigate the energetic impacts of sexual vs parthenogenetic (egg development without fertilization) reproduction on leaf insect food consumption. To do so, researchers could measure the daily food intake of individual adult female leaf insects from female only (parthenogenetic) and mixed (sexual) populations. Unfortunately, the available individual leaf insects varied substantially in body size which was expected to increase the variability of daily food intake of treatment groups. Consequently, the researchers also measured the body mass of the individuals as a covariate, thereby providing a means by which daily food consumption could be standardized for body mass. ANCOVA attempts to reduce unexplained variability by standardising the response to the treatment by the effects of the specific covariate condition. Thus ANCOVA provides a means of exercising some statistical control over the variability when it is either not possible or not desirable to exercise experimental control (such as blocking or using otherwise homogeneous observations).</p>
</div>
<div id="null-hypothesis" class="section level2">
<h2>Null hypothesis</h2>
<p><strong>Factor A: the main treatment effect</strong></p>
<ul>
<li><span class="math inline">\(H_0(A):\mu_1(adj)=\mu_2(adj)=\ldots=\mu_i(adj)=\mu(adj)\)</span></li>
</ul>
<p>The adjusted population group means are all equal. The mean of population <span class="math inline">\(1\)</span> adjusted for the covariate is equal to that of population <span class="math inline">\(2\)</span> adjusted for the covariate and so on, and thus all population means adjusted for the covariate are equal to an overall adjusted mean. If the effect of the <span class="math inline">\(i\)</span>-th group is the difference between the <span class="math inline">\(i\)</span>-th group adjusted mean and the overall adjusted mean (<span class="math inline">\(\alpha_i(adj)=\mu_i(adj)−\mu(adj)\)</span>) then the <span class="math inline">\(H_0\)</span> can alternatively be written as:</p>
<ul>
<li><span class="math inline">\(H_0(A):\alpha_1(adj)=\alpha_2(adj)=\ldots=\alpha_i(adj)=0\)</span></li>
</ul>
<p>The effect of each group equals zero. If one or more of the <span class="math inline">\(\alpha_i(adj)\)</span> are different from zero (the response mean for this treatment differs from the overall response mean), the null hypothesis is not true, indicating that the treatment does affect the response variable.</p>
<p><strong>Factor B: the covariate effect</strong></p>
<ul>
<li><span class="math inline">\(H_0(B):\beta_1(pooled)=0\)</span></li>
</ul>
<p>The pooled population slope equals zero. Note, that this null hypothesis is rarely of much interest. It is precisely because of this nuisance relationship that ANCOVA designs are applied.</p>
</div>
<div id="linear-models" class="section level2">
<h2>Linear models</h2>
<p>One or more covariates can be incorporated into single factor, nested, factorial and partly nested designs in order to reduce the unexplained variation. Fundamentally, the covariate(s) are purely used to adjust the response values prior to the regular analysis. The difficulty is in determining the appropriate adjustments. Following is a list of the appropriate linear models and adjusted response calculations for a range of ANCOVA designs. Note that these linear models do not include interactions involving the covariates as these are assumed to be zero. The inclusion of these interaction terms is a useful means of testing the homogeneity of slopes assumption.</p>
<ul>
<li><p><em>Single categorical and single covariate</em></p>
<ul>
<li><p>Linear model: <span class="math inline">\(y_{ij}=\mu + \alpha_i + \beta(x_{ij}-\bar{x}) + \epsilon_{ij}\)</span></p></li>
<li><p>Adjustments: <span class="math inline">\(y_{ij(adj)}=y_{ij} - b(x_{ij} - \bar{x})\)</span></p></li>
</ul></li>
<li><p><em>Single categorical and two covariates</em></p>
<ul>
<li><p>Linear model: <span class="math inline">\(y_{ij}=\mu + \alpha_i + \beta_{YX}(x_{ij}-\bar{x}) + \beta_{YZ}(z_{ij}-\bar{z}) + \epsilon_{ij}\)</span></p></li>
<li><p>Adjustments: <span class="math inline">\(y_{ij(adj)}=y_{ij} - b_{YX}(x_{ij} - \bar{x}) - b_{YZ}(z_{ij} - \bar{z})\)</span></p></li>
</ul></li>
<li><p><em>Factorial designs</em></p>
<ul>
<li><p>Linear model: <span class="math inline">\(y_{ij}=\mu + \alpha_i + \gamma_j + (\alpha\gamma)_{ij}+ \beta(x_{ijk}-\bar{x}) + \epsilon_{ijk}\)</span></p></li>
<li><p>Adjustments: <span class="math inline">\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \bar{x})\)</span></p></li>
</ul></li>
<li><p><em>Nested designs</em></p>
<ul>
<li><p>Linear model: <span class="math inline">\(y_{ijk}=\mu + \alpha_i + \gamma_{j(i)} + \beta(x_{ijk}-\bar{x}) + \epsilon_{ijk}\)</span></p></li>
<li><p>Adjustments: <span class="math inline">\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \bar{x})\)</span></p></li>
</ul></li>
<li><p><em>Partly nested designs</em></p>
<ul>
<li><p>Linear model: <span class="math inline">\(y_{ijkl}=\mu + \alpha_i + \gamma_{j(i)} + \delta_k + (\alpha\delta)_{ik} + (\gamma\delta)_{j(i)k} + \beta(x_{ijk}-\bar{x}) + \epsilon_{ijkl}\)</span></p></li>
<li><p>Adjustments: <span class="math inline">\(y_{ijk(adj)}=y_{ijkl} - b_{between}(x_{i} - \bar{x}) - b_{within}(x_{ijk} - \bar{x}_i)\)</span></p></li>
</ul></li>
</ul>
</div>
<div id="analysis-of-variance" class="section level2">
<h2>Analysis of variance</h2>
<p>In ANCOVA, the total variability of the response variable is sequentially partitioned into components explained by each of the model terms, starting with the covariate and is therefore equivalent to performing a regular analysis of variance on the response variables that have been adjusted for the covariate. The appropriate unexplained residuals and therefore the appropriate <em>F-ratios</em> for each factor differ according to the different null hypotheses associated with different linear models as well as combinations of fixed and random factors in the model (see the following tables). Note that since the covariate levels measured are typically different for each group, ANCOVA designs are inherently non-orthogonal (unbalanced). Consequently, sequential (Type I sums of squares) should not be used. For very simple Ancova designs that incorporate a single categorical and single covariate, Type I sums of squares can be used provided the covariate appears in the linear model first (and thus is partitioned out last) as we are typically not interested in estimating this effect.</p>
<pre class="r"><code>&gt; ancova_table
          df       MS       F-ratio (A&amp;B fixed) F-ratio (B fixed) 
Factor A  &quot;a-1&quot;    &quot;MS A&quot;   &quot;(MS A)/(MS res)&quot;   &quot;(MS A)/(MS res)&quot; 
Factor B  &quot;1&quot;      &quot;MS B&quot;   &quot;(MS B)/(MS res)&quot;   &quot;(MS B)/(MS res)&quot; 
Factor AB &quot;a-1&quot;    &quot;MS AB&quot;  &quot;(MS AB)/(MS res)&quot;  &quot;(MS AB)/(MS res)&quot;
Residual  &quot;(n-2)a&quot; &quot;MS res&quot; &quot;&quot;                  &quot;&quot;                </code></pre>
<p>The corresponding <code>R</code> syntax is given below.</p>
<pre class="r"><code>&gt; anova(lm(DV ~ B * A, dataset))
&gt; # OR
&gt; anova(aov(DV ~ B * A, dataset))
&gt; # OR (make sure not using treatment contrasts)
&gt; Anova(lm(DV ~ B * A, dataset), type = &quot;III&quot;)</code></pre>
</div>
<div id="assumptions" class="section level2">
<h2>Assumptions</h2>
<p>As ANCOVA designs are essentially regular ANOVA designs that are first adjusted (centered) for the covariate(s), ANCOVA designs inherit all of the underlying assumptions of the appropriate ANOVA design. Specifically, hypothesis tests assume that:</p>
<ul>
<li><p>The appropriate residuals are normally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator, see the above tables) should be used to explore normality. Scale transformations are often useful.</p></li>
<li><p>The appropriate residuals are equally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.</p></li>
<li><p>The appropriate residuals are independent of one another.</p></li>
<li><p>The relationship between the response variable and the covariate should be linear. Linearity can be explored using scatterplots and residual plots should reveal no patterns.</p></li>
<li><p>For repeated measures and other designs in which treatment levels within blocks can not be be randomly ordered, the variance/covariance matrix is assumed to display sphericity.</p></li>
<li><p>For designs that utilise blocking, it is assumed that there are no block by within block interactions.</p></li>
</ul>
<p><strong>Homogeneity of Slopes</strong></p>
<p>In addition to the above assumptions, ANCOVA designs also assume that slopes of relationships between the response variable and the covariate(s) are the same for each treatment level (group). That is, all the trends are parallel. If the individual slopes deviate substantially from each other (and thus the overall slope), then adjustments made to each of the observations are nonsensical. This situation is analogous to an interaction between two or more factors. In ANCOVA, interactions involving the covariate suggest that the nature of the relationship between the response and the covariate differs between the levels of the categorical treatment. More importantly, they also indicate that whether or not there is an effect of the treatment depends on what range of the covariate you are focussed on. Clearly then, it is not possible to make conclusions about the main effects of treatments in the presence of such interactions. The assumption of homogeneity of slopes can be examined via interaction plots or more formally, by testing hypotheses about the interactions between categorical variables and the covariate(s). There are three broad approaches for dealing with ANCOVA designs with heterogeneous slopes and selection depends on the primary focus of the study.</p>
<ol style="list-style-type: decimal">
<li><p>When the primary objective of the analysis is to investigate the effects of categorical treatments, it is possible to adopt an approach similar to that taken when exploring interactions in multiple regression. The effect of treatments can be examined at specific values of the covariate (such as the mean and <span class="math inline">\(\pm\)</span> one standard deviation). This approach is really only useful at revealing broad shifts in patterns over the range of the covariate and if the selected values of the covariate do not have some inherent clinical meaning (selected arbitrarily), then the outcomes can be of only limited clinical interest.</p></li>
<li><p>Alternatively, the <em>Johnson-Neyman technique</em> (or Wilxon modification thereof) procedure indicates the ranges of the covariate over which the individual regression lines of pairs of treatment groups overlap or cross. Although less powerful than the previous approach, the <em>Wilcox(J-N)</em> procedure has the advantage of revealing the important range (ranges for which the groups are different and not different) of the covariate rather than being constrained by specific levels selected.</p></li>
<li><p>Use contrast treatments to split up the interaction term into its constituent contrasts for each level of the treatment. Essentially this compares each of the treatment level slopes to the slope from the “control” group and is useful if the primary focus is on the relationships between the response and the covariate.</p></li>
</ol>
<p><strong>Similar covariate ranges</strong></p>
<p>Adjustments made to the response means in an attempt to statistically account for differences in the covariate involve predicting mean response values along displaced linear relationships between the overall response and covariate variables. The degree of trend displacement for any given group is essentially calculated by multiplying the overall regression slope by the degree of difference between the overall covariate mean and the mean of the covariate for that group. However, when the ranges of the covariate within each of the groups differ substantially from one another, these adjustments are effectively extrapolations and therefore of unknown reliability. If a simple ANOVA of the covariate modelled against the categorical factor indicates that the covariate means differ significantly between groups, it may be necessary to either remove extreme observations or reconsider the analysis.</p>
<p><strong>Robust ANCOVA</strong></p>
<p>ANCOVA based on rank transformed data can be useful for accommodating data with numerous problematic outliers. Nevertheless, problems about the difficulties of detecting interactions from rank transformed data, obviously have implications for inferential tests of homogeneity of slopes. Randomisation tests that maintain response0covariate pairs and repeatedly randomise these observations amongst the levels of the treatments can also be useful, particularly when there is doubt over the independence of observations. Both planned and unplanned comparisons follow those of other ANOVA chapters without any real additional complications. Notably, recent implementations of the <em>Tukey’s test</em> (within <code>R</code>) accommodate unbalanced designs and thus negate the need for some of the more complicated and specialised techniques that have been highlighted in past texts.</p>
</div>
</div>
<div id="data-generation" class="section level1">
<h1>Data generation</h1>
<p>Consider an experimental design aimed at exploring the effects of a categorical variable with three levels (Group A, Group B and Group C) on a response. From previous studies, we know that the response is influenced by another variable (covariate). Unfortunately, it was not possible to ensure that all sampling units were the same degree of the covariate. Therefore, in an attempt to account for this anticipated extra source of variability, we measured the level of the covariate for each sampling unit. Actually, in allocating treatments to the various treatment groups, we tried to ensure a similar mean and range of the covariate within each group.</p>
<pre class="r"><code>&gt; set.seed(123)
&gt; n &lt;- 10
&gt; p &lt;- 3
&gt; A.eff &lt;- c(40, -15, -20)
&gt; beta &lt;- -0.45
&gt; sigma &lt;- 4
&gt; B &lt;- rnorm(n * p, 0, 15)
&gt; A &lt;- gl(p, n, lab = paste(&quot;Group&quot;, LETTERS[1:3]))
&gt; mm &lt;- model.matrix(~A + B)
&gt; data &lt;- data.frame(A = A, B = B, Y = as.numeric(c(A.eff, beta) %*% t(mm)) + rnorm(n * p, 0, 4))
&gt; data$B &lt;- data$B + 20
&gt; head(data)
        A        B        Y
1 Group A 11.59287 45.48907
2 Group A 16.54734 40.37341
3 Group A 43.38062 33.05922
4 Group A 21.05763 43.03660
5 Group A 21.93932 42.41363
6 Group A 45.72597 31.17787</code></pre>
<div id="exploratory-data-analysis" class="section level2">
<h2>Exploratory data analysis</h2>
<pre class="r"><code>&gt; library(car)
&gt; scatterplot(Y ~ B | A, data = data)</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/exp_data-1.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; boxplot(Y ~ A, data)
&gt; 
&gt; # OR via ggplot
&gt; library(ggplot2)</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/exp_data-2.png" width="672" /></p>
<pre class="r"><code>&gt; ggplot(data, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/exp_data-3.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; ggplot(data, aes(y = Y, x = A)) + geom_boxplot()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/exp_data-4.png" width="672" /></p>
<p><strong>Conclusions</strong></p>
<p>There is no evidence of obvious non-normality. The assumption of linearity seems reasonable. The variability of the three groups seems approximately equal. The slopes (<span class="math inline">\(Y\)</span> vs B trends) appear broadly similar for each treatment group.</p>
<p>We can explore inferential evidence of unequal slopes by examining estimated effects of the interaction between the categorical variable and the covariate. Note, pay no attention to the main effects - only the interaction. Even though I intend to illustrate Bayesian analyses here, for such a simple model, it is considerably simpler to use traditional OLS for testing for the presence of an interaction.</p>
<pre class="r"><code>&gt; anova(lm(Y ~ B * A, data = data))
Analysis of Variance Table

Response: Y
          Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    
B          1  989.99  989.99  92.6782 1.027e-09 ***
A          2 2320.05 1160.02 108.5956 9.423e-13 ***
B:A        2   51.36   25.68   2.4041    0.1118    
Residuals 24  256.37   10.68                       
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>There is very little evidence to suggest that the assumption of equal slopes will be inappropriate.</p>
</div>
</div>
<div id="model-fitting" class="section level1">
<h1>Model fitting</h1>
<p>The observed response (<span class="math inline">\(y_i\)</span>) are assumed to be drawn from a normal distribution with a given mean (<span class="math inline">\(\mu\)</span>) and standard deviation (<span class="math inline">\(\sigma\)</span>). The expected values are themselves determined by the linear predictor (<span class="math inline">\(\boldsymbol X \boldsymbol \beta\)</span>). In this case, <span class="math inline">\(\boldsymbol \beta\)</span> represents the vector of <span class="math inline">\(\beta\)</span>’s - the intercept associated with the first group, the (effects) differences between this intercept and the intercepts for each other group as well as the slope associated with the continuous covariate. <span class="math inline">\(\boldsymbol X\)</span> is the model matrix. MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying ‘uninformative’ priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (<span class="math inline">\(100\)</span>) for both the intercept and the treatment effect and a wide half-cauchy (<span class="math inline">\(\text{scale}=5\)</span>) for the standard deviation.</p>
<p><span class="math display">\[y_i \sim N(\mu_i,\sigma),  \]</span></p>
<p>where <span class="math inline">\(\mu_i=\beta_0 +\boldsymbol \beta \boldsymbol X\)</span>. The assumed priors are: <span class="math inline">\(\beta \sim N(0,100)\)</span> and <span class="math inline">\(\sigma \sim \text{Cauchy}(0,5)\)</span>. Note, exploratory data analysis suggests that while the intercept (intercept of Group A) and categorical predictor effects (differences between intercepts of each of the Group and Group A’s intercept) could be drawn from a similar distribution (with mean in the <span class="math inline">\(10\)</span>’s and variances in the <span class="math inline">\(100\)</span>’s), the slope (effect associated with Group A linear relationship) is likely to be an order of magnitude less. We might therefore be tempted to provide different priors for the intercept, categorical effects and slope effect. For a simple model such as this, it is unlikely to be necessary. However, for more complex models, where prior specification becomes more critical, separate priors would probably be necessary.</p>
<p>We proceed to code the model into <code>JAGS</code> (remember that in this software normal distribution are parameterised in terms of precisions <span class="math inline">\(\tau\)</span> rather than variances, where <span class="math inline">\(\tau=\frac{1}{\sigma^2}\)</span>). Note the following example as group means calculated as derived posteriors.</p>
<pre class="r"><code>&gt; modelString = &quot;
+   model {
+   #Likelihood
+   for (i in 1:n) {
+   y[i]~dnorm(mean[i],tau)
+   mean[i] &lt;- inprod(beta[],X[i,])
+   }
+   #Priors
+   for (i in 1:ngroups) {
+   beta[i] ~ dnorm(0, 1.0E-6) 
+   }
+   sigma ~ dunif(0, 100)
+   tau &lt;- 1 / (sigma * sigma)
+   }
+   &quot;
&gt; 
&gt; ## write the model to a text file
&gt; writeLines(modelString, con = &quot;ancovaModel.txt&quot;)</code></pre>
<p>Arrange the data as a list (as required by <code>JAGS</code>). As input, <code>JAGS</code> will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.</p>
<pre class="r"><code>&gt; X &lt;- model.matrix(~A + B, data)
&gt; data.list &lt;- with(data, list(y = Y, X = X, n = nrow(data), ngroups = ncol(X)))</code></pre>
<p>Define the nodes (parameters and derivatives) to monitor and the chain parameters.</p>
<pre class="r"><code>&gt; params &lt;- c(&quot;beta&quot;, &quot;sigma&quot;)
&gt; nChains = 2
&gt; burnInSteps = 3000
&gt; thinSteps = 1
&gt; numSavedSteps = 15000  #across all chains
&gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&gt; nIter
[1] 10500</code></pre>
<p>Start the <code>JAGS</code> model (check the model, load data into the model, specify the number of chains and compile the model). Load the <code>R2jags</code> package.</p>
<pre class="r"><code>&gt; library(R2jags)</code></pre>
<p>Now run the <code>JAGS</code> code via the <code>R2jags</code> interface. Note that the first time jags is run after the <code>R2jags</code> package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.</p>
<pre class="r"><code>&gt; data.r2jags &lt;- jags(data = data.list, inits = NULL, parameters.to.save = params,
+     model.file = &quot;ancovaModel.txt&quot;, n.chains = nChains, n.iter = nIter,
+     n.burnin = burnInSteps, n.thin = thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 30
   Unobserved stochastic nodes: 5
   Total graph size: 224

Initializing model
&gt; 
&gt; print(data.r2jags)
Inference for Bugs model at &quot;ancovaModel.txt&quot;, fit using jags,
 2 chains, each with 10500 iterations (first 3000 discarded)
 n.sims = 15000 iterations saved
         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
beta[1]   51.001   1.529  47.977  50.009  50.995  52.016  53.980 1.001 15000
beta[2]  -16.254   1.623 -19.455 -17.342 -16.259 -15.170 -13.090 1.001 10000
beta[3]  -20.656   1.667 -23.941 -21.752 -20.672 -19.566 -17.330 1.001 15000
beta[4]   -0.484   0.048  -0.577  -0.516  -0.484  -0.453  -0.389 1.001 15000
sigma      3.607   0.526   2.740   3.236   3.546   3.912   4.793 1.001  7400
deviance 160.601   3.509 155.859 158.002 159.905 162.478 169.218 1.001 15000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 6.2 and DIC = 166.8
DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
</div>
<div id="mcmc-diagnostics" class="section level1">
<h1>MCMC diagnostics</h1>
<p>In addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.</p>
<ul>
<li><p><em>Traceplots</em> for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.</p></li>
<li><p><em>Autocorrelation</em> plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of <span class="math inline">\(0\)</span> represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of <span class="math inline">\(1\)</span>). A lag of <span class="math inline">\(1\)</span> represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).</p></li>
<li><p><em>Potential scale reduction factor</em> (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than <span class="math inline">\(1.05\)</span>. If there are values of <span class="math inline">\(1.05\)</span> or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.</p></li>
</ul>
<p>Prior to examining the summaries, we should have explored the convergence diagnostics. We use the package <code>mcmcplots</code> to obtain density and trace plots for the effects model as an example. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters, e.g. <span class="math inline">\(\boldsymbol \beta\)</span>.</p>
<pre class="r"><code>&gt; library(mcmcplots)
&gt; denplot(data.r2jags, parms = c(&quot;beta&quot;))</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_diag-1.png" width="672" /></p>
<pre class="r"><code>&gt; traplot(data.r2jags, parms = c(&quot;beta&quot;))</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_diag-2.png" width="672" /></p>
<p>Trace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters (such as <span class="math inline">\(\beta\)</span>s).</p>
<pre class="r"><code>&gt; data.mcmc = as.mcmc(data.r2jags)
&gt; #Raftery diagnostic
&gt; raftery.diag(data.mcmc)
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta[1]  2        3689  3746         0.985     
 beta[2]  2        3938  3746         1.050     
 beta[3]  2        3853  3746         1.030     
 beta[4]  2        3811  3746         1.020     
 deviance 2        3895  3746         1.040     
 sigma    5        5552  3746         1.480     


[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta[1]  2        3770  3746         1.010     
 beta[2]  2        3729  3746         0.995     
 beta[3]  2        3811  3746         1.020     
 beta[4]  2        3895  3746         1.040     
 deviance 2        3855  3746         1.030     
 sigma    4        5247  3746         1.400     </code></pre>
<p>The Raftery diagnostics for each chain estimate that we would require no more than <span class="math inline">\(5000\)</span> samples to reach the specified level of confidence in convergence. As we have <span class="math inline">\(10500\)</span> samples, we can be confidence that convergence has occurred.</p>
<pre class="r"><code>&gt; #Autocorrelation diagnostic
&gt; autocorr.diag(data.mcmc)
            beta[1]      beta[2]      beta[3]       beta[4]     deviance
Lag 0   1.000000000  1.000000000  1.000000000  1.0000000000  1.000000000
Lag 1   0.017910611 -0.003186598  0.009149022  0.0039919666  0.266991768
Lag 5  -0.004399550 -0.002747041 -0.001891657 -0.0213261543  0.005499734
Lag 10 -0.001972741  0.005855050 -0.004887402  0.0186597337 -0.008683579
Lag 50 -0.002269863  0.015348324 -0.001446494 -0.0004828212 -0.010725173
              sigma
Lag 0   1.000000000
Lag 1   0.382742913
Lag 5   0.007377659
Lag 10 -0.001255836
Lag 50  0.003892668</code></pre>
<p>A lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).</p>
</div>
<div id="model-validation" class="section level1">
<h1>Model validation</h1>
<p>Model validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. For more complex models (those that contain multiple effects), it is also advisable to plot the residuals against each of the individual predictors. For sampling designs that involve sample collection over space or time, it is also a good idea to explore whether there are any temporal or spatial patterns in the residuals.</p>
<p>There are numerous situations (e.g. when applying specific variance-covariance structures to a model) where raw residuals do not reflect the interior workings of the model. Typically, this is because they do not take into account the variance-covariance matrix or assume a very simple variance-covariance matrix. Since the purpose of exploring residuals is to evaluate the model, for these cases, it is arguably better to draw conclusions based on standardized (or studentised) residuals. Unfortunately the definitions of standardised and studentised residuals appears to vary and the two terms get used interchangeably. I will adopt the following definitions:</p>
<ul>
<li><p><strong>Standardised residuals</strong>. The raw residuals divided by the true standard deviation of the residuals (which of course is rarely known).</p></li>
<li><p><strong>Studentised residuals</strong>. The raw residuals divided by the standard deviation of the residuals. Note that <strong>externally studentised residuals</strong> are calculated by dividing the raw residuals by a unique standard deviation for each observation that is calculated from regressions having left each successive observation out.</p></li>
<li><p><strong>Pearson residuals</strong>. The raw residuals divided by the standard deviation of the response variable.</p></li>
</ul>
<p>he mark of a good model is being able to predict well. In an ideal world, we would have sufficiently large sample size as to permit us to hold a fraction (such as <span class="math inline">\(25\)</span>%) back thereby allowing us to train the model on <span class="math inline">\(75\)</span>% of the data and then see how well the model can predict the withheld <span class="math inline">\(25\)</span>%. Unfortunately, such a luxury is still rare. The next best option is to see how well the model can predict the observed data. Models tend to struggle most with the extremes of trends and have particular issues when the extremes approach logical boundaries (such as zero for count data and standard deviations). We can use the fitted model to generate random predicted observations and then explore some properties of these compared to the actual observed data.</p>
<p>Rather than dublicate this for both additive and multiplicative models, we will only explore the multiplicative model. Residuals are not computed directly within <code>JAGS</code>. However, we can calculate them manually form the posteriors.</p>
<pre class="r"><code>&gt; library(dplyr)
&gt; mcmc = data.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%
+     dplyr:::select(contains(&quot;beta&quot;), sigma) %&gt;% as.matrix
&gt; # generate a model matrix
&gt; newdata = data
&gt; Xmat = model.matrix(~A + B, newdata)
&gt; ## get median parameter estimates
&gt; coefs = apply(mcmc[, 1:4], 2, median)
&gt; fit = as.vector(coefs %*% t(Xmat))
&gt; resid = data$Y - fit
&gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_residuals-1.png" width="672" /></p>
<p>Residuals against predictors</p>
<pre class="r"><code>&gt; library(tidyr)
&gt; mcmc = data.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%
+     dplyr:::select(contains(&quot;beta&quot;), sigma) %&gt;% as.matrix
&gt; # generate a model matrix
&gt; newdata = newdata
&gt; Xmat = model.matrix(~A + B, newdata)
&gt; ## get median parameter estimates
&gt; coefs = apply(mcmc[, 1:4], 2, median)
&gt; fit = as.vector(coefs %*% t(Xmat))
&gt; resid = data$Y - fit
&gt; newdata = newdata %&gt;% cbind(fit, resid)
&gt; ggplot(newdata) + geom_point(aes(y = resid, x = A)) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_residuals2-1.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; ggplot(newdata) + geom_point(aes(y = resid, x = B)) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_residuals2-2.png" width="672" /></p>
<p>And now for studentised residuals</p>
<pre class="r"><code>&gt; mcmc = data.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%
+     dplyr:::select(contains(&quot;beta&quot;), sigma) %&gt;% as.matrix
&gt; # generate a model matrix
&gt; newdata = data
&gt; Xmat = model.matrix(~A + B, newdata)
&gt; ## get median parameter estimates
&gt; coefs = apply(mcmc[, 1:4], 2, median)
&gt; fit = as.vector(coefs %*% t(Xmat))
&gt; resid = data$Y - fit
&gt; sresid = resid/sd(resid)
&gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit)) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_residuals3-1.png" width="672" /></p>
<p>For this simple model, the studentised residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.</p>
<pre class="r"><code>&gt; mcmc = data.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%
+     dplyr:::select(contains(&quot;beta&quot;), sigma) %&gt;% as.matrix
&gt; # generate a model matrix
&gt; Xmat = model.matrix(~A + B, data)
&gt; ## get median parameter estimates
&gt; coefs = mcmc[, 1:4]
&gt; fit = coefs %*% t(Xmat)
&gt; ## draw samples from this model
&gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i,
+     ], mcmc[i, &quot;sigma&quot;]))
&gt; newdata = data.frame(A = data$A, B = data$B, yRep) %&gt;% gather(key = Sample,
+     value = Value, -A, -B)
&gt; ggplot(newdata) + geom_violin(aes(y = Value, x = A, fill = &quot;Model&quot;),
+     alpha = 0.5) + geom_violin(data = data, aes(y = Y, x = A,
+     fill = &quot;Obs&quot;), alpha = 0.5) + geom_point(data = data, aes(y = Y,
+     x = A), position = position_jitter(width = 0.1, height = 0),
+     color = &quot;black&quot;) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_rep-1.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; ggplot(newdata) + geom_violin(aes(y = Value, x = B, fill = &quot;Model&quot;,
+     group = B, color = A), alpha = 0.5) + geom_point(data = data,
+     aes(y = Y, x = B, group = B, color = A)) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_rep-2.png" width="672" /></p>
<p>The predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.</p>
<pre class="r"><code>&gt; library(bayesplot)
&gt; mcmc_intervals(data.r2jags$BUGSoutput$sims.matrix, regex_pars = &quot;beta|sigma&quot;)</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_rep2-1.png" width="672" /></p>
<pre class="r"><code>&gt; mcmc_areas(data.r2jags$BUGSoutput$sims.matrix, regex_pars = &quot;beta|sigma&quot;)</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_rep2-2.png" width="672" /></p>
</div>
<div id="parameter-estimates" class="section level1">
<h1>Parameter estimates</h1>
<p>Although all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and <span class="math inline">\(95\)</span>% credibility intervals.</p>
<pre class="r"><code>&gt; mcmcpvalue &lt;- function(samp) {
+     ## elementary version that creates an empirical p-value for the
+     ## hypothesis that the columns of samp have mean zero versus a general
+     ## multivariate distribution with elliptical contours.
+ 
+     ## differences from the mean standardized by the observed
+     ## variance-covariance factor
+ 
+     ## Note, I put in the bit for single terms
+     if (length(dim(samp)) == 0) {
+         std &lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),
+             transpose = TRUE)
+         sqdist &lt;- colSums(std * std)
+         sum(sqdist[-1] &gt; sqdist[1])/length(samp)
+     } else {
+         std &lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),
+             transpose = TRUE)
+         sqdist &lt;- colSums(std * std)
+         sum(sqdist[-1] &gt; sqdist[1])/nrow(samp)
+     }
+ 
+ }</code></pre>
<p>First, we look at the results from the additive model.</p>
<pre class="r"><code>&gt; print(data.r2jags)
Inference for Bugs model at &quot;ancovaModel.txt&quot;, fit using jags,
 2 chains, each with 10500 iterations (first 3000 discarded)
 n.sims = 15000 iterations saved
         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
beta[1]   51.001   1.529  47.977  50.009  50.995  52.016  53.980 1.001 15000
beta[2]  -16.254   1.623 -19.455 -17.342 -16.259 -15.170 -13.090 1.001 10000
beta[3]  -20.656   1.667 -23.941 -21.752 -20.672 -19.566 -17.330 1.001 15000
beta[4]   -0.484   0.048  -0.577  -0.516  -0.484  -0.453  -0.389 1.001 15000
sigma      3.607   0.526   2.740   3.236   3.546   3.912   4.793 1.001  7400
deviance 160.601   3.509 155.859 158.002 159.905 162.478 169.218 1.001 15000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 6.2 and DIC = 166.8
DIC is an estimate of expected predictive error (lower deviance is better).
&gt; 
&gt; # OR
&gt; library(broom)
&gt; tidyMCMC(as.mcmc(data.r2jags), conf.int = TRUE, conf.method = &quot;HPDinterval&quot;)
# A tibble: 6 x 5
  term     estimate std.error conf.low conf.high
  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 beta[1]    51.0      1.53     48.0      53.9  
2 beta[2]   -16.3      1.62    -19.5     -13.1  
3 beta[3]   -20.7      1.67    -23.9     -17.3  
4 beta[4]    -0.484    0.0478   -0.577    -0.389
5 deviance  161.       3.51    155.      167.   
6 sigma       3.61     0.526     2.69      4.70 </code></pre>
<p><strong>Conclusions</strong></p>
<ul>
<li><p>The intercept of the first group (Group A) is <span class="math inline">\(51\)</span>.</p></li>
<li><p>The mean of the second group (Group B) is <span class="math inline">\(-16.3\)</span> units greater than (A).</p></li>
<li><p>The mean of the third group (Group C) is <span class="math inline">\(-20.7\)</span> units greater than (A).</p></li>
<li><p>A one unit increase in B in Group A is associated with a <span class="math inline">\(-0.484\)</span> units increase in <span class="math inline">\(Y\)</span>.</p></li>
</ul>
<p>The <span class="math inline">\(95\)</span>% confidence interval for the effects of Group B, Group C and the partial slope associated with B do not overlapp with 0 implying a significant difference between group A and groups B, C and a significant negative relationship with B. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.</p>
<pre class="r"><code>&gt; ## since values are less than zero
&gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, &quot;beta[2]&quot;])  # effect of (B-A = 0)
[1] 0
&gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, &quot;beta[3]&quot;])  # effect of (C-A = 0)
[1] 0
&gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, &quot;beta[4]&quot;])  # effect of (slope = 0)
[1] 0
&gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, 2:4])  # effect of (model)
[1] 0</code></pre>
<p>There is evidence that the reponse differs between the groups.</p>
</div>
<div id="graphical-summaries" class="section level1">
<h1>Graphical summaries</h1>
<p>A nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group.</p>
<pre class="r"><code>&gt; mcmc = data.r2jags$BUGSoutput$sims.matrix
&gt; ## Calculate the fitted values
&gt; newdata = expand.grid(A = levels(data$A), B = seq(min(data$B), max(data$B),
+     len = 100))
&gt; Xmat = model.matrix(~A + B, newdata)
&gt; coefs = mcmc[, c(&quot;beta[1]&quot;, &quot;beta[2]&quot;, &quot;beta[3]&quot;, &quot;beta[4]&quot;)]
&gt; fit = coefs %*% t(Xmat)
&gt; newdata = newdata %&gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = &quot;HPDinterval&quot;))
&gt; 
&gt; ggplot(newdata, aes(y = estimate, x = B, fill = A)) + geom_ribbon(aes(ymin = conf.low,
+     ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(&quot;Y&quot;) +
+     scale_x_continuous(&quot;B&quot;) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_post1-1.png" width="672" /></p>
<p>As this is simple single factor ANOVA, we can simple add the raw data to this figure. For more complex designs with additional predictors, it is necessary to plot partial residuals.</p>
<pre class="r"><code>&gt; ## Calculate partial residuals fitted values
&gt; fdata = rdata = data
&gt; fMat = rMat = model.matrix(~A + B, fdata)
&gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))
&gt; resid = as.vector(data$Y - apply(coefs, 2, median) %*% t(rMat))
&gt; rdata = rdata %&gt;% mutate(partial.resid = resid + fit)
&gt; 
&gt; ggplot(newdata, aes(y = estimate, x = B, fill = A)) + geom_point(data = rdata,
+     aes(y = partial.resid, x = B, color = A)) + geom_ribbon(aes(ymin = conf.low,
+     ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(&quot;Y&quot;) +
+     scale_x_continuous(&quot;B&quot;) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_post2-1.png" width="672" /></p>
</div>
<div id="posteriors" class="section level1">
<h1>Posteriors</h1>
<p>In frequentist statistics, when we have more than two groups, we are typically not only interested in whether there is evidence for an overall “effect” of a factor - we are also interested in how various groups compare to one another. To explore these trends, we either compare each group to each other in a pairwise manner (controlling for family-wise Type I error rates) or we explore an independent subset of the possible comparisons. Although these alternate approaches can adequately address a specific research agenda, often they impose severe limitations and compromises on the scope and breadth of questions that can be asked of your data. The reason for these limitations is that in a frequentist framework, any single hypothesis carries with it a (nominally) <span class="math inline">\(5\)</span>% chance of a false rejection (since it is based on long-run frequency). Thus, performing multiple tests are likely to compound this error rate. The point is, that each comparison is compared to its own probability distribution (and each carries a <span class="math inline">\(5\)</span>% error rate). By contrast, in Bayesian statistics, all comparisons (contrasts) are drawn from the one (hopefully stable and convergent) posterior distribution and this posterior is invariant to the type and number of comparisons drawn. Hence, the theory clearly indicates that having generated our posterior distribution, we can then query this distribution in any way that we wish thereby allowing us to explore all of our research questions simultaneously.</p>
<p>Bayesian “contrasts” can be performed either:</p>
<ul>
<li><p>within the Bayesian sampling model or</p></li>
<li><p>construct them from the returned MCMC samples (they are drawn from the posteriors)</p></li>
</ul>
<p>Only the latter will be demonstrated as it provides a consistent approach across all routines. In order to allow direct comparison to the frequentist equivalents, I will explore the same set of planned and <em>Tukey</em>’s test comparisons described here. For the “planned comparison” we defined two contrasts: 1) group B vs group C; and 2) group A vs the average of groups B and C. Of course each of these could be explored at multiple values of B, however, since we fit an additive model (which assumes that the slopes are homogeneous), the contrasts will be constant throughout the domain of B.</p>
<p>Lets start by comparing each group to each other group in a pairwise manner. Arguably the most elegant way to do this is to generate a Tukey’s contrast matrix. This is a model matrix specific to comparing each group to each other group. Again, since the lines are parallel, it does not really matter what level of B we estimate these efffects at - so lets use the mean B.</p>
<pre class="r"><code>&gt; mcmc = data.r2jags$BUGSoutput$sims.matrix
&gt; coefs &lt;- as.matrix(mcmc)[, 1:4]
&gt; newdata &lt;- data.frame(A = levels(data$A), B = mean(data$B))
&gt; # A Tukeys contrast matrix
&gt; library(multcomp)
&gt; tuk.mat &lt;- contrMat(n = table(newdata$A), type = &quot;Tukey&quot;)
&gt; Xmat &lt;- model.matrix(~A + B, data = newdata)
&gt; pairwise.mat &lt;- tuk.mat %*% Xmat
&gt; pairwise.mat
                  (Intercept) AGroup B AGroup C B
Group B - Group A           0        1        0 0
Group C - Group A           0        0        1 0
Group C - Group B           0       -1        1 0
&gt; 
&gt; mcmc_areas(coefs %*% t(pairwise.mat))</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/posterior1-1.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; (comps = tidyMCMC(coefs %*% t(pairwise.mat), conf.int = TRUE, conf.method = &quot;HPDinterval&quot;))
# A tibble: 3 x 5
  term              estimate std.error conf.low conf.high
  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 Group B - Group A   -16.3       1.62   -19.5     -13.1 
2 Group C - Group A   -20.7       1.67   -23.9     -17.3 
3 Group C - Group B    -4.40      1.69    -7.68     -1.04
&gt; 
&gt; ggplot(comps, aes(y = estimate, x = term)) + geom_pointrange(aes(ymin = conf.low,
+     ymax = conf.high)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) +
+     scale_y_continuous(&quot;Effect size&quot;) + scale_x_discrete(&quot;&quot;) + coord_flip() +
+     theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/posterior1-2.png" width="672" /></p>
<p>With a couple of modifications, we could also express this as percentage changes. A percentage change represents the change (difference between groups) divided by one of the groups (determined by which group you want to express the percentage change to). Hence, we generate an additional mcmc matrix that represents the cell means for the divisor group (group we want to express change relative to). Since the <code>tuk.mat</code> defines comparisons as <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span> pairs, if we simply replace all the <span class="math inline">\(-1\)</span> with <span class="math inline">\(0\)</span>, the eventual matrix multiplication will result in estimates of the divisor cell means instread of the difference. We can then divide the original mcmc matrix above with this new divisor mcmc matrix to yeild a mcmc matrix of percentage change.</p>
<pre class="r"><code>&gt; # Modify the tuk.mat to replace -1 with 0.  This will allow us to get a
&gt; # mcmc matrix of ..
&gt; tuk.mat[tuk.mat == -1] = 0
&gt; comp.mat &lt;- tuk.mat %*% Xmat
&gt; comp.mat
                  (Intercept) AGroup B AGroup C        B
Group B - Group A           1        1        0 19.29344
Group C - Group A           1        0        1 19.29344
Group C - Group B           1        0        1 19.29344
&gt; 
&gt; comp.mcmc = 100 * (coefs %*% t(pairwise.mat))/coefs %*% t(comp.mat)
&gt; (comps = tidyMCMC(comp.mcmc, conf.int = TRUE, conf.method = &quot;HPDinterval&quot;))
# A tibble: 3 x 5
  term              estimate std.error conf.low conf.high
  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 Group B - Group A    -64.3      8.74    -82.4    -48.0 
2 Group C - Group A    -99.0     12.6    -124.     -74.8 
3 Group C - Group B    -21.4      9.02    -39.2     -4.13
&gt; 
&gt; ggplot(comps, aes(y = estimate, x = term)) + geom_pointrange(aes(ymin = conf.low,
+     ymax = conf.high)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) +
+     scale_y_continuous(&quot;Effect size (%)&quot;) + scale_x_discrete(&quot;&quot;) + coord_flip() +
+     theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/posterior2-1.png" width="672" /></p>
<p>And now for the specific planned comparisons (Group B vs Group C as well as Group A vs the average of Groups B and C). This is achieved by generating our own contrast matrix (defining the contributions of each group to each contrast).</p>
<pre class="r"><code>&gt; c.mat = rbind(c(0, 1, -1), c(1/2, -1/3, -1/3))
&gt; c.mat
     [,1]       [,2]       [,3]
[1,]  0.0  1.0000000 -1.0000000
[2,]  0.5 -0.3333333 -0.3333333
&gt; 
&gt; mcmc = data.r2jags$BUGSoutput$sims.matrix
&gt; coefs &lt;- as.matrix(mcmc)[, 1:4]
&gt; newdata &lt;- data.frame(A = levels(data$A), B = mean(data$B))
&gt; Xmat &lt;- model.matrix(~A + B, data = newdata)
&gt; c.mat = c.mat %*% Xmat
&gt; c.mat
     (Intercept)   AGroup B   AGroup C         B
[1,]   0.0000000  1.0000000 -1.0000000  0.000000
[2,]  -0.1666667 -0.3333333 -0.3333333 -3.215574
&gt; 
&gt; (comps = tidyMCMC(as.mcmc(coefs %*% t(c.mat)), conf.int = TRUE, conf.method = &quot;HPDinterval&quot;))
# A tibble: 2 x 5
  term  estimate std.error conf.low conf.high
  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 var1      4.40     1.69      1.04      7.68
2 var2      5.36     0.790     3.80      6.93</code></pre>
</div>
<div id="finite-population-standard-deviations" class="section level1">
<h1>Finite population standard deviations</h1>
<p>Variance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).</p>
<p>Finite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (<span class="citation">Gelman and others (2005)</span>). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.</p>
<pre><code>      beta[1]   beta[2]   beta[3]    beta[4] deviance    sigma
[1,] 49.12140 -12.79223 -18.26477 -0.4972722 161.2762 3.888826
[2,] 51.03351 -16.80051 -20.03944 -0.4767683 156.2198 2.958015
[3,] 51.55756 -16.80292 -20.00531 -0.4479209 161.2724 3.984268
[4,] 50.15508 -15.15637 -21.01837 -0.4787121 158.5376 3.943798
[5,] 52.94683 -17.04043 -22.95279 -0.5209229 157.8834 3.194266
[6,] 52.16920 -17.91313 -23.53270 -0.4678091 159.4251 3.239537
# A tibble: 3 x 5
  term     estimate std.error conf.low conf.high
  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 sd.A         3.12     1.18     0.739      5.41
2 sd.B         7.12     0.703    5.73       8.49
3 sd.resid     3.46     0.169    3.26       3.79
# A tibble: 3 x 5
  term     estimate std.error conf.low conf.high
  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 sd.A         22.9      6.62     8.09      34.1
2 sd.B         52.3      4.54    43.1       61.2
3 sd.resid     24.9      3.22    20.8       31.9</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/eff_pop1-1.png" width="672" /></p>
<p>Approximately <span class="math inline">\(22.9\)</span>% of the total finite population standard deviation is due to <span class="math inline">\(x\)</span>.</p>
</div>
<div id="r-squared" class="section level1">
<h1>R squared</h1>
<p>In a frequentist context, the <span class="math inline">\(R^2\)</span> value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, <span class="math inline">\(R^2\)</span> is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an <span class="math inline">\(R^2\)</span> greater than <span class="math inline">\(100\)</span>%. <span class="citation">Gelman et al. (2019)</span> proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.</p>
<p>So in the standard regression model notation of:</p>
<p><span class="math display">\[ y_i \sim \text{Normal}(\boldsymbol X \boldsymbol \beta, \sigma),\]</span></p>
<p>the <span class="math inline">\(R^2\)</span> could be formulated as</p>
<p><span class="math display">\[ R^2 = \frac{\sigma^2_f}{\sigma^2_f + \sigma^2_e},\]</span></p>
<p>where <span class="math inline">\(\sigma^2_f=\text{var}(\boldsymbol X \boldsymbol \beta)\)</span>, and for normal models <span class="math inline">\(\sigma^2_e=\text{var}(y-\boldsymbol X \boldsymbol \beta)\)</span></p>
<pre class="r"><code>&gt; Xmat = model.matrix(~A + B, data)
&gt; wch = grep(&quot;beta&quot;, colnames(mcmc))
&gt; coefs = mcmc[, wch]
&gt; fit = coefs %*% t(Xmat)
&gt; resid = sweep(fit, 2, data$Y, &quot;-&quot;)
&gt; var_f = apply(fit, 1, var)
&gt; var_e = apply(resid, 1, var)
&gt; R2 = var_f/(var_f + var_e)
&gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = &quot;HPDinterval&quot;)
# A tibble: 1 x 5
  term  estimate std.error conf.low conf.high
  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 var1     0.905    0.0148    0.877     0.922
&gt; 
&gt; # for comparison with frequentist
&gt; summary(lm(Y ~ A + B, data))

Call:
lm(formula = Y ~ A + B, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-6.4381 -2.2244 -0.6829  2.1732  8.6607 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  51.00608    1.44814   35.22  &lt; 2e-16 ***
AGroup B    -16.25472    1.54125  -10.55 6.92e-11 ***
AGroup C    -20.65596    1.57544  -13.11 5.74e-13 ***
B            -0.48399    0.04526  -10.69 5.14e-11 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 3.44 on 26 degrees of freedom
Multiple R-squared:  0.9149,    Adjusted R-squared:  0.9051 
F-statistic: 93.22 on 3 and 26 DF,  p-value: 4.901e-14</code></pre>
</div>
<div id="dealing-with-heterogeneous-slopes" class="section level1">
<h1>Dealing with heterogeneous slopes</h1>
<p>Generate the data with heterogeneous slope effects.</p>
<pre class="r"><code>&gt; set.seed(123)
&gt; n &lt;- 10
&gt; p &lt;- 3
&gt; A.eff &lt;- c(40, -15, -20)
&gt; beta &lt;- c(-0.45, -0.1, 0.5)
&gt; sigma &lt;- 4
&gt; B &lt;- rnorm(n * p, 0, 15)
&gt; A &lt;- gl(p, n, lab = paste(&quot;Group&quot;, LETTERS[1:3]))
&gt; mm &lt;- model.matrix(~A * B)
&gt; data1 &lt;- data.frame(A = A, B = B, Y = as.numeric(c(A.eff, beta) %*% t(mm)) + rnorm(n * p, 0, 4))
&gt; data1$B &lt;- data1$B + 20
&gt; head(data1)
        A        B        Y
1 Group A 11.59287 45.48907
2 Group A 16.54734 40.37341
3 Group A 43.38062 33.05922
4 Group A 21.05763 43.03660
5 Group A 21.93932 42.41363
6 Group A 45.72597 31.17787</code></pre>
<div id="exploratory-data-analysis-1" class="section level2">
<h2>Exploratory data analysis</h2>
<pre class="r"><code>&gt; scatterplot(Y ~ B | A, data = data1)</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/exp_het-1.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; boxplot(Y ~ A, data1)</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/exp_het-2.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; # OR via ggplot
&gt; ggplot(data1, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/exp_het-3.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; ggplot(data1, aes(y = Y, x = A)) + geom_boxplot()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/exp_het-4.png" width="672" /></p>
<p>The slopes (<span class="math inline">\(Y\)</span> vs B trends) do appear to differ between treatment groups - in particular, Group C seems to portray a different trend to Groups A and B.</p>
<pre class="r"><code>&gt; anova(lm(Y ~ B * A, data = data1))
Analysis of Variance Table

Response: Y
          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
B          1  442.02  442.02  41.380 1.187e-06 ***
A          2 2760.60 1380.30 129.217 1.418e-13 ***
B:A        2  285.75  142.87  13.375 0.0001251 ***
Residuals 24  256.37   10.68                      
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>There is strong evidence to suggest that the assumption of equal slopes is violated.</p>
</div>
<div id="fitting-the-model" class="section level2">
<h2>Fitting the model</h2>
<pre class="r"><code>&gt; modelString2 = &quot;
+   model {
+   #Likelihood
+   for (i in 1:n) {
+   y[i]~dnorm(mean[i],tau)
+   mean[i] &lt;- inprod(beta[],X[i,])
+   }
+   #Priors
+   for (i in 1:ngroups) {
+   beta[i] ~ dnorm(0, 1.0E-6) 
+   }
+   sigma ~ dunif(0, 100)
+   tau &lt;- 1 / (sigma * sigma)
+   }
+   &quot;
&gt; 
&gt; ## write the model to a text file
&gt; writeLines(modelString2, con = &quot;ancovaModel2.txt&quot;)</code></pre>
<p>Arrange the data as a list (as required by <code>JAGS</code>). As input, <code>JAGS</code> will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.</p>
<pre class="r"><code>&gt; X &lt;- model.matrix(~A * B, data1)
&gt; data1.list &lt;- with(data1, list(y = Y, X = X, n = nrow(data1), ngroups = ncol(X)))</code></pre>
<p>Define the nodes (parameters and derivatives) to monitor and the chain parameters.</p>
<pre class="r"><code>&gt; params &lt;- c(&quot;beta&quot;, &quot;sigma&quot;)
&gt; nChains = 2
&gt; burnInSteps = 3000
&gt; thinSteps = 1
&gt; numSavedSteps = 15000  #across all chains
&gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&gt; nIter
[1] 10500</code></pre>
<p>Start the <code>JAGS</code> model (check the model, load data into the model, specify the number of chains and compile the model).</p>
<pre class="r"><code>&gt; data1.r2jags &lt;- jags(data = data1.list, inits = NULL, parameters.to.save = params,
+     model.file = &quot;ancovaModel2.txt&quot;, n.chains = nChains, n.iter = nIter,
+     n.burnin = burnInSteps, n.thin = thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 30
   Unobserved stochastic nodes: 7
   Total graph size: 286

Initializing model
&gt; 
&gt; print(data1.r2jags)
Inference for Bugs model at &quot;ancovaModel2.txt&quot;, fit using jags,
 2 chains, each with 10500 iterations (first 3000 discarded)
 n.sims = 15000 iterations saved
         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
beta[1]   48.194   2.035  44.200  46.864  48.200  49.531  52.217 1.001 15000
beta[2]  -10.562   2.884 -16.240 -12.453 -10.586  -8.688  -4.814 1.001  8100
beta[3]  -26.538   2.568 -31.636 -28.207 -26.525 -24.858 -21.431 1.001 15000
beta[4]   -0.351   0.082  -0.512  -0.404  -0.351  -0.297  -0.188 1.001 15000
beta[5]   -0.271   0.110  -0.491  -0.344  -0.270  -0.198  -0.055 1.001 15000
beta[6]    0.270   0.117   0.039   0.194   0.270   0.346   0.500 1.001 15000
sigma      3.454   0.535   2.601   3.074   3.396   3.757   4.689 1.002  1800
deviance 157.761   4.417 151.465 154.544 156.990 160.166 168.119 1.001  3000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 9.8 and DIC = 167.5
DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
</div>
<div id="mcmc-diagnostics-1" class="section level2">
<h2>MCMC diagnostics</h2>
<pre class="r"><code>&gt; denplot(data1.r2jags, parms = c(&quot;beta&quot;))</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_diag_het-1.png" width="672" /></p>
<pre class="r"><code>&gt; traplot(data1.r2jags, parms = c(&quot;beta&quot;))</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_diag_het-2.png" width="672" /></p>
<p>Trace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters (such as <span class="math inline">\(\beta\)</span>s).</p>
<pre class="r"><code>&gt; data1.mcmc = as.mcmc(data1.r2jags)
&gt; #Raftery diagnostic
&gt; raftery.diag(data1.mcmc)
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta[1]  2        3853  3746         1.030     
 beta[2]  2        3689  3746         0.985     
 beta[3]  2        3895  3746         1.040     
 beta[4]  2        3649  3746         0.974     
 beta[5]  2        3918  3746         1.050     
 beta[6]  2        3770  3746         1.010     
 deviance 2        3938  3746         1.050     
 sigma    4        5018  3746         1.340     


[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta[1]  2        3853  3746         1.030     
 beta[2]  2        3570  3746         0.953     
 beta[3]  2        3811  3746         1.020     
 beta[4]  2        3770  3746         1.010     
 beta[5]  2        3770  3746         1.010     
 beta[6]  2        3895  3746         1.040     
 deviance 2        3981  3746         1.060     
 sigma    4        5131  3746         1.370     </code></pre>
<p>The Raftery diagnostics for each chain estimate that we would require no more than <span class="math inline">\(5000\)</span> samples to reach the specified level of confidence in convergence. As we have <span class="math inline">\(10500\)</span> samples, we can be confidence that convergence has occurred.</p>
<pre class="r"><code>&gt; #Autocorrelation diagnostic
&gt; autocorr.diag(data1.mcmc)
            beta[1]      beta[2]     beta[3]      beta[4]       beta[5]
Lag 0   1.000000000  1.000000000 1.000000000  1.000000000  1.0000000000
Lag 1  -0.002520665 -0.007698073 0.001992162  0.000509790 -0.0005326877
Lag 5   0.001007950  0.009095032 0.001511518 -0.006890623  0.0025773251
Lag 10 -0.011280919  0.007907450 0.005969613 -0.006999313  0.0040454668
Lag 50 -0.012861369 -0.019813696 0.002604518 -0.008791380 -0.0136623372
            beta[6]     deviance        sigma
Lag 0   1.000000000  1.000000000 1.0000000000
Lag 1   0.004381248  0.332075434 0.4518687724
Lag 5  -0.001182603  0.032092130 0.0351574955
Lag 10 -0.004191097  0.003338842 0.0005457235
Lag 50  0.002636154 -0.005426687 0.0039447210</code></pre>
</div>
<div id="model-validation-1" class="section level2">
<h2>Model validation</h2>
<pre class="r"><code>&gt; mcmc = data1.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%
+     dplyr:::select(contains(&quot;beta&quot;), sigma) %&gt;% as.matrix
&gt; # generate a model matrix
&gt; newdata1 = data1
&gt; Xmat = model.matrix(~A * B, newdata1)
&gt; ## get median parameter estimates
&gt; coefs = apply(mcmc[, 1:6], 2, median)
&gt; fit = as.vector(coefs %*% t(Xmat))
&gt; resid = data1$Y - fit
&gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_residuals_het-1.png" width="672" /></p>
<p>Residuals against predictors</p>
<pre class="r"><code>&gt; mcmc = data1.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%
+     dplyr:::select(contains(&quot;beta&quot;), sigma) %&gt;% as.matrix
&gt; # generate a model matrix
&gt; newdata1 = newdata1
&gt; Xmat = model.matrix(~A * B, newdata1)
&gt; ## get median parameter estimates
&gt; coefs = apply(mcmc[, 1:6], 2, median)
&gt; fit = as.vector(coefs %*% t(Xmat))
&gt; resid = data1$Y - fit
&gt; newdata1 = newdata1 %&gt;% cbind(fit, resid)
&gt; ggplot(newdata1) + geom_point(aes(y = resid, x = A)) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_residuals2_het-1.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; ggplot(newdata1) + geom_point(aes(y = resid, x = B)) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_residuals2_het-2.png" width="672" /></p>
<p>And now for studentised residuals</p>
<pre class="r"><code>&gt; mcmc = data1.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%
+     dplyr:::select(contains(&quot;beta&quot;), sigma) %&gt;% as.matrix
&gt; # generate a model matrix
&gt; newdata1 = data1
&gt; Xmat = model.matrix(~A * B, newdata1)
&gt; ## get median parameter estimates
&gt; coefs = apply(mcmc[, 1:6], 2, median)
&gt; fit = as.vector(coefs %*% t(Xmat))
&gt; resid = data1$Y - fit
&gt; sresid = resid/sd(resid)
&gt; ggplot() + geom_point(data1 = NULL, aes(y = sresid, x = fit)) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_residuals3_het-1.png" width="672" /></p>
<p>For this simple model, the studentised residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.</p>
<pre class="r"><code>&gt; mcmc = data1.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%
+     dplyr:::select(contains(&quot;beta&quot;), sigma) %&gt;% as.matrix
&gt; # generate a model matrix
&gt; Xmat = model.matrix(~A * B, data1)
&gt; ## get median parameter estimates
&gt; coefs = mcmc[, 1:6]
&gt; fit = coefs %*% t(Xmat)
&gt; ## draw samples from this model
&gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data1), fit[i,
+     ], mcmc[i, &quot;sigma&quot;]))
&gt; newdata1 = data.frame(A = data1$A, B = data1$B, yRep) %&gt;% gather(key = Sample,
+     value = Value, -A, -B)
&gt; ggplot(newdata1) + geom_violin(aes(y = Value, x = A, fill = &quot;Model&quot;),
+     alpha = 0.5) + geom_violin(data = data1, aes(y = Y, x = A,
+     fill = &quot;Obs&quot;), alpha = 0.5) + geom_point(data = data1, aes(y = Y,
+     x = A), position = position_jitter(width = 0.1, height = 0),
+     color = &quot;black&quot;) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_rep_het-1.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; ggplot(newdata1) + geom_violin(aes(y = Value, x = B, fill = &quot;Model&quot;,
+     group = B, color = A), alpha = 0.5) + geom_point(data = data1,
+     aes(y = Y, x = B, group = B, color = A)) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_rep_het-2.png" width="672" /></p>
<p>The predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.</p>
<pre class="r"><code>&gt; mcmc_intervals(data1.r2jags$BUGSoutput$sims.matrix, regex_pars = &quot;beta|sigma&quot;)</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_rep2_het-1.png" width="672" /></p>
<pre class="r"><code>&gt; mcmc_areas(data1.r2jags$BUGSoutput$sims.matrix, regex_pars = &quot;beta|sigma&quot;)</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_rep2_het-2.png" width="672" /></p>
</div>
<div id="parameter-estimates-1" class="section level2">
<h2>Parameter estimates</h2>
<p>First, we look at the results from the additive model.</p>
<pre class="r"><code>&gt; print(data1.r2jags)
Inference for Bugs model at &quot;ancovaModel2.txt&quot;, fit using jags,
 2 chains, each with 10500 iterations (first 3000 discarded)
 n.sims = 15000 iterations saved
         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
beta[1]   48.194   2.035  44.200  46.864  48.200  49.531  52.217 1.001 15000
beta[2]  -10.562   2.884 -16.240 -12.453 -10.586  -8.688  -4.814 1.001  8100
beta[3]  -26.538   2.568 -31.636 -28.207 -26.525 -24.858 -21.431 1.001 15000
beta[4]   -0.351   0.082  -0.512  -0.404  -0.351  -0.297  -0.188 1.001 15000
beta[5]   -0.271   0.110  -0.491  -0.344  -0.270  -0.198  -0.055 1.001 15000
beta[6]    0.270   0.117   0.039   0.194   0.270   0.346   0.500 1.001 15000
sigma      3.454   0.535   2.601   3.074   3.396   3.757   4.689 1.002  1800
deviance 157.761   4.417 151.465 154.544 156.990 160.166 168.119 1.001  3000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 9.8 and DIC = 167.5
DIC is an estimate of expected predictive error (lower deviance is better).
&gt; 
&gt; # OR
&gt; tidyMCMC(as.mcmc(data1.r2jags), conf.int = TRUE, conf.method = &quot;HPDinterval&quot;)
# A tibble: 8 x 5
  term     estimate std.error conf.low conf.high
  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 beta[1]    48.2      2.03    44.2      52.2   
2 beta[2]   -10.6      2.88   -16.3      -4.94  
3 beta[3]   -26.5      2.57   -31.6     -21.4   
4 beta[4]    -0.351    0.0816  -0.510    -0.187 
5 beta[5]    -0.271    0.110   -0.491    -0.0541
6 beta[6]     0.270    0.117    0.0436    0.503 
7 deviance  158.       4.42   151.      167.    
8 sigma       3.45     0.535    2.51      4.50  </code></pre>
<p><strong>Conclusions</strong></p>
<ul>
<li><p>The intercept of the first group (Group A) is <span class="math inline">\(48.2\)</span>.</p></li>
<li><p>The mean of the second group (Group B) is <span class="math inline">\(-10.6\)</span> units greater than (A).</p></li>
<li><p>The mean of the third group (Group C) is <span class="math inline">\(-26.5\)</span> units greater than (A).</p></li>
<li><p>A one unit increase in B in Group A is associated with a <span class="math inline">\(-0.351\)</span> units increase in <span class="math inline">\(Y\)</span>.</p></li>
<li><p>difference in slope between Group B and Group A <span class="math inline">\(-0.270\)</span>.</p></li>
<li><p>difference in slope between Group C and Group A <span class="math inline">\(0.270\)</span>.</p></li>
</ul>
<p>The <span class="math inline">\(95\)</span>% confidence interval for the effects of Group B, Group C and the partial slope associated with B do not overlapp with <span class="math inline">\(0\)</span> implying a significant difference between group A and groups B, C (at the mean level of predictor B) and a significant negative relationship with B (for Group A). The slope associated with Group B was not found to be significantly different from that associated with Group A, however, the slope associated with Group C was found to be significantly less negative than the slope associated with Group A. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.</p>
<pre class="r"><code>&gt; ## since values are less than zero
&gt; mcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, &quot;beta[2]&quot;])  # effect of (B-A = 0)
[1] 0.0009333333
&gt; mcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, &quot;beta[3]&quot;])  # effect of (C-A = 0)
[1] 0
&gt; mcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, &quot;beta[4]&quot;])  # effect of (slope = 0)
[1] 0.0003333333
&gt; mcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, &quot;beta[5]&quot;])  # effect of (slopeB - slopeA = 0)
[1] 0.0152
&gt; mcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, &quot;beta[6]&quot;])  # effect of (slopeC - slopeA = 0)
[1] 0.0232
&gt; mcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, 2:6])  # effect of (model)
[1] 0</code></pre>
<p>There is evidence that the reponse differs between the groups.</p>
</div>
<div id="graphical-summaries-1" class="section level2">
<h2>Graphical summaries</h2>
<pre class="r"><code>&gt; mcmc = data1.r2jags$BUGSoutput$sims.matrix
&gt; ## Calculate the fitted values
&gt; newdata1 = expand.grid(A = levels(data1$A), B = seq(min(data1$B), max(data1$B),
+     len = 100))
&gt; Xmat = model.matrix(~A * B, newdata1)
&gt; coefs = mcmc[, c(&quot;beta[1]&quot;, &quot;beta[2]&quot;, &quot;beta[3]&quot;, &quot;beta[4]&quot;, &quot;beta[5]&quot;,
+     &quot;beta[6]&quot;)]
&gt; fit = coefs %*% t(Xmat)
&gt; newdata1 = newdata1 %&gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = &quot;HPDinterval&quot;))
&gt; 
&gt; ggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_ribbon(aes(ymin = conf.low,
+     ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(&quot;Y&quot;) +
+     scale_x_continuous(&quot;B&quot;) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_post1_het-1.png" width="672" /></p>
<p>As this is simple single factor ANOVA, we can simple add the raw data to this figure. For more complex designs with additional predictors, it is necessary to plot partial residuals.</p>
<pre class="r"><code>&gt; ## Calculate partial residuals fitted values
&gt; fdata1 = rdata1 = data1
&gt; fMat = rMat = model.matrix(~A * B, fdata1)
&gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))
&gt; resid = as.vector(data1$Y - apply(coefs, 2, median) %*% t(rMat))
&gt; rdata1 = rdata1 %&gt;% mutate(partial.resid = resid + fit)
&gt; 
&gt; ggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_point(data = rdata1,
+     aes(y = partial.resid, x = B, color = A)) + geom_ribbon(aes(ymin = conf.low,
+     ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(&quot;Y&quot;) +
+     scale_x_continuous(&quot;B&quot;) + theme_classic()</code></pre>
<p><img src="/JAGS/ancova-jags/2020-02-01-ancova-jags_files/figure-html/mcmc_post2_het-1.png" width="672" /></p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references hanging-indent">
<div id="ref-gelman2019r">
<p>Gelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” <em>The American Statistician</em> 73 (3): 307–9.</p>
</div>
<div id="ref-gelman2005analysis">
<p>Gelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” <em>The Annals of Statistics</em> 33 (1): 1–53.</p>
</div>
<div id="ref-plummer2004jags">
<p>Plummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”</p>
</div>
<div id="ref-su2015package">
<p>Su, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” <em>R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags</em>.</p>
</div>
</div>
</div>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/tutorials/">tutorials</a>
  
  <a class="badge badge-light" href="/tags/jags/">JAGS</a>
  
  <a class="badge badge-light" href="/tags/ancova/">ancova</a>
  
  <a class="badge badge-light" href="/tags/factor-analysis/">factor analysis</a>
  
</div>



    
      








  
  
    
  
  





  
  
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="/authors/andrea-gabrio/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/stan/ancova-stan/ancova-stan/">Analysis of Covariance - STAN</a></li>
          
          <li><a href="/jags/single-factor-anova-jags/single-factor-anova-jags/">Single Factor Anova - JAGS</a></li>
          
          <li><a href="/stan/single-factor-anova-stan/single-factor-anova-stan/">Single Factor Anova - STAN</a></li>
          
          <li><a href="/jags/multiple-linear-regression-jags/multiple-linear-regression-jags/">Multiple Linear Regression - JAGS</a></li>
          
          <li><a href="/jags/simple-linear-regression-jags/simple-linear-regression-jags/">Simple Linear Regression - JAGS</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3394a224b26ce58ff36f44c54743e0ab.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">

  <p class="powered-by">
    &#169; Andrea Gabrio &middot;
    <code>2020</code> 
    &middot; Based on the 
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
