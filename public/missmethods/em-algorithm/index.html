<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.1">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Andrea Gabrio">

  
  
  
    
  
  <meta name="description" content="Patterns of incomplete data in practice often do not have the forms that allow explicit Maximum Likelihood(ML) estimates to be calculated. Suppose we have a model for the complete data \(Y\), with density \(f(Y\mid \theta)\), indexed by the set of unknown parameters \(\theta\). Writing \(Y=(Y_0,Y_1)\) in terms of the observed \(Y_0\) and missing \(Y_1\) components, and assuming that the missingness mechanism is Missing At Random(MAR), we want to maximise the likelihood">

  
  <link rel="alternate" hreflang="en-us" href="/missmethods/em-algorithm/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:450,700|Oswald+Sans:600,700|Roboto+Mono:550,700">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.548c5488bf2acb3767aba941aacbd58f.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academic.3f1befffb0882d6c3372ec9dda375740.css">
  

  
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/missmethods/em-algorithm/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Andrea Gabrio">
  <meta property="og:url" content="/missmethods/em-algorithm/">
  <meta property="og:title" content="Expectation Maximisation Algorithm | Andrea Gabrio">
  <meta property="og:description" content="Patterns of incomplete data in practice often do not have the forms that allow explicit Maximum Likelihood(ML) estimates to be calculated. Suppose we have a model for the complete data \(Y\), with density \(f(Y\mid \theta)\), indexed by the set of unknown parameters \(\theta\). Writing \(Y=(Y_0,Y_1)\) in terms of the observed \(Y_0\) and missing \(Y_1\) components, and assuming that the missingness mechanism is Missing At Random(MAR), we want to maximise the likelihood"><meta property="og:image" content="/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2016-04-27T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2016-04-27T00:00:00&#43;00:00">
  

  


  





  <title>Expectation Maximisation Algorithm | Andrea Gabrio</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Andrea Gabrio</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/post/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/research/"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/publication/"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/talk/"><span>Talks</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Software</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/missingHE/"><span>missingHE</span></a>
            </li>
            
          </ul>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Tutorials</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/JAGS/"><span>JAGS</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/OpenBUGS/"><span>OpenBUGS</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/STAN/"><span>STAN</span></a>
            </li>
            
          </ul>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/missingdata/"><span>Missing Data</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Expectation Maximisation Algorithm</h1>

  

  
    



<meta content="2016-04-27 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2016-04-27 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>Apr 27, 2016</time>
  </span>
  

  

  

  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    <a href="/categories/rubric/">rubric</a></span>
  

  
    

  

</div>

    











  



<div class="btn-links mb-3">
  
  








  





<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/missmethods/em-algorithm/emalgorithm.bib">
  Cite
</button>















</div>


  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      


<p>Patterns of incomplete data in practice often do not have the forms that allow explicit <em>Maximum Likelihood</em>(ML) estimates to be calculated. Suppose we have a model for the complete data <span class="math inline">\(Y\)</span>, with density <span class="math inline">\(f(Y\mid \theta)\)</span>, indexed by the set of unknown parameters <span class="math inline">\(\theta\)</span>. Writing <span class="math inline">\(Y=(Y_0,Y_1)\)</span> in terms of the observed <span class="math inline">\(Y_0\)</span> and missing <span class="math inline">\(Y_1\)</span> components, and assuming that the missingness mechanism is <em>Missing At Random</em>(MAR), we want to maximise the likelihood</p>
<p>\[
L\left(\theta \mid Y_0 \right) = \int f\left(Y_0, Y_1 \mid \theta \right)dY_1
\]</p>
<p>with respect to <span class="math inline">\(\theta\)</span>. When the likelihood is differentiable and unimodal, ML estimates can be found by solving the likelihood equation</p>
<p>\[
D_l\left(\theta \mid Y_0 \right) \equiv \frac{\partial ln L\left(\theta \mid Y_0 \right)}{\partial \theta} = 0,
\]</p>
<p>while, if a closed-form solution cannot be found, iterative methods can be applied. One of these methods is the popular <em>Expectation Maximisation</em>(EM) algorithm (<span class="citation">Dempster, Laird, and Rubin (1977)</span>).</p>
<p>The EM algorithm is a general iterative method for ML estimation in incomplete data problems. The basic idea behind it is based on a sequence of steps:</p>
<ul>
<li><p>Replace missing values by estimated values</p></li>
<li><p>Estimate the parameters</p></li>
<li><p>Re-estimate the missing values assuming the new parameter estimates are correct</p></li>
<li><p>Re-estimate parameters</p></li>
</ul>
<p>The procedure is then iterated until apparent convergence. Each iteration of EM consists of an <em>expectation step</em> (E step) and a <em>maximisation step</em> (M step) which ensure that, under general conditions, each iteration increases the loglikelihood <span class="math inline">\(l(\theta \mid Y_0)\)</span>. In addition, if the loglikelihood is bounded, the sequence <span class="math inline">\(\{l(\theta_t \mid Y_0), t=(0,1,\ldots)\}\)</span> converges to a stationary value of <span class="math inline">\(l(\theta \mid Y_0)\)</span>.</p>
<div id="the-e-step-and-the-m-step" class="section level2">
<h2>The E step and the M step</h2>
<p>The M step simply consists of performing ML estimation of <span class="math inline">\(\theta\)</span> as if there were no missing data, that is, after they had been filled in. The E step finds the conditional expectation of the missing values given the observed data and current estimated parameters. In practice, EM does not necessarily substitute the missing values themselves but its key idea is that they are generally not <span class="math inline">\(Y_0\)</span> but the functions of <span class="math inline">\(Y_0\)</span> appearing in the complete data loglikelihood <span class="math inline">\(l(\theta \mid Y)\)</span>. Specifically, let <span class="math inline">\(\theta_t\)</span> be the current estimate of <span class="math inline">\(\theta\)</span>, then the E step finds the expected complete data loglikelihood if <span class="math inline">\(\theta\)</span> were <span class="math inline">\(\theta_t\)</span>:</p>
<p>\[
Q\left(\theta \mid \theta_t \right) = \int l\left(\theta \mid Y \right)f\left(Y_0 \mid Y_1 , \theta = \theta_t \right)dY_0.
\]</p>
<p>The M step determines <span class="math inline">\(\theta_{t+1}\)</span> by maximising this expected complete data loglikelihood:</p>
<p>\[
Q\left(\theta_{t+1} \mid \theta_t \right) \geq Q\left(\theta \mid \theta_t \right),
\]</p>
<p>for all <span class="math inline">\(\theta\)</span>.</p>
<div id="univariate-normal-data-example" class="section level3">
<h3>Univariate Normal Data Example</h3>
<p>Suppose <span class="math inline">\(y_i\)</span> form a an iid sample from a Normal distribution with population mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, for <span class="math inline">\(i=1,\ldots,n_{cc}\)</span> observed units and <span class="math inline">\(i=n_{cc}+1,\ldots,n\)</span> missing units. Under the assumption that the missingness mechanism is ignorable, the expectation of each missing <span class="math inline">\(y_i\)</span> given <span class="math inline">\(Y_{obs}\)</span> and <span class="math inline">\(\theta=(\mu,\sigma^2)\)</span> is <span class="math inline">\(\mu\)</span>. Since the loglikelihood based on all <span class="math inline">\(y_i\)</span> is linear in the sufficient statistics <span class="math inline">\(\sum_{i=1}^n y_i\)</span> and <span class="math inline">\(\sum_{i=1}^n y^2_i\)</span>, the E step of the algorithm calculates</p>
<p>\[
E\left(\sum_{i=1}^{n}y_i \mid \theta_t, Y_0 \right) = \sum_{i=1}^{n_{cc}}y_i + (n-n_{cc})\mu_t
\]</p>
<p>and</p>
<p>\[
E\left(\sum_{i=1}^{n}y^2_i \mid \theta_t, Y_0 \right) = \sum_{i=1}^{n_{cc}}y^2_i + (n-n_{cc})\left(\mu^2_t + \sigma^2_t \right)
\]</p>
<p>for current estimates <span class="math inline">\(\theta_t=(\mu_t,\sigma_t)\)</span> of the parameters. Note that simply substituting <span class="math inline">\(\mu_t\)</span> for the missing values <span class="math inline">\(y_{n_{cc}+1},\ldots,y_n\)</span> is not correct since the term <span class="math inline">\((n-n_{cc})(\sigma_t^2)\)</span> is omitted. Without missing data, the ML estimate of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are <span class="math inline">\(\frac{\sum_{i=1}^ny_i}{n}\)</span> and <span class="math inline">\(\frac{\sum_{i=1}^ny^2_i}{n}-\left(\frac{\sum_{i=1}^ny_i}{n}\right)^2\)</span>, respectively. The M step uses the same expressions based on the current expectations of the sufficient statistics calculated in the E step. Thus, the M step calculates</p>
<p>\[
\mu_{t+1} = \frac{E\left(\sum_{i=1}^n y_i \mid \theta_t, Y_0 \right)}{n}
\]</p>
<p>and</p>
<p>\[
\sigma^2_{t+1} = \frac{E\left(\sum_{i=1}^n y^2_i \mid \theta_t, Y_0 \right)}{n} - \mu^2_{t+1}.
\]</p>
<p>Setting <span class="math inline">\(\mu_t=\mu_{t+1}=\hat{\mu}\)</span> and <span class="math inline">\(\sigma_t=\sigma_{t+1}=\hat{\sigma}\)</span> in these equations shows that a fixed point of these iterations is <span class="math inline">\(\hat{\mu}=\frac{\sum_{i=1}^{n_{cc}}y_i}{n_{cc}}\)</span> and <span class="math inline">\(\hat{\sigma}^2=\frac{\sum_{i=1}^{n_{cc}}y^2_i}{n_{cc}} - \hat{\mu}^2\)</span>, which are the ML estimates of the parameters from <span class="math inline">\(Y_0\)</span> assuming MAR and distinctness of the parameters.</p>
</div>
</div>
<div id="extensions-of-em" class="section level2">
<h2>Extensions of EM</h2>
<p>There are a variety of applications where the M step does not have a simple computational form. In such cases, one way to avoid an iterative M step is to increase the Q function, rather than maximising it at each iteration, which corresponds to a <em>Generalised Expectation Maximisation</em>(GEM) algorithm. GEM inceases the likelihood at each iteration but appropriate convergence is not guaranteed without further specification of the process of increasing the Q function. One specific case of GEM is the <em>Expectation Conditional Maximisation</em>(ECM) algorithm (<span class="citation">Meng and Rubin (1993)</span>), which replaces the M step with a sequence of <span class="math inline">\(S\)</span> conditional maximisation (CM) steps, each of which maximises the Q function over <span class="math inline">\(\theta\)</span> but with some vector function of <span class="math inline">\(\theta\)</span>, say <span class="math inline">\(g_s(\theta)\)</span>, fixed at its previous values for <span class="math inline">\(s=1,\ldots,S\)</span>. Very briefly, assume that we have a parameter <span class="math inline">\(\theta\)</span> that can be partitioned into subvectors <span class="math inline">\(\theta=(\theta_1,\ldots,\theta_S)\)</span>, then we can take the <span class="math inline">\(s\)</span>-th of the CM steps to be maximisation with respect to <span class="math inline">\(\theta_s\)</span> with all other parameters held fixed. Alternatively, it may be useful to take the <span class="math inline">\(s\)</span>-th of the CM steps to be simultaneous maximisation over all of the subvectors expect <span class="math inline">\(\theta_s\)</span>, which is fixed. Because the ECM increases Q, it belongs to the class of GEM algorithms and therefore monotonically increases the likelihood of <span class="math inline">\(\theta\)</span>. When the set of functions <span class="math inline">\(g\)</span> is “space filling” in the sense that it allows unconstrained maximisation over <span class="math inline">\(\theta\)</span> in its parameter space, ECM converges to a stationary point under the same conditions ensuring convergence of EM.</p>
<p>The <em>Expectation Conditional Maximisation Either</em>(ECME) algorithm (<span class="citation">Liu and Rubin (1994)</span>) is another version of GEM, which replaces some of the CM steps of ECM, maximising the constrained expected complete data loglikelihood function, with steps that maximise the correspondingly constrained actual likelihood function. The algorithm has stable monotone convergence and basic simplicity implementation relative to competing faster converging methods, and can have faster convergence rate than EM or ECM, measured using either the number of iterations or actual computer time. The The <em>Alternative Expectation Conditional Maximisation</em>(AECM) algorithm (<span class="citation">Meng and Van Dyk (1997)</span>) builds on the ECME idea by maximising functions other than Q or L in particular CM steps, corresponding to varying definitions of what constitutes missing data. An iteration of AECM consists of cycles, each consisting of an E step with a particular definition of complete and missing data, followed by CM steps, which can result in enhanced computational efficiency.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-dempster1977maximum">
<p>Dempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the Em Algorithm.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 39 (1): 1–22.</p>
</div>
<div id="ref-liu1994ecme">
<p>Liu, Chuanhai, and Donald B Rubin. 1994. “The Ecme Algorithm: A Simple Extension of Em and Ecm with Faster Monotone Convergence.” <em>Biometrika</em> 81 (4): 633–48.</p>
</div>
<div id="ref-meng1993maximum">
<p>Meng, Xiao-Li, and Donald B Rubin. 1993. “Maximum Likelihood Estimation via the Ecm Algorithm: A General Framework.” <em>Biometrika</em> 80 (2): 267–78.</p>
</div>
<div id="ref-meng1997algorithm">
<p>Meng, Xiao-Li, and David Van Dyk. 1997. “The Em Algorithm—an Old Folk-Song Sung to a Fast New Tune.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 59 (3): 511–67.</p>
</div>
</div>
</div>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/expectation-maximisation-algorithm/">Expectation Maximisation Algorithm</a>
  
  <a class="badge badge-light" href="/tags/likelihood-based-methods-ignorable/">Likelihood Based Methods Ignorable</a>
  
</div>



    
      



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/missmethods/bayesian-methods/">Bayesian Iterative Simulation Methods</a></li>
          
          <li><a href="/missmethods/likelihood-based-methods-ignorable2/">Introduction to Bayesian Inference</a></li>
          
          <li><a href="/missmethods/likelihood-based-methods-ignorable/">Introduction to Maximum Likelihood Estimation</a></li>
          
          <li><a href="/missmethods/likelihood-based-methods-ignorable3/">Likelihood Based Inference with Incomplete Data</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script async defer src="//maps.googleapis.com/maps/api/js?key=AIzaSyAYShZdrjjE_TojzlN30gOZCZjvTBD3b3c"></script>
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/gmaps.js/0.4.25/gmaps.min.js" integrity="sha256-7vjlAeb8OaTrCXZkCNun9djzuB2owUsaO72kXaFDBJs=" crossorigin="anonymous"></script>
      
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.9ef1b53ee2bde6c7f33b150c6ba4d452.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">

  <p class="powered-by">
    

    &#169; Andrea Gabrio 2019. Based on the 
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
