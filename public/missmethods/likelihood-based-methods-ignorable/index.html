<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.1">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Andrea Gabrio">

  
  
  
    
  
  <meta name="description" content="A possible approach to analyse missing data is to use methods based on the likelihood function under specific modelling assumptions. In this section, I review maximum likelihood methods based on fully observed data alone.
Maximum Likelihood Methods for Complete DataLet \(Y\) denote the set of data, which are assumed to be generated according to a certain probability density function \(f(Y= y,\mid \theta)=f(y \mid \theta)\) indexed by the set of parameters \(\theta\), which lies on the parameter space \(\Theta\) (i.">

  
  <link rel="alternate" hreflang="en-us" href="/missmethods/likelihood-based-methods-ignorable/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:450,700|Oswald+Sans:600,700|Roboto+Mono:550,700">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.a71200610c21759266bff163bb521d95.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academic.4fe06d02a41da8ea4cf58ceef0b5213f.css">
  

  
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/missmethods/likelihood-based-methods-ignorable/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Andrea Gabrio">
  <meta property="og:url" content="/missmethods/likelihood-based-methods-ignorable/">
  <meta property="og:title" content="Introduction to Maximum Likelihood Estimation | Andrea Gabrio">
  <meta property="og:description" content="A possible approach to analyse missing data is to use methods based on the likelihood function under specific modelling assumptions. In this section, I review maximum likelihood methods based on fully observed data alone.
Maximum Likelihood Methods for Complete DataLet \(Y\) denote the set of data, which are assumed to be generated according to a certain probability density function \(f(Y= y,\mid \theta)=f(y \mid \theta)\) indexed by the set of parameters \(\theta\), which lies on the parameter space \(\Theta\) (i."><meta property="og:image" content="/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2016-04-27T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2016-04-27T00:00:00&#43;00:00">
  

  


  





  <title>Introduction to Maximum Likelihood Estimation | Andrea Gabrio</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Andrea Gabrio</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/post/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/research/"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/publication/"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/talk/"><span>Talks</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Software</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/missingHE/"><span>missingHE</span></a>
            </li>
            
          </ul>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Tutorials</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/JAGS/"><span>JAGS</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/OpenBUGS/"><span>OpenBUGS</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/STAN/"><span>STAN</span></a>
            </li>
            
          </ul>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/missingdata/"><span>Missing Data</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Introduction to Maximum Likelihood Estimation</h1>

  

  
    



<meta content="2016-04-27 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2016-04-27 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>Apr 27, 2016</time>
  </span>
  

  

  

  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    <a href="/categories/rubric/">rubric</a></span>
  

  
    

  

</div>

    











  



<div class="btn-links mb-3">
  
  








  





<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/missmethods/likelihood-based-methods-ignorable/mle.bib">
  Cite
</button>















</div>


  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      


<p>A possible approach to analyse missing data is to use methods based on the likelihood function under specific modelling assumptions. In this section, I review maximum likelihood methods based on fully observed data alone.</p>
<div id="maximum-likelihood-methods-for-complete-data" class="section level2">
<h2>Maximum Likelihood Methods for Complete Data</h2>
<p>Let <span class="math inline">\(Y\)</span> denote the set of data, which are assumed to be generated according to a certain probability density function <span class="math inline">\(f(Y= y,\mid \theta)=f(y \mid \theta)\)</span> indexed by the set of parameters <span class="math inline">\(\theta\)</span>, which lies on the parameter space <span class="math inline">\(\Theta\)</span> (i.e. set of values of <span class="math inline">\(\theta\)</span> for which <span class="math inline">\(f(y\mid \theta)\)</span> is a proper density function). The <em>Likelihood</em> function, indicated with <span class="math inline">\(L(\theta \mid y)\)</span>, is defined as any function of <span class="math inline">\(\theta \in \Theta\)</span> proportional that is to <span class="math inline">\(f(y \mid \theta)\)</span>. Note that, in contrast to the density function which is defined as a function of the data <span class="math inline">\(Y\)</span> given the values of the parameters <span class="math inline">\(\theta\)</span>, instead the likelihood is defined as a function of the parameters <span class="math inline">\(\theta\)</span> for fixed data <span class="math inline">\(y\)</span>. In addition, the <em>loglikelihood</em> function, indicated with <span class="math inline">\(l(\theta\mid y)\)</span> is defined as the natural logarithm of <span class="math inline">\(L(\theta \mid y)\)</span>.</p>
<div id="univariate-normal-example" class="section level3">
<h3>Univariate Normal Example</h3>
<p>The joint density function of <span class="math inline">\(n\)</span> independent and identially distributed units <span class="math inline">\(y=(y_1,\ldots,y_n)\)</span> from a Normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, is</p>
<p>\[
f(y \mid \mu, \sigma^2)=\frac{1}{\sqrt{\left(2\pi\sigma^2\right)^n}}\text{exp}\left(-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2} \right),
\]</p>
<p>and therefore the loglikelihood is</p>
<p>\[
l(\mu, \sigma^2 \mid y)= -\frac{n}{2}\text{ln}(2\pi)-\frac{n}{2}\text{ln}(\sigma^2)-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2},
\]</p>
<p>which is considered as a function of <span class="math inline">\(\theta=(\mu,\sigma^2)\)</span> for fixed data <span class="math inline">\(y\)</span>.</p>
</div>
<div id="multivariate-normal-example" class="section level3">
<h3>Multivariate Normal Example</h3>
<p>If the sample considered has dimension <span class="math inline">\(J&gt;1\)</span>, e.g. we have a set of idependent and identically distributed variables <span class="math inline">\(y=(y_{ij})\)</span>, for <span class="math inline">\(i=1,\ldots,n\)</span> units and <span class="math inline">\(j=1,\ldots,J\)</span> variables, which comes from a Multivariate Normal distribution with mean vector <span class="math inline">\(\mu=(\mu_1,\ldots\mu_J)\)</span> and covariance matrix <span class="math inline">\(\Sigma=(\sigma_{jk})\)</span> for $ j=1,,J, k=1,,K$ and <span class="math inline">\(J=K\)</span>, then its density function is</p>
<p>\[
f(y \mid \mu, \Sigma)=\frac{1}{\sqrt{\left(2\pi \right)^{nK}\left(\mid \Sigma \mid \right)^n}} \text{exp}\left(-\frac{1}{2}\sum_{i=1}^{n}(y_i-\mu)\Sigma^{-1}(y_i-\mu)^{T} \right),
\]</p>
<p>where <span class="math inline">\(|\Sigma|\)</span> denotes the determinant of the matrix <span class="math inline">\(\Sigma\)</span> and the superscript <span class="math inline">\(T\)</span> denotes the transpose of a matrix or vector, while <span class="math inline">\(y_i\)</span> denotes the row vector of observed values for unit <span class="math inline">\(i\)</span>. The loglikelihood of <span class="math inline">\(\theta=(\mu,\Sigma)\)</span> is</p>
<p>\[
l(\mu,\Sigma \mid y)= - \frac{n}{2}\text{ln}(2\pi) - \frac{n}{2}\text{ln}(|\Sigma|)-\frac{1}{2}\sum_{i=1}^{n}(y_i-\mu)\Sigma^{-1}(y_i-\mu)^T.
\]</p>
</div>
</div>
<div id="mle-estimation" class="section level2">
<h2>MLE estimation</h2>
<p>Finding the maximum value of <span class="math inline">\(\theta\)</span> that is most likely to have generated the data <span class="math inline">\(y\)</span>, corresponding to maximising the likelihood or <em>Maximum Likelihood Estimation</em>(MLE), is a standard approach to make inference about <span class="math inline">\(\theta\)</span>. Suppose a specific value for the parameter <span class="math inline">\(\hat{\theta}\)</span> such that <span class="math inline">\(L(\hat{\theta}\mid y)\geq L(\theta \mid y)\)</span> for any other value of <span class="math inline">\(\theta\)</span>. This implies that the observed data <span class="math inline">\(y\)</span> is at least as likely under <span class="math inline">\(\hat{\theta}\)</span> as under any other value of <span class="math inline">\(\theta\)</span>, i.e. <span class="math inline">\(\hat{\theta}\)</span> is the value best supported by the data. More specifically, a maximum likelihood estimate of <span class="math inline">\(\theta\)</span> is a value of <span class="math inline">\(\theta \in \Theta\)</span> that maximises the likelihood <span class="math inline">\(L(\theta \mid y)\)</span> or, equivalently, that maximises the loglikelihood <span class="math inline">\(l(\theta \mid y)\)</span>. In general, when the likelihood is differentiable and bounded from above, typically the MLE can be found by differentiating <span class="math inline">\(L(\theta \mid y)\)</span> or <span class="math inline">\(l(\theta \mid y)\)</span> with respect to <span class="math inline">\(\theta\)</span>, setting the result equal to zero, and solving for <span class="math inline">\(\theta\)</span>. The resulting equation, <span class="math inline">\(D_l(\theta)=\frac{\partial l(\theta \mid y)}{\partial \theta}=0\)</span>, is known as the <em>likelihood equation</em> and the derivative of the loglikelihood as the <em>score function</em>. When <span class="math inline">\(\theta\)</span> consists in a set of <span class="math inline">\(j=1,\ldots,J\)</span> components, then the likelihood equation corresponds to a set of <span class="math inline">\(J\)</span> simultaneous equations, obtained by differentiating <span class="math inline">\(l(\theta \mid y)\)</span> with respect to each component of <span class="math inline">\(\theta\)</span>.</p>
<div id="univariate-normal-example-1" class="section level3">
<h3>Univariate Normal Example</h3>
<p>Recall that, for a Normal sample with <span class="math inline">\(n\)</span> units, the loglikelihood is indexed by the set of parameters <span class="math inline">\(\theta=(\mu,\sigma^2)\)</span> and has the form</p>
<p>\[
l(\mu, \sigma^2 \mid y)= -\frac{n}{2}\text{ln}(2\pi)-\frac{n}{2}\text{ln}(\sigma^2)-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2}.
\]</p>
<p>Next, the MLE can be derived by first differentiating <span class="math inline">\(l(\theta \mid y)\)</span> with respect to <span class="math inline">\(\mu\)</span> and set the result equal to zero, that is</p>
<p>\[
\frac{\partial l(\theta \mid y)}{\partial \mu}= -\frac{2}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)(-1)=\frac{\sum_{i=1}^n y_i - n\mu}{\sigma^2}=0,
\]</p>
<p>Next, after simplifying a bit, we can retrieve the solution</p>
<p>\[
\hat{\mu}=\frac{1}{n}\sum_{i=1}^n y_i=\bar{y},
\]</p>
<p>which corresponds to the sample mean of the observations. Next, we differentiate <span class="math inline">\(l(\theta \mid y)\)</span> with respect to <span class="math inline">\(\sigma^2\)</span>, that is we set</p>
<p>\[
\frac{\partial l(\theta \mid y)}{\partial \sigma^2}= -\frac{n}{2\sigma^2}+\frac{1}{2(\sigma^2)^2}\sum_{i=1}^n (y_i-\mu)^2=0.
\]</p>
<p>We then simplify and move things around to get</p>
<p>\[
\frac{1}{\sigma^3}\sum_{i=1}^n(y_i-\mu)^2=\frac{n}{\sigma} \;\;\; \rightarrow \;\;\; \sigma^2=\frac{1}{n}\sum_{i=1}^n(y_i-\mu)^2.
\]</p>
<p>Finally, we replace <span class="math inline">\(\mu\)</span> in the expression above with the value <span class="math inline">\(\hat{\mu}=\bar{y}\)</span> found before and obtain the solution</p>
<p>\[
\hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^n(y_i-\bar{y})^2=s^2,
\]</p>
<p>which, however, is a biased estimator of <span class="math inline">\(\sigma^2\)</span> and therefore is often replaced with the unbiased estimator <span class="math inline">\(\frac{s^2}{(n-1)}\)</span>. In particular, given a population parameter <span class="math inline">\(\theta\)</span>, the estimator <span class="math inline">\(\hat{\theta}\)</span> for <span class="math inline">\(\theta\)</span> is said to be unbiased when <span class="math inline">\(E[\hat{\theta}]=\theta\)</span>. This is the case, for example, of the sample mean <span class="math inline">\(\hat{\mu}=\bar{y}\)</span> which is an unbiased estimator of the population mean <span class="math inline">\(\mu\)</span>:</p>
<p>\[
E\left[\hat{\mu} \right]=E\left[\frac{1}{n}\sum_{i=1}^n y_i \right]=\frac{1}{n}\sum_{i=1}^n E\left[y_i \right]=\frac{1}{n} (n\mu)=\mu.
\]</p>
<p>However, this is not true for the sample variance <span class="math inline">\(s^2\)</span>. This can be seen by first rewriting the expression of the estimator as</p>
<p>\[
\hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^n (y_i^2 -2y_i\bar{y}+\bar{y}^2)=\frac{1}{n}\sum_{i=1}^n y_i^2 -2\bar{y}\sum_{i=1}^n y_i + \frac{1}{n}n\bar{y}^2=\frac{1}{n}\sum_{i=1}^n y_i^2 - \bar{y}^2,
\]</p>
<p>and then by computing the expectation of this quantity:</p>
<p>\[
E\left[\hat{\sigma}^2 \right]=E\left[\frac{1}{n}\sum_{i=1}^n y_i^2 - \bar{y}^2 \right]=\frac{1}{n}\sum_{i=1}^n E\left[y_i^2 \right] - E\left[\bar{y}^2 \right]=\frac{1}{n}\sum_{i=1}^n (\sigma^2 + \mu^2) - (\frac{\sigma^2}{n}+\mu^2)=\frac{1}{n}\left(n\sigma^2+n\mu^2\right) - \frac{\sigma^2}{n}-\mu^2=\frac{(n-1)\sigma^2}{n}.
\]</p>
<p>The above result is obtained by pluggin in the expression for the variance of a general variable <span class="math inline">\(y\)</span> and retrieving the expression for <span class="math inline">\(E[y^2]\)</span> as a function of the variance and <span class="math inline">\(E[y]^2\)</span>. More specifically, given that</p>
<p>\[
Var(y)=\sigma^2=E\left[y^2 \right]-E\left[y \right]^2,
\]</p>
<p>then we know that for <span class="math inline">\(y\)</span>, <span class="math inline">\(E\left[y^2 \right]=\sigma^2+E[y]^2\)</span>, and similarly we can derive the same expression for <span class="math inline">\(\bar{y}\)</span>. However, we can see that <span class="math inline">\(\hat{\sigma}^2\)</span> is biased by a factor of <span class="math inline">\((n-1)/n\)</span>. Thus, an unbiased estimator for <span class="math inline">\(\sigma^2\)</span> is given by multiplying <span class="math inline">\(\hat{\sigma}^2\)</span> by <span class="math inline">\(\frac{n}{(n-1)}\)</span>, which gives the unbiased estimator <span class="math inline">\(\hat{\sigma}^{2\star}=\frac{s^2}{n-1}\)</span>, where <span class="math inline">\(E\left[\hat{\sigma}^{2\star}\right]=\sigma^2\)</span>.</p>
</div>
<div id="multivariate-normal-example-1" class="section level3">
<h3>Multivariate Normal Example</h3>
<p>The same procedure can be applied to an independent and identically distributed multivariate sample <span class="math inline">\(y=(y_{ij})\)</span>, for <span class="math inline">\(i=1,\ldots,n\)</span> units and <span class="math inline">\(j=1,\ldots,J\)</span> variables (<span class="citation">Anderson (1962)</span>,<span class="citation">Rao et al. (1973)</span>,<span class="citation">Gelman et al. (2013)</span>). It can be shown that, maximising the loglikelihood with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> yields the MLEs</p>
<p>\[
\hat{\mu}=\bar{y} \;\;\; \text{and} \;\;\; \Sigma=\frac{(n-1)\hat{\sigma}^{2\star}}{n},
\]</p>
<p>where <span class="math inline">\(\bar{y}=(\bar{y}_1,\ldots,\bar{y}_{J})\)</span> is the row vectors of sample means and <span class="math inline">\(\hat{\sigma}^{2\star}=(s^{\star_{jk}})\)</span> is the sample covariance matrix with <span class="math inline">\(jk\)</span>-th element <span class="math inline">\(s^\star_{jk}=\frac{\Sigma_{i=1}^n(y_{ij} - \bar{y}_j)}{(n-1)}\)</span>. In addition, in general, given a function <span class="math inline">\(g(\theta)\)</span> of the parameter <span class="math inline">\(\theta\)</span>, if <span class="math inline">\(\hat{\theta}\)</span> is a MLE of <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(g(\hat{\theta})\)</span> is a MLE of <span class="math inline">\(g(\theta)\)</span>.</p>
</div>
</div>
<div id="conditional-distribution-of-a-bivariate-normal" class="section level2">
<h2>Conditional Distribution of a Bivariate Normal</h2>
<p>Consider an indpendent and identically distributed sample formed by two variables <span class="math inline">\(y=(y_1,y_2)\)</span>, each measured on <span class="math inline">\(i=1\ldots,n\)</span> units, which come from a Bivariate Normal distribution with mean vector and covariance matrix</p>
<p>\[
\mu=(\mu_1,\mu_2) \;\;\; \text{and} \;\;\; \Sigma = \begin{pmatrix} \sigma^2_1 &amp; \rho\sigma_1\sigma_2 \\ \rho\sigma_2\sigma_1 &amp; \sigma_2^2 \ \end{pmatrix},
\]</p>
<p>where <span class="math inline">\(\rho\)</span> is a correlation parameter between the two variables. Thus, intuitive MLEs for these parameters are</p>
<p>\[
\hat{\mu}_j=\bar{y}_j \;\;\; \text{and} \;\;\; \hat{\sigma}_{jk}=\frac{(n-1)s_{jk}}{n},
\]</p>
<p>where <span class="math inline">\(\sigma^2_j=\sigma_{jj}\)</span>, <span class="math inline">\(\rho\sigma_{j}\sigma_{k}=\sigma_{jk}\)</span>, for <span class="math inline">\(j,k=1,2\)</span>. By properites of the Bivariate Normal distribution (<span class="citation">Ord and Stuart (1994)</span>), the marginal distribution of <span class="math inline">\(y_1\)</span> and the conditional distribution of <span class="math inline">\(y_2 \mid y_1\)</span> are</p>
<p>\[
y_1 \sim \text{Normal}\left(\mu_1,\sigma^2_1 \right) \;\;\; \text{and} \;\;\; y_2 \mid y_1 \sim \text{Normal}\left(\mu_2 + \beta(y_1-\mu_1 \right), \sigma^2_2 - \sigma^2_1\beta^2),
\]</p>
<p>where <span class="math inline">\(\beta=\rho\frac{\sigma_2}{\sigma_1}\)</span> is the parameter that quantifies the linear dependence between the two variables. The MLEs of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2_2\)</span> can also be derived from the likelihood based on the conditional distribution of <span class="math inline">\(y_2 \mid y_1\)</span>, which have strong connections with the least squares estimates derived in a multiple linear regression framework.</p>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2>Multiple Linear Regression</h2>
<p>Suppose the data consist in <span class="math inline">\(n\)</span> units measured on an outcome variable <span class="math inline">\(y\)</span> and a set of <span class="math inline">\(J\)</span> covariates <span class="math inline">\(x=(x_{1},\ldots,x_{J})\)</span> and assume that the distribution of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span> is Normal with mean <span class="math inline">\(\mu_i=\beta_0+\sum_{j=1}^J\beta_jx_{ij}\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. The loglikelihood of <span class="math inline">\(\theta=(\beta,\sigma^2)\)</span> given the observed data <span class="math inline">\((y,x)\)</span> is given by</p>
<p>\[
l(\theta \mid y) = -\frac{n}{2}\text{ln}(2\pi) -\frac{n}{2}\text{ln}(\sigma^2) - \frac{\sum_{i=1}^n \left(y_i - \mu_i \right)^2}{2\sigma^2}.
\]</p>
<p>Maximising this expression with respect to <span class="math inline">\(\theta\)</span>, the MLEs are found to be equal to the least squares estimates of the intercept and regression coefficients. Using a matrix notation for the <span class="math inline">\(n\)</span>-th vector of the outcome values <span class="math inline">\(Y\)</span> and the <span class="math inline">\(n\times (J+1)\)</span> matrix of the covariate values (including the constant term), then the MLEs are:</p>
<p>\[
\hat{\beta}=(X^{T}X)^{-1}X^{T}Y \;\;\; \text{and} \;\;\; \hat{\sigma}^{2}=\frac{(Y-X\hat{\beta})(Y-X\hat{\beta})}{n},
\]</p>
<p>where the numerator of the fraction is known as the <em>Residual Sum of Squares</em>(RSS). Because the denominator of is equal to <span class="math inline">\(n\)</span>, the MLE of <span class="math inline">\(\sigma^2\)</span> does not correct for the loss of degrees of freedom when estimating the <span class="math inline">\(J+1\)</span> location parameters. Thus, the MLE should instead divide the RSS by <span class="math inline">\(n-(J+1)\)</span> to obtain an unbiased estimator. An extension of standard multiple linear regression is the so called <em>weighted</em> multiple linear regression, in which the regression variance is assumed to be equal to<span class="math inline">\(\frac{\sigma^2}{w_i}\)</span>, for <span class="math inline">\((w_i) &gt; 0\)</span>. Thus, the variable <span class="math inline">\((y_i-\mu)\sqrt{w_i}\)</span> is Normally distributed with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, and the loglikelihood is</p>
<p>\[
l(\theta \mid y)= - \frac{n}{2}\text{ln}(2\pi) - \frac{n}{2}\text{ln}(\sigma^2) - \frac{\sum_{i=1}^n w_i(y_i - \mu_i)^2}{2\sigma^2}.
\]</p>
<p>Maximising this function yields MLEs given by the weighted least squares estimates</p>
<p>\[
\hat{\beta}=\left(X^{T}WX\right)^{-1}\left(X^{T}WY \right) \;\;\; \text{and} \;\;\; \sigma^{2}=\frac{\left(Y-X\hat{\beta}\right)^{T}W\left(Y-X\hat{\beta}\right)}{n},
\]</p>
<p>where <span class="math inline">\(W=\text{Diag}(w_1,\ldots,w_n)\)</span>.</p>
</div>
<div id="generalised-linear-models" class="section level2">
<h2>Generalised Linear Models</h2>
<p>Consider the previous example where we had an outcome variable <span class="math inline">\(y\)</span> and a set of <span class="math inline">\(J\)</span> covariates, each measured on <span class="math inline">\(n\)</span> units. A more general class of models, compare with the Normal model, assumes that, given <span class="math inline">\(x\)</span>, the values of <span class="math inline">\(y\)</span> are an independent sample from a regular exponential family distribution</p>
<p>\[
f(y \mid x,\beta,\phi)=\text{exp}\left(\frac{\left(y\delta\left(x,\beta \right) - b\left(\delta\left(x,\beta\right)\right)\right)}{\phi} + c\left(y,\phi\right)\right),
\]</p>
<p>where <span class="math inline">\(\delta()\)</span> and <span class="math inline">\(b()\)</span> are known functions that determine the distribution of <span class="math inline">\(y\)</span>, and <span class="math inline">\(c()\)</span> is a known function indexed by a scale parameter <span class="math inline">\(\phi\)</span>. The mean of <span class="math inline">\(y\)</span> is assumed to linearly relate to the covariates via</p>
<p>\[
E\left[y \mid x,\beta,\phi \right]=g^{-1}\left(\beta_0 + \sum_{j=1}^J\beta_jx_{j} \right),
\]</p>
<p>where <span class="math inline">\(E\left[y \mid x,\beta,\phi \right]=\mu_i\)</span> and <span class="math inline">\(g()\)</span> is a known one to one function which is called <em>link function</em> because it “links” the expectation of <span class="math inline">\(y\)</span> to a linear combination of the covariates. The canonical link function</p>
<p>\[
g_c(\mu_i)=\delta(x_{i},\beta)=\beta_0+\sum_{j=1}^J\beta_jx_{ij},
\]</p>
<p>which is obtained by setting <span class="math inline">\(g()\)</span> equal to the inverse of the derivative of <span class="math inline">\(b()\)</span> with respect to its argument. Examples of canonical links include</p>
<ul>
<li><p>Normal linear regression: <span class="math inline">\(g_c=\text{identity}\)</span>, <span class="math inline">\(b(\delta)=\frac{\delta^2}{2},\phi=\sigma^2\)</span></p></li>
<li><p>Poisson regression: <span class="math inline">\(g_c=\log\)</span>, <span class="math inline">\(b(\delta)=\text{exp}(\delta),\phi=1\)</span></p></li>
<li><p>Logistic regression: <span class="math inline">\(g_c=\text{logit}\)</span>, <span class="math inline">\(b(\delta)=\log(1+\text{exp}(\delta)),\phi=1\)</span></p></li>
</ul>
<p>The loglikelihood of <span class="math inline">\(\theta=(\beta,\phi)\)</span> given the observed data <span class="math inline">\((y,x)\)</span>, is</p>
<p>\[
l(\theta \mid y,x)=\sum_{i=1}^n \left[\frac{\left(y_i\delta\left(x_i,\beta\right)-b\left(\delta\left(x_i,\beta\right)\right) \right)}{\phi}+c\left(y_i,\phi\right)\right],
\]</p>
<p>which for non-normal cases does not have explicit maxima and numerical maximisation can be achieved using iterative algorithms.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-anderson1962introduction">
<p>Anderson, Theodore Wilbur. 1962. “An Introduction to Multivariate Statistical Analysis.” Wiley New York.</p>
</div>
<div id="ref-gelman2013bayesian">
<p>Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. <em>Bayesian Data Analysis</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-ord1994kendall">
<p>Ord, Keith, and Alan Stuart. 1994. “Kendall’s Advanced Theory of Statistics: Distribution Theory.” Edward Arnold.</p>
</div>
<div id="ref-rao1973linear">
<p>Rao, Calyampudi Radhakrishna, Calyampudi Radhakrishna Rao, Mathematischer Statistiker, Calyampudi Radhakrishna Rao, and Calyampudi Radhakrishna Rao. 1973. <em>Linear Statistical Inference and Its Applications</em>. Vol. 2. Wiley New York.</p>
</div>
</div>
</div>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/maximum-likelihood-estimation/">Maximum Likelihood Estimation</a>
  
  <a class="badge badge-light" href="/tags/likelihood-based-methods-ignorable/">Likelihood Based Methods Ignorable</a>
  
</div>



    
      



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/missmethods/likelihood-based-methods-ignorable3/">Likelihood Based Inference with Incomplete Data</a></li>
          
          <li><a href="/missmethods/bayesian-methods/">Bayesian Iterative Simulation Methods</a></li>
          
          <li><a href="/missmethods/em-algorithm/">Expectation Maximisation Algorithm</a></li>
          
          <li><a href="/missmethods/likelihood-based-methods-ignorable2/">Introduction to Bayesian Inference</a></li>
          
          <li><a href="/missmethods/likelihood-based-methods-nonignorable/">Likelihood Based Inference with Incomplete Data (Nonignorable)</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3394a224b26ce58ff36f44c54743e0ab.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">

  <p class="powered-by">
    &#169; Andrea Gabrio &middot;
    <code>2020</code> 
    &middot; Based on the 
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
