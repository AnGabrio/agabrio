<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Missmethods on Andrea Gabrio</title>
    <link>/missmethods/</link>
    <description>Recent content in Missmethods on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/missmethods/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Augmented Inverse Probability Weighting</title>
      <link>/missmethods/augmented-inverse-probability-weighting/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/augmented-inverse-probability-weighting/</guid>
      <description>


&lt;p&gt;A general problem associated with the implementatio of &lt;em&gt;Inverse Probability Weighting&lt;/em&gt; (IPW) methods is that information in some available data is ignored by focussing only on the complete cases (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;). This has provided room to extend these methods to make a more efficient use of the available information through the incorporation of an “augmentation” term, which lead to the development of the so called &lt;em&gt;Augmented Inverse Probability Weighting&lt;/em&gt; (AIPW) methods. These approaches extend IPW methods by creating predictions from a model to recove the information in the incomplete units and applying IPW to the residuals from the model (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Considering the IPW &lt;em&gt;Generalised Estimating Equation&lt;/em&gt; (GEE)&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^{n_r} = w_i(\hat{\alpha})D_i(x_i,\beta)(y_i-g(x_i,\beta))=0,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(w_i(\hat{\alpha})=\frac{1}{p(x_i,z_i \mid \hat{\alpha})}\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(p(x_i,z_i \mid \hat{\alpha})\)&lt;/span&gt; an estimate of the probability of being a complete unit estimated for example using logistic regressions of the missingness indicator &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; on the vectors of the covariate and auxiliary variables &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;, respectively. A problem of this IPW estimator is that it has poor small sample properties when the propensity score gets close to zero or one for some observations, which will lead to high variance in the estimator. AIPW methods can provide estimators of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; which are more efficient than their nonaugmented IPW versions. In general, AIPW estimating functions provide a method for constructing estimators of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; based on two terms:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The usual IPW term &lt;span class=&#34;math inline&#34;&gt;\(p(x_i,z_i \mid \hat{\alpha})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An augmentation term &lt;span class=&#34;math inline&#34;&gt;\(g^\star(x_i,\beta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The basis for the first term is a complete data unbiased estimating function for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, whereas the basis for the second term is some function of the observed data chosen so it has conditional mean of zero given the complete data (&lt;span class=&#34;citation&#34;&gt;Molenberghs et al. (2014)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;doubly-robust-estimators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Doubly Robust Estimators&lt;/h2&gt;
&lt;p&gt;An important class of AIPW methods is known as &lt;em&gt;doubly robust&lt;/em&gt; estimators, which have desirable robustness properties (&lt;span class=&#34;citation&#34;&gt;Robins, Rotnitzky, and Laan (2000)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Robins and Rotnitzky (2001)&lt;/span&gt;). The key feature of these estimators is that they relax the assumption that the model of the missingness probabilities is correctly specified, although requiring additional assumptions on the model for &lt;span class=&#34;math inline&#34;&gt;\(y_i \mid x_i\)&lt;/span&gt;. For example, doubly robust estimators for a population mean parameter &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; could be obtained as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Fit a logistic regression model for the probability of observing &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; to derive the individual weights &lt;span class=&#34;math inline&#34;&gt;\(w_i(\hat{\alpha})\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fit a generalized linear model for the outcome of responders in function of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; using weights &lt;span class=&#34;math inline&#34;&gt;\(w_i(\hat{\alpha})\)&lt;/span&gt; and let &lt;span class=&#34;math inline&#34;&gt;\(g^\star(x_i,\beta)\)&lt;/span&gt; denote the fitted values for subject &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take the sample average of the fitted values &lt;span class=&#34;math inline&#34;&gt;\(g^\star(x_i,\beta)\)&lt;/span&gt; of both respondents and nonrespondents as an estimate of the population mean &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Doubly robust estimators require the specification of two models: one for the missingness probability and another for the distribution of the incomplete data. When the augmentation term &lt;span class=&#34;math inline&#34;&gt;\(g^\star(x_i,\beta)\)&lt;/span&gt; is selected and modelled correctly according to the distribution of the complete data, the resulting estimator of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is consistent even if the model of missingness is misspecified. On the other hand, if the model of missingness is correctly specified, the augmentation term no longer needs to be correctly specified to yield consistent estimators of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Scharfstein, Daniels, and Robins (2003)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Bang and Robins (2005)&lt;/span&gt;). Doubly robust estimators therefore allow to obtain an unbiased estimating function for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; if either the model for the incomplete data or the model for the missingness mechanism has been correctly specified.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose the full data consists of a single outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and an additional variable &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; and that the objective is to estimate the population outcome mean &lt;span class=&#34;math inline&#34;&gt;\(\mu=\text{E}[y]\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is partially observed (while &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is always fully observed), individuals may fall into one of two missingness patterns &lt;span class=&#34;math inline&#34;&gt;\(r=(r_{y},r_{z})\)&lt;/span&gt;, namely &lt;span class=&#34;math inline&#34;&gt;\(r=(1,1)\)&lt;/span&gt; if both variables are observed or &lt;span class=&#34;math inline&#34;&gt;\(r=(1,0)\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is missing. Let &lt;span class=&#34;math inline&#34;&gt;\(c=1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(r=(1,1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c=0\)&lt;/span&gt; otherwise, so that the observed data can be summarised as &lt;span class=&#34;math inline&#34;&gt;\((c,cy,z)\)&lt;/span&gt;. Assuming that missingness only depends on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p(c=1 \mid y,z)=p(c=1 \mid z)=\pi(z),
\]&lt;/p&gt;
&lt;p&gt;then the missing data mechanism is &lt;em&gt;Missing At Random&lt;/em&gt; (MAR). Under these conditions, consider the consistent IPW complete case estimating equation&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n\frac{c_i}{\pi(z_i \mid \hat{\alpha})}(y_i-\mu)=0,
\]&lt;/p&gt;
&lt;p&gt;which can be used to weight the contribution of each complete case by the inverse of &lt;span class=&#34;math inline&#34;&gt;\(\pi(z_i \mid \hat{\alpha})\)&lt;/span&gt;, typically estimated via logistic regressions. A general problem of this type of estimators is that they discard all the available data among the non-completers and are therefore inefficient. However, it is possible to augment the simple IPW complete case estimating equation to improve efficiency. The optimal estimator for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; within this class is the solution to the estimating equation&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n \left(\frac{c_i}{\pi(z_i \mid \hat{\alpha})}(y_i-\mu) - \frac{c_i-\pi(z_i \mid \hat{\alpha})}{\pi(z_i \mid \hat{\alpha})}\text{E}[(y_i-\mu)\mid z_i] \right),
\]&lt;/p&gt;
&lt;p&gt;which leads to the estimator&lt;/p&gt;
&lt;p&gt;\[
\mu_{aipw}=\frac{1}{n}\sum_{i=1}^n \left(\frac{c_iy_i}{\pi(z_i\mid \hat{\alpha})} - \frac{c_i - \pi(z_i\mid \hat{\alpha})}{\pi(z_i\mid \hat{\alpha})} \text{E}[y_i \mid z_i] \right).
\]&lt;/p&gt;
&lt;p&gt;The conditional expectation &lt;span class=&#34;math inline&#34;&gt;\(\text{E}[y_i \mid z_i]\)&lt;/span&gt; is not known and must be estimated from the data. Under a &lt;em&gt;Missing At Random&lt;/em&gt; (MAR) assumption we have that &lt;span class=&#34;math inline&#34;&gt;\(\text{E}[y \mid z]=\text{E}[y \mid z, c=1]\)&lt;/span&gt;, that is the conditional expecation of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is the same as that among the completers. Thus, we can specify a model &lt;span class=&#34;math inline&#34;&gt;\(m(z,\xi)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\text{E}[y \mid z]\)&lt;/span&gt;, indexed by the parameter &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;, that can be estimated from the completers. If &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is continuous, a simple choice is to estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{\xi}\)&lt;/span&gt; by OLS from the completers. The AIPW estimator for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; then becomes&lt;/p&gt;
&lt;p&gt;\[
\mu_{aipw}=\frac{1}{n}\sum_{i=1}^n \left(\frac{c_iy_i}{\pi(z_i\mid \hat{\alpha})} - \frac{c_i - \pi(z_i\mid \hat{\alpha})}{\pi(z_i\mid \hat{\alpha})} m(z_i\mid \hat{\xi}) \right).
\]&lt;/p&gt;
&lt;p&gt;It can be shown that this estimator is more efficient that the simple IPW complete case estimator for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and that it has a double robustness property. This ensures that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{aipw}\)&lt;/span&gt; is a consitent estimator of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; if &lt;strong&gt;either&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the model &lt;span class=&#34;math inline&#34;&gt;\(\pi(z\mid\alpha)\)&lt;/span&gt; is correctly specified, &lt;strong&gt;or&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the model &lt;span class=&#34;math inline&#34;&gt;\(m(z\mid \xi)\)&lt;/span&gt; is correctly specified.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To see a derivation of the double robustness property I put here a link to some nice &lt;a href=&#34;https://www4.stat.ncsu.edu/~davidian/st790/notes/chap5.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conlcusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conlcusions&lt;/h2&gt;
&lt;p&gt;As all weighting methods, such as IPW, AIPW methods are &lt;em&gt;semiparametric&lt;/em&gt; methods that aim to achieve robustness and good performance over more general classes of population distributions. However, semiparametric estimators can be less efficient and less powerful than &lt;em&gt;Maximum Likelihood&lt;/em&gt; or &lt;em&gt;Bayesian&lt;/em&gt; estimators under a well specified parametric model. With missing data, &lt;span class=&#34;citation&#34;&gt;Rubin (1976)&lt;/span&gt; results show that likelihood-based methods perform uniformly well over any &lt;em&gt;Missing At Random&lt;/em&gt; (MAR) missingness distribution, and the user does not need to specify that distribution. However, semiparametric methods that relax assumptions about the data must in turn assume a specific form for the distribution of missingness. It has been argued that, for these semiparametric methods to gain a substantial advantage over well-specified likelihood methods, the parametric model has to be grossly misspecified (&lt;span class=&#34;citation&#34;&gt;Meng (2000)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-bang2005doubly&#34;&gt;
&lt;p&gt;Bang, Heejung, and James M Robins. 2005. “Doubly Robust Estimation in Missing Data and Causal Inference Models.” &lt;em&gt;Biometrics&lt;/em&gt; 61 (4): 962–73.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-meng2000missing&#34;&gt;
&lt;p&gt;Meng, Xiao-Li. 2000. “Missing Data: Dial M for???” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 95 (452): 1325–30.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-molenberghs2014handbook&#34;&gt;
&lt;p&gt;Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. &lt;em&gt;Handbook of Missing Data Methodology&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins2001comment&#34;&gt;
&lt;p&gt;Robins, James M, and Andrea Rotnitzky. 2001. “Comment on the Bickel and Kwon Article,‘Inference for Semiparametric Models: Some Questions and an Answer’.” &lt;em&gt;Statistica Sinica&lt;/em&gt; 11 (4): 920–36.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins2000profile&#34;&gt;
&lt;p&gt;Robins, James M, Andrea Rotnitzky, and Mark van der Laan. 2000. “On Profile Likelihood: Comment.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 95 (450): 477–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1976inference&#34;&gt;
&lt;p&gt;Rubin, Donald B. 1976. “Inference and Missing Data.” &lt;em&gt;Biometrika&lt;/em&gt; 63 (3): 581–92.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-scharfstein2003incorporating&#34;&gt;
&lt;p&gt;Scharfstein, Daniel O, Michael J Daniels, and James M Robins. 2003. “Incorporating Prior Beliefs About Selection Bias into the Analysis of Randomized Trials with Missing Outcomes.” &lt;em&gt;Biostatistics&lt;/em&gt; 4 (4): 495–512.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Available Case Analysis</title>
      <link>/missmethods/available-case-analysis/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/available-case-analysis/</guid>
      <description>


&lt;p&gt;Complete case analysis (CCA) can be particularly inefficient for data sets with a large number of variables which are partially observed. An alternative approach that can be used to conduct univariate analyses in known as &lt;em&gt;Available Case Analysis&lt;/em&gt; (ACA), which uses all the available cases, separately for each variable under examination, to estimate the quantities of interest.&lt;/p&gt;
&lt;p&gt;The main drawback of ACA is that the sample used to perform the analysis varies from variable to variable according to the patterns of missing data, which generates problems of comparability across variables if the missingness mechanism is not &lt;em&gt;missing completely at random&lt;/em&gt; (MCAR), i.e. the missing data probabilities depend on the variables under study. While estimates of means and variances can be easily computed, measures of covariation need to be adjusted. In particular, for estimating sample covariances, this approach is known as &lt;em&gt;pairwise deletion&lt;/em&gt; or &lt;em&gt;pairwise inclusion&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;pairwise-measures-of-covariation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pairwise measures of covariation&lt;/h2&gt;
&lt;p&gt;One possible approach to estimate pairwise measures of covariation for &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_k\)&lt;/span&gt; is to use only those units &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n_{ac}\)&lt;/span&gt; for which both variables are observed (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;). For example, one can compute pairwise sample covariances as:&lt;/p&gt;
&lt;p&gt;\[
s^{ac}_{jk} = \frac{\sum_{i \in I_{ac}}(y_{ij}-\bar{y}_{j}^{ac})(y_{ik}-\bar{y}_{k}^{ac})}{(n_{ac}-1)},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(I_{ac}\)&lt;/span&gt; is the set of &lt;span class=&#34;math inline&#34;&gt;\(n_{ac}\)&lt;/span&gt; with both &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_k\)&lt;/span&gt; observed, while the sample means &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{ac}_{j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{ac}_{k}\)&lt;/span&gt; are calculated over this set of units. We can also estimate the sample correlation&lt;/p&gt;
&lt;p&gt;\[
r^{\star}_{jk} = \frac{s^{ac}_{jk}}{\sqrt{s^2_{j}s^{2}_{k}}},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s^2_{j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s^2_{k}\)&lt;/span&gt; are the sample variances computed over the sets of observed units &lt;span class=&#34;math inline&#34;&gt;\(I_{j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(I_{k}\)&lt;/span&gt;, respectively. A problem of this type of correlation estimate is that it can lie outside the range &lt;span class=&#34;math inline&#34;&gt;\((-1,1)\)&lt;/span&gt;, which is typically addressed by computing &lt;em&gt;pairwise correlations&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Wilks (1932)&lt;/span&gt;), where variances are estimated from the set of units with both variables observed &lt;span class=&#34;math inline&#34;&gt;\(I_{jk}\)&lt;/span&gt;, i.e. &lt;/p&gt;
&lt;p&gt;\[
r^{ac}_{jk} = \frac{s^{ac}_{jk}}{\sqrt{s^{2,ac}_{j}s^{2,ac}_{k}}}.
\]&lt;/p&gt;
&lt;p&gt;In addition, we could also replace the sample means &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{ac}_{j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{ac}_{k}\)&lt;/span&gt;, evaluated on the common set of units &lt;span class=&#34;math inline&#34;&gt;\(I_{jk}\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{k}\)&lt;/span&gt;, which are evaluated on the sets of units &lt;span class=&#34;math inline&#34;&gt;\(I_{j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(I_{k}\)&lt;/span&gt;, respectively. This leads to the following estimates for the sample covariances (&lt;span class=&#34;citation&#34;&gt;Matthai (1951)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;\[
s^{\star}_{jk} = \frac{\sum_{i \in I_{ac}}(y_{ij}-\bar{y}_{j})(y_{ik}-\bar{y}_{k})}{(n_{ac}-1)},
\]&lt;/p&gt;
&lt;p&gt;Pairwise AC estimates aim at recovering information from partially-observed units that are lost by CCA. However, when considered together, the estimates suffer from inconsistencies that undermine the validity of these methods. For example, pairwise correlation matrices may be not positive definite. Because parameters are estimated from different sets of units, different approaches can be used to obtain estimate of the measures of uncertainty (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;AC estimates allow to make use of all the available evidence in the data and may be more efficient that CCA when the missingness mechanism is MCAR and correlations are modest (&lt;span class=&#34;citation&#34;&gt;Kim and Curry (1977)&lt;/span&gt;). However, when correlations are more substantial, ACA may become even less efficient than CCA (&lt;span class=&#34;citation&#34;&gt;Haitovsky (1968)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Azen and Van Guilder (1981)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-azen1981conclusions&#34;&gt;
&lt;p&gt;Azen, S, and M Van Guilder. 1981. “Conclusions Regarding Algorithms for Handling Incomplete Data.” &lt;em&gt;1981 Proceedings of the Statistical Computing Section&lt;/em&gt;, 53–56.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-haitovsky1968missing&#34;&gt;
&lt;p&gt;Haitovsky, Yoel. 1968. “Missing Data in Regression Analysis.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Methodological)&lt;/em&gt; 30 (1): 67–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kim1977treatment&#34;&gt;
&lt;p&gt;Kim, Jae-On, and James Curry. 1977. “The Treatment of Missing Data in Multivariate Analysis.” &lt;em&gt;Sociological Methods &amp;amp; Research&lt;/em&gt; 6 (2): 215–40.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-matthai1951estimation&#34;&gt;
&lt;p&gt;Matthai, Abraham. 1951. “Estimation of Parameters from Incomplete Data with Application to Design of Sample Surveys.” &lt;em&gt;Sankhyā: The Indian Journal of Statistics&lt;/em&gt;, 145–52.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wilks1932moments&#34;&gt;
&lt;p&gt;Wilks, Samuel S. 1932. “Moments and Distributions of Estimates of Population Parameters from Fragmentary Samples.” &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt; 3 (3): 163–95.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Complete Case Analysis</title>
      <link>/missmethods/complete-case-analysis/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/complete-case-analysis/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Complete case analysis&lt;/em&gt; (CCA), also known as &lt;em&gt;case&lt;/em&gt; or &lt;em&gt;listwise deletion&lt;/em&gt; (LD), is one of the oldest methods to handle missing data and consists in discarding any unit or case whose information is incomplete. Only the cases with observed values for all the variables under consideration are used in the analysis. For example, suppose we have a data set formed by &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; individuals and that we want to fit a linear regression on some outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; using some other variables &lt;span class=&#34;math inline&#34;&gt;\(x_{i1},\ldots,x_{ik}\)&lt;/span&gt; as covariates. CCA uses only the subset of cases with observed values on all the variables included in the analysis (&lt;em&gt;completers&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;CCA has been a quite popular approach to deal with missingness, mainly because it is very easy to implement (used by default in many statistical programs) and it allows the comparison of different univariate statistics in a straightforward way (calculated on a common set of cases). However, there are a number of potential disadvantages which threatens the validity of this method:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Bias, when the missing data mechanism is not &lt;em&gt;missing completely at random&lt;/em&gt; (MCAR) and the completers are not a random samples of all the cases&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Loss of efficiency, due to the potential loss of information in discarding the incomplete cases.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;CCA may be justified when the loss of precision and bias are minimal, which is more likley to occur when the proportion of completers is high, although it is difficult to formulate rules that apply in general circumstances. Indeed, both the degree of loss of precision and bias depend not only on the fraction of completers and missingness patterns, but also on the extent to which complete and incomplete cases differ and the parameters of interest.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_{cc}\)&lt;/span&gt; be an estimate of a parameter of interest from the completers. One might measure the increase in variance of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_{cc}\)&lt;/span&gt; with respect to the estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; that would be obtained in the absence of missing values. Using the notation of &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;\[
\text{Var}(\hat{\theta}_{cc}) = \text{Var}(\hat{\theta})(1 + \Delta^{\star}_{cc}),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Delta^{\star}_{cc}\)&lt;/span&gt; is the proportional increase in variance from the loss of information. A more practical measure of the loss of inofrmation is &lt;span class=&#34;math inline&#34;&gt;\(\Delta_{cc}\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;\[
\text{Var}(\hat{\theta}_{cc}) = \text{Var}(\hat{\theta}_{eff})(1 + \Delta_{cc}),
\]&lt;/p&gt;
&lt;p&gt;and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_{eff}\)&lt;/span&gt; is an efficient estimate based on all the available data.&lt;/p&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;p&gt;Consider bivariate normal monotone data &lt;span class=&#34;math inline&#34;&gt;\(\bf y = (y_1,y_2)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(n_{cc}\)&lt;/span&gt; out of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; cases are complete and &lt;span class=&#34;math inline&#34;&gt;\(n - n_{cc}\)&lt;/span&gt; cases have observed values only on &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;. Assume for simplicity that the missingness mechanism is MCAR and that the mean of &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; is estimated by the empirical mean from the complete cases &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{cc}_j\)&lt;/span&gt;. Then, the loss in sample size for estimating the mean of &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;\[
\Delta_{cc}(\bar{y}_1) = \frac{n - n_{cc}}{n_{cc}},
\]&lt;/p&gt;
&lt;p&gt;so that if half the cases are missing, the variance is doubled. For the mean of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt;, the loss of information alos depends on the squared correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}\)&lt;/span&gt; between the variables: (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;\[
\Delta_{cc}(\bar{y}_2) \approx \frac{(n - n_{cc})\rho^{2}}{n_{cc}(1 - \rho^{2}) + n_{cc}\rho^{2}}.
\]&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Delta_{cc}(\bar{y}_2)\)&lt;/span&gt; varies from zero (when &lt;span class=&#34;math inline&#34;&gt;\(\rho=0\)&lt;/span&gt;) to &lt;span class=&#34;math inline&#34;&gt;\(\Delta_{cc}(\bar{y}_1)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2} \rightarrow 1\)&lt;/span&gt;. However, for the regression coefficients of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; we have that &lt;span class=&#34;math inline&#34;&gt;\(\Delta_{cc}=0\)&lt;/span&gt; since the incomplete observations of &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; provide no information for estimating the parameters of the regression of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2&lt;/h2&gt;
&lt;p&gt;For inference about the population mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, the bias of CCA depends on the proportion of the completers &lt;span class=&#34;math inline&#34;&gt;\(\pi_{cc}\)&lt;/span&gt; and the extent to which complete and incomplete cases differ on the variables of interest. Suppose a variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is partially-observed and that we partition the data into the subset of the completers &lt;span class=&#34;math inline&#34;&gt;\(y_{cc}\)&lt;/span&gt; and incompleters &lt;span class=&#34;math inline&#34;&gt;\(y_{ic}\)&lt;/span&gt;, with associated population means &lt;span class=&#34;math inline&#34;&gt;\(\mu_{cc}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ic}\)&lt;/span&gt;, respectively. The overall mean can be written as a weighted average of the means of the two subsets&lt;/p&gt;
&lt;p&gt;\[
\mu = \pi_{cc}\mu_{cc} + (1 - \pi_{cc})\mu_{ic}.
\]&lt;/p&gt;
&lt;p&gt;The bias of CCA is then equal to the expected fraction of incomplete cases multiplied by the differences in the means for complete and incomplete cases&lt;/p&gt;
&lt;p&gt;\[
\mu_{cc} - \mu = (1 - \pi_{cc})(\mu_{cc} - \mu_{ic}).&lt;br /&gt;
\]&lt;/p&gt;
&lt;p&gt;Under MCAR, we have that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{cc} = \mu_{ic}\)&lt;/span&gt; and therefore the bias is zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 3&lt;/h2&gt;
&lt;p&gt;Consider the estimation of the regression of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x_1,\ldots,x_K\)&lt;/span&gt; from data with potential missing values on all variables and with the regression function correctly specified. The bias of CCA for estimating the regression coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_1,\ldots,\beta_K\)&lt;/span&gt; associated with the covariates is null if the probbaility of being a completer depends on the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;s but not &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, since the analysis conditions on the values of the covariates (&lt;span class=&#34;citation&#34;&gt;Glynn and Laird (1986)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;White and Carlin (2010)&lt;/span&gt;). This class of missing data mechanisms includes &lt;em&gt;missing not at random&lt;/em&gt; (MNAR), where the probability that a covariate is missing depends on the value of that covariate. However, CCA is biased if the probability of being a completer depends on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; after conditioning on the covariates. A nice example of this particular topic and its implications for the analysis has been provided by professor &lt;a href=&#34;https://thestatsgeek.com/about-thestatsgeek-com/&#34;&gt;Bartlett&lt;/a&gt; using some nice &lt;a href=&#34;http://thestatsgeek.com/wp-content/uploads/2016/08/Jonathan-Bartlett-28-06-2013.pdf&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The main virtue of case deletion is simplicity. If a missing data problem can be resolved by discarding only a small part of the sample, then the method can be quite effective. However, even in that situation, one should explore the data (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;). The discarded information from incomplete cases can be used to study whether the complete cases are plausibly a random subsample of the original sample, that is, whether MCAR is a reasonable assumption. A simple procedure is to compare the distribution of a particular variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; based on complete cases with the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; based on incomplete cases for which &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is recorded. Significant differences indicate that the MCAR assumption is invalid, and the complete-case analysis yields potentially biased estimates. Such tests are useful but have limited power when the sample of incomplete cases is small. Also the tests can offer no direct evidence on the validity of the &lt;em&gt;missing at random&lt;/em&gt; (MAR) assumption.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-glynn1986regression&#34;&gt;
&lt;p&gt;Glynn, RJ, and NM Laird. 1986. “Regression Estimates and Missing Data: Complete Case Analysis.” &lt;em&gt;Cambridge MA: Harvard School of Public Health, Department of Biostatistics&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-white2010bias&#34;&gt;
&lt;p&gt;White, Ian R, and John B Carlin. 2010. “Bias and Efficiency of Multiple Imputation Compared with Complete-Case Analysis for Missing Covariate Values.” &lt;em&gt;Statistics in Medicine&lt;/em&gt; 29 (28): 2920–31.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Augmentation via MCMC</title>
      <link>/missmethods/data-augmentation-mcmc/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/data-augmentation-mcmc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Explicit Single Imputation</title>
      <link>/missmethods/mean-imputation/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/mean-imputation/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;All case deletion methods, such as &lt;em&gt;Complete Case Analysis&lt;/em&gt;(CCA) or &lt;em&gt;Available Case Analysis&lt;/em&gt;(ACA) make no use of units with partially observed data, when estimating the marginal distribution of the variables under study or the covariation between variables. Clearly, this is inefficient and a tempting alternative would be to &lt;em&gt;impute&lt;/em&gt; or “fill in” the unobserved data with some plausible values. When a single value is used to replace each missing data, we talk about &lt;em&gt;Single Imputation&lt;/em&gt;(SI) methods and, according to the precedure used to generate these imputations, different SI methods can be used. In general, the idea of imputing the missing values is really appealing as it allows to recover the full sample on which standard complete data methods can be applied to derive the estimates of interest.&lt;/p&gt;
&lt;p&gt;However, it is important to be aware of the potential problems of imputing missing data without a clear understanding about the process underlying the values we want to impute, which is the key factor to determine whether the selected approach would be plausible in the context considered. Indeed, imputation should be conceptualised as draws from a predictive distribution of the missing values and require methods for creating a predictive distribution for the imputation based on the observed data. According to &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;, these predictive distributions can be created using&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Explicit modelling&lt;/em&gt;, when the distribution is based on formal statistical models which make the underlying assumptions explicit.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Implicit modelling&lt;/em&gt;, when the distribution is based on an algorithm which implicitly relies on some underlying model assumptions.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this part, we focus on some of the most popular &lt;em&gt;Explicit Single Imputation&lt;/em&gt; methods. These include: &lt;em&gt;Mean Imputation&lt;/em&gt;(SI-M), where means from the observed data are used as imputed values; &lt;em&gt;Regression Imputation&lt;/em&gt;(SI-R), where missing values are replaced with values predicited from a regression of the missing variable on some other observed variables; and &lt;em&gt;Stochastic Regression Imputation&lt;/em&gt;(SI-SR), where unobserved data are substituted with the predicted values from a regression imputation plus a randomly selected residual drawn to reflect uncertainty in the predicted values.&lt;/p&gt;
&lt;div id=&#34;mean-imputation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean Imputation&lt;/h2&gt;
&lt;p&gt;The simplest type of SI-M consists in replacing the missing values in a variable with the mean of the observed units from the same variable, a method known as &lt;em&gt;Unconditional Mean Imputation&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;). Let &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt; be the value of variable &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, such that the unconditional mean based on the observed values of &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; is given by &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_j\)&lt;/span&gt;. The sample mean of the observed and imputed values is then &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{m}_j=\bar{y}^{ac}_j\)&lt;/span&gt;, i.e. the estimate from ACA, while the sample variance is given by&lt;/p&gt;
&lt;p&gt;\[
s^{m}_{j}=s^{ac}_{j}\frac{(n^{ac}-1)}{(n-1)},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s^{ac}_j\)&lt;/span&gt; is the sample variance estimated from the &lt;span class=&#34;math inline&#34;&gt;\(n^{ac}\)&lt;/span&gt; available units. Under a &lt;em&gt;Missing Completely At Random&lt;/em&gt;(MCAR) assumption, &lt;span class=&#34;math inline&#34;&gt;\(s^{ac}_j\)&lt;/span&gt; is a consistent estimator of the tru variance so that the sample variance from the imputed data &lt;span class=&#34;math inline&#34;&gt;\(s^m_j\)&lt;/span&gt; systematically underestimates the true variance by a factor of &lt;span class=&#34;math inline&#34;&gt;\(\frac{(n^{ac}-1)}{(n-1)}\)&lt;/span&gt;, which clearly comes from the fact that missing data are imputed using values at the centre of the distribution. The imputation distorts theempirical distribution of the observed values as well as any quantities that are not linear in the data (e.g. variances, percentiles, measures of shape). The sampel covariance of &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_k\)&lt;/span&gt; from the imputed data is&lt;/p&gt;
&lt;p&gt;\[
s^{m}_{jk}=s^{ac}_{jk}\frac{(n^{as}_{jk}-1)}{(n-1)},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(n^{ac}_{jk}\)&lt;/span&gt; is the number of units with both variables observed and &lt;span class=&#34;math inline&#34;&gt;\(s^{ac}_{jk}\)&lt;/span&gt; is the corresponding covariance estimate from ACA. Under MCAR &lt;span class=&#34;math inline&#34;&gt;\(s^{ac}_{jk}\)&lt;/span&gt; is a consistent estimator of the true covariance, so that &lt;span class=&#34;math inline&#34;&gt;\(s^{m}_{jk}\)&lt;/span&gt; underestimates the magnitude of the covariance by a factor of &lt;span class=&#34;math inline&#34;&gt;\(\frac{(n^{ac}_{jk}-1)}{(n-1)}\)&lt;/span&gt;. Obvious adjustments for the variance (&lt;span class=&#34;math inline&#34;&gt;\(\frac{(n-1)}{(n^{ac}_j-1)}\)&lt;/span&gt;) and the covariance (&lt;span class=&#34;math inline&#34;&gt;\(\frac{(n-1)}{(n^{ac}_{jk}-1)}\)&lt;/span&gt;) yield ACA estimates, which could lead to covariance matrices that are not positive definite.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-imputation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression Imputation&lt;/h2&gt;
&lt;p&gt;An improvement over SI-M is to impute each missing data using the conditional means given the observed values, a method known SI-R or &lt;em&gt;Conditional Mean Imputation&lt;/em&gt;. To be precise, it would also be possible to impute conditional means without using a regression approach, for example by grouping individuals into adjustment classes (analogous to weighting methods) based on the observed data and then impute the missing values using the observed means in each adjustment class (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;). However, for the sake of simplicity, here we will assume that SI-R and conditional mean imputation are the same.&lt;/p&gt;
&lt;p&gt;To generate imputations under SI-R, consider a set of &lt;span class=&#34;math inline&#34;&gt;\(J-1\)&lt;/span&gt; fully observed response variables &lt;span class=&#34;math inline&#34;&gt;\(y_1,\ldots,y_{J-1}\)&lt;/span&gt; and a partially observed response variable &lt;span class=&#34;math inline&#34;&gt;\(y_J\)&lt;/span&gt; which has the first &lt;span class=&#34;math inline&#34;&gt;\(n_{cc}\)&lt;/span&gt; units observed and the remaiing &lt;span class=&#34;math inline&#34;&gt;\(n-n_{cc}\)&lt;/span&gt; units missing. SI-R computes the regression of &lt;span class=&#34;math inline&#34;&gt;\(y_J\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1,\ldots,y_{J-1}\)&lt;/span&gt; based on the &lt;span class=&#34;math inline&#34;&gt;\(n_{cc}\)&lt;/span&gt; complete units and then fills in the missing values as predictions from the regression. For example, for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the missing value &lt;span class=&#34;math inline&#34;&gt;\(y_{iJ}\)&lt;/span&gt; is imputed using&lt;/p&gt;
&lt;p&gt;\[
\hat{y}_{iJ}=\hat{\beta}_{J0}+\sum_{j=1}^{J-1}\hat{\beta}_{Jj}y_{ij},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{J0}\)&lt;/span&gt; is the intercept and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{Jj}\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; coefficient of of the regression of &lt;span class=&#34;math inline&#34;&gt;\(y_J\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1,\ldots,y_{J-1}\)&lt;/span&gt; based on the &lt;span class=&#34;math inline&#34;&gt;\(n_{cc}\)&lt;/span&gt; units.&lt;/p&gt;
&lt;p&gt;An extension of regression imputation to a general pattern of missing data is known as &lt;em&gt;Buck’s method&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Buck (1960)&lt;/span&gt;). This approach first estimates the population mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; from the sample mean and covariance matrix of the complete units and then uses these estimates to calculate the OLS regressions of the missing variables on the observed variables for each missing data pattern. Predictions of the missing data for each observation are obtained by replacing the values of the present variables in the regressions. The average of the observed and imputed values from this method are consistent estimates of the means and MCAR and mild assumptions about the moments of the distribution (&lt;span class=&#34;citation&#34;&gt;Buck (1960)&lt;/span&gt;). They are also consistent when the missingness mechanism depends on observed variables, i.e. under a &lt;em&gt;Missing At Random&lt;/em&gt;(MAR) assumption, although addtional assumptions are required in this case (e.g. using linear regressions it assumes that the “true” regression of the missing varables on the observed variables is linear).&lt;/p&gt;
&lt;p&gt;The filled in data from Buck’s method typically yield reasonable estimates of means, while the sample variances and covariances are biased, although the bias is less than the one associated with unconditional mean imputation. Specifically, the sample variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2,SI-R}_j\)&lt;/span&gt; from the imputed data underestimates the true variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_j\)&lt;/span&gt; by a factor of &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n-1}\sum_{i=1}^n\sigma^{2}_{ji}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_{ji}\)&lt;/span&gt; is the residual variance from regressing &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; on the variables observed in unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt; is missing and zero if &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt; is observed. The sample covariance of &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_k\)&lt;/span&gt; has a bias of &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n-1}\sum_{i=1}^n\sigma_{jki}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{jki}\)&lt;/span&gt; is the residual covariance from the multivariate regression of &lt;span class=&#34;math inline&#34;&gt;\((y_{ij},y_{ik})\)&lt;/span&gt; on the variables observed in unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; if both variables are missing and zero otherwise. A consistent estimator of &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; can be constructed under MCAR by replacing consistent estimates of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_{ji}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{jki}\)&lt;/span&gt; in the expressions for bias and then adding the resulting quantities to the sample covariance matrix of the filled-in data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stochastic-regression-imputation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stochastic Regression Imputation&lt;/h2&gt;
&lt;p&gt;Any type of mean or regression imputation will lead to bias when the interest is in the tails of the distributions because “best prediction” imputation systematically underestimates variability and standard errors calculated from the imputed data are typically too small. These considerations suggest an alternative imputation strategy, where imputed values are drawn from a predictive distribution of a plausible set of values rather than from the centre of the distribution. This is the idea behind SI-SR, which imputes a conditional draw&lt;/p&gt;
&lt;p&gt;\[
\hat{y}_{iJ}=\hat{\beta}_{J0}+\sum_{j=1}^{J-1}\hat{\beta}_{Jj}y_{ij}+z_{iJ},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(z_{iJ}\)&lt;/span&gt; is a random normal deviate with mean 0 and variance &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2_J\)&lt;/span&gt;, the residual variance from the regression of &lt;span class=&#34;math inline&#34;&gt;\(y_J\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1,\ldots,y_{J-1}\)&lt;/span&gt; based on the complete units. The addition of the random deviate makes the imputation a random draw from the predictive distribution of the missing values, rather than the mean, which is likely to ameliorate the distortion of the predictive distributions (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;Consider a bivariate normal monotone missing data with &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; fully observed and &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt; missing for a fraction &lt;span class=&#34;math inline&#34;&gt;\(\lambda=\frac{(n-n_{cc})}{n}\)&lt;/span&gt; and a MCAR mechanism. The following table shows the large sample bias of standard OLS estimates obtained from the filled-in data about the mean, the variance of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt;, the regression coefficient of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;, and the regression coefficient of &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt;, using four different single imputation methods: uncondtional mean (UM), unconditional draw (UD), conditional mean (CM), and conditional draw (CD).&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Bivariate normal monotone MCAR data; large sample bias of four imputation methods.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
mu_2
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
sigma_2
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
beta_21
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
beta_12
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
UM
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
-lambda * sigma_2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
-lambda * beta_21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
UD
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
-lambda * beta_21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
-lambda * beta_21
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CM
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
-lambda * (1-rho^2) * sigma_2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
((lambda * (1-rho^2)) / (1-lambda * (1-rho^2)) ) * beta_12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CD
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Under MCAR, all four methods yield consistent estimates of &lt;span class=&#34;math inline&#34;&gt;\(\mu_2\)&lt;/span&gt; but both UM and CM underestimate the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma_2\)&lt;/span&gt;, UD leads to attenuation of the regression coefficients, while CD yields consistent estimates of all four parameters. However, CD has some important drawbacks. First, adding random draws to the conditional mean imputations is inefficient as the large sample variance of the CD estimates of &lt;span class=&#34;math inline&#34;&gt;\(\mu_2\)&lt;/span&gt; can be shown (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;) to be&lt;/p&gt;
&lt;p&gt;\[
\frac{[1-\lambda\rho^2+(1-\rho^2)\lambda(1-\lambda)]\sigma_2}{n_{cc}},
\]&lt;/p&gt;
&lt;p&gt;which is larger than the large sample sampling variance of the CM estimate of &lt;span class=&#34;math inline&#34;&gt;\(\mu_2\)&lt;/span&gt;, namely &lt;span class=&#34;math inline&#34;&gt;\(\frac{[1-\lambda\rho^2]\sigma_2}{n_{cc}}\)&lt;/span&gt;. Second, the standard errors of the CD estimates from the imputed data are too small because they do not incorporate imputation uncertainty.&lt;/p&gt;
&lt;p&gt;When the analysis involves units with some covariates missing and other observed, it is common practice to condition on the observed covariates when generating the imputations for the missing covariates. It is also possible to condition on the outcome &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to impute missing covariates, even if the final objective is to regress &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; on the full set of covariates and conditioning on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; will lead to bias when conditional means are imputed. However, if predictive draws are imputed, this approach will yield consistent estimates of the regression coefficients. Imputing missing covariates using the means by conditioning only the observed covariates (and not also on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) also yields consistent estimates of the regression coefficients under certain conditions, although these are typically less efficient then those from CCA, but yields inconsistent estimates of other parameters such as variances and correlations (&lt;span class=&#34;citation&#34;&gt;Little (1992)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;According to &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;, imputation should generally be&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Conditional&lt;/strong&gt; on observed variables, to reduce bias, improve precision and preserve association between variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Multivariate&lt;/strong&gt;, to preserve association between missing variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Draws&lt;/strong&gt; from the predictive distributions rather than means, to provide valid estimates of a wide range of estimands.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Nevertheless, a main problem of SI methods is that inferences based on the imputed data do not account for imputation uncertainty and standard errors are therefore systematically underestimated, p-values of tests are too significant and confidence intervals are too narrow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-buck1960method&#34;&gt;
&lt;p&gt;Buck, Samuel F. 1960. “A Method of Estimation of Missing Values in Multivariate Data Suitable for Use with an Electronic Computer.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Methodological)&lt;/em&gt; 22 (2): 302–6.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little1992regression&#34;&gt;
&lt;p&gt;Little, Roderick JA. 1992. “Regression with Missing X’s: A Review.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 87 (420): 1227–37.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Implicit Single Imputation</title>
      <link>/missmethods/last-value-carried-forward/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/last-value-carried-forward/</guid>
      <description>


&lt;p&gt;All case deletion methods, such as &lt;em&gt;Complete Case Analysis&lt;/em&gt;(CCA) or &lt;em&gt;Available Case Analysis&lt;/em&gt;(ACA) make no use of units with partially observed data, when estimating the marginal distribution of the variables under study or the covariation between variables. Clearly, this is inefficient and a tempting alternative would be to &lt;em&gt;impute&lt;/em&gt; or “fill in” the unobserved data with some plausible values. When a single value is used to replace each missing data, we talk about &lt;em&gt;Single Imputation&lt;/em&gt;(SI) methods and, according to the precedure used to generate these imputations, different SI methods can be used. In general, the idea of imputing the missing values is really appealing as it allows to recover the full sample on which standard complete data methods can be applied to derive the estimates of interest.&lt;/p&gt;
&lt;p&gt;However, it is important to be aware of the potential problems of imputing missing data without a clear understanding about the process underlying the values we want to impute, which is the key factor to determine whether the selected approach would be plausible in the context considered. Indeed, imputation should be conceptualised as draws from a predictive distribution of the missing values and require methods for creating a predictive distribution for the imputation based on the observed data. According to &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;, these predictive distributions can be created using&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Explicit modelling&lt;/em&gt;, when the distribution is based on formal statistical models which make the underlying assumptions explicit.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Implicit modelling&lt;/em&gt;, when the distribution is based on an algorithm which implicitly relies on some underlying model assumptions.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this part, we focus on some of the most popular &lt;em&gt;Implicit Single Imputation&lt;/em&gt; methods. These include: &lt;em&gt;Hot Deck Imputation&lt;/em&gt;(SI-HD), where missing values are imputed using observed values from similar responding units in the sample; &lt;em&gt;Substitution&lt;/em&gt;(SI-S), where nonresponding units are replaced with alternative units not yet selected into the sample; &lt;em&gt;Cold Deck Imputation&lt;/em&gt;(SI-CD), where missing values are replaced with a constant value from an external source; &lt;em&gt;Composite Methods&lt;/em&gt;, which combine procedures from the previous approaches. We will specifically focus on SI-HD methods, which are the most popular among these.&lt;/p&gt;
&lt;div id=&#34;hot-deck-imputation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hot Deck Imputation&lt;/h2&gt;
&lt;p&gt;SI-HD procedures refer to the deck of match &lt;a href=&#34;https://en.wikipedia.org/wiki/Punched_card#Hollerith&amp;#39;s_early_punched_card_formats&#34;&gt;Hollerith cards&lt;/a&gt; for the donors available for a nonrespondent. Suppose that a sample of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; out of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; units is selected and that &lt;span class=&#34;math inline&#34;&gt;\(n_{cc}\)&lt;/span&gt; out of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; are recorded. Given an equal probability sampling scheme, the mean of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; can be estimated from the filled-in data as the mean of the responding and the imputed units&lt;/p&gt;
&lt;p&gt;\[
\bar{y}_{HD}=\frac{(n_{cc}\bar{y}_{cc}+(n-n_{cc})\bar{y}^{\star})}{n},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{cc}\)&lt;/span&gt; is the mean of the responding units, and &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^\star=\sum_{i=1}^{n_{cc}}\frac{H_iy_i}{n-n_{cc}}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(H_i\)&lt;/span&gt; is the number of times &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is used as substitute for a missing value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{n_{cc}}H_i=n-n_{cc}\)&lt;/span&gt; being the number of missing units. The proprties of &lt;span class=&#34;math inline&#34;&gt;\(bar{y}_{HD}\)&lt;/span&gt; depend on the procedure used to generate the numbers &lt;span class=&#34;math inline&#34;&gt;\(H_i\)&lt;/span&gt; and in general the mean and sampling variance of this estimator can be written as&lt;/p&gt;
&lt;p&gt;\[
E[\bar{y}_{HD}]=E[E[\bar{y}_{HD}\mid y_{obs}]] ;;; \text{and} ;;; Var(\bar{y}_{HD})=Var(E[\bar{y}_{HD} \mid y_{obs}]) + E[Var(\bar{y}_{HD} \mid y_{obs})],
\]&lt;/p&gt;
&lt;p&gt;where the inner expectations and variances are taken over the distribution of &lt;span class=&#34;math inline&#34;&gt;\(H_i\)&lt;/span&gt; given the observed data &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}\)&lt;/span&gt;, and the outer expectations and variances are taken over the model distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. The term &lt;span class=&#34;math inline&#34;&gt;\(E[Var(\bar{y}_{HD} \mid y_{obs})]\)&lt;/span&gt; represents the additional sampling variance from the stochastic imputation procedure. Examples of these procedures include &lt;em&gt;predictive mean matching&lt;/em&gt; or PMM(&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;) and &lt;em&gt;last value carried forward&lt;/em&gt; or LVCF(&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;predictive-mean-matching&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Predictive Mean Matching&lt;/h3&gt;
&lt;p&gt;A general approach to hot-deck imputation is to define a metric &lt;span class=&#34;math inline&#34;&gt;\(d(i,j)\)&lt;/span&gt; measuring the distance between units based on observed variables &lt;span class=&#34;math inline&#34;&gt;\(x_{i1},\ldots,x_{iJ}\)&lt;/span&gt; and then choose the imputed values that come from responding units close to the unit with the missing value, i.e. we choose the imputed value for &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; from a &lt;em&gt;donor pool&lt;/em&gt; of units &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; that are such that &lt;span class=&#34;math inline&#34;&gt;\(y_j,x_1,\ldots,x_J\)&lt;/span&gt; are observed and &lt;span class=&#34;math inline&#34;&gt;\(d(i,j)\)&lt;/span&gt; is less than some value &lt;span class=&#34;math inline&#34;&gt;\(d_0\)&lt;/span&gt;. Varying the value for &lt;span class=&#34;math inline&#34;&gt;\(d_0\)&lt;/span&gt; can control the number of available donors &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. When the choice of the metric has the form&lt;/p&gt;
&lt;p&gt;\[
d(i,j)=(\hat{y}(x_i)-\hat{y}(x_j))^2,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}(x_i)\)&lt;/span&gt; is the predicted value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; from the regression of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; from the complete units, then the procedure is known as PMM. A powerful aspect of this metric is that it weights predictors according to their ability to predict the missing variable, which allows to have some protection against misspecification of the regression of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, even though better approaches are available when good matches to donor units cannot be found or the sample size is small.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;last-value-carried-forward&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Last Value Carried Forward&lt;/h3&gt;
&lt;p&gt;Longitudinal data are often subject to attrition when units leave the study prematurely. Let &lt;span class=&#34;math inline&#34;&gt;\(y_i=(y_{i1},\ldots,y_{iJ})\)&lt;/span&gt; be a &lt;span class=&#34;math inline&#34;&gt;\((J\times1)\)&lt;/span&gt; vector of partially-observed outcomes for subject &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, and denote with &lt;span class=&#34;math inline&#34;&gt;\(y_{i,obs}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{i,mis}\)&lt;/span&gt; the observed and missing components of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(y=(y_{i,obs},y_{i,mis})\)&lt;/span&gt;. Define the indicator variable &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; taking value 0 for complete units and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; if subject &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; drops out between &lt;span class=&#34;math inline&#34;&gt;\(j-1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; time points. LVCF, also called &lt;em&gt;last observation carried forward&lt;/em&gt;(&lt;span class=&#34;citation&#34;&gt;Pocock (2013)&lt;/span&gt;), imputes all missing values for individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (for whom &lt;span class=&#34;math inline&#34;&gt;\(m_i=j\)&lt;/span&gt;) using the last recorded value for that unit, that is&lt;/p&gt;
&lt;p&gt;\[
\hat{y}_{it}=y_{i,j-1},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(t=j,\ldots,J\)&lt;/span&gt;. Although simple, this approach makes the often unrealistic assumption that the value of the outcome remains unchanged after dropout.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;According to &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;, imputation should generally be&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Conditional&lt;/strong&gt; on observed variables, to reduce bias, improve precision and preserve association between variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Multivariate&lt;/strong&gt;, to preserve association between missing variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Draws&lt;/strong&gt; from the predictive distributions rather than means, to provide valid estimates of a wide range of estimands.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Nevertheless, a main problem of SI methods is that inferences based on the imputed data do not account for imputation uncertainty and standard errors are therefore systematically underestimated, p-values of tests are too significant and confidence intervals are too narrow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pocock2013clinical&#34;&gt;
&lt;p&gt;Pocock, Stuart J. 2013. &lt;em&gt;Clinical Trials: A Practical Approach&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Inverse Probability Weighting</title>
      <link>/missmethods/inverse-probability-weighting/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/inverse-probability-weighting/</guid>
      <description>


&lt;p&gt;In certain cases, it is possible to reduce biases from case deletion by the application of weights. After incomplete cases are removed, the remaining complete cases can be weighted so that their distribution more closely resembles that of the full sample with respect to auxiliary variables. &lt;em&gt;Weighting methods&lt;/em&gt; can eliminate bias due to differential response related to the variables used to model the response probabilities, but it cannot correct for biases related to variables that are unused or unmeasured (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;). &lt;span class=&#34;citation&#34;&gt;Robins, Rotnitzky, and Zhao (1994)&lt;/span&gt; introduced &lt;em&gt;Inverse Probability Weighting&lt;/em&gt; (IPW) as a weighted regression approach that require an explicit model for the missingness but relaxes some of the parametric assumptions in the data model. Their method is an extension of &lt;em&gt;Generalized Estimating Equations&lt;/em&gt; (GEE), a popular technique for modeling marginal or populationaveraged relationships between a response variable and predictors (&lt;span class=&#34;citation&#34;&gt;Zeger, Liang, and Albert (1988)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(y_i=(y_{i1},\ldots,y_{iK})\)&lt;/span&gt; denote a vector of variables for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; subject to missing values with &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; being fully observed for &lt;span class=&#34;math inline&#34;&gt;\(i=1\ldots,n_r\)&lt;/span&gt; units and partially-observed for &lt;span class=&#34;math inline&#34;&gt;\(i=n_r+1,\ldots,n\)&lt;/span&gt; units. Define &lt;span class=&#34;math inline&#34;&gt;\(m_i=1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is incomplete and &lt;span class=&#34;math inline&#34;&gt;\(m_i=0\)&lt;/span&gt; if complete. Let &lt;span class=&#34;math inline&#34;&gt;\(x_i=(x_{i1},\ldots,x_{ip})\)&lt;/span&gt; denote a vector of fully observed covariates and suppose the interest is in estimating the mean of the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;, having the form &lt;span class=&#34;math inline&#34;&gt;\(g(x_i,\beta)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(g()\)&lt;/span&gt; is a possibly non-linear regression function indexed by a parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; of dimension &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Let also &lt;span class=&#34;math inline&#34;&gt;\(z_i=(z_{i1},\ldots,z_{iq})\)&lt;/span&gt; be a vector of fully observed auxiliary variables that potentially predictive of missingness but are not included in the model for &lt;span class=&#34;math inline&#34;&gt;\(y_i \mid x_i\)&lt;/span&gt;. When there are no missing data, a consistent estimate of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is given by the solution to the following GEE, under mild regularity conditions (&lt;span class=&#34;citation&#34;&gt;Liang and Zeger (1986)&lt;/span&gt;),&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n = D_i(x_i,\beta)(y_i-g(x_i,\beta))=0,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(D_i(x_i,\beta)\)&lt;/span&gt; is a suitably chosen &lt;span class=&#34;math inline&#34;&gt;\((d\times k)\)&lt;/span&gt; matrix of known functions of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. With missing data, the equation is applied only to the complete cases (&lt;span class=&#34;math inline&#34;&gt;\(n_{r}\)&lt;/span&gt;), which yields consistent estimates provided that&lt;/p&gt;
&lt;p&gt;\[
p(m_i=1 \mid x_i,y_i,z_i,\phi)=p(m_i=1\mid x_i,\phi),
\]&lt;/p&gt;
&lt;p&gt;that is, missingness does not depend on &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; after conditioning on &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. IPW GEE methods (&lt;span class=&#34;citation&#34;&gt;Robins and Rotnitzky (1995)&lt;/span&gt;) replace the equation with&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^{n_r} = w_i(\hat{\alpha})D_i(x_i,\beta)(y_i-g(x_i,\beta))=0,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(w_i(\hat{\alpha})=\frac{1}{p(x_i,z_i \mid \hat{\alpha})}\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(p(x_i,z_i \mid \hat{\alpha})\)&lt;/span&gt; being an estimate of the probability of being a complete unit, obtained for example via logistic regressions on &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;. If the logistic regression is correctly specified, IPW GEE yields a consistent estimator of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; provided that&lt;/p&gt;
&lt;p&gt;\[
p(m_i=1 \mid x_i,y_i,z_i,\phi)=p(m_i=1\mid x_i,z_i\phi).
\]&lt;/p&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose the full data consists of a single outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and an additional variable &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; and that the objective is to estimate the population outcome mean &lt;span class=&#34;math inline&#34;&gt;\(\mu=\text{E}[y]\)&lt;/span&gt;. If data were fully observed for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; individuals, an obvious estimator of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; would be the sample outcome mean&lt;/p&gt;
&lt;p&gt;\[
\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i,
\]&lt;/p&gt;
&lt;p&gt;which is equivalent to the solution to the estimating equation &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^n(y_i-\mu)=0\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is partially observed (while &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is always fully observed), individuals may fall into one of two missingness patterns &lt;span class=&#34;math inline&#34;&gt;\(r=(r_{y},r_{z})\)&lt;/span&gt;, namely &lt;span class=&#34;math inline&#34;&gt;\(r=(1,1)\)&lt;/span&gt; if both variables are observed or &lt;span class=&#34;math inline&#34;&gt;\(r=(1,0)\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is missing. Let &lt;span class=&#34;math inline&#34;&gt;\(c=1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(r=(1,1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c=0\)&lt;/span&gt; otherwise, so that the observed data can be summarised as &lt;span class=&#34;math inline&#34;&gt;\((c,cy,z)\)&lt;/span&gt;. Assuming that missingness only depends on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p(c=1 \mid y,z)=p(c=1 \mid z)=\pi(z),
\]&lt;/p&gt;
&lt;p&gt;then the missing data mechanism is &lt;em&gt;Missing At Random&lt;/em&gt; (MAR). Under these conditions, the sample mean of the complete cases &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{cc}=\frac{\sum_{i=1}^nc_iy_i}{c_i}\)&lt;/span&gt;, i.e. the solution to the equation &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^nc_i(y_i-\mu)=0\)&lt;/span&gt;, is not a consistent estimator of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. To correct for this, the IPW complete case estimating equation&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n\frac{c_i}{\pi(z_i)}(y_i-\mu)=0,
\]&lt;/p&gt;
&lt;p&gt;can be used to weight the contribution of each complete case by the inverse of &lt;span class=&#34;math inline&#34;&gt;\(\pi(z_i)\)&lt;/span&gt;. The solution of the equation corresponds to the IPW estimator&lt;/p&gt;
&lt;p&gt;\[
\mu_{ipw}=\left(\sum_{i=1}^n \frac{c_i}{\pi(z_i)} \right)^{-1} \sum_{i=1}^n \frac{c_iy_i}{\pi(z_i)},
\]&lt;/p&gt;
&lt;p&gt;which is unbiased under MAR and for &lt;span class=&#34;math inline&#34;&gt;\(\pi(z)&amp;gt;0\)&lt;/span&gt;. In case you want to have a look at the &lt;a href=&#34;https://www4.stat.ncsu.edu/~davidian/st790/notes/chap5.pdf&#34;&gt;proof&lt;/a&gt; of this I put here the link. In most situations &lt;span class=&#34;math inline&#34;&gt;\(\pi(z_i)\)&lt;/span&gt; is not known and must be estimated from the data, typically posing some model for &lt;span class=&#34;math inline&#34;&gt;\(p(c=1 \mid z, \hat{\alpha})\)&lt;/span&gt;, indexed by some parameter &lt;span class=&#34;math inline&#34;&gt;\(\hat{\alpha}\)&lt;/span&gt;, for example a logistic regression&lt;/p&gt;
&lt;p&gt;\[
\text{logit}(\pi)=\alpha_0 + \alpha_1z.
\]&lt;/p&gt;
&lt;p&gt;Of course, if the model for &lt;span class=&#34;math inline&#34;&gt;\(\pi(z)\)&lt;/span&gt; is misspecified, &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ipw}\)&lt;/span&gt; can be an inconsistent estimator. In addition, IPW methods typically used data only from the completers discarding all the partially observed values, which is clearly inefficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Thus, IPW estimators can correct for the bias of unweighted estimators due to the dependence of the missingness mechanism on &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;). The basic intuition of IPW methods is that each subject’s contribution to the weighted &lt;em&gt;Complete Case Analysis&lt;/em&gt; (CCA) is replicated &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; times in order to account once for herself and &lt;span class=&#34;math inline&#34;&gt;\((1-w_i)\)&lt;/span&gt; times for those subjects with the same responses and covariates who are missing. These models are called &lt;em&gt;semiparametric&lt;/em&gt; because they apart from requiring the regression equation to have a specific form, they do not specify any probability distribution for the response variable (&lt;span class=&#34;citation&#34;&gt;Molenberghs et al. (2014)&lt;/span&gt;). Older GEE methods can accommodate missing values only if they are &lt;em&gt;Missing Completely At Random&lt;/em&gt; (MCAR), while more recent methods allow them to be MAR or even &lt;em&gt;Missing Not At Random&lt;/em&gt; (MNAR), provided that a model for the missingness is correctly specified (&lt;span class=&#34;citation&#34;&gt;Robins, Rotnitzky, and Zhao (1995)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Rotnitzky, Robins, and Scharfstein (1998)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-liang1986longitudinal&#34;&gt;
&lt;p&gt;Liang, Kung-Yee, and Scott L Zeger. 1986. “Longitudinal Data Analysis Using Generalized Linear Models.” &lt;em&gt;Biometrika&lt;/em&gt; 73 (1): 13–22.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-molenberghs2014handbook&#34;&gt;
&lt;p&gt;Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. &lt;em&gt;Handbook of Missing Data Methodology&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins1995semiparametric&#34;&gt;
&lt;p&gt;Robins, James M, and Andrea Rotnitzky. 1995. “Semiparametric Efficiency in Multivariate Regression Models with Missing Data.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 90 (429): 122–29.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins1994estimation&#34;&gt;
&lt;p&gt;Robins, James M, Andrea Rotnitzky, and Lue Ping Zhao. 1994. “Estimation of Regression Coefficients When Some Regressors Are Not Always Observed.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 89 (427): 846–66.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins1995analysis&#34;&gt;
&lt;p&gt;———. 1995. “Analysis of Semiparametric Regression Models for Repeated Outcomes in the Presence of Missing Data.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 90 (429): 106–21.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rotnitzky1998semiparametric&#34;&gt;
&lt;p&gt;Rotnitzky, Andrea, James M Robins, and Daniel O Scharfstein. 1998. “Semiparametric Regression for Repeated Outcomes with Nonignorable Nonresponse.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 93 (444): 1321–39.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-zeger1988models&#34;&gt;
&lt;p&gt;Zeger, Scott L, Kung-Yee Liang, and Paul S Albert. 1988. “Models for Longitudinal Data: A Generalized Estimating Equation Approach.” &lt;em&gt;Biometrics&lt;/em&gt;, 1049–60.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Joint Multiple Imputation</title>
      <link>/missmethods/joint-multiple-imputation/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/joint-multiple-imputation/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Multiple Imputation&lt;/em&gt;(MI) refers to the procedure of replacing each missing value by a set of &lt;span class=&#34;math inline&#34;&gt;\(H\geq 2\)&lt;/span&gt; imputed values. These are ordered in the sense that &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets can be created from the sets of imputations, where the first imputed value replaces the missing value in the first completed data set, the second imputed value in the second completed data set, and so on. Next, standard complete data methods are used to analyse each completed data set. When the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; sets of imputations are repeated random draws from the predictive distribution of the missing data under a particular model of missingness, the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences can be combined to form one inference that properly reflects uncertainty due to missing values under that model. In general, MI procedures can be summarised in three main steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Specify an &lt;strong&gt;imputation model&lt;/strong&gt; to generate &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; imputed values, typically taken as random draws from the predictive distribution of the missing values given the observed values, and create &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets using these imputations and the observed data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Analyse each completed data sets using standard complete data methods based on an &lt;strong&gt;analysis model&lt;/strong&gt;, and derive &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pool together the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences into a single inference using standard MI formulas, which ensure that missing data uncertainty is taken into account&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mi was first proposed by Rubin (&lt;span class=&#34;citation&#34;&gt;Rubin (1978)&lt;/span&gt;) and has become more popular over time (&lt;span class=&#34;citation&#34;&gt;Rubin (1996)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;), as well as the focus of research for methodological and practical applications in a variety of fields (&lt;span class=&#34;citation&#34;&gt;Herzog and Rubin (1983)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Rubin and Schenker (1987)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Schafer (1999)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Carpenter and Kenward (2012)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Molenberghs et al. (2014)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Van Buuren (2018)&lt;/span&gt;). MI shares both advantages of &lt;em&gt;Single Imputaiton&lt;/em&gt; (SI) methods and solves both disadvantages. Indeed, like SI, MI methods allow the analyst to use familiar complete data methods when analysing the completed data sets. The only disadvantage of MI compared with SI methods is that it takes more time to generate the imputations and analyse the completed data sets. However, &lt;span class=&#34;citation&#34;&gt;Rubin (2004)&lt;/span&gt; showed that in order to obtain sufficiently precise estimates, a relatively small number of imputations (typically &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt;) is required. For example, considering a situation with &lt;span class=&#34;math inline&#34;&gt;\(\lambda=50\%\)&lt;/span&gt; missing information and &lt;span class=&#34;math inline&#34;&gt;\(H=10\)&lt;/span&gt; imputations, the efficiency of MI can be shown to be equal to &lt;span class=&#34;math inline&#34;&gt;\((1+\frac{\lambda}{H})^{-1}=95\%\)&lt;/span&gt;. In addition, in today’s computing environments, the work of analysing the completed data sets is quite modest since it involves performing the same task &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; times. Thus, once a precedure to combine multiple completed data sets is established, the additonal time and effort to handle &lt;span class=&#34;math inline&#34;&gt;\(50\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(20\)&lt;/span&gt;, or &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; imputations if often of little consequence.&lt;/p&gt;
&lt;p&gt;In the first step of MI, imputations should ideally be created as repeated draws from the &lt;em&gt;posterior predictive distribution&lt;/em&gt; of the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}\)&lt;/span&gt; given the observed values &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}\)&lt;/span&gt;, each repetition being an independent drawing of the parameters and missing values. In practice, implicit imputation models can also be used in place of explicit imputation models (&lt;span class=&#34;citation&#34;&gt;Herzog and Rubin (1983)&lt;/span&gt;). In the second step, each completed data set is analysed using the same complete data method that would be used in the absence of missingness. Finally, in the last step, standard procedures should be used to combine the compelted data inferences into a single one. The simplest and most popular method for combining the reuslts of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets is known as &lt;em&gt;Rubin’s rules&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Rubin (2004)&lt;/span&gt;), which can be explained with a simple example.&lt;/p&gt;
&lt;div id=&#34;rubins-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rubin’s rules&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_h\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(h=1,\ldots,H\)&lt;/span&gt;, be the completed data estimates and sampling variances for a scalar estimand &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, calculated from &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; repeated imputations under a given imputation model. Then, according to Rubin’s rules, the combined estimate is simply the average of the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data estimates, that is&lt;/p&gt;
&lt;p&gt;\[
\bar{\theta}_{H}=\frac{1}{H}\sum_{h=1}^{H}\hat{\theta}_{h}.
\]&lt;/p&gt;
&lt;p&gt;Because the imputations under MI are conditional draws, under a good imputaton model, they provide valid estimates for a wide range of estimands. In addition, the averaging over &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; imputed data sets increases the efficiency of estimation over that obtained from a single completed data set. The variability associated with the pooled estimate has two components: the &lt;em&gt;average within-imputation variance&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\bar{V}_H\)&lt;/span&gt; and the &lt;em&gt;between-imputation variance&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(B_H\)&lt;/span&gt;, defined as&lt;/p&gt;
&lt;p&gt;\[
\bar{V}_{H}=\frac{1}{H}\sum_{h=1}^{H}V_{h} \;\;\; \text{and} \;\;\; B_{H}=\frac{1}{H-1}\sum_{h=1}^{H}(\hat{\theta}_{h}-\bar{\theta}_{H})^2.
\]&lt;/p&gt;
&lt;p&gt;The total variability associated with &lt;span class=&#34;math inline&#34;&gt;\(\bar{\theta}_H\)&lt;/span&gt; is the computed as&lt;/p&gt;
&lt;p&gt;\[
T_{H}=\bar{V}_H + \frac{H+1}{H}B_{H},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\((1+\frac{1}{H})\)&lt;/span&gt; is an adjustment factor for finite due to estimating &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\bar{\theta}_H\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\lambda}_H=(1+\frac{1}{H})\frac{B_H}{T_H}\)&lt;/span&gt; is known as the &lt;em&gt;fraction of missing information&lt;/em&gt; and is an estimate of the fraction of information about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that is missing due to nonresponse. For large sample sizes and scalar quantities like &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the reference distribution for interval estimates and significance tests is a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution&lt;/p&gt;
&lt;p&gt;\[
(\theta - \bar{\theta}_H)\frac{1}{\sqrt{T^2_H}} \sim t_v,
\]&lt;/p&gt;
&lt;p&gt;where the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be approximated with the quantity &lt;span class=&#34;math inline&#34;&gt;\(v=(H-1)\left(1+\frac{1}{H+1}\frac{\bar{V}_H}{B_H} \right)^2\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Rubin and Schenker (1987)&lt;/span&gt;). In small data sets, an improved version of &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be obtained as &lt;span class=&#34;math inline&#34;&gt;\(v^\star=(\frac{1}{v}+\frac{1}{\hat{v}_{obs}})^{-1}\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;\[
\hat{v}_{obs}=(1-\hat{\lambda}_{H})\left(\frac{v_{com}+1}{v_{com}+3}\right)v_{com},
\]&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(v_{com}\)&lt;/span&gt; being the degrees of freedom for appropriate or exact &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; inferences about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; when there are no missing values (&lt;span class=&#34;citation&#34;&gt;Barnard and Rubin (1999)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The validity of MI rests on how the imputations are created and how that procedure relates to the model used to subsequently analyze the data. Creating MIs often requires special algorithms (&lt;span class=&#34;citation&#34;&gt;Schafer (1997)&lt;/span&gt;). In general, they should be drawn from a distribution for the missing data that reflects uncertainty about the parameters of the data model. Recall that with SI methods, it is desirable to impute from the conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(p(y_{mis}\mid y_{obs},\hat{\theta})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is an estimate derived from the observed data. MI extends this approach by first simulating &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; independent plausible values for the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_1,\ldots,\theta_H\)&lt;/span&gt; and then drawing the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}^h\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(p(y_{mis}\mid y_{obs}, \theta_h)\)&lt;/span&gt;. Treating parameters as random rather than fixed is
an essential part of MI. For this reason, it is natural (but not essential) to motivate MI from the Bayesian perspective, in which the state of knowledge about parameters is represented through a posterior distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;joint-multiple-imputation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Joint Multiple Imputation&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-barnard1999miscellanea&#34;&gt;
&lt;p&gt;Barnard, John, and Donald B Rubin. 1999. “Miscellanea. Small-Sample Degrees of Freedom with Multiple Imputation.” &lt;em&gt;Biometrika&lt;/em&gt; 86 (4): 948–55.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-carpenter2012multiple&#34;&gt;
&lt;p&gt;Carpenter, James, and Michael Kenward. 2012. &lt;em&gt;Multiple Imputation and Its Application&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-herzog1983using&#34;&gt;
&lt;p&gt;Herzog, Thomas N, and Donald B Rubin. 1983. “Using Multiple Imputations to Handle Nonresponse in Sample Surveys.” &lt;em&gt;Incomplete Data in Sample Surveys&lt;/em&gt; 2: 209–45.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-molenberghs2014handbook&#34;&gt;
&lt;p&gt;Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. &lt;em&gt;Handbook of Missing Data Methodology&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1978multiple&#34;&gt;
&lt;p&gt;Rubin, Donald B. 1978. “Multiple Imputations in Sample Surveys a Phenomenological Bayesian Approach to Nonresponse.” &lt;em&gt;Proceedings of the Survey Research Methods Section of the American Statistical Association&lt;/em&gt; 1: 20–34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1996multiple&#34;&gt;
&lt;p&gt;———. 1996. “Multiple Imputation After 18 Years.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 91 (434): 473–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin2004multiple&#34;&gt;
&lt;p&gt;———. 2004. &lt;em&gt;Multiple Imputation for Nonresponse in Surveys&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1987interval&#34;&gt;
&lt;p&gt;Rubin, Donald B, and Nathaniel Schenker. 1987. “Interval Estimation from Multiply Imputed Data: A Case Study Using Census Agriculture Industry Codes.” &lt;em&gt;Journal of Official Statistics&lt;/em&gt; 3 (4): 375.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer1997analysis&#34;&gt;
&lt;p&gt;Schafer, Joseph L. 1997. &lt;em&gt;Analysis of Incomplete Multivariate Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer1999multiple&#34;&gt;
&lt;p&gt;———. 1999. “Multiple Imputation: A Primer.” &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt; 8 (1): 3–15.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2018flexible&#34;&gt;
&lt;p&gt;Van Buuren, Stef. 2018. &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multiple Imputation by Chained Equations</title>
      <link>/missmethods/multiple-imputation-by-chained-equations/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/multiple-imputation-by-chained-equations/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Multiple Imputation&lt;/em&gt;(MI) refers to the procedure of replacing each missing value by a set of &lt;span class=&#34;math inline&#34;&gt;\(H\geq 2\)&lt;/span&gt; imputed values. These are ordered in the sense that &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets can be created from the sets of imputations, where the first imputed value replaces the missing value in the first completed data set, the second imputed value in the second completed data set, and so on. Next, standard complete data methods are used to analyse each completed data set. When the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; sets of imputations are repeated random draws from the predictive distribution of the missing data under a particular model of missingness, the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences can be combined to form one inference that properly reflects uncertainty due to missing values under that model. In general, MI procedures can be summarised in three main steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Specify an &lt;strong&gt;imputation model&lt;/strong&gt; to generate &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; imputed values, typically taken as random draws from the predictive distribution of the missing values given the observed values, and create &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets using these imputations and the observed data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Analyse each completed data sets using standard complete data methods based on an &lt;strong&gt;analysis model&lt;/strong&gt;, and derive &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pool together the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences into a single inference using standard MI formulas, which ensure that missing data uncertainty is taken into account&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mi was first proposed by Rubin (&lt;span class=&#34;citation&#34;&gt;Rubin (1978)&lt;/span&gt;) and has become more popular over time (&lt;span class=&#34;citation&#34;&gt;Rubin (1996)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;), as well as the focus of research for methodological and practical applications in a variety of fields (&lt;span class=&#34;citation&#34;&gt;Herzog and Rubin (1983)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Rubin and Schenker (1987)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Schafer (1999)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Carpenter and Kenward (2012)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Molenberghs et al. (2014)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Van Buuren (2018)&lt;/span&gt;). MI shares both advantages of &lt;em&gt;Single Imputaiton&lt;/em&gt; (SI) methods and solves both disadvantages. Indeed, like SI, MI methods allow the analyst to use familiar complete data methods when analysing the completed data sets. The only disadvantage of MI compared with SI methods is that it takes more time to generate the imputations and analyse the completed data sets. However, &lt;span class=&#34;citation&#34;&gt;Rubin (2004)&lt;/span&gt; showed that in order to obtain sufficiently precise estimates, a relatively small number of imputations (typically &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt;) is required. For example, considering a situation with &lt;span class=&#34;math inline&#34;&gt;\(\lambda=50\%\)&lt;/span&gt; missing information and &lt;span class=&#34;math inline&#34;&gt;\(H=10\)&lt;/span&gt; imputations, the efficiency of MI can be shown to be equal to &lt;span class=&#34;math inline&#34;&gt;\((1+\frac{\lambda}{H})^{-1}=95\%\)&lt;/span&gt;. In addition, in today’s computing environments, the work of analysing the completed data sets is quite modest since it involves performing the same task &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; times. Thus, once a precedure to combine multiple completed data sets is established, the additonal time and effort to handle &lt;span class=&#34;math inline&#34;&gt;\(50\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(20\)&lt;/span&gt;, or &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; imputations if often of little consequence.&lt;/p&gt;
&lt;p&gt;In the first step of MI, imputations should ideally be created as repeated draws from the &lt;em&gt;posterior predictive distribution&lt;/em&gt; of the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}\)&lt;/span&gt; given the observed values &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}\)&lt;/span&gt;, each repetition being an independent drawing of the parameters and missing values. In practice, implicit imputation models can also be used in place of explicit imputation models (&lt;span class=&#34;citation&#34;&gt;Herzog and Rubin (1983)&lt;/span&gt;). In the second step, each completed data set is analysed using the same complete data method that would be used in the absence of missingness. Finally, in the last step, standard procedures should be used to combine the compelted data inferences into a single one. The simplest and most popular method for combining the reuslts of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets is known as &lt;em&gt;Rubin’s rules&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Rubin (2004)&lt;/span&gt;), which can be explained with a simple example.&lt;/p&gt;
&lt;div id=&#34;rubins-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rubin’s rules&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_h\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(h=1,\ldots,H\)&lt;/span&gt;, be the completed data estimates and sampling variances for a scalar estimand &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, calculated from &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; repeated imputations under a given imputation model. Then, according to Rubin’s rules, the combined estimate is simply the average of the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data estimates, that is&lt;/p&gt;
&lt;p&gt;\[
\bar{\theta}_{H}=\frac{1}{H}\sum_{h=1}^{H}\hat{\theta}_{h}.
\]&lt;/p&gt;
&lt;p&gt;Because the imputations under MI are conditional draws, under a good imputaton model, they provide valid estimates for a wide range of estimands. In addition, the averaging over &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; imputed data sets increases the efficiency of estimation over that obtained from a single completed data set. The variability associated with the pooled estimate has two components: the &lt;em&gt;average within-imputation variance&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\bar{V}_H\)&lt;/span&gt; and the &lt;em&gt;between-imputation variance&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(B_H\)&lt;/span&gt;, defined as&lt;/p&gt;
&lt;p&gt;\[
\bar{V}_{H}=\frac{1}{H}\sum_{h=1}^{H}V_{h} \;\;\; \text{and} \;\;\; B_{H}=\frac{1}{H-1}\sum_{h=1}^{H}(\hat{\theta}_{h}-\bar{\theta}_{H})^2.
\]&lt;/p&gt;
&lt;p&gt;The total variability associated with &lt;span class=&#34;math inline&#34;&gt;\(\bar{\theta}_H\)&lt;/span&gt; is the computed as&lt;/p&gt;
&lt;p&gt;\[
T_{H}=\bar{V}_H + \frac{H+1}{H}B_{H},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\((1+\frac{1}{H})\)&lt;/span&gt; is an adjustment factor for finite due to estimating &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\bar{\theta}_H\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\lambda}_H=(1+\frac{1}{H})\frac{B_H}{T_H}\)&lt;/span&gt; is known as the &lt;em&gt;fraction of missing information&lt;/em&gt; and is an estimate of the fraction of information about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that is missing due to nonresponse. For large sample sizes and scalar quantities like &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the reference distribution for interval estimates and significance tests is a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution&lt;/p&gt;
&lt;p&gt;\[
(\theta - \bar{\theta}_H)\frac{1}{\sqrt{T^2_H}} \sim t_v,
\]&lt;/p&gt;
&lt;p&gt;where the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be approximated with the quantity &lt;span class=&#34;math inline&#34;&gt;\(v=(H-1)\left(1+\frac{1}{H+1}\frac{\bar{V}_H}{B_H} \right)^2\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Rubin and Schenker (1987)&lt;/span&gt;). In small data sets, an improved version of &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be obtained as &lt;span class=&#34;math inline&#34;&gt;\(v^\star=(\frac{1}{v}+\frac{1}{\hat{v}_{obs}})^{-1}\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;\[
\hat{v}_{obs}=(1-\hat{\lambda}_{H})\left(\frac{v_{com}+1}{v_{com}+3}\right)v_{com},
\]&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(v_{com}\)&lt;/span&gt; being the degrees of freedom for appropriate or exact &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; inferences about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; when there are no missing values (&lt;span class=&#34;citation&#34;&gt;Barnard and Rubin (1999)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The validity of MI rests on how the imputations are created and how that procedure relates to the model used to subsequently analyze the data. Creating MIs often requires special algorithms (&lt;span class=&#34;citation&#34;&gt;Schafer (1997)&lt;/span&gt;). In general, they should be drawn from a distribution for the missing data that reflects uncertainty about the parameters of the data model. Recall that with SI methods, it is desirable to impute from the conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(p(y_{mis}\mid y_{obs},\hat{\theta})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is an estimate derived from the observed data. MI extends this approach by first simulating &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; independent plausible values for the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_1,\ldots,\theta_H\)&lt;/span&gt; and then drawing the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}^h\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(p(y_{mis}\mid y_{obs}, \theta_h)\)&lt;/span&gt;. Treating parameters as random rather than fixed is
an essential part of MI. For this reason, it is natural (but not essential) to motivate MI from the Bayesian perspective, in which the state of knowledge about parameters is represented through a posterior distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-imputation-by-chained-equations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Imputation by Chained Equations&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-barnard1999miscellanea&#34;&gt;
&lt;p&gt;Barnard, John, and Donald B Rubin. 1999. “Miscellanea. Small-Sample Degrees of Freedom with Multiple Imputation.” &lt;em&gt;Biometrika&lt;/em&gt; 86 (4): 948–55.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-carpenter2012multiple&#34;&gt;
&lt;p&gt;Carpenter, James, and Michael Kenward. 2012. &lt;em&gt;Multiple Imputation and Its Application&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-herzog1983using&#34;&gt;
&lt;p&gt;Herzog, Thomas N, and Donald B Rubin. 1983. “Using Multiple Imputations to Handle Nonresponse in Sample Surveys.” &lt;em&gt;Incomplete Data in Sample Surveys&lt;/em&gt; 2: 209–45.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-molenberghs2014handbook&#34;&gt;
&lt;p&gt;Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. &lt;em&gt;Handbook of Missing Data Methodology&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1978multiple&#34;&gt;
&lt;p&gt;Rubin, Donald B. 1978. “Multiple Imputations in Sample Surveys a Phenomenological Bayesian Approach to Nonresponse.” &lt;em&gt;Proceedings of the Survey Research Methods Section of the American Statistical Association&lt;/em&gt; 1: 20–34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1996multiple&#34;&gt;
&lt;p&gt;———. 1996. “Multiple Imputation After 18 Years.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 91 (434): 473–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin2004multiple&#34;&gt;
&lt;p&gt;———. 2004. &lt;em&gt;Multiple Imputation for Nonresponse in Surveys&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1987interval&#34;&gt;
&lt;p&gt;Rubin, Donald B, and Nathaniel Schenker. 1987. “Interval Estimation from Multiply Imputed Data: A Case Study Using Census Agriculture Industry Codes.” &lt;em&gt;Journal of Official Statistics&lt;/em&gt; 3 (4): 375.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer1997analysis&#34;&gt;
&lt;p&gt;Schafer, Joseph L. 1997. &lt;em&gt;Analysis of Incomplete Multivariate Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer1999multiple&#34;&gt;
&lt;p&gt;———. 1999. “Multiple Imputation: A Primer.” &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt; 8 (1): 3–15.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2018flexible&#34;&gt;
&lt;p&gt;Van Buuren, Stef. 2018. &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Weighting Adjustments</title>
      <link>/missmethods/weighting-adjustments/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/weighting-adjustments/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The notion of reducing bias due to missingness through &lt;em&gt;reweighting methods&lt;/em&gt; has its root in the survey literature and the basic idea is closely related to weighting in randomisation inference for finite population surveys (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;). In particular, in probability sampling, a unit selected from a target population with probability &lt;span class=&#34;math inline&#34;&gt;\(\pi_i\)&lt;/span&gt; can be thought as “representing” &lt;span class=&#34;math inline&#34;&gt;\(\pi^{-1}_i\)&lt;/span&gt; units in the population and hence should be given weight &lt;span class=&#34;math inline&#34;&gt;\(\pi^{-1}_i\)&lt;/span&gt; when estimating population quantities. For example, in a stratified random sample, a selected unit in stratum &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; represents &lt;span class=&#34;math inline&#34;&gt;\(\frac{N_j}{n_j}\)&lt;/span&gt; population units, where &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; indicates the units sampled from the &lt;span class=&#34;math inline&#34;&gt;\(N_j\)&lt;/span&gt; population units in stratum &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots,J\)&lt;/span&gt;. The population total &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; can then be estimated by the weighted sum&lt;/p&gt;
&lt;p&gt;\[
T = \sum_{i=1}^{n}y_i\pi^{-1}_i,
\]&lt;/p&gt;
&lt;p&gt;known as the Horvitz-Thompson estimate (&lt;span class=&#34;citation&#34;&gt;Horvitz and Thompson (1952)&lt;/span&gt;), while the stratified mean can be written as&lt;/p&gt;
&lt;p&gt;\[
\bar{y}_{w} = \frac{1}{n}\sum_{i=1}^{n}w_iy_i,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(w_i=\frac{n\pi^{-1}_i}{\sum_{k=1}^n\pi^{-1}_k}\)&lt;/span&gt; is the sampling weight attached to the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th unit scaled tosum up to the sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Weighting class estimators extend this approach to handle missing data such that, if the probabilities of response for unit &lt;span class=&#34;math inline&#34;&gt;\(\phi_i\)&lt;/span&gt; were known, then the probability of selection and response is &lt;span class=&#34;math inline&#34;&gt;\(\pi_i\phi_i\)&lt;/span&gt; and we have&lt;/p&gt;
&lt;p&gt;\[
\bar{y}_{w} = \frac{1}{n_r}\sum_{i=1}^{n_r}w_iy_i,
\]&lt;/p&gt;
&lt;p&gt;where the sum is now over responding units and &lt;span class=&#34;math inline&#34;&gt;\(w_i=\frac{n_r(\pi_i\phi_i)^{-1}}{\sum_{k=1}^{n_r}(\pi_k\phi_k)^{-1}}\)&lt;/span&gt;. In practice, the response probability &lt;span class=&#34;math inline&#34;&gt;\(\phi_i\)&lt;/span&gt; is not known and is typically estimated based on the information available for respondents and nonrespondents (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;weighting-class-estimator-of-the-mean&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weighting Class Estimator of the Mean&lt;/h2&gt;
&lt;p&gt;A simple reweighting approach is to partition the sample into &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; “weighting classes” according to the variables observed for respondents and nonrespondents. If &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; is the sample size, &lt;span class=&#34;math inline&#34;&gt;\(n_{rj}\)&lt;/span&gt; the number of respondents in class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(n_r=\sum_{j=1}^Jr_j\)&lt;/span&gt;, then a simple estimator of the response probability for units in class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is given by &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_{rj}}{n_j}\)&lt;/span&gt;. Thus, responding units in class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; receive weight &lt;span class=&#34;math inline&#34;&gt;\(w_i=\frac{n_r(\pi_i\hat{\phi}_i)^{-1}}{\sum_{k=1}^{n_r}(\pi_k\hat{\phi}_k)^{-1}}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\phi}_i=\frac{n_{rj}}{n_j}\)&lt;/span&gt; for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. The weighting class estimate of the mean is then&lt;/p&gt;
&lt;p&gt;\[
\bar{y}_{w} = \frac{1}{n_r}\sum_{i=1}^{n_r}w_iy_i,
\]&lt;/p&gt;
&lt;p&gt;which is unbiased under the &lt;em&gt;quasirandomisation&lt;/em&gt; assumption (&lt;span class=&#34;citation&#34;&gt;Oh and Scheuren (1983)&lt;/span&gt;), which requires respondents in weighting class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; to be a random sample of the sampled units, i.e. data are &lt;em&gt;Missing Completely At Random&lt;/em&gt; (MCAR) within adjustment class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. Weighting class adjustments are simple because the same weights are obtained regardless of the outcome tp which they are applied, but these are inefficient and generally involves an increase in sampling variance for outcomes that are weakly related to the weighting class variable. Assuming random sampling within weighting classes, a constant variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; for an outcome &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and ignoring sampling variation in the weights, the increase in sampling variance of a sample mean is&lt;/p&gt;
&lt;p&gt;\[
\text{Var}\left(\frac{1}{n_{r}}\sum_{i=1}^{n_{r}}w_iy_i \right) = \frac{\sigma^2}{n_{r}^2}\left(\sum_{i=1}^{n_{r}}w_{i}^{2} \right) = \frac{\sigma^2}{n_{r}}(1+\text{cv}^2(w_i)),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{cv}(w_i)\)&lt;/span&gt; is the coefficient of variation of the weights (scaled to average one), which is a rough measure of the proportional increase in sampling variance due to weighting (&lt;span class=&#34;citation&#34;&gt;Kish (1992)&lt;/span&gt;). When the weighting class variable is predictive of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, weighting methods can lead to a reduction in sampling variance. &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt; summarise the effect of weighting on the bias and sampling variance of an estimated mean, according to whether the associations between the adjustment cells and the outcome &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and missing indicator &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; are high or low.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Effect of weighting adjustments on bias and sampling variance of a mean.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Low (y)
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
High (y)
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Low (m)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bias: /, var: /
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bias: /, var: -
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
High (m)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bias: /, var: +
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bias: -, var: -
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Thus, weighting is only effective when the outcome is associated with the adjustment cell variable because otherwise the sampling variance is increased with no bias reduction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;propensity-weighting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Propensity Weighting&lt;/h2&gt;
&lt;p&gt;In some settings, weighting class estimates cannot be feasibly derived by all recorded variables X because the number of classes become too large and some may include cells with nonrespondents but no respondents for which the nonresponse weight is infinite. The theory of propensity scores (&lt;span class=&#34;citation&#34;&gt;Rosenbaum and Rubin (1983)&lt;/span&gt;) provides a prescription for choosing the coarsest reduction of the variables to a weighting class variable &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. Suppose the data are &lt;em&gt;Missing At Random&lt;/em&gt; (MAR) such that&lt;/p&gt;
&lt;p&gt;\[
p(m\mid X,y,\phi)=p(m\mid X,\phi),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; are unknown parameters and define the nonresponse propensity for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;\[
\rho(x_i,\phi)=p(m_i=1 \mid \phi),
\]&lt;/p&gt;
&lt;p&gt;assuming that this is strictly positive for all values of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. Then, it can be shown that&lt;/p&gt;
&lt;p&gt;\[
p(m\mid \rho(X,\phi),y,\phi)=p(m\mid \rho(X,\phi),\phi),
\]&lt;/p&gt;
&lt;p&gt;so that respondents are a random subsample within strata defined by the propensity score &lt;span class=&#34;math inline&#34;&gt;\(\rho(X,\phi)\)&lt;/span&gt;. In practice the parameter &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is unknown and must be estimated from sample data, for example via logistic, probit or robit regressions of the missingness indicator &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; based on respondent and nonrespondent data (&lt;span class=&#34;citation&#34;&gt;Liu (2004)&lt;/span&gt;). A variant of this procedure is to weight respondents &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; directly by the inverse of the estimated propensity score &lt;span class=&#34;math inline&#34;&gt;\(\rho(X,\hat{\phi})^{-1}\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Cassel, Sarndal, and Wretman (1983)&lt;/span&gt;), which allows to remove bias but may cause two problems: 1) estimates may be associated with very high sampling variances due to nonrespondents with low response propensity estimates receiving large nonresponse weights; 2) more reliance on correct model specification of the propensity score regression than response propensity stratification.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-cassel1983some&#34;&gt;
&lt;p&gt;Cassel, Claes M, Carl-Erik Sarndal, and Jan H Wretman. 1983. “Some Uses of Statistical Models in Connection with the Nonresponse Problem.” &lt;em&gt;Incomplete Data in Sample Surveys&lt;/em&gt; 3: 143–60.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-horvitz1952generalization&#34;&gt;
&lt;p&gt;Horvitz, Daniel G, and Donovan J Thompson. 1952. “A Generalization of Sampling Without Replacement from a Finite Universe.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 47 (260): 663–85.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kish1992weighting&#34;&gt;
&lt;p&gt;Kish, Leslie. 1992. “Weighting for Unequal Pi.” &lt;em&gt;Journal of Official Statistics&lt;/em&gt; 8 (2): 183.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-liu2004robit&#34;&gt;
&lt;p&gt;Liu, Chuanhai. 2004. “Robit Regression: A Simple Robust Alternative to Logistic and Probit Regression.” &lt;em&gt;Applied Bayesian Modeling and Casual Inference from Incomplete-Data Perspectives&lt;/em&gt;, 227–38.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-oh1983weighting&#34;&gt;
&lt;p&gt;Oh, H, and F Scheuren. 1983. “Weighting Adjustment for Unit Nonresponse. Chap. 13 in Vol. 2, Part 4 of Incomplete Data in Sample Surveys, Edited by William G. Madow, Harold Nisselson, and Ingram Olkin.” New York: Academic Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rosenbaum1983central&#34;&gt;
&lt;p&gt;Rosenbaum, Paul R, and Donald B Rubin. 1983. “The Central Role of the Propensity Score in Observational Studies for Causal Effects.” &lt;em&gt;Biometrika&lt;/em&gt; 70 (1): 41–55.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
