<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Multiple Imputation by Chained Equations on Andrea Gabrio</title>
    <link>/tags/multiple-imputation-by-chained-equations/</link>
    <description>Recent content in Multiple Imputation by Chained Equations on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/multiple-imputation-by-chained-equations/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Joint Multiple Imputation</title>
      <link>/missmethods/joint-multiple-imputation/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/joint-multiple-imputation/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Multiple Imputation&lt;/em&gt;(MI) refers to the procedure of replacing each missing value by a set of &lt;span class=&#34;math inline&#34;&gt;\(H\geq 2\)&lt;/span&gt; imputed values. These are ordered in the sense that &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets can be created from the sets of imputations, where the first imputed value replaces the missing value in the first completed data set, the second imputed value in the second completed data set, and so on. Next, standard complete data methods are used to analyse each completed data set. When the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; sets of imputations are repeated random draws from the predictive distribution of the missing data under a particular model of missingness, the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences can be combined to form one inference that properly reflects uncertainty due to missing values under that model. In general, MI procedures can be summarised in three main steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Specify an &lt;strong&gt;imputation model&lt;/strong&gt; to generate &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; imputed values, typically taken as random draws from the predictive distribution of the missing values given the observed values, and create &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets using these imputations and the observed data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Analyse each completed data sets using standard complete data methods based on an &lt;strong&gt;analysis model&lt;/strong&gt;, and derive &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pool together the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences into a single inference using standard MI formulas, which ensure that missing data uncertainty is taken into account&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mi was first proposed by Rubin (&lt;span class=&#34;citation&#34;&gt;Rubin (1978)&lt;/span&gt;) and has become more popular over time (&lt;span class=&#34;citation&#34;&gt;Rubin (1996)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;), as well as the focus of research for methodological and practical applications in a variety of fields (&lt;span class=&#34;citation&#34;&gt;Herzog and Rubin (1983)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Rubin and Schenker (1987)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Schafer (1999)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Carpenter and Kenward (2012)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Molenberghs et al. (2014)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Van Buuren (2018)&lt;/span&gt;). MI shares both advantages of &lt;em&gt;Single Imputaiton&lt;/em&gt; (SI) methods and solves both disadvantages. Indeed, like SI, MI methods allow the analyst to use familiar complete data methods when analysing the completed data sets. The only disadvantage of MI compared with SI methods is that it takes more time to generate the imputations and analyse the completed data sets. However, &lt;span class=&#34;citation&#34;&gt;Rubin (2004)&lt;/span&gt; showed that in order to obtain sufficiently precise estimates, a relatively small number of imputations (typically &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt;) is required. For example, considering a situation with &lt;span class=&#34;math inline&#34;&gt;\(\lambda=50\%\)&lt;/span&gt; missing information and &lt;span class=&#34;math inline&#34;&gt;\(H=10\)&lt;/span&gt; imputations, the efficiency of MI can be shown to be equal to &lt;span class=&#34;math inline&#34;&gt;\((1+\frac{\lambda}{H})^{-1}=95\%\)&lt;/span&gt;. In addition, in today’s computing environments, the work of analysing the completed data sets is quite modest since it involves performing the same task &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; times. Thus, once a precedure to combine multiple completed data sets is established, the additonal time and effort to handle &lt;span class=&#34;math inline&#34;&gt;\(50\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(20\)&lt;/span&gt;, or &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; imputations if often of little consequence.&lt;/p&gt;
&lt;p&gt;In the first step of MI, imputations should ideally be created as repeated draws from the &lt;em&gt;posterior predictive distribution&lt;/em&gt; of the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}\)&lt;/span&gt; given the observed values &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}\)&lt;/span&gt;, each repetition being an independent drawing of the parameters and missing values. In practice, implicit imputation models can also be used in place of explicit imputation models (&lt;span class=&#34;citation&#34;&gt;Herzog and Rubin (1983)&lt;/span&gt;). In the second step, each completed data set is analysed using the same complete data method that would be used in the absence of missingness. Finally, in the last step, standard procedures should be used to combine the compelted data inferences into a single one. The simplest and most popular method for combining the reuslts of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets is known as &lt;em&gt;Rubin’s rules&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Rubin (2004)&lt;/span&gt;), which can be explained with a simple example.&lt;/p&gt;
&lt;div id=&#34;rubins-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rubin’s rules&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_h\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(h=1,\ldots,H\)&lt;/span&gt;, be the completed data estimates and sampling variances for a scalar estimand &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, calculated from &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; repeated imputations under a given imputation model. Then, according to Rubin’s rules, the combined estimate is simply the average of the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data estimates, that is&lt;/p&gt;
&lt;p&gt;\[
\bar{\theta}_{H}=\frac{1}{H}\sum_{h=1}^{H}\hat{\theta}_{h}.
\]&lt;/p&gt;
&lt;p&gt;Because the imputations under MI are conditional draws, under a good imputaton model, they provide valid estimates for a wide range of estimands. In addition, the averaging over &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; imputed data sets increases the efficiency of estimation over that obtained from a single completed data set. The variability associated with the pooled estimate has two components: the &lt;em&gt;average within-imputation variance&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\bar{V}_H\)&lt;/span&gt; and the &lt;em&gt;between-imputation variance&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(B_H\)&lt;/span&gt;, defined as&lt;/p&gt;
&lt;p&gt;\[
\bar{V}_{H}=\frac{1}{H}\sum_{h=1}^{H}V_{h} \;\;\; \text{and} \;\;\; B_{H}=\frac{1}{H-1}\sum_{h=1}^{H}(\hat{\theta}_{h}-\bar{\theta}_{H})^2.
\]&lt;/p&gt;
&lt;p&gt;The total variability associated with &lt;span class=&#34;math inline&#34;&gt;\(\bar{\theta}_H\)&lt;/span&gt; is the computed as&lt;/p&gt;
&lt;p&gt;\[
T_{H}=\bar{V}_H + \frac{H+1}{H}B_{H},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\((1+\frac{1}{H})\)&lt;/span&gt; is an adjustment factor for finite due to estimating &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\bar{\theta}_H\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\lambda}_H=(1+\frac{1}{H})\frac{B_H}{T_H}\)&lt;/span&gt; is known as the &lt;em&gt;fraction of missing information&lt;/em&gt; and is an estimate of the fraction of information about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that is missing due to nonresponse. For large sample sizes and scalar quantities like &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the reference distribution for interval estimates and significance tests is a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution&lt;/p&gt;
&lt;p&gt;\[
(\theta - \bar{\theta}_H)\frac{1}{\sqrt{T^2_H}} \sim t_v,
\]&lt;/p&gt;
&lt;p&gt;where the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be approximated with the quantity &lt;span class=&#34;math inline&#34;&gt;\(v=(H-1)\left(1+\frac{1}{H+1}\frac{\bar{V}_H}{B_H} \right)^2\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Rubin and Schenker (1987)&lt;/span&gt;). In small data sets, an improved version of &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be obtained as &lt;span class=&#34;math inline&#34;&gt;\(v^\star=(\frac{1}{v}+\frac{1}{\hat{v}_{obs}})^{-1}\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;\[
\hat{v}_{obs}=(1-\hat{\lambda}_{H})\left(\frac{v_{com}+1}{v_{com}+3}\right)v_{com},
\]&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(v_{com}\)&lt;/span&gt; being the degrees of freedom for appropriate or exact &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; inferences about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; when there are no missing values (&lt;span class=&#34;citation&#34;&gt;Barnard and Rubin (1999)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The validity of MI rests on how the imputations are created and how that procedure relates to the model used to subsequently analyze the data. Creating MIs often requires special algorithms (&lt;span class=&#34;citation&#34;&gt;Schafer (1997)&lt;/span&gt;). In general, they should be drawn from a distribution for the missing data that reflects uncertainty about the parameters of the data model. Recall that with SI methods, it is desirable to impute from the conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(p(y_{mis}\mid y_{obs},\hat{\theta})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is an estimate derived from the observed data. MI extends this approach by first simulating &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; independent plausible values for the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_1,\ldots,\theta_H\)&lt;/span&gt; and then drawing the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}^h\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(p(y_{mis}\mid y_{obs}, \theta_h)\)&lt;/span&gt;. Treating parameters as random rather than fixed is
an essential part of MI. For this reason, it is natural (but not essential) to motivate MI from the Bayesian perspective, in which the state of knowledge about parameters is represented through a posterior distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;joint-multiple-imputation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Joint Multiple Imputation&lt;/h2&gt;
&lt;p&gt;Joint MI starts from the assumption that the data can be described by a multivariate distribution which in many cases, mostly for practical reasons, corresponds to assuming a multivariate Normal distribution. The general idea is that, for a general missing data pattern $ r$, missingness may occur anywhere in the multivariate outcome vector $ y=(y_1,,y_J)$, so that the distribution from which imputations should be drawn varies based on the observed variables in each pattern. For example, given $ r=(0,0,1,1)$, then imputations should be drawn from the bivariate distribution of the missing variables given the observed variables in that pattern, that is from &lt;span class=&#34;math inline&#34;&gt;\(f(y^{mis}_1,y^{mis}_2 \mid y^{obs}_3, y^{obs}_4, \phi_{12})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\phi_{12}\)&lt;/span&gt; is the probability of being in pattern $ r$ where the first two variables are missing.&lt;/p&gt;
&lt;p&gt;Consider the multivariate Normal distribution &lt;span class=&#34;math inline&#34;&gt;\(y \sim N(\mu,\Sigma)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\Sigma)\)&lt;/span&gt; represent the vector of the parameters of interest which need to be identified. Indeed, for non-monotone missing data, $ $ cannot be generally identified based on the observed data directly $ y^{obs}$, and the typical solution is to iterate imputation and parameter estimation using a general algorithm known as &lt;em&gt;data augmentation&lt;/em&gt;(&lt;span class=&#34;citation&#34;&gt;Tanner and Wong (1987)&lt;/span&gt;). Following &lt;span class=&#34;citation&#34;&gt;Van Buuren (2018)&lt;/span&gt;, the general procedure of the algorithm can be summarised as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Define some plausible starting values for all parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=(\mu_0,\Sigma_0)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At each iteration &lt;span class=&#34;math inline&#34;&gt;\(t=1,\ldots,T\)&lt;/span&gt;, draw &lt;span class=&#34;math inline&#34;&gt;\(h=1,\ldots,H\)&lt;/span&gt; imputations for each missing value from the predictive distribution of the missing data given the observed data and the current value of the parameters at &lt;span class=&#34;math inline&#34;&gt;\(t-1\)&lt;/span&gt;, that is&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\hat{y}^{mis}_{t} \sim p(y^{mis} \mid y^{obs},\theta_{t-1})
\]&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Re-estimate the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; using the observed and imputed data at &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; based on the multivariate Normal model, that is&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\hat{\theta}_{t} \sim p(\theta \mid y^{obs}, \hat{y}^{mis}_{t})
\]&lt;/p&gt;
&lt;p&gt;And reiterate the steps 2 and 3 until convergence, where the stopping rule typically consists in imposing that the change in the parameters between iterations &lt;span class=&#34;math inline&#34;&gt;\(t-1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; should be smaller than a predefined “small” threshold &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. &lt;span class=&#34;citation&#34;&gt;Schafer (1997)&lt;/span&gt; showed that imputations generated under the multivariate Normal model can be robust to non-normal data, even though it is generally more efficient to transform the data towards normality, especially when the parameters of interest are difficult to estimate, such as quantiles and variances.&lt;/p&gt;
&lt;p&gt;The multivariate Normal model is also often applied to categorical data, with different types of specifications that have been proposed in the literature (&lt;span class=&#34;citation&#34;&gt;Schafer (1997)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Horton, Lipsitz, and Parzen (2003)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Allison (2005)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Bernaards, Belin, and Schafer (2007)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Yucel, He, and Zaslavsky (2008)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Demirtas (2009)&lt;/span&gt;). For examples, missing data in contingency tables can be imputed using log-linear models (&lt;span class=&#34;citation&#34;&gt;Schafer (1997)&lt;/span&gt;); mixed continuous-categorical data can be imputed under the general location model which combines a log-linear and multivariate Normal model (&lt;span class=&#34;citation&#34;&gt;Olkin, Tate, and others (1961)&lt;/span&gt;); two-way imputation can be applied to missing test item responses by imputing missing categorical data by conditioning on the row and column sum scores of the multivariate data (&lt;span class=&#34;citation&#34;&gt;Van Ginkel et al. (2007)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-allison2005imputation&#34;&gt;
&lt;p&gt;Allison, Paul D. 2005. “Imputation of Categorical Variables with Proc Mi.” &lt;em&gt;SUGI 30 Proceedings&lt;/em&gt; 113 (30): 1–14.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-barnard1999miscellanea&#34;&gt;
&lt;p&gt;Barnard, John, and Donald B Rubin. 1999. “Miscellanea. Small-Sample Degrees of Freedom with Multiple Imputation.” &lt;em&gt;Biometrika&lt;/em&gt; 86 (4): 948–55.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bernaards2007robustness&#34;&gt;
&lt;p&gt;Bernaards, Coen A, Thomas R Belin, and Joseph L Schafer. 2007. “Robustness of a Multivariate Normal Approximation for Imputation of Incomplete Binary Data.” &lt;em&gt;Statistics in Medicine&lt;/em&gt; 26 (6): 1368–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-carpenter2012multiple&#34;&gt;
&lt;p&gt;Carpenter, James, and Michael Kenward. 2012. &lt;em&gt;Multiple Imputation and Its Application&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-demirtas2009rounding&#34;&gt;
&lt;p&gt;Demirtas, Hakan. 2009. “Rounding Strategies for Multiply Imputed Binary Data.” &lt;em&gt;Biometrical Journal: Journal of Mathematical Methods in Biosciences&lt;/em&gt; 51 (4): 677–88.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-herzog1983using&#34;&gt;
&lt;p&gt;Herzog, Thomas N, and Donald B Rubin. 1983. “Using Multiple Imputations to Handle Nonresponse in Sample Surveys.” &lt;em&gt;Incomplete Data in Sample Surveys&lt;/em&gt; 2: 209–45.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-horton2003potential&#34;&gt;
&lt;p&gt;Horton, Nicholas J, Stuart R Lipsitz, and Michael Parzen. 2003. “A Potential for Bias When Rounding in Multiple Imputation.” &lt;em&gt;The American Statistician&lt;/em&gt; 57 (4): 229–32.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-molenberghs2014handbook&#34;&gt;
&lt;p&gt;Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. &lt;em&gt;Handbook of Missing Data Methodology&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-olkin1961multivariate&#34;&gt;
&lt;p&gt;Olkin, Ingram, Robert Fleming Tate, and others. 1961. “Multivariate Correlation Models with Mixed Discrete and Continuous Variables.” &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt; 32 (2): 448–65.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1978multiple&#34;&gt;
&lt;p&gt;Rubin, Donald B. 1978. “Multiple Imputations in Sample Surveys a Phenomenological Bayesian Approach to Nonresponse.” &lt;em&gt;Proceedings of the Survey Research Methods Section of the American Statistical Association&lt;/em&gt; 1: 20–34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1996multiple&#34;&gt;
&lt;p&gt;———. 1996. “Multiple Imputation After 18 Years.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 91 (434): 473–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin2004multiple&#34;&gt;
&lt;p&gt;———. 2004. &lt;em&gt;Multiple Imputation for Nonresponse in Surveys&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1987interval&#34;&gt;
&lt;p&gt;Rubin, Donald B, and Nathaniel Schenker. 1987. “Interval Estimation from Multiply Imputed Data: A Case Study Using Census Agriculture Industry Codes.” &lt;em&gt;Journal of Official Statistics&lt;/em&gt; 3 (4): 375.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer1997analysis&#34;&gt;
&lt;p&gt;Schafer, Joseph L. 1997. &lt;em&gt;Analysis of Incomplete Multivariate Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer1999multiple&#34;&gt;
&lt;p&gt;———. 1999. “Multiple Imputation: A Primer.” &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt; 8 (1): 3–15.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tanner1987calculation&#34;&gt;
&lt;p&gt;Tanner, Martin A, and Wing Hung Wong. 1987. “The Calculation of Posterior Distributions by Data Augmentation.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 82 (398): 528–40.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2018flexible&#34;&gt;
&lt;p&gt;Van Buuren, Stef. 2018. &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2007two&#34;&gt;
&lt;p&gt;Van Ginkel, Joost R, L Andries Van der Ark, Klaas Sijtsma, and Jeroen K Vermunt. 2007. “Two-Way Imputation: A Bayesian Method for Estimating Missing Scores in Tests and Questionnaires, and an Accurate Approximation.” &lt;em&gt;Computational Statistics &amp;amp; Data Analysis&lt;/em&gt; 51 (8): 4013–27.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-yucel2008using&#34;&gt;
&lt;p&gt;Yucel, Recai M, Yulei He, and Alan M Zaslavsky. 2008. “Using Calibration to Improve Rounding in Imputation.” &lt;em&gt;The American Statistician&lt;/em&gt; 62 (2): 125–29.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multiple Imputation by Chained Equations</title>
      <link>/missmethods/multiple-imputation-by-chained-equations/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/multiple-imputation-by-chained-equations/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Multiple Imputation&lt;/em&gt;(MI) refers to the procedure of replacing each missing value by a set of &lt;span class=&#34;math inline&#34;&gt;\(H\geq 2\)&lt;/span&gt; imputed values. These are ordered in the sense that &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets can be created from the sets of imputations, where the first imputed value replaces the missing value in the first completed data set, the second imputed value in the second completed data set, and so on. Next, standard complete data methods are used to analyse each completed data set. When the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; sets of imputations are repeated random draws from the predictive distribution of the missing data under a particular model of missingness, the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences can be combined to form one inference that properly reflects uncertainty due to missing values under that model. In general, MI procedures can be summarised in three main steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Specify an &lt;strong&gt;imputation model&lt;/strong&gt; to generate &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; imputed values, typically taken as random draws from the predictive distribution of the missing values given the observed values, and create &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets using these imputations and the observed data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Analyse each completed data sets using standard complete data methods based on an &lt;strong&gt;analysis model&lt;/strong&gt;, and derive &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pool together the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences into a single inference using standard MI formulas, which ensure that missing data uncertainty is taken into account&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mi was first proposed by Rubin (&lt;span class=&#34;citation&#34;&gt;Rubin (1978)&lt;/span&gt;) and has become more popular over time (&lt;span class=&#34;citation&#34;&gt;Rubin (1996)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;), as well as the focus of research for methodological and practical applications in a variety of fields (&lt;span class=&#34;citation&#34;&gt;Herzog and Rubin (1983)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Rubin and Schenker (1987)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Schafer (1999)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Carpenter and Kenward (2012)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Molenberghs et al. (2014)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Van Buuren (2018)&lt;/span&gt;). MI shares both advantages of &lt;em&gt;Single Imputaiton&lt;/em&gt; (SI) methods and solves both disadvantages. Indeed, like SI, MI methods allow the analyst to use familiar complete data methods when analysing the completed data sets. The only disadvantage of MI compared with SI methods is that it takes more time to generate the imputations and analyse the completed data sets. However, &lt;span class=&#34;citation&#34;&gt;Rubin (2004)&lt;/span&gt; showed that in order to obtain sufficiently precise estimates, a relatively small number of imputations (typically &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt;) is required. For example, considering a situation with &lt;span class=&#34;math inline&#34;&gt;\(\lambda=50\%\)&lt;/span&gt; missing information and &lt;span class=&#34;math inline&#34;&gt;\(H=10\)&lt;/span&gt; imputations, the efficiency of MI can be shown to be equal to &lt;span class=&#34;math inline&#34;&gt;\((1+\frac{\lambda}{H})^{-1}=95\%\)&lt;/span&gt;. In addition, in today’s computing environments, the work of analysing the completed data sets is quite modest since it involves performing the same task &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; times. Thus, once a precedure to combine multiple completed data sets is established, the additonal time and effort to handle &lt;span class=&#34;math inline&#34;&gt;\(50\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(20\)&lt;/span&gt;, or &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; imputations if often of little consequence.&lt;/p&gt;
&lt;p&gt;In the first step of MI, imputations should ideally be created as repeated draws from the &lt;em&gt;posterior predictive distribution&lt;/em&gt; of the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}\)&lt;/span&gt; given the observed values &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}\)&lt;/span&gt;, each repetition being an independent drawing of the parameters and missing values. In practice, implicit imputation models can also be used in place of explicit imputation models (&lt;span class=&#34;citation&#34;&gt;Herzog and Rubin (1983)&lt;/span&gt;). In the second step, each completed data set is analysed using the same complete data method that would be used in the absence of missingness. Finally, in the last step, standard procedures should be used to combine the compelted data inferences into a single one. The simplest and most popular method for combining the reuslts of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets is known as &lt;em&gt;Rubin’s rules&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Rubin (2004)&lt;/span&gt;), which can be explained with a simple example.&lt;/p&gt;
&lt;div id=&#34;rubins-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rubin’s rules&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_h\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(h=1,\ldots,H\)&lt;/span&gt;, be the completed data estimates and sampling variances for a scalar estimand &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, calculated from &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; repeated imputations under a given imputation model. Then, according to Rubin’s rules, the combined estimate is simply the average of the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data estimates, that is&lt;/p&gt;
&lt;p&gt;\[
\bar{\theta}_{H}=\frac{1}{H}\sum_{h=1}^{H}\hat{\theta}_{h}.
\]&lt;/p&gt;
&lt;p&gt;Because the imputations under MI are conditional draws, under a good imputaton model, they provide valid estimates for a wide range of estimands. In addition, the averaging over &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; imputed data sets increases the efficiency of estimation over that obtained from a single completed data set. The variability associated with the pooled estimate has two components: the &lt;em&gt;average within-imputation variance&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\bar{V}_H\)&lt;/span&gt; and the &lt;em&gt;between-imputation variance&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(B_H\)&lt;/span&gt;, defined as&lt;/p&gt;
&lt;p&gt;\[
\bar{V}_{H}=\frac{1}{H}\sum_{h=1}^{H}V_{h} \;\;\; \text{and} \;\;\; B_{H}=\frac{1}{H-1}\sum_{h=1}^{H}(\hat{\theta}_{h}-\bar{\theta}_{H})^2.
\]&lt;/p&gt;
&lt;p&gt;The total variability associated with &lt;span class=&#34;math inline&#34;&gt;\(\bar{\theta}_H\)&lt;/span&gt; is the computed as&lt;/p&gt;
&lt;p&gt;\[
T_{H}=\bar{V}_H + \frac{H+1}{H}B_{H},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\((1+\frac{1}{H})\)&lt;/span&gt; is an adjustment factor for finite due to estimating &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\bar{\theta}_H\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\lambda}_H=(1+\frac{1}{H})\frac{B_H}{T_H}\)&lt;/span&gt; is known as the &lt;em&gt;fraction of missing information&lt;/em&gt; and is an estimate of the fraction of information about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that is missing due to nonresponse. For large sample sizes and scalar quantities like &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the reference distribution for interval estimates and significance tests is a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution&lt;/p&gt;
&lt;p&gt;\[
(\theta - \bar{\theta}_H)\frac{1}{\sqrt{T^2_H}} \sim t_v,
\]&lt;/p&gt;
&lt;p&gt;where the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be approximated with the quantity &lt;span class=&#34;math inline&#34;&gt;\(v=(H-1)\left(1+\frac{1}{H+1}\frac{\bar{V}_H}{B_H} \right)^2\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Rubin and Schenker (1987)&lt;/span&gt;). In small data sets, an improved version of &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be obtained as &lt;span class=&#34;math inline&#34;&gt;\(v^\star=(\frac{1}{v}+\frac{1}{\hat{v}_{obs}})^{-1}\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;\[
\hat{v}_{obs}=(1-\hat{\lambda}_{H})\left(\frac{v_{com}+1}{v_{com}+3}\right)v_{com},
\]&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(v_{com}\)&lt;/span&gt; being the degrees of freedom for appropriate or exact &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; inferences about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; when there are no missing values (&lt;span class=&#34;citation&#34;&gt;Barnard and Rubin (1999)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The validity of MI rests on how the imputations are created and how that procedure relates to the model used to subsequently analyze the data. Creating MIs often requires special algorithms (&lt;span class=&#34;citation&#34;&gt;Schafer (1997)&lt;/span&gt;). In general, they should be drawn from a distribution for the missing data that reflects uncertainty about the parameters of the data model. Recall that with SI methods, it is desirable to impute from the conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(p(y_{mis}\mid y_{obs},\hat{\theta})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is an estimate derived from the observed data. MI extends this approach by first simulating &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; independent plausible values for the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_1,\ldots,\theta_H\)&lt;/span&gt; and then drawing the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}^h\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(p(y_{mis}\mid y_{obs}, \theta_h)\)&lt;/span&gt;. Treating parameters as random rather than fixed is
an essential part of MI. For this reason, it is natural (but not essential) to motivate MI from the Bayesian perspective, in which the state of knowledge about parameters is represented through a posterior distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-imputation-by-chained-equations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Imputation by Chained Equations&lt;/h2&gt;
&lt;p&gt;MI by Chained Equations, also known as &lt;em&gt;Fully Conditional Specification&lt;/em&gt;(FCS), imputes multivariate missing data on a variable-by-variable basis, and therefore requires the specification of an imputation model for each incomplete variable to create imputations per variable in an iterative fashion (&lt;span class=&#34;citation&#34;&gt;Van Buuren (2007)&lt;/span&gt;). In contrast to Joint MI, MICE specifies the multivariate distribution for the outcome and missingness pattern &lt;span class=&#34;math inline&#34;&gt;\(p(y,r\mid \theta, \phi)\)&lt;/span&gt;, indexed by the parameter vectors of the outcome (&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) and missingness models (&lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;), through a set of conditional densities &lt;span class=&#34;math inline&#34;&gt;\(p(y_j \mid y_{-j},r,\theta_j, \phi_j)\)&lt;/span&gt;, which is used to impute &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; given the other variables. Starting from a random draw from the marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;, imputation is then carried out by iterating over the conditionally specified imputation models for each &lt;span class=&#34;math inline&#34;&gt;\(y_j=(y_2,\ldots,y_J)\)&lt;/span&gt; separately given the set of all the other variables &lt;span class=&#34;math inline&#34;&gt;\(y_{-j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Tha main idea of MICE is to directly draw the missing data from the predictive distribution of conditional densities, therefore avoiding the need to specify a joint multivariate model for all the data. Different approaches can be used to implement MICE. For example, a possible strategy is the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start at iteration &lt;span class=&#34;math inline&#34;&gt;\(t=0\)&lt;/span&gt; by drawing randomly from the the distribution of the missing data given the observed data and all other variables, according to some probability model for each variable &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;, that is&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\hat{y}^{mis}_{j,0} \sim p(y^{mis}_{j} \mid y^{obs}_{j}, y_{-j}, r)
\]&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;At each iteration &lt;span class=&#34;math inline&#34;&gt;\(t=1,\ldots,T\)&lt;/span&gt; and for each variable &lt;span class=&#34;math inline&#34;&gt;\(j=\ldots,J\)&lt;/span&gt;, set&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\hat{y}^{mis}_{-j,t}=\left(\hat{y}_{1,t},\ldots, \hat{y}_{j-1,t}, \hat{y}_{j+1,t}, \ldots, \hat{y}_{J,t} \right)
\]&lt;/p&gt;
&lt;p&gt;as the currently completed data except &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Draw &lt;span class=&#34;math inline&#34;&gt;\(h=1,\ldots,H\)&lt;/span&gt; imputations for each variable &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; from the predictive distribution of the missing data given the observed data and the currently imputed data at &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, that is&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\hat{y}^{mis}_{j,t} \sim p(y^{mis}_{j} \mid y^{obs}_{j}, \hat{y}_{-j,t}, r)
\]&lt;/p&gt;
&lt;p&gt;and repeat the steps 2 and 3 until convergence. It is important to stress out that MICE is essentially a &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt;(MCMC) algorithm (&lt;span class=&#34;citation&#34;&gt;Brooks et al. (2011)&lt;/span&gt;), where the state space is the collection of all imputed values. More specifically, when the conditional distributions of all variables are compatible with a joint multivariate distribution, the algorithm corresponds to a Gibbs sampler, a Bayesian simulation method that samples from the conditional distributions in order to obtain samples from the joint multivariate distribution of all variables via some conditional factorisation of the latter (&lt;span class=&#34;citation&#34;&gt;Casella and George (1992)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Gilks, Richardson, and Spiegelhalter (1996)&lt;/span&gt;). A potential issue of MICE is that, since the conditional distributions are specified freely by the user, these may not be compatible with a joint distribution and therefore it is not clear from which distribution the algorithm is sampling from. However, a general advatage of MICE is that it gives freedom to the user for the specification of the univariate models for the variables, which can be tailored to handle different types of variabes (e.g. continuous and categorical) and different statistical issues for each variable (e.g. skewness and non-liner associations).&lt;/p&gt;
&lt;p&gt;Regardless of the theoretical implications of MICE, as a MCMC method, the algorithm converges to a stationary distribution when three conditions are satisfied (&lt;span class=&#34;citation&#34;&gt;Roberts (1996)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Brooks et al. (2011)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The chain is &lt;em&gt;irreducible&lt;/em&gt;, i.e. must be able to reach any state from any state in the state space&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The chain is &lt;em&gt;aperiodic&lt;/em&gt;, i.e. must be able to return to each state after some unknown number of steps or transitions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The chain is &lt;em&gt;recurrent&lt;/em&gt;, i.e. there is probability of one of eventually returning to each state after some number of steps&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Typically periodicity and non-recurrence can be a problem in MICE when the imputation models are not compatible, possibly leading to different inferences based on the stopping point of the chain or to non-stationary behaviours of the chain.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-barnard1999miscellanea&#34;&gt;
&lt;p&gt;Barnard, John, and Donald B Rubin. 1999. “Miscellanea. Small-Sample Degrees of Freedom with Multiple Imputation.” &lt;em&gt;Biometrika&lt;/em&gt; 86 (4): 948–55.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brooks2011handbook&#34;&gt;
&lt;p&gt;Brooks, Steve, Andrew Gelman, Galin Jones, and Xiao-Li Meng. 2011. &lt;em&gt;Handbook of Markov Chain Monte Carlo&lt;/em&gt;. CRC press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-carpenter2012multiple&#34;&gt;
&lt;p&gt;Carpenter, James, and Michael Kenward. 2012. &lt;em&gt;Multiple Imputation and Its Application&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-casella1992explaining&#34;&gt;
&lt;p&gt;Casella, George, and Edward I George. 1992. “Explaining the Gibbs Sampler.” &lt;em&gt;The American Statistician&lt;/em&gt; 46 (3): 167–74.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gilks1996introducing&#34;&gt;
&lt;p&gt;Gilks, Walter R, Sylvia Richardson, and David J Spiegelhalter. 1996. “Introducing Markov Chain Monte Carlo.” &lt;em&gt;Markov Chain Monte Carlo in Practice&lt;/em&gt; 1: 19.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-herzog1983using&#34;&gt;
&lt;p&gt;Herzog, Thomas N, and Donald B Rubin. 1983. “Using Multiple Imputations to Handle Nonresponse in Sample Surveys.” &lt;em&gt;Incomplete Data in Sample Surveys&lt;/em&gt; 2: 209–45.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-molenberghs2014handbook&#34;&gt;
&lt;p&gt;Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. &lt;em&gt;Handbook of Missing Data Methodology&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-roberts1996markov&#34;&gt;
&lt;p&gt;Roberts, Gareth O. 1996. “Markov Chain Concepts Related to Sampling Algorithms.” &lt;em&gt;Markov Chain Monte Carlo in Practice&lt;/em&gt; 57.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1978multiple&#34;&gt;
&lt;p&gt;Rubin, Donald B. 1978. “Multiple Imputations in Sample Surveys a Phenomenological Bayesian Approach to Nonresponse.” &lt;em&gt;Proceedings of the Survey Research Methods Section of the American Statistical Association&lt;/em&gt; 1: 20–34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1996multiple&#34;&gt;
&lt;p&gt;———. 1996. “Multiple Imputation After 18 Years.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 91 (434): 473–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin2004multiple&#34;&gt;
&lt;p&gt;———. 2004. &lt;em&gt;Multiple Imputation for Nonresponse in Surveys&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1987interval&#34;&gt;
&lt;p&gt;Rubin, Donald B, and Nathaniel Schenker. 1987. “Interval Estimation from Multiply Imputed Data: A Case Study Using Census Agriculture Industry Codes.” &lt;em&gt;Journal of Official Statistics&lt;/em&gt; 3 (4): 375.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer1997analysis&#34;&gt;
&lt;p&gt;Schafer, Joseph L. 1997. &lt;em&gt;Analysis of Incomplete Multivariate Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer1999multiple&#34;&gt;
&lt;p&gt;———. 1999. “Multiple Imputation: A Primer.” &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt; 8 (1): 3–15.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2007multiple&#34;&gt;
&lt;p&gt;Van Buuren, Stef. 2007. “Multiple Imputation of Discrete and Continuous Data by Fully Conditional Specification.” &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt; 16 (3): 219–42.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2018flexible&#34;&gt;
&lt;p&gt;———. 2018. &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
