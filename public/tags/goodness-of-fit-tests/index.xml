<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>goodness of fit tests on Andrea Gabrio</title>
    <link>/tags/goodness-of-fit-tests/</link>
    <description>Recent content in goodness of fit tests on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>`{year}`</copyright>
    <lastBuildDate>Wed, 12 Feb 2020 21:13:14 -0500</lastBuildDate>
    
	    <atom:link href="/tags/goodness-of-fit-tests/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Goodness of fit tests - JAGS</title>
      <link>/jags/gof-tests-jags/gof-tests-jags/</link>
      <pubDate>Wed, 12 Feb 2020 21:13:14 -0500</pubDate>
      
      <guid>/jags/gof-tests-jags/gof-tests-jags/</guid>
      <description>


&lt;p&gt;This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. &lt;code&gt;BUGS&lt;/code&gt; (Bayesian inference Using &lt;em&gt;Gibbs Sampling&lt;/em&gt;) is an algorithm and supporting language (resembling &lt;code&gt;R&lt;/code&gt;) dedicated to performing the Gibbs sampling implementation of &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; (MCMC) method. Dialects of the &lt;code&gt;BUGS&lt;/code&gt; language are implemented within three main projects:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OpenBUGS&lt;/strong&gt; - written in component pascal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;JAGS&lt;/strong&gt; - (Just Another Gibbs Sampler) - written in &lt;code&gt;C++&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;STAN&lt;/strong&gt; - a dedicated Bayesian modelling framework written in &lt;code&gt;C++&lt;/code&gt; and implementing &lt;em&gt;Hamiltonian&lt;/em&gt; MCMC samplers.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of &lt;code&gt;R&lt;/code&gt;, and thus, they are best accessed from within &lt;code&gt;R&lt;/code&gt; itself. As such there are multiple packages devoted to interfacing with the various software implementations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2OpenBUGS&lt;/em&gt; - interfaces with &lt;code&gt;OpenBUGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2jags&lt;/em&gt; - interfaces with &lt;code&gt;JAGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;rstan&lt;/em&gt; - interfaces with &lt;code&gt;STAN&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial will demonstrate how to fit models in &lt;code&gt;JAGS&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Plummer (2004)&lt;/span&gt;) using the package &lt;code&gt;R2jags&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Su et al. (2015)&lt;/span&gt;) as interface, which also requires to load some other packages.&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The analyses described in previous tutorials have all involved response variables that implicitly represent normally distributed and continuous population responses. In this context, continuous indicates that (at least in theory), any value of measurement down to an infinite number of decimal places is possible. Population responses can also be categorical such that the values could be logically or experimentally constrained to a set number of discrete possibilities. For example, individuals in a population can be categorized as either male or female, reaches in a stream could be classified as either riffles, runs or pools and salinity levels of sites might be categorized as either high, medium or low. Typically, categorical response variables are tallied up to generate the frequency of replicates in each of the possible categories. From above, we would tally up the frequency of males and females, the number of riffles, runs and pools and the high, medium and low salinity sites. Hence, rather than model data in which a response was measured from each replicate in the sample (as was the case for previous analyses in this series), frequency analyses model data on the frequency of replicates in each possible category. Furthermore, frequency data follow a Poisson distribution rather than a normal distribution. The Poisson distribution is a symmetrical distribution in which only discrete integer values are possible and whose variance is equal to its mean.&lt;/p&gt;
&lt;p&gt;Since the mean and variance of a Poisson distribution are equal, distributions with higher expected values are shorter and wider than those with smaller means. Note that a Poisson distribution with an expected less than less than &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt; will be obviously asymmetrical as a Poisson distribution is bounded to the left by zero. This has important implications for the reliability of frequency analyses when sample sizes are low. The frequencies expected for each category are determined by the size of the sample and the nature of the (null) hypothesis. For example, if the null hypothesis is that there are three times as many females as males in a population (ratio of &lt;span class=&#34;math inline&#34;&gt;\(3:1\)&lt;/span&gt;), then a sample of &lt;span class=&#34;math inline&#34;&gt;\(110\)&lt;/span&gt; individuals would be expected to yield &lt;span class=&#34;math inline&#34;&gt;\(0.75\times110=82.5\)&lt;/span&gt; females and &lt;span class=&#34;math inline&#34;&gt;\(0.25\times110=27.5\)&lt;/span&gt; males.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-chi-square-statistic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Chi-square statistic&lt;/h2&gt;
&lt;p&gt;The degree of difference between the observed (o) and expected (e) sample category frequencies is represented by the chi-square (&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;) statistic.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \chi^2=\sum\frac{(o-e)^2}{e}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is a relative measure that is standardised by the magnitude of the expected frequencies. When the null hypothesis is true (typically this represents the situation when there are no effects or patterns of interest in the population response category frequencies), and we have sampled in an unbiased manner, we might expect the observed category frequencies in the sample to be very similar (if not equal) to the expected frequencies and thus, the chi-square value should be close to zero. Likewise, repeated sampling from such a population is likely to yield chi-square values close to zero and large chi-square values should be relatively rare. As such, the chi-square statistic approximately follows a &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution, a mathematical probability distribution representing the frequency (and thus probability) of all possible ranges of chi-square statistics that could result when the null hypothesis is true.&lt;br /&gt;
The &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p(x) = \frac{1}{2^{\frac{n}{2}}\gamma(\frac{n}{2})}x^{\frac{n}{2-1}}e^{-\frac{x}{2}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that the location AND shape are both determined by a single parameter (the sample size, n which is also equal to the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(+ 1\)&lt;/span&gt;). The &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution is an asymmetrical distribution bounded by zero and infinity and whose exact shape is determined by the degrees of freedom (calculated as the total number of categories minus &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;). Note also that the peak of a chi-square distribution is not actually at zero (although it does approach it when the degrees of freedom is equal to zero). Initially, this might seem counter intuitive. We might expect that when a null hypothesis is true, the most common chi-square value will be zero. However, the &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution takes into account the expected natural variability in a population as well as the nature of sampling (in which multiple samples should yield slightly different results). The more categories there are, the more likely that the observed and expected values will differ. It could be argued that when there are a large number of categories, samples in which all the observed frequencies are very close to the expected frequencies are a little suspicious and may represent dishonesty on the part of the researcher (Indeed the extraordinary conformity of Gregor Mendelâ€™s pea experiments have been subjected to such skepticism).&lt;/p&gt;
&lt;p&gt;By comparing any given sample chi-square statistic to its appropriate &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution, the probability that the observed category frequencies could have be collected from a population with a specific ratio of frequencies (for example &lt;span class=&#34;math inline&#34;&gt;\(3:1\)&lt;/span&gt;) can be estimated. As is the case for most hypothesis tests, probabilities lower than &lt;span class=&#34;math inline&#34;&gt;\(0.05\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;%) are considered unlikely and suggest that the sample is unlikely to have come from a population characterized by the null hypothesis. Chi-squared tests are typically one-tailed tests focusing on the right-hand tail as we are primarily interested in the probability of obtaining large chi-square values. Nevertheless, it is also possible to focus on the left-hand tail so as to investigate whether the observed values are “too good to be true”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assumptions&lt;/h2&gt;
&lt;p&gt;A chi-square statistic will follow a &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution approximately provided that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;All observations are classified independently of one another. The classification of one replicate should not be influenced by or related to the classification of other replicates. Random sampling should address this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No more than &lt;span class=&#34;math inline&#34;&gt;\(20\)&lt;/span&gt;% of the expected frequencies are less than five. &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distributions do not reliably approximate the distribution of all possible chi-square values under those circumstances (Expected frequencies less than five result in asymmetrical sampling distributions (since they must be truncated at zero) and thus potentially unrepresentative χ2 distributions). Since the expected values are a function of sample sizes, meeting this assumption is a matter of ensuring sufficient replication. When sample sizes or other circumstances beyond control lead to a violation of this assumption, numerous options are available.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;goodness-of-fit-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goodness of fit tests&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Homogeneous frequencies tests&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Homogeneous frequencies tests (often referred to as goodness of fit tests) are used to test null hypotheses that the category frequencies observed within a single variable could arise from a population displaying a specific ratio of frequencies. The null hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;) is that the observed frequencies come from a population with a specific ratio of frequencies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distributional conformity - Kolmogorov-Smirnov tests&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Strictly, goodness of fit tests are used to examine whether a frequency/sampling distribution is homogeneous with some declared distribution. For example, we might use a goodness of fit test to formally investigate whether the distribution of a response variable deviates substantially from a normal distribution. In this case, frequencies of responses in a set of pre-defined bin ranges are compared to those frequencies expected according to the mathematical model of a normal distribution. Since calculations of these expected frequencies also involve estimates of population mean and variance (both required to determine the mathematical formula), a two degree of freedom loss is incurred (hence &lt;span class=&#34;math inline&#34;&gt;\(df=n−2\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;contingency-tables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contingency tables&lt;/h2&gt;
&lt;p&gt;Contingency tables are used to investigate the associations between two or more categorical variables. That is, they test whether the patterns of frequencies in one categorical variable differ between different levels of other categorical variable(s) or ould the variables be independent of another. In this way, they are analogous to interactions in factorial linear models (such as factorial ANOVA). Contingency tables test the null hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;) that the categorical variables are independent of (not associated with) one another. Note that analyses of contingency tables do not empirically distinguish between response and predictor variables (analogous to correlation), yet causality can be implied when logical and justified by interpretation. As an example, contingency tables could be used to investigate whether incidences of hair and eye color in a population are associated with one another (is one hair color type more commonly observed with a certain eye color). In this case, neither hair color nor eye color influence one another, their incidences are both controlled by a separate set of unmeasured factors. By contrast, an association between the presence or absence of a species of frog and the level of salinity (high, medium or low) could imply that salinity effects the distribution of that species of frog - but not vice versa. Sample replicates are cross-classified according to the levels (categories) of multiple categorical variables. The data are conceptualized as a table (hence the name) with the rows representing the levels of one variable and the column the levels of the other variable(s) such that the cells represent the category combinations. The expected frequency of any given cell is calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{\text{(row total)} \times \text{(column total)}}{\text{(grand total)}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thereafter, the chi-square calculations are calculated as described above and the chi-square value is compared to a &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution with &lt;span class=&#34;math inline&#34;&gt;\((r−1)(c−1)\)&lt;/span&gt; degrees of freedom. Contingency tables involving more than two variables have multiple interaction levels and thus multiple potential sources of independence. For example, in a three-way contingency table between variables A, B and C, there are four interactions (A:B, A:C, B:C and A:B:C). Such designs are arguably more appropriately analysed using log-linear models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Odds ratios&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The chi-square test provides an indication of whether or not the occurrences in one set of categories are likely to be associated with other sets of categories (an interaction between two or more categorical variables), yet does not provide any indication of how strongly the variables are associated (magnitude of the effect). Furthermore, for variables with more than two categories (e.g. high, medium, low), there is no indication of which category combinations contribute most to the associations. This role is provided by odds ratios which are essentially a measure of effect size. Odds refer the likelihood of a specific event or outcome occurring (such as the odds of a species being present) versus the odds of it not occurring (and thus the occurrence of an alternative outcome) and are calculated as &lt;span class=&#34;math inline&#34;&gt;\(\frac{\pi_j}{(1-\pi_j)}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\pi_j\)&lt;/span&gt; refers to the probability of the event occurring. For example, we could calculate the odds of frogs being present in highly saline habitats as the probability of frogs being present divided by the probability of them being absent. Similarly, we could calculate the likelihood of frog presence (odds) within low salinity habitats. The ratio of two of these likelihoods (odds ratio) can then be used to compare whether the likelihood of one outcome (frog presence) is the same for both categories (salinity levels). For example, is the likelihood of frogs being present in highly saline habitats the same as the probability of them being present in habitats with low levels of salinity. In so doing, the odds ratio is a measure of effect size that describes the strength of an association between pairs of cross-classification levels. Although odds and thus odds ratios (&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) are technically derived from probabilities, they can also be estimated using cell frequencies (&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \theta = \frac{n_{11}n_{22}}{n_{12}n_{21}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or alternatively&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \theta = \frac{(n_{11}+0.5)(n_{22}+0.5)}{(n_{12} + 0.5)(n_{21} + 0.5)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt; is a small constant added to prevent division by zero. An odds ratio of one indicates that the event or occurrence (presence of frogs) is equally likely in both categories (high and low salinity habitats). Odds ratios greater than one signify that the event or occurrence is more likely in the first than second category and vice verse for odds ratios less than one. For example, when comparing the presence/absence of frogs in low versus high salinity habitats, an odds ratio of &lt;span class=&#34;math inline&#34;&gt;\(5.8\)&lt;/span&gt; would suggest that frogs are &lt;span class=&#34;math inline&#34;&gt;\(5.8\)&lt;/span&gt; times more likely to be present in low salinity habitats than those that highly saline. The distribution of odds ratios (which range from &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;) is not symmetrical around the null position (&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;) thereby precluding confidence interval and standard error calculations. Instead, these measures are calculated from log transformed (natural log) odds ratios (the distribution of which is a standard normal distribution centered around &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;) and then converted back into a linear scale by anti-logging. Odds ratios can only be calculated between category pairs from two variables and therefore &lt;span class=&#34;math inline&#34;&gt;\(2 \times 2\)&lt;/span&gt; contingency tables (tables with only two rows and two columns). However, tables with more rows and columns can be accommodate by splitting the table up into partial tables of specific category pair combinations. Odds ratios (and confidence intervals) are then calculated from each pairing, notwithstanding their lack of independence. For example, if there were three levels of salinity (high, medium and low), the odds ratios from three partial tables (high vs medium, high vs low, medium vs low) could be calculated.&lt;/p&gt;
&lt;p&gt;Since odds ratios only explore pairwise patterns within two-way interactions, odds ratios for multi-way (three or more variables) tables are considerably more complex to calculate and interpret. Partial tables between two of the variables (e.g frog presence/absence and high/low salinity) are constructed for each level of a third (season: summer/winter). This essentially removes the effect of the third variable by holding it constant. Associations in partial tables are therefore referred to as conditional associations - since the outcomes (associated or independent) from each partial table are explicitly conditional on the level of the third variable at which they were tested.&lt;/p&gt;
&lt;p&gt;Specific contributions to a lack of independence (significant associations) can also be investigated by exploring the residuals. Recall that residuals are the difference between the observed values (frequencies) and those predicted or expected when the null hypothesis is true (no association between variables). Hence the magnitude of each residual indicates how much each of the cross classification combinations differs from what is expected. The residuals are typically standardized (by dividing by the square of the expected frequencies) to enable individual residuals to be compared relative to one another. Large residuals (in magnitude) indicate large deviations from what is expected when the null hypothesis is true and thus also indicate large influences (contributions) to the overall association. The sign (&lt;span class=&#34;math inline&#34;&gt;\(+\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(-\)&lt;/span&gt;) of the residual indicates whether the frequencies were higher or lower than expected.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;g-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;G tests&lt;/h2&gt;
&lt;p&gt;An alternative to the chi-square test for goodness of fit and contingency table analyses is the G-test. The G-test is based on a log likelihood-ratio test. A log likelihood ratio is a ratio of maximum likelihoods of the alternative and null hypotheses. More simply, a log likelihood ratio test essentially examines how likely (the probability) the alternative hypothesis (representing an effect) is compared to how likely the null hypothesis (no effect) is given the collected data. The G2 statistic is calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ G^2 = 2 \sum o \; ln\frac{o}{e}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where o and e are the observed and expected sample category frequencies respectively and ln denotes the natural logarithm (base e). When the null hypothesis is true, the G2 statistic approximately follows a theoretical &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution with the same degrees of freedom as the corresponding chi-square statistic. The G2 statistic (which is twice the value of the log-likelihood ratio) is arguably more appropriate than the chi-square statistic as it is closely aligned with the theoretical basis of the χ2 distribution (for which the chi-squared statistic is a convenient approximation). For large sample sizes, G2 and &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; statistics are equivalent, however the former is a better approximation of the theoretical chi2 distribution when the difference between the observed and expected is less than the expected frequencies (ie &lt;span class=&#34;math inline&#34;&gt;\(|o−e|&amp;lt;e\)&lt;/span&gt;). Nevertheless, G-tests operate under the same assumptions are the chi-square statistic and thus very small sample sizes (expected values less than &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;) are still problematic. G-tests have the additional advantage that they can be used additively with more complex designs and a thus more extensible than the chi-squared statistic.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;small-sample-sizes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Small sample sizes&lt;/h2&gt;
&lt;p&gt;As discussed previously, both the &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; and G2 statistics are poor approximations of theoretical &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distributions when sample sizes are very small. Under these circumstances a number of alternative options are available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the issue has arisen due to a large number of category levels in one or more of the variables, some categories could be combined together.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fishers exact test which essentially calculates the probability of obtaining the cell frequencies given the observed marginal totals in &lt;span class=&#34;math inline&#34;&gt;\(2 \times 2\)&lt;/span&gt; tables. The calculations involved in such tests are extremely tedious as they involve calculating probabilities from hypergeometric distributions (discrete distributions describing the number of successes from sequences of samples drawn with out replacement) for all combinations of cell values that result in the given marginal totals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Yates’ continuity correction calculates the test statistic after adding and subtracting &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt; from observed values less than and greater than expected values respectively. Yates’ correction can only be applied to designs with a single degree of freedom (goodness-of-fit designs with two categories or &lt;span class=&#34;math inline&#34;&gt;\(2 \times 2\)&lt;/span&gt; tables) and for goodness-of-fit tests provide p-values that are closer to those of an exact binomial. However, they typically yield over inflated p-values in contingency tables and so have gone out of favour.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Williams’ correction is applied by dividing the test statistic by &lt;span class=&#34;math inline&#34;&gt;\(1+(p2−1)6nv\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the number of categories, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the total sample size (total of observed frequencies) and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is the number of degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\((p−1)\)&lt;/span&gt;. Williams’ corrections can be applied to designs with greater than one degree of freedom, and are considered marginally more appropriate than Yates’ corrections if corrections are insisted.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Randomisation tests in which the sample test statistic (either &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; or G2) is compared to a probability distribution generated by repeatedly calculating the test statistic from an equivalent number of observations drawn from a population (sampling with replacement) with the specific ratio of category frequencies defined by the null hypothesis. Significance is thereafter determined by the proportion of the randomised test statistic values that are greater than or equal to the value of the statistic that is based on observed data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Log-linear modelling (as a form of generalized linear model)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-generation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data generation&lt;/h1&gt;
&lt;p&gt;Goodness of fit tests are concerned with comparing the observed frequencies with those expected on the basis of a specific null hypothesis. So lets now fabricate a motivating scenario and some data. We will create a scenario that involves items classified into one of three groups (A, B and C). The number of items in each classification group are then tallied up. Out of a total of &lt;span class=&#34;math inline&#34;&gt;\(47\)&lt;/span&gt; items, &lt;span class=&#34;math inline&#34;&gt;\(15\)&lt;/span&gt; where of type A, &lt;span class=&#34;math inline&#34;&gt;\(9\)&lt;/span&gt; where of type B and &lt;span class=&#34;math inline&#34;&gt;\(23\)&lt;/span&gt; where of type C. We could evaluate a parity (a &lt;span class=&#34;math inline&#34;&gt;\(1:1:1\)&lt;/span&gt; ratio from these data. In a frequentist context, this might involve testing a null hypothesis that the observed data could have come from a population with a &lt;span class=&#34;math inline&#34;&gt;\(1:1\)&lt;/span&gt; item ratio. In this case the probability would be the probability of obtaining the observed ratio of frequencies when the null hypothesis is true. In a Bayesian context, there are numerous ways that we could tackle these data. We would be evaluating the evidence for the null hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(1:1:1\)&lt;/span&gt; item ratio) given the observed by estimating the degree of freedom from a chi-square distribution. Alternatively, we could estimate the value of the three population fractions which are expected to be &lt;span class=&#34;math inline&#34;&gt;\(1/3, 1/3, 1/3\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(1:1:1\)&lt;/span&gt;. We will explore this option first and then explore the chi-square approach second. To extend the example, lets also explore a &lt;span class=&#34;math inline&#34;&gt;\(1:1:2\)&lt;/span&gt; ratio. We start by generating the observed data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #the observed frequences of A and B
&amp;gt; obs &amp;lt;- c(15,9,23)
&amp;gt; obs
[1] 15  9 23&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-population-fractions---binomial-distribution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimating population fractions - binomial distribution&lt;/h1&gt;
&lt;p&gt;The binomial distribution represents the distribution of possible densities (probabilities) for the number of successes p out of a total of n independent trials. In this case, it can be used to model the number of items of each group (A, B and C) out of a total of &lt;span class=&#34;math inline&#34;&gt;\(47\)&lt;/span&gt; items. The prior distribution for &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; would be a beta distribution (values range from &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;) with shape parameters a and b the hyperpriors of which follow vague (flat, imprecise) gamma distributions.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ obs_i \sim \text{Bin}(p_i,n_i),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(p_i\sim \text{Beta}(a,b)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a,b \sim \text{Gamma}(1,0.01)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;The data should logically follow a binomial distribution (since the observations are counts of positive events out of a total).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;We now translate the likelihood model into &lt;code&gt;JAGS&lt;/code&gt; code and store the code in an external file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; modelString=&amp;quot;
+ model {
+   #Likelihood
+  for (i in 1:nGroups) {
+    obs[i] ~ dbin(p[i],n[i])
+    p[i] ~ dbeta(a[i],b[i])
+    a[i] ~ dgamma(1,0.01)
+    b[i] ~ dgamma(1,0.01)
+  }
+  }
+ &amp;quot;
&amp;gt; ## write the model to a text file 
&amp;gt; writeLines(modelString,con=&amp;quot;chi2model.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The likelihood model indicates that the observed counts are modeled by a binomial distribution with a probability of p (fraction) from n trials (items).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The prior on each p is defined as a beta distribution with shape parameters a and b&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The hyperpriors for each a and b are drawn from imprecise (vague, flat) gamma distributions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Define the data list. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; # The observed item frequencies
&amp;gt; obs &amp;lt;- c(15, 9, 23)
&amp;gt; data.list &amp;lt;- list(obs = obs, n = c(47, 47, 47), nGroups = 3)
&amp;gt; data.list
$obs
[1] 15  9 23

$n
[1] 47 47 47

$nGroups
[1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the parameters to monitor and the chain details&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params &amp;lt;- c(&amp;quot;p&amp;quot;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 1000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 5000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fit the model in &lt;code&gt;JAGS&lt;/code&gt; using the function &lt;code&gt;jags&lt;/code&gt; in the package &lt;code&gt;R2jags&lt;/code&gt; (which should be loaded first).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(R2jags)
&amp;gt; # Fit the model for the 1:1:1 ratio
&amp;gt; data.r2jags &amp;lt;- jags(data = data.list, inits = NULL, parameters.to.save = params, 
+                     model.file = &amp;quot;chi2model.txt&amp;quot;,n.chains = nChains, n.iter = nIter, 
+                     n.burnin = burnInSteps, n.thin = thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 3
   Unobserved stochastic nodes: 9
   Total graph size: 18

Initializing model
&amp;gt; 
&amp;gt; print(data.r2jags)
Inference for Bugs model at &amp;quot;chi2model.txt&amp;quot;, fit using jags,
 2 chains, each with 2500 iterations (first 1000 discarded)
 n.sims = 3000 iterations saved
         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff
p[1]       0.323   0.064  0.205  0.276  0.321  0.365  0.449 1.003   570
p[2]       0.204   0.053  0.117  0.168  0.199  0.234  0.322 1.047    38
p[3]       0.496   0.073  0.350  0.448  0.497  0.545  0.636 1.001  3000
deviance  15.119   2.384 12.535 13.373 14.490 16.130 21.782 1.005   320

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 2.8 and DIC = 18.0
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: Initially, we should focus our attention on the Rhat and n.eff columns. These are the scale reduction and number of effective samples respectively and they provide an indication of the degree of mixing or coverage of the samples. Ideally, the n.eff values should be approximately equal to the number of saved samples (in this case &lt;span class=&#34;math inline&#34;&gt;\(4701\)&lt;/span&gt;), and the Rhat values should be approximately &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (complete convergence). Whilst the actual values are likely to differ substantially from run to run (due to the stochastic nature of the way the chains traverse the posterior distribution), on this occasion, the n.eff of the first two probability parameters (p[1] and p[2]) are substantially lower than &lt;span class=&#34;math inline&#34;&gt;\(4700\)&lt;/span&gt;. Hence, the samples of these parameters may not accurately reflect the posterior distribution. We might consider altering one or more of the chain behavioural paramters (such as the thinning rate), alter the model definition (or priors) itself.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-evaluation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model evaluation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(mcmcplots)
&amp;gt; denplot(data.r2jags, parms = c(&amp;quot;p&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/gof-tests-jags/2020-02-01-gof-tests-jags_files/figure-html/mcmc_diag-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(data.r2jags, parms = c(&amp;quot;p&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/gof-tests-jags/2020-02-01-gof-tests-jags_files/figure-html/mcmc_diag-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; raftery.diag(as.mcmc(data.r2jags))
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 

You need a sample size of at least 3746 with these values of q, r and s

[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 

You need a sample size of at least 3746 with these values of q, r and s
&amp;gt; 
&amp;gt; autocorr.diag(as.mcmc(data.r2jags))
         deviance        p[1]        p[2]        p[3]
Lag 0  1.00000000  1.00000000  1.00000000  1.00000000
Lag 1  0.67862022  0.79329782  0.75266787  0.79501632
Lag 5  0.22316853  0.37163355  0.31066379  0.35879998
Lag 10 0.05470517  0.15225282  0.11155263  0.17106406
Lag 50 0.02781881 -0.00722609 -0.09133568 -0.03525663&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: Minimum required number of MCMC samples to ensure that sufficient samples had been collected to achieve good accuracy is &lt;span class=&#34;math inline&#34;&gt;\(3746\)&lt;/span&gt;. We had &lt;span class=&#34;math inline&#34;&gt;\(5000\)&lt;/span&gt; per chain (&lt;span class=&#34;math inline&#34;&gt;\(5000\times3=15000\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params &amp;lt;- c(&amp;quot;p&amp;quot;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 1000
&amp;gt; thinSteps = 50
&amp;gt; numSavedSteps = 5000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; data.r2jags &amp;lt;- jags(data = data.list, inits = NULL, parameters.to.save = params, 
+                     model.file = &amp;quot;chi2model.txt&amp;quot;, n.chains = nChains, n.iter = nIter,
+                     n.burnin = burnInSteps, n.thin = thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 3
   Unobserved stochastic nodes: 9
   Total graph size: 18

Initializing model
&amp;gt; 
&amp;gt; print(data.r2jags)
Inference for Bugs model at &amp;quot;chi2model.txt&amp;quot;, fit using jags,
 2 chains, each with 125000 iterations (first 1000 discarded), n.thin = 50
 n.sims = 4960 iterations saved
         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff
p[1]       0.325   0.067  0.200  0.279  0.322  0.369  0.459 1.001  5000
p[2]       0.204   0.056  0.105  0.164  0.201  0.240  0.324 1.002  2100
p[3]       0.491   0.070  0.353  0.443  0.490  0.539  0.630 1.001  5000
deviance  15.223   2.356 12.524 13.457 14.647 16.360 21.253 1.002  1700

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 2.8 and DIC = 18.0
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: Rhat and n.eff are now much better for the probability parameters. The estimated fractions for A, B and C are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A: 0.327 (0.207, 0.466)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;B: 0.200 (0.104, 0.323)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;C: 0.491 (0.355, 0.625)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Collectively, the fractions of 1/3, 1/3 and 1/3 do not fall within these ranges. However, collectively the fractions 1/4, 1/4, 2/4 do fall comfortably within these ranges. This suggests that the population ratio is more likely to be 1:1:2 than 1:1:1.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;chi-square&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Chi-square&lt;/h1&gt;
&lt;p&gt;An appropriate test statistic for comparing an observed (o) frequency ratio to an expected (e) frequency ratio is the chi-square &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; statistic. In effect, the chi-square statistic (which incorporates the variability in the data in to measure of the difference between observed and expected) becomes the input for the likelihood model. Whilst we could simply pass &lt;code&gt;JAGS&lt;/code&gt; the chi-square statistic, by parsing the observed and expected values and having the chi-square value calculated within &lt;code&gt;JAGS&lt;/code&gt; data, the resulting &lt;code&gt;JAGS&lt;/code&gt; code is more complete and able to accommodate other scenarios. So if, chisq is the chi-square statistic and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the degrees of freedom (and thus expected value of the &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution), then the likelihood model is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \text{chisq} \sim \chi^2(k),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(k \sim \text{Unif}(0.01,100)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;exploratory-data-analysis-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;So lets calculate the expected frequencies as a means to evaluate this assumption. The expected values are calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ e=\text{total counts} \times \text{expected fraction}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is clear that in neither case are any of the expected frequencies less than &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;. Therefore, we would conclude that probabilities derived from the &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution are likely to be reliable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;We now translate the likelihood model into &lt;code&gt;JAGS&lt;/code&gt; code and store the code in an external file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; modelString2=&amp;quot;
+ data {
+ for (i in 1:n){
+      resid[i] &amp;lt;- pow(obs[i]-exp[i],2)/exp[i]
+    }
+    chisq &amp;lt;- sum(resid)
+ }
+ model {
+   #Likelihood
+   chisq  ~ dchisqr(k)
+   #Priors
+   k ~ dunif(0.01,100)
+  }
+ &amp;quot;
&amp;gt; ## write the model to a text file 
&amp;gt; writeLines(modelString2,con=&amp;quot;chi2model2.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First of all, the standardized residuals and chi-square statistic are calculated according to the formula listed above.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The likelihood model indicates that the chi-squared statistic can be modeled by a &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution with a centrality parameter of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The prior on &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is defined as a uniform (thus vague) flat prior whose values could range from &lt;span class=&#34;math inline&#34;&gt;\(0.01\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(100\)&lt;/span&gt; (all with equal probability).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Define the data list. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; # The observed item frequencies
&amp;gt; obs &amp;lt;- c(15, 9, 23)
&amp;gt; # The expected item frequencies (for a 1:1:1 ratio)
&amp;gt; exp &amp;lt;- rep(sum(obs) * 1/3, 3)
&amp;gt; data.list &amp;lt;- list(obs = obs, exp = exp, n = 3)
&amp;gt; data.list
$obs
[1] 15  9 23

$exp
[1] 15.66667 15.66667 15.66667

$n
[1] 3
&amp;gt; 
&amp;gt; # The expected item frequencies (for a 1:1:2 ratio)
&amp;gt; exp &amp;lt;- sum(obs) * c(1/4, 1/4, 2/4)
&amp;gt; data.list1 &amp;lt;- list(obs = obs, exp = exp, n = 3)
&amp;gt; data.list1
$obs
[1] 15  9 23

$exp
[1] 11.75 11.75 23.50

$n
[1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the parameters to monitor and the chain details&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params &amp;lt;- c(&amp;quot;chisq&amp;quot;, &amp;quot;resid&amp;quot;, &amp;quot;k&amp;quot;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 1000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 5000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fit the model in &lt;code&gt;JAGS&lt;/code&gt; using the function &lt;code&gt;jags&lt;/code&gt; in the package &lt;code&gt;R2jags&lt;/code&gt; (which should be loaded first).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; # Fit the model for the 1:1:1 ratio
&amp;gt; data.r2jags2 &amp;lt;- jags(data = data.list, inits = NULL, parameters.to.save = params, 
+                     model.file = &amp;quot;chi2model2.txt&amp;quot;,n.chains = nChains, n.iter = nIter, 
+                     n.burnin = burnInSteps, n.thin = thinSteps)
Compiling data graph
   Resolving undeclared variables
   Allocating nodes
   Initializing
   Reading data back into data table
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 1
   Unobserved stochastic nodes: 1
   Total graph size: 14

Initializing model
&amp;gt; 
&amp;gt; print(data.r2jags2)
Inference for Bugs model at &amp;quot;chi2model2.txt&amp;quot;, fit using jags,
 2 chains, each with 2500 iterations (first 1000 discarded)
 n.sims = 3000 iterations saved
         mu.vect sd.vect  2.5%   25%   50%    75%  97.5%  Rhat n.eff
chisq      6.298   0.000 6.298 6.298 6.298  6.298  6.298 1.000     1
k          8.293   3.611 2.306 5.686 7.915 10.559 16.165 1.001  3000
resid[1]   0.028   0.000 0.028 0.028 0.028  0.028  0.028 1.000     1
resid[2]   2.837   0.000 2.837 2.837 2.837  2.837  2.837 1.000     1
resid[3]   3.433   0.000 3.433 3.433 3.433  3.433  3.433 1.000     1
deviance   5.338   1.428 4.346 4.444 4.809  5.647  9.405 1.001  3000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 1.0 and DIC = 6.4
DIC is an estimate of expected predictive error (lower deviance is better).
&amp;gt; 
&amp;gt; # Fit the model for the 1:1:2 ratio
&amp;gt; data.r2jags2.1 &amp;lt;- jags(data = data.list1, inits = NULL, parameters.to.save = params, 
+                     model.file = &amp;quot;chi2model2.txt&amp;quot;,n.chains = nChains, n.iter = nIter, 
+                     n.burnin = burnInSteps, n.thin = thinSteps)
Compiling data graph
   Resolving undeclared variables
   Allocating nodes
   Initializing
   Reading data back into data table
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 1
   Unobserved stochastic nodes: 1
   Total graph size: 14

Initializing model
&amp;gt; 
&amp;gt; print(data.r2jags2.1)
Inference for Bugs model at &amp;quot;chi2model2.txt&amp;quot;, fit using jags,
 2 chains, each with 2500 iterations (first 1000 discarded)
 n.sims = 3000 iterations saved
         mu.vect sd.vect  2.5%   25%   50%   75% 97.5%  Rhat n.eff
chisq      1.553   0.000 1.553 1.553 1.553 1.553 1.553 1.000     1
k          3.455   1.909 0.549 2.007 3.185 4.612 7.908 1.002  3000
resid[1]   0.899   0.000 0.899 0.899 0.899 0.899 0.899 1.000     1
resid[2]   0.644   0.000 0.644 0.644 0.644 0.644 0.644 1.000     1
resid[3]   0.011   0.000 0.011 0.011 0.011 0.011 0.011 1.000     1
deviance   3.883   1.426 2.870 2.976 3.334 4.230 8.006 1.003   920

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 1.0 and DIC = 4.9
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-evaluation-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model evaluation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; denplot(data.r2jags2, parms = c(&amp;quot;k&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/gof-tests-jags/2020-02-01-gof-tests-jags_files/figure-html/mcmc_diag_v2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(data.r2jags2, parms = c(&amp;quot;k&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/gof-tests-jags/2020-02-01-gof-tests-jags_files/figure-html/mcmc_diag_v2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; raftery.diag(as.mcmc(data.r2jags2))
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 

You need a sample size of at least 3746 with these values of q, r and s

[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 

You need a sample size of at least 3746 with these values of q, r and s
&amp;gt; 
&amp;gt; autocorr.diag(as.mcmc(data.r2jags2))
       chisq    deviance           k resid[1] resid[2] resid[3]
Lag 0    NaN  1.00000000  1.00000000      NaN      NaN      NaN
Lag 1    NaN  0.42415918  0.27295298      NaN      NaN      NaN
Lag 5    NaN -0.01961156 -0.01627609      NaN      NaN      NaN
Lag 10   NaN -0.03086926 -0.01043329      NaN      NaN      NaN
Lag 50   NaN -0.01409259 -0.02172076      NaN      NaN      NaN&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: The trace plots show what appears to be “random noise” about the parameter value. There is no real suggestion of a step or dramatic change in the trend direction along the length of the sampling chain. The samples seem relatively stable. Thus it would seem that the chains are well mixed and have converged. The density plot (for &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;) is not symmetrical. This suggests that the mean is not a good point estimate for this parameter - the median would be better.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-model-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring model parameters&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; print(data.r2jags2)
Inference for Bugs model at &amp;quot;chi2model2.txt&amp;quot;, fit using jags,
 2 chains, each with 2500 iterations (first 1000 discarded)
 n.sims = 3000 iterations saved
         mu.vect sd.vect  2.5%   25%   50%    75%  97.5%  Rhat n.eff
chisq      6.298   0.000 6.298 6.298 6.298  6.298  6.298 1.000     1
k          8.293   3.611 2.306 5.686 7.915 10.559 16.165 1.001  3000
resid[1]   0.028   0.000 0.028 0.028 0.028  0.028  0.028 1.000     1
resid[2]   2.837   0.000 2.837 2.837 2.837  2.837  2.837 1.000     1
resid[3]   3.433   0.000 3.433 3.433 3.433  3.433  3.433 1.000     1
deviance   5.338   1.428 4.346 4.444 4.809  5.647  9.405 1.001  3000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 1.0 and DIC = 6.4
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: The median degrees of freedom (&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;) was &lt;span class=&#34;math inline&#34;&gt;\(8.00\)&lt;/span&gt; with a &lt;span class=&#34;math inline&#34;&gt;\(95\)&lt;/span&gt;% spread of &lt;span class=&#34;math inline&#34;&gt;\(2.31-16.16\)&lt;/span&gt;. This interval does not include the value of &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; (expected value of the chi2 distribution for this hypothesis). Hence there is evidence that the population ratio deviates from a 1:1:1 ratio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; print(data.r2jags2.1)
Inference for Bugs model at &amp;quot;chi2model2.txt&amp;quot;, fit using jags,
 2 chains, each with 2500 iterations (first 1000 discarded)
 n.sims = 3000 iterations saved
         mu.vect sd.vect  2.5%   25%   50%   75% 97.5%  Rhat n.eff
chisq      1.553   0.000 1.553 1.553 1.553 1.553 1.553 1.000     1
k          3.455   1.909 0.549 2.007 3.185 4.612 7.908 1.002  3000
resid[1]   0.899   0.000 0.899 0.899 0.899 0.899 0.899 1.000     1
resid[2]   0.644   0.000 0.644 0.644 0.644 0.644 0.644 1.000     1
resid[3]   0.011   0.000 0.011 0.011 0.011 0.011 0.011 1.000     1
deviance   3.883   1.426 2.870 2.976 3.334 4.230 8.006 1.003   920

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 1.0 and DIC = 4.9
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: The median degrees of freedom (&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;) was &lt;span class=&#34;math inline&#34;&gt;\(3.31\)&lt;/span&gt; with a &lt;span class=&#34;math inline&#34;&gt;\(95\)&lt;/span&gt;% spread of &lt;span class=&#34;math inline&#34;&gt;\(0.57-7.61\)&lt;/span&gt;. This interval comfortably includes the value of &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; (expected value of the chi2 distribution for this hypothesis). Hence there is no evidence that the population ratio deviates from a 1:1:2 ratio.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploration-of-the-trends&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploration of the trends&lt;/h2&gt;
&lt;p&gt;There are a number of avenues we could take in order to explore the data and models further. One thing we could do is calculate the probability that &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is greater than $24 (the expected value) for each hypothesis. This can be done either by modifying the &lt;code&gt;JAGS&lt;/code&gt; code to include a derivative that uses the step function, or we can derive it within &lt;code&gt;R&lt;/code&gt; from the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; samples. Lets explore the latter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; k &amp;lt;- data.r2jags2$BUGSoutput$sims.matrix[, &amp;quot;k&amp;quot;]
&amp;gt; pr &amp;lt;- sum(k &amp;gt; 2)/length(k)
&amp;gt; pr
[1] 0.9813333
&amp;gt; 
&amp;gt; k &amp;lt;- data.r2jags2.1$BUGSoutput$sims.matrix[, &amp;quot;k&amp;quot;]
&amp;gt; pr1 &amp;lt;- sum(k &amp;gt; 2)/length(k)
&amp;gt; pr1
[1] 0.7513333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: the probability that the expected value exceeds &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; for the 1:1:1 hypothesis is &lt;span class=&#34;math inline&#34;&gt;\(0.982\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(98.2\)&lt;/span&gt;%). There is an &lt;span class=&#34;math inline&#34;&gt;\(98.2\)&lt;/span&gt;% likelihood that the population is not 1:1:1.
We could also compare the two alternative hypotheses. The 1:1:2 hypothesis has lower DIC and is therefore considered a better fit (&lt;span class=&#34;math inline&#34;&gt;\(4.7\)&lt;/span&gt; vs &lt;span class=&#34;math inline&#34;&gt;\(6.4\)&lt;/span&gt;). This is a difference in DIC of around &lt;span class=&#34;math inline&#34;&gt;\(1.7\)&lt;/span&gt; units. So the data have higher support for a 1:1:2 population ratio than a 1:1:1 ratio.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-plummer2004jags&#34;&gt;
&lt;p&gt;Plummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-su2015package&#34;&gt;
&lt;p&gt;Su, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” &lt;em&gt;R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
