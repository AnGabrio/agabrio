<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian statistics on Andrea Gabrio</title>
    <link>/tags/bayesian-statistics/</link>
    <description>Recent content in Bayesian statistics on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>`{year}`</copyright>
    <lastBuildDate>Fri, 07 Aug 2020 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/bayesian-statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>What is Bayesian inference?</title>
      <link>/post/update-july/</link>
      <pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-july/</guid>
      <description>


&lt;p&gt;What is probability ? The answer to this question is generally acknowledged to be the one that respects the so called Kolmogorov axioms which can be brutally simplified to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Probabilities are non-negative&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Probabilities sum to one&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The joint probability of disjoint events is the sum of the probabilities of the events&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One of the ways in which Bayesian statistics differs from classical statistics is in the &lt;strong&gt;interpretation&lt;/strong&gt; of probability. Differences in interpretation continue to be controversial, are critical to the distinction between Bayesian and non-Bayesian statistics.&lt;/p&gt;
&lt;p&gt;In classical statistics probability is often understood as a &lt;em&gt;property of the phenomenon being studied&lt;/em&gt;: for instance, the probability that a tossed coin will come up heads is a characteristic of the coin. Thus, by tossing the coin many times under more or less identical conditions, and noting the result of each toss, we can estimate the probability of a head, with the precision of the estimate monotonically increasing with the number of tosses. In this view, probability is the limit of a long-run, relative frequency; i.e. if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is an event of interest (e.g. the coin lands heads up) then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \text{Pr}(A) = \lim_{n\rightarrow\infty}\frac{m}{n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is the probabilty of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is the number of times we observe the event &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of repetitions. Given this definition of probability, we can understand why classicial statistics is sometimes referred to as &lt;em&gt;frequentist&lt;/em&gt; and &lt;em&gt;objectivist&lt;/em&gt;. However, historians of science stress that at least two notions of probability were under development from the late &lt;span class=&#34;math inline&#34;&gt;\(1600\)&lt;/span&gt;s onwards: the objectivist view described above, and a subjectivist view. With regard to the latter, we can consider different ‘degrees’ of belief to interpret probability, ‘from the very neighborhourhood of certainty and demonstration, quite down to improbability and unlikeliness, even to the confines of impossibility’. For Locke, ‘Probability is likeliness to be true’, a definition in which (repeated) games of chance play no part. For Bernoulli, ‘Probability is degree of certainty and differs from absolute certainty as the part differs from the whole’, it being unequivocal that the ‘certainty’ referred to is a state of mind, but, critically, (1) varied from person to person (depending on one’s knowledge and experience) and (2) was quantifiable. Ramsey and de Finetti, working independently, showed that subjective probability is not just any set of subjective beliefs, but beliefs that conform to the axioms of probability. The Ramsey-de Finetti Theorem states that if &lt;span class=&#34;math inline&#34;&gt;\(p_1, p_2, \ldots\)&lt;/span&gt; are a set of betting quotients on hypotheses &lt;span class=&#34;math inline&#34;&gt;\(h_1, h_2,\ldots\)&lt;/span&gt; , then if the &lt;span class=&#34;math inline&#34;&gt;\(p_j\)&lt;/span&gt; do not satisfy the probability axioms, there exists a betting strategy and a set of stakes such that whoever follows this betting strategy will lose a finite sum whatever the truth values of the hypotheses turn out to be. In de Finetti’s terminology, subjective probabilities that fail to conform to the axioms of probability are incoherent or inconsistent. Thus, subjective probabilities are whatever a particular person believes, provided they satisfy the axioms of probability. Thus, if I do not update my subjective beliefs in light of new information (data) in a manner consistent with the probability axioms, and you can convince me to gamble with you, you have the opportunity to take advantage of my irrationality, and are guaranteed to profit at my expense. That is, while probability may be subjective, Bayes Rule governs how rational people should update subjective beliefs.&lt;/p&gt;
&lt;div id=&#34;subjective-probability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Subjective probability&lt;/h2&gt;
&lt;p&gt;Bayesian probability statements are thus about states of mind over states of the world, and not about states of the world per se. Indeed, whatever one believes about determinism or chance in social processes, the meaningful uncertainty is that which resides in our brains, upon which we will base decisions and actions. This is why, in one of the more memorable and strongest statements of the subjectivist position, de Finetti writes &lt;strong&gt;probability does not exist&lt;/strong&gt;: “The abandonment of superstitious beliefs about … Fairies and Witches was an essential step along the road to scientific thinking. Probability, too, if regarded as something endowed with some kind of objective existence, is not less a misleading misconception, an illusory attempt to exteriorize or materialize our true probabilistic beliefs. In investigating the reasonableness of our own modes of thought and behaviour under uncertainty, all we require, and all that we are reasonably entitled to, is consistency among these beliefs, and their reasonable relation to any kind of relevant objective data”.&lt;/p&gt;
&lt;p&gt;The use of subjective probability also means that Bayesians can report probabilities without a “practically unlimited” sequence of observations. What is the frequentist probability of the truth of the proposition “Jackson was the eighth president”? Since there is only one relevant experiment for this problem, the frequentist probability is either zero (if Jackson was not the eighth president) or one (if Jackson was the eighth president). Non-trivial frequentist probabilities, it seems, are reserved for phenomena that are standardized and repeatable. Bayes Theorem itself is uncontroversial: it is merely an accounting identity that follows from the axioms of probability discussed above, plus the following additional definition.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Conditional probability&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; be events with &lt;span class=&#34;math inline&#34;&gt;\(P(B)&amp;gt;0\)&lt;/span&gt;. Then the conditional probability of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(A\mid B) = \frac{P(A \cap B)}{P(B)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The following two useful results are also implied by the probability axioms, plus the definition of conditional probability&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Multiplication rule&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(A \cap B) = P(A\mid B)P(B) = P(B\mid A)P(A)\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Law of total probability&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(B) = P(A\cap B)+ P\overline{(A\cap B)} = P(B\mid A)P(A) + P(B \mid \overline{A})P(\overline{A})\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayes-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayes theorem&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Bayes Theorem&lt;/em&gt; can now be stated, following immediately from the definition of conditional probability. If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are events with &lt;span class=&#34;math inline&#34;&gt;\(P(B)&amp;gt;0\)&lt;/span&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we consider the event &lt;span class=&#34;math inline&#34;&gt;\(A=H\)&lt;/span&gt; to be an hypothesis and the event &lt;span class=&#34;math inline&#34;&gt;\(B=E\)&lt;/span&gt; to be observing some evidence, then &lt;span class=&#34;math inline&#34;&gt;\(Pr(H\mid E)\)&lt;/span&gt; is the probability of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; after obtaining &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\text{Pr}(H)\)&lt;/span&gt; is the prior probability of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; before considering &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;. The conditional probability on the left-hand side of the theorem, &lt;span class=&#34;math inline&#34;&gt;\(\text{Pr}(H\mid E)\)&lt;/span&gt;, is usually referred to as the posterior probability of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;. Bayes Theorem thus supplies a solution to the general problem of inference or induction, providing a mechanism for learning about the plausibility of a hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; from data &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In most analyses in the social sciences, we want to learn about a continuous parameter, rather than the discrete parameters considered in the discussion thus far. Examples include the mean of a continuous variable, a proportion (a continuous parameter on the unit interval), a correlation, or a regression coefficient. In general, let the unknown parameter be &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and denote the data available for analysis as &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y = (y_1, \ldots , y_n)\)&lt;/span&gt;. In the case of continuous parameters, beliefs about the parameter are represented as probability density
functions or pdfs; we denote the prior pdf as &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; and the posterior pdf as &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid \boldsymbol y)\)&lt;/span&gt;. Then, Bayes Theorem for a continuous parameter is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p(\theta \mid \boldsymbol y) = \frac{p(\boldsymbol y \mid \theta) p(\theta)}{\int p(\boldsymbol y \mid \theta) p(\theta) d\theta}\]&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;which is often approximated by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p(\theta \mid \boldsymbol y) \propto p(\boldsymbol y \mid \theta) p(\theta) \]&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;where the proportionality constant is &lt;span class=&#34;math inline&#34;&gt;\(\left[ \int p(\boldsymbol y \mid \theta) p(\theta) d\theta \right]^{-1}\)&lt;/span&gt; which ensures that the posterior density integrates to one, as a proper probability density. The first term on the right hand side of the Equation is the &lt;em&gt;likelihood function&lt;/em&gt;, the probability density of the data &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y\)&lt;/span&gt;, considered as a function of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. This formulation of Bayes Rule highlights a particularly elegant feature of the Bayesian approach, showing how the likelihood function &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol y|\theta)\)&lt;/span&gt; can be “inverted” to generate a
probability statement about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, given data &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y\)&lt;/span&gt;. Thus, from a Bayesian perspective, likelihood based analyses of data assume prior ignorance, although seldom is this assumption made explicit, even if it were plausible. In other cases, when working with the so-called conjugate priors in the exponential family, the mean of the posterior distribution is a precision-weighted average of the prior and the likelihood. Suppose a prior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; belongs to a class of parametric of densities, &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;. More specifically, the prior density is said to be conjugate with respect to a likelihood &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol y\mid \theta)\)&lt;/span&gt; if the posterior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid \boldsymbol y )\)&lt;/span&gt; is also in &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Bayesian statistical inference is equivalent to combining information, marrying the information in the prior with the information in the data, with the relative contributions of prior and data to the posterior being proportional to their respective precisions. That is, Bayesian analysis with conjugate priors over a parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is equivalent to taking a precision-weighted average of prior information about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and the information in the data about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Thus:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Thus, when prior beliefs about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; are ‘vague’, ‘diffuse’, or, in the limit, uninformative, the posterior density will be dominated by the likelihood (i.e. the data contains much more information than the prior about the parameters);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When prior information is available, the posterior incorporates it, and rationally, in the sense of being consistent with the laws of probability via Bayes Theorem. In fact, when prior beliefs are quite precise relative to the data, it is possible that the likelihood is largely ignored, and the posterior distribution will look almost exactly like the prior&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note also that via Bayes Rule, if a particular region of the parameter space has zero prior probability, then it also has zero posterior probability. This feature of Bayesian updating
has been dubbed &lt;em&gt;Cromwell’s Rule&lt;/em&gt; by Lindley. The point here is that posterior distributions can sometimes look quite unusual, depending on the form of the prior and the likelihood for a particular problem. The fact that a posterior distribution may have a peculiar shape is of no great concern in a Bayesian analysis: provided one is updating prior beliefs via Bayes Rule, all is well. Unusual looking posterior distributions might suggest that one’s prior distribution was poorly specified, but, as a general rule, one should be extremely wary of engaging this kind of procedure. Bayes Rule is a procedure for generating posterior distributions over parameters in light of data. Although one can always re-run a Bayesian analysis with different priors (and indeed, this is usually a good idea), Bayesian procedures should not be used to hunt for priors that generate the most pleasing looking posterior distribution given a particular data set and likelihood. Indeed, such a practice would amount to an inversion of the Bayesian approach: i.e. if the researcher has strong ideas as to what values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; are more likely than others, aside from the information in the data, then that auxiliary information should be considered a prior, with Bayes Rule providing a procedure for rationally combining that auxiliary information with the information in the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-updating-of-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian updating of information&lt;/h2&gt;
&lt;p&gt;Bayesian procedures are often equivalent to combining the information in one set of data with another set of data. In fact, if prior beliefs represent the result of a previous data analysis (or perhaps many previous data analyses), then Bayesian analysis is equivalent to &lt;em&gt;pooling information&lt;/em&gt;. This is a particularly compelling feature of Bayesian analysis, and one that takes on special significance when working with cojugate priors. In these cases, Bayesian procedures accumulate information in the sense that the posterior distribution is more precise than either the prior distribution or the likelihood alone. Further, as the amount of data increases, say through repeated applications of the data generation process, the posterior precision will continue to increase, eventually overwhelming any non-degenerate prior; the upshot is that analysts with different (non-degenerate) prior beliefs over a parameter will eventually find their beliefs coinciding, provided they (1) see enough data and (2) update their beliefs using Bayes Theorem. In this way Bayesian analysis has been proclaimed as a model for scientific practice acknowledging that while reasonable people may differ (at least prior to seeing data), our views will tend to converge as scientific knowledge accumulates, provided we update our views rationally, consistent with the laws of probability.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters-as-random-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameters as random variables&lt;/h2&gt;
&lt;p&gt;One of the critical ways in which Bayesian statistical inference differs from frequentist
inference is that the result of a Bayesian analysis, the posterior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid \boldsymbol y)\)&lt;/span&gt; is just that, a probability density. Given a subjectivist interpretation of probabilty that most Bayesians adopt, the ‘randomness’ summarized by the posterior density is a reflection of the researcher’s uncertainty over &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, conditional on having observed data. Contrast the frequentist approach, in which &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is not random, but a fixed (but unknown) property of a population from which we randomly sample data &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y\)&lt;/span&gt;. Repeated applications of the sampling process, if undertaken, would yield different y, and different sample based estimates of θ, denoted &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta} = \hat{\theta}(\boldsymbol y)\)&lt;/span&gt;, this notation reminding us that estimates of parameters are functions of data. In the frequentist scheme, the &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}(\boldsymbol y)\)&lt;/span&gt; vary randomly across data sets (or would, if repeated sampling was undertaken), while the parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is a constant feature of the population from which data sets are drawn. The distribution of values of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}(\boldsymbol y)\)&lt;/span&gt; that would result from repeated application of the sampling process is called the sampling distribution, and is the basis of inference in the frequentist approach; the standard deviation of the sampling distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is the standard error of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt;, which plays a key role in frequentist inference. The Bayesian approach does not rely on how &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; might vary over repeated applications of random sampling. Instead, Bayesian procedures center on a simple question: “what should I believe about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in light of the data available for analysis, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y\)&lt;/span&gt; ?”&lt;/p&gt;
&lt;p&gt;The critical point to grasp is that in the Bayesian approach, the roles of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; are reversed relative to their roles in classical, frequentist inference: &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is random, in the sense that the researcher is uncertain about its value, while &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is fixed, a feature of the data at hand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;So, we have seen a couple of interesting things about Bayesian statistics which people may not be aware of. First, Bayesian statistics is a scientific approach in that it provides a rational way to update subjective beliefs based on the available evidence through Bayes theorem which conforms the rules of probability. This ensures the scientific credibility of the posterior results while also providing a way to solve the inductive problem of learning from the data and update our belief about a parameter/hypothesis. Second, in contrast to the classical approach, Bayesian statistics do not rely on asymptotic results of a series of repeateable events in order to hold and therefore can be used to answer questions which do not have any meaning in the context of repeated events. Finally, Bayesian statistics sees any unknown quantity (e.g. parameters) as random variables and attach to them a probability distribution expressing the uncertainty around the estimates. Since the entire posterior distribution is derived based on Bayes theorem, this ensures correct propagation of uncertainty from the data and prior and does not require the additional step of classical statistics of deriving uncertainty measures in an “artifical way” or relying on asymptotic results.&lt;/p&gt;
&lt;p&gt;I hope this was a bit interesting for those who would like to get more familiar with the Bayesian philosophy and its underlying implications in terms of statistical assumptions and methods. Of course, being a Bayesian, this is the best way to go for me when doing an analysis and I would love to see more people embracing the Bayesian way as a new way of thinking statistics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/qav3a2OPBdZoQ/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why be Bayesian?</title>
      <link>/post/update-july/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-july/</guid>
      <description>


&lt;p&gt;Many times I have been asked by co-workers and people around me who are a bit familiar with statistics why I choose to be Bayesian and whether I feel confident in using this approach for my data analysis rather than the most widely accepted frequentist methods, at least in my research area. Well, I am sure there are many valid arguments I could use to reply to this question but if I have to summarise my answer in two words I would say: &lt;strong&gt;why not&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;Now, a bit more into the details for those who were not extremely annoyed by my previous sentence. So, I truly believe that the Bayesian approach can be considered as a complement rather than a substitute to the frequentist paradigm. The main reason is relate to its much stronger links with probability theory compared with the classical approach in that not only are sampling distributions required for summaries of data, but also a wide range of distributions are used to represent prior opinion about proportions, event rates, and other unknown quantities. In a nutshell, the key difference between the two approaches is how they confront the concept of &lt;strong&gt;probability&lt;/strong&gt; of a certain event. In fact, although there is general consensus about the &lt;em&gt;rules&lt;/em&gt; of probability, that there is no universal &lt;em&gt;concept&lt;/em&gt; of probability, and two quite different definitions come from the frequentist and Bayesian approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The most widely known definition is: the proportion of times a will occur in an infinitely long series of repeated identical situations. This is known as the &lt;strong&gt;frequentist&lt;/strong&gt; perspective, as it rests on the &lt;em&gt;frequency&lt;/em&gt; with which specific events occur.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In contrast, the &lt;strong&gt;Bayesian&lt;/strong&gt; approach rests on an essentially subjective interpretation of probability, which is allowed to express generic uncertainty or &lt;em&gt;degree of belief&lt;/em&gt; about any unknown but potentially observable quantity, whether or not it is one of a number of repeatable experiments.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Rather than debating on philosophical debates about the foundations of statistics I prefer to focus on those aspects which I believe make the Bayesian approach, if not more intuitive than the frequentist counterpart, at least more attractive. Be worn I am not trying to start a war as I think both approaches could be used without the need to completely discard the other. The simple fact of being able to choose between two methods, rather than restricting themselves to a single option, seems a good enough reason for me to advocate the use of &lt;strong&gt;both&lt;/strong&gt; approaches. I terms of your own knowledge, experience and skills, You do not gain anything by saying “I will never be Bayesian” or “I will never be a frequentist”. On the contrary, by opening your mind and explore the use of one or the other method you will be able to have more options at your disposal that you can use to tackle the different problems you will face in your analyses.&lt;/p&gt;
&lt;p&gt;For the purpose of this post I just want to highlight some aspects which make the Bayesian approach particularly useful and, in some cases, even arguably preferable than the frequentist approach. Note that I am well aware there could be cases where the opposite holds and this is precisely why I believe it is important that statisticians should become familiar with both methods. By doing so they will be able to overcome the limitations/concerns associated with one method for a specific problem at hand using the instruments made available from the other method. Since I am a Bayesian, here I want to report the reasons and situations in which the Bayesian approach could provide a powerful tool.&lt;/p&gt;
&lt;p&gt;Let us start with a quick recap of the basic principle behind Bayesian methods. Bayesian statistical analysis relies on &lt;strong&gt;Bayes’s Theorem&lt;/strong&gt;, which tells us how to update prior
beliefs about parameters and hypotheses in light of data, to yield posterior beliefs. The theorem itself is utterly uncontroversial and follows directly from the conventional definition of conditional probability. If &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is some object of interest, but subject to uncertainty, e.g. a parameter, a hypothesis, a model, a data point, then Bayes Theorem tells us how to rationally
revise prior beliefs about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;, in light of the data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, to yield posterior beliefs &lt;span class=&#34;math inline&#34;&gt;\(p(y \mid \theta)\)&lt;/span&gt;. In this way Bayes Theorem provides a solution to the general problem of &lt;em&gt;induction&lt;/em&gt;, while in the specific case of statistical inference, Bayes Theorem provides a solution to problem of &lt;em&gt;how to learn from data&lt;/em&gt;. Thus, in a general sense, Bayesian statistical analysis is remarkably simple and even elegant, relying on this same simple recipe in each and every application.&lt;/p&gt;
&lt;p&gt;As I see it, there are a few major reasons why statisticians should consider learning about the Bayesian approach to statistical inference, and in the social sciences in particular:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Bayesian inference is simple and direct&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The result of a Bayesian analysis is a posterior probability statement, ‘posterior’ in the literal sense, in that such a statement characterizes beliefs after looking at data. Examples include: the posterior probability that a regression coefficient is positive, negative or lies in a particular interval; the posterior probability that a subject belongs to a particular latent class; the posterior probabilities that a particular statistical model is true model among a
family of statistical models.&lt;/p&gt;
&lt;p&gt;Note that the posterior probability statements produced by a Bayesian analysis are probability
statements over the quantities or objects of direct substantive interest to the researcher (e.g. parameters, hypotheses, models, predictions from models). Bayesian procedures condition on the data at hand to produce posterior probability statements about parameters and hypotheses. Frequentist procedures do just the reverse: one conditions on a null hypothesis to assess the plausibility of the data one observes (and more ‘extreme’ data sets that one did not observe but we might have had we done additional sampling), with another step of reasoning required to either reject or fail to reject the null hypothesis. Thus, compared to frequentist procedures, Bayesian procedures are simple and straightforward, at least conceptually.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical modeling&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The prior density also provides a way for model expansion when we work with data sets that pool data over multiple units and/or time periods. Data sets of this sort abound in the social sciences. Individuals live in different locations, with environmental factors that are constant for anyone within that location, but vary across locations. key question in research of this type is how the causal structure that operates at one level of analysis (e.g. individuals) varies across a ‘higher’ level of analysis (e.g. localities or time periods). The Bayesian approach to statistical inference is extremely well-suited to answering this question. Recall that in the Bayesian approach parameters are always random variables, typically (and most basically) in the sense that the researcher is unsure as to their value, but can characterize that uncertainty in the form of a prior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can replace the prior with a stochastic model formalizing the researcher’s assumptions about the way that parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; might vary across groups &lt;span class=&#34;math inline&#34;&gt;\(j = 1,..., J\)&lt;/span&gt; , perhaps as a function of observable characteristics of the groups; e.g., &lt;span class=&#34;math inline&#34;&gt;\(\theta_j \sim f (z_j, \gamma )\)&lt;/span&gt;, where now &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is a set of unknown hyperparameters. That is, the model is now comprised of a nested hierarchy of stochastic relations: the data from unit &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;, are modeled as a function of covariates and parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; , while cross-unit heterogeneity in the &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; is modeled as function of unit-specific covariates &lt;span class=&#34;math inline&#34;&gt;\(z_j\)&lt;/span&gt; and hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;. Models of this sort are known to Bayesians as &lt;em&gt;hierarchical models&lt;/em&gt;, but go by many different names in different parts of the social sciences depending on the specific form of the model and the estimation strategy being used (e.g. ‘random’ or ‘varying’ coefficients models, ‘multilevel’ or ‘mixed’ models). Compared with the frequentist counterpart, thanks to the use of &lt;em&gt;Markov chain Monte Carlo&lt;/em&gt; (MCMC) methods, Bayesian computation for these models has also become rather simple. Indeed, MCMC algorithms have proven themselves amazingly powerful and flexible, and have brought wide classes of models and data sets out of the ‘too hard’ basket. Other modelling examples include data sets with lots of missing data, or models with lots of parameters, model with latent variables, mixture
models, and flexible semi-and non-parametric models.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Statistically significant?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Frequentist inference asks assuming hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; is true, how often would we obtain a result at least as extreme as the result actually obtained?’, where ‘extreme’ is relative to the hypothesis being tested. If results such as the one obtained are sufficiently rare under hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; (e.g. generate a sufficiently small p value), then we conclude that &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; is incorrect, rejecting it in favor of some alternative hypothesis. Indeed, we teach our students to say that when the preceding conditions hold, we have a &lt;em&gt;statistically significant&lt;/em&gt; result. My experience is that in substituting this phrase for the much longer textbook definition, people quickly forget the frequentist underpinnings of what it is they are really asserting, and, hence seldom question whether the appeal to the long-run, repeated sampling properties of a statistical procedure is logical or realistic.&lt;/p&gt;
&lt;p&gt;In the Bayesian approach we condition on the data at hand to assess the plausibility of a hypothesis (via Bayes Rule), while the frequentist approach conditions on a hypothesis to assess the plausibility of the data (or more extreme data sets), with another step of reasoning required to either reject or fail to reject hypotheses. The frequentist p-value is the relative frequency of obtaining a result at least as extreme as the result actually obtained, assuming hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; to be true, where the sampling distribution of the result tells us how to assess relative frequencies of possible different results, under &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;. But what about cases where repeated sampling makes no sense, even as a thought experiment?&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Intervals&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Recall that in the frequentist approach, parameters are fixed characteristics of populations, so &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; either lies in the interval or it doesn’t. The correct interpretation of a frequentist confidence interval concerns the repeated sampling characteristics of a sample statistic. In the case of a &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; confidence interval, the correct frequentist interpretation is that &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; confidence intervals one would draw in repeated samples will include &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. Now, is the &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; confidence interval that one constructs from the data set at hand one of the lucky &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; that actually contains &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, or not? No ones knows.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Rational subjectivity&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, aside from acknowledging the subjectivity inherent to the general scientific exercise, the Bayesian approach rests on a subjective notion of probability, but demands that subjective
beliefs conform to the laws of probability. Put differently, in the Bayesian approach, the subjectivity of scientists is acknowledged, but simultaneously insists that subjectivity be
rational, in the sense that when confronted with evidence, subjective beliefs are updated rationally, in accord with the axioms of probability. Again, it is in this sense that Bayesian procedures offer a more direct path to inference; as I put it earlier, the Bayesian approach lets researchers mean what they say and say what they mean. For instance, the statement, having looked at the data, I am &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; sure that &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is included in an interval is a natural product of a Bayesian analysis, a characterization of the researcher’s beliefs about a parameter in formal, probabilistic terms, rather than a statement about the repeated sampling properties of a statistical procedure.&lt;/p&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The mathematics and computation underlying Bayesian analysis has been dramatically simplified
via a suite of MCMC algorithms. The combination of the popularization of MCMC and vast
increases in the computing power available to social scientists means that Bayesian analysis
is now well and truly part of the mainstream of quantitative social science. Despite these important pragmatic reasons for adopting the Bayesian approach, it is important to remember that MCMC algorithms are Bayesian algorithms: they are tools that simplify the computation of posterior densities. So, before we can fully and sensibly exploit the power of MCMC algorithms, it is important that we understand the foundations of Bayesian inference.&lt;/p&gt;
&lt;p&gt;This time I went overboard with the discussion but I thought it could be interesting to clarify here the key points, in my opinion, which make the Bayesian approach not only valid and efficient, but even a powerful tool that, once grasped the underlying phylosophy, can be used to overcome the difficulties of standard methods, especially when dealing with complex analyses.&lt;/p&gt;
&lt;p&gt;So what are you waiting for? do not sit in your frequentist comfort zone but expand your statistical knowledge! Evolve!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/u1k1kpDZSw5sA/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian methods for addressing missing data in health economic evaluations</title>
      <link>/talk/albacete2019/</link>
      <pubDate>Tue, 11 Jun 2019 10:00:00 +0000</pubDate>
      
      <guid>/talk/albacete2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations</title>
      <link>/publication/gabrio2019c/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2019c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Statistical Economic Evaluation Methods for Health Technology Assessment</title>
      <link>/publication/gabrio2019b/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2019b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Full Bayesian Model to Handle Structural Ones and Missingness in Economic Evaluations from Individual-Level Data</title>
      <link>/publication/gabrio2019a/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2019a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Full Bayesian Model to Handle Structural Ones and Missingness in Health Economic Evaluations from Individual-Level Data</title>
      <link>/talk/hesymposium2018/</link>
      <pubDate>Mon, 05 Feb 2018 10:00:00 +0000</pubDate>
      
      <guid>/talk/hesymposium2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>missingHE</title>
      <link>/missinghe/</link>
      <pubDate>Mon, 05 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/missinghe/</guid>
      <description>&lt;p&gt;&lt;code&gt;missingHE&lt;/code&gt; is a &lt;code&gt;R&lt;/code&gt; package aimed at providing some useful tools to analysts in order to handle missing outcome data under a Full Bayesian framework in economic evaluations. The package relies on the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;R2jags&lt;/code&gt; to implement Bayesian methods via the statistical software &lt;code&gt;JAGS&lt;/code&gt;. The package allows to obtain inferences using Markov Chain Monte Carlo (MCMC) methods under a range of modelling approaches and missing data assumptions. The package also contains functions specifically defined to assess model fit and possible issues in model convergence as well as to summarise the main results from the economic analysis.&lt;/p&gt;
&lt;p&gt;Missing data are iteratively imputed using data augmentation methods according to the type of model, distribution and missingness assumptions specified by the user using different arguments in the functions of the package. The posterior distribution of the main quantities of interest (e.g. some suitable measures of costs and clinical benefits) is then summarised to assess the cost-effectiveness of a new intervention ($t=2$) against a standard intervention ($t=1$).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;missingHE&lt;/code&gt; produces plots which compares the observed and imputed values for both cost and benefit measures in each treatment intervention considered to detect possible concerns about the plausibility of the imputation methods. In addition, the output of &lt;code&gt;missingHE&lt;/code&gt; cab be analysed using different funtions in the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;BCEA&lt;/code&gt; which produces a synthesis of the decision process given the current evidence and uncertainty, as well as several indicators that can be used to perform Probabilistic Sensitivity Analysis to parameter and model uncertainty.&lt;/p&gt;



  




&lt;figure&gt;

&lt;img src=&#34;/img/imputed.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Example of a graphical output from missingHE&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h1 id=&#34;example&#34;&gt;Example&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(missingHE)
model.sel &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;selection&lt;/span&gt;(data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MenSS, model.eff &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; e &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; u.0, model.cost &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; e, model.me &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; me &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, model.mc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mc &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, 
                       type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;MAR&amp;#34;&lt;/span&gt;, n.chains &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, n.iter &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10000&lt;/span&gt;, n.burnin &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;, dist_e &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;norm&amp;#34;&lt;/span&gt;, dist_c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;norm&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;summary&lt;/span&gt;(model.sel)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt; Cost&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;effectiveness analysis summary 
 
 Comparator intervention&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; intervention &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; 
 Reference intervention&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; intervention &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; 
 
 Parameter estimates under MAR assumption
 
 Comparator intervention 
               mean     sd      LB      UB
mean.effects  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.874&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.017&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;0.846&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;0.901&lt;/span&gt;
mean.costs   &lt;span style=&#34;color:#ae81ff&#34;&gt;238.34&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;52.432&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;153.541&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;325.355&lt;/span&gt;

 Reference intervention 
                  mean    sd      LB      UB
mean.effects.1   &lt;span style=&#34;color:#ae81ff&#34;&gt;0.917&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.022&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;0.881&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;0.953&lt;/span&gt;
mean.costs.1   &lt;span style=&#34;color:#ae81ff&#34;&gt;186.825&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;41.26&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;119.672&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;254.125&lt;/span&gt;

 Incremental results 
                   mean     sd       LB     UB
delta.effects     &lt;span style=&#34;color:#ae81ff&#34;&gt;0.043&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.028&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;-0.003&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;0.089&lt;/span&gt;
delta.costs     &lt;span style=&#34;color:#ae81ff&#34;&gt;-51.514&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;67.025&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;-162.862&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;58.327&lt;/span&gt;
ICER          &lt;span style=&#34;color:#ae81ff&#34;&gt;-1198.431&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;news-and-updates-about-missinghe&#34;&gt;News and updates about &lt;code&gt;missingHE&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;From 25/09/2019, the updated version (1.2.1) of &lt;code&gt;missingHE&lt;/code&gt; has become available on &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE/&#34;&gt;CRAN&lt;/a&gt;, which allows to perform posterior predictive checks for each type of model as a further way to assess the fit of the model to the observed data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The checks can be done by first setting the optional argument &lt;code&gt;ppc = TRUE&lt;/code&gt; when fitting the model using one of the main function of the package. For example, when using &lt;code&gt;selection&lt;/code&gt; to fit selection models you would have something like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;model.sel &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;selection&lt;/span&gt;(data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data, model.eff &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; e &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; age, model.cost &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; age &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; e, model.me &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; me &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; age, model.mc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mc &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; age, 
dist_e &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;norm&amp;#34;&lt;/span&gt;, dist_c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gamma&amp;#34;&lt;/span&gt;, type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;MAR&amp;#34;&lt;/span&gt;, n.iter &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10000&lt;/span&gt;, ppc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then you can use the function &lt;code&gt;ppc&lt;/code&gt; to perform different types of posterior predictive checks that you can choose among a set of pre-specified types using the &lt;code&gt;type&lt;/code&gt; argument. For example, if we want to compare histograms of the empirical and predictive distributions of the effectiveness variable in one arm (e.g. control), then we can type&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;ppc&lt;/span&gt;(model.sel, type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;histogram&amp;#34;&lt;/span&gt;, outcome &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;effects_arm1&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and we get something like this&lt;/p&gt;



  




&lt;figure&gt;

&lt;img src=&#34;/img/plotpred.png&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Example of posterior predictive checks in missingHE&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;From 07/01/2020, the updated version (1.3.2) of &lt;code&gt;missingHE&lt;/code&gt; has become available on &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE/&#34;&gt;CRAN&lt;/a&gt;, which allows to choose among more distributions for the effectiveness measures, including continuous (Gamma, Weibull, Exponential, Logistic), discrete (Poisson, Negative Binomial) and binary (Bernoulli) health outcomes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, we can choose to specify a selection model assuming a Bernoulli distribution for the effects (if this is a binary outcome) and a LogNormak distribution for the costs&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;model.sel &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;selection&lt;/span&gt;(data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data, model.eff &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; e &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; age, model.cost &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; age &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; e, model.me &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; me &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, model.mc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mc &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, 
dist_e &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bern&amp;#34;&lt;/span&gt;, dist_c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;lnorm&amp;#34;&lt;/span&gt;, type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;MAR&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;From 30/04/2020, the updated version (1.4.0) of &lt;code&gt;missingHE&lt;/code&gt; has become available on &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE/&#34;&gt;CRAN&lt;/a&gt;, which allows to perform fit random effects for each type of model implemented. The random terms can be specified using the following notation&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;model.sel &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;selection&lt;/span&gt;(data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data, model.eff &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; e &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; age &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (age &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; site), model.cost &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; age &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; e &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (age &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; e &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; site), 
model.me &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; me &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; age &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; site), model.mc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mc &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; age &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; age &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; site), 
dist_e &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;norm&amp;#34;&lt;/span&gt;, dist_c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gamma&amp;#34;&lt;/span&gt;, type &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;MAR&amp;#34;&lt;/span&gt;, n.iter &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10000&lt;/span&gt;, ppc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I borrowed this notation, alongside with a couple of internal functions, from the &lt;code&gt;lme4&lt;/code&gt; package. The terms inside the brackets on the left of the bar are the terms for which the random effects are assumed (these must also be included as fixed effects). The term on the right of the bar is the clustering variable over which the random effects are specified.&lt;/p&gt;
&lt;p&gt;For example the formula &lt;code&gt; + (age | site)&lt;/code&gt; specifies random effects for the intercept and &lt;code&gt;age&lt;/code&gt; across the values of the &lt;code&gt;site&lt;/code&gt; variable. It aslo possible to specify random slope only models (i.e. remove the random intercept) by adding the term &lt;code&gt;0 +&lt;/code&gt; inside the brackets on the left of the bar.&lt;/p&gt;
&lt;p&gt;All functions in the package have been updated to take into account the possibility that random effects are specified and to perform diagnostic and posterior predictive checks based on the random effects if these are included. In addition, a new generic function called &lt;code&gt;coef&lt;/code&gt; is now available to extract the fixed or random effect terms from the effectiveness and cost models for each type of model in &lt;code&gt;missingHE&lt;/code&gt;. For example, we can extract summary statistics for the fixed effects from the fitted selection model by using the command&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;coef&lt;/span&gt;(model.sel, random &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FALSE&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;which prints something like this&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;Comparator
&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;Comparator&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;Effects
              mean    sd  lower  &lt;span style=&#34;color:#a6e22e&#34;&gt;upper
&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;&lt;/span&gt;(Intercept)  &lt;span style=&#34;color:#ae81ff&#34;&gt;4.520&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2.128&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1.694&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;7.584&lt;/span&gt;
age         &lt;span style=&#34;color:#ae81ff&#34;&gt;-0.059&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.011&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;-0.081&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;-0.037&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;Comparator&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;Costs
                mean      sd     lower    &lt;span style=&#34;color:#a6e22e&#34;&gt;upper
&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;&lt;/span&gt;(Intercept)  &lt;span style=&#34;color:#ae81ff&#34;&gt;553.614&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;576.375&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;-412.631&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2118.222&lt;/span&gt;
age           &lt;span style=&#34;color:#ae81ff&#34;&gt;-9.534&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;32.682&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;-75.701&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;50.304&lt;/span&gt;
e           &lt;span style=&#34;color:#ae81ff&#34;&gt;-934.280&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;428.726&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;-1749.524&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;-85.378&lt;/span&gt;


&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;Reference
&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;Reference&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;Effects
              mean    sd  lower &lt;span style=&#34;color:#a6e22e&#34;&gt;upper
&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;&lt;/span&gt;(Intercept) &lt;span style=&#34;color:#ae81ff&#34;&gt;-1.294&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2.381&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;-4.411&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2.091&lt;/span&gt;
age          &lt;span style=&#34;color:#ae81ff&#34;&gt;0.032&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.100&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;-0.094&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.155&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;Reference&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;Costs
                mean      sd     lower    &lt;span style=&#34;color:#a6e22e&#34;&gt;upper
&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;&lt;/span&gt;(Intercept)  &lt;span style=&#34;color:#ae81ff&#34;&gt;273.504&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;387.796&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;-349.418&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1047.288&lt;/span&gt;
age           &lt;span style=&#34;color:#ae81ff&#34;&gt;-9.138&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;36.223&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;-78.510&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;56.514&lt;/span&gt;
e           &lt;span style=&#34;color:#ae81ff&#34;&gt;-264.332&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;421.044&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;-1094.129&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;571.148&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If we set &lt;code&gt;random = TRUE&lt;/code&gt;, then summary statistics for the random effects terms are printed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;From 10/06/2020 a new version (1.4.1) of &lt;code&gt;missingHE&lt;/code&gt; is available to download from my &lt;a href=&#34;https://github.com/AnGabrio/missingHE&#34;&gt;GitHub&lt;/a&gt; page, which includes three vignettes providing some tutorials on how to use the functions of the package. Each vignette is specifically designed to help different types of users:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The first vignette is named &lt;em&gt;Introduction_to_missingHE&lt;/em&gt; and is designed to provide some introductory summary about the use of the functions of the package based on the default settings, what the user needs to specify and how to interpret and extract the results. See the &lt;a href=&#34;/files/Introduction_to_missingHE.pdf&#34; target=&#34;_blank&#34;&gt; pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second vignette is named &lt;em&gt;Fitting_MNAR_models_in_missingHE&lt;/em&gt; and is deisgned to help those who would like to explore MNAR assumptions and how this can be done within each main function of the package. See the &lt;a href=&#34;/files/Fitting_MNAR_models_in_missingHE.pdf&#34; target=&#34;_blank&#34;&gt; pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The third vinette is named &lt;em&gt;Model_customisation_in_missingHE&lt;/em&gt; and is designed for those who are already familiar with the package but who would like to customise the functions in a more flexile way, for example by including random effects, using different priors or modelling assumptions. See  the &lt;a href=&#34;/files/Model_customisation_in_missingHE.pdf&#34; target=&#34;_blank&#34;&gt; pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Soon, this version will also be uploaded on CRAN as well. In the meantime, the pdf files of these vignettes can be accessed from my website.&lt;/p&gt;
&lt;h1 id=&#34;installation&#34;&gt;Installation&lt;/h1&gt;
&lt;p&gt;There are two ways of installing &lt;code&gt;missingHE&lt;/code&gt;. A &lt;em&gt;stable&lt;/em&gt; version (currently 1.4.0) is packaged and available from &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE/&#34;&gt;CRAN&lt;/a&gt;. You can simply type on your &lt;code&gt;R&lt;/code&gt; terminal&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;install.packages&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;missingHE&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The second way involves using the &lt;em&gt;development&lt;/em&gt; version of &lt;code&gt;missingHE&lt;/code&gt;, which is available from &lt;a href=&#34;https://github.com/AnGabrio/missingHE&#34;&gt;GitHub&lt;/a&gt; - this will usually be updated more frequently and may be continuously tested. On Windows machines, you need to install a few dependencies, including Rtools first, e.g. by running&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;pkgs &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;R2jags&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ggplot2&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gridExtra&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BCEA&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ggmcmc&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;loo&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Rtools&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;devtools&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;utils&amp;#34;&lt;/span&gt;)
repos &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://cran.rstudio.com&amp;#34;&lt;/span&gt;) 
&lt;span style=&#34;color:#a6e22e&#34;&gt;install.packages&lt;/span&gt;(pkgs,repos&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;repos,dependencies &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Depends&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;before installing the package using &lt;code&gt;devtools&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;devtools&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;install_github&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;AnGabrio/missingHE&amp;#34;&lt;/span&gt;, build_vignettes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The optional argument &lt;code&gt;build_vignettes = TRUE&lt;/code&gt; allows to install the vignettes of the package locally on your computer. These consist in brief tutorials to guid the user on how to use and customise the models in missingHE using different functions of the package. Once the package is installed, they can be accessed by using the command&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;browseVignettes&lt;/span&gt;(package &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;missingHE&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;All models implemented in missingHE are written in the &lt;code&gt;BUGS&lt;/code&gt; language using the software &lt;code&gt;JAGS&lt;/code&gt;, which needs to be installed from its own repository and instructions for installations under different OS can be found online. Once installed, the software is called in missingHE via the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;R2jags&lt;/code&gt;. Note that the missingHE package is currently under active development and therefore it is advisable to reinstall the package directly from &lt;a href=&#34;https://github.com/AnGabrio/missingHE&#34;&gt;GitHub&lt;/a&gt; before each use to ensure that you are using the most updated version.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
