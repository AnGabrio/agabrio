<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>generalised linear models on Andrea Gabrio</title>
    <link>/tags/generalised-linear-models/</link>
    <description>Recent content in generalised linear models on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Feb 2020 21:13:14 -0500</lastBuildDate>
    
	    <atom:link href="/tags/generalised-linear-models/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Generalised Linear Models part II - JAGS</title>
      <link>/jags/glm2-jags/glm2-jags/</link>
      <pubDate>Fri, 14 Feb 2020 21:13:14 -0500</pubDate>
      
      <guid>/jags/glm2-jags/glm2-jags/</guid>
      <description>


&lt;p&gt;This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. &lt;code&gt;BUGS&lt;/code&gt; (Bayesian inference Using &lt;em&gt;Gibbs Sampling&lt;/em&gt;) is an algorithm and supporting language (resembling &lt;code&gt;R&lt;/code&gt;) dedicated to performing the Gibbs sampling implementation of &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; (MCMC) method. Dialects of the &lt;code&gt;BUGS&lt;/code&gt; language are implemented within three main projects:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OpenBUGS&lt;/strong&gt; - written in component pascal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;JAGS&lt;/strong&gt; - (Just Another Gibbs Sampler) - written in &lt;code&gt;C++&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;STAN&lt;/strong&gt; - a dedicated Bayesian modelling framework written in &lt;code&gt;C++&lt;/code&gt; and implementing &lt;em&gt;Hamiltonian&lt;/em&gt; MCMC samplers.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of &lt;code&gt;R&lt;/code&gt;, and thus, they are best accessed from within &lt;code&gt;R&lt;/code&gt; itself. As such there are multiple packages devoted to interfacing with the various software implementations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2OpenBUGS&lt;/em&gt; - interfaces with &lt;code&gt;OpenBUGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2jags&lt;/em&gt; - interfaces with &lt;code&gt;JAGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;rstan&lt;/em&gt; - interfaces with &lt;code&gt;STAN&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial will demonstrate how to fit models in &lt;code&gt;JAGS&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Plummer (2004)&lt;/span&gt;) using the package &lt;code&gt;R2jags&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Su et al. (2015)&lt;/span&gt;) as interface, which also requires to load some other packages.&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Whilst in many instances, count data can be approximated reasonably well by a normal distribution (particularly if the counts are all above zero and the mean count is greater than about &lt;span class=&#34;math inline&#34;&gt;\(20\)&lt;/span&gt;), more typically, when count data are modelled via normal distribution certain undesirable characteristics arise that are a consequence of the nature of discrete non-negative data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Expected (predicted) values and confidence bands less than zero are illogical, yet these are entirely possible from a normal distribution&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The distribution of count data are often skewed when their mean is low (in part because the distribution is truncated to the left by zero) and variance usually increases with increasing mean (variance is typically proportional to mean in count data). By contrast, the Gaussian (normal) distribution assumes that mean and variance are unrelated and thus estimates (particularly of standard error) might well be reasonable inaccurate.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Poisson regression is a type of &lt;strong&gt;generalised linear model&lt;/strong&gt; (GLM) in which a non-negative integer (natural number) response is modelled against a linear predictor via a specific link function. The linear predictor is typically a linear combination of effects parameters. The role of the link function is to transform the expected values of the response y (which is on the scale of (&lt;span class=&#34;math inline&#34;&gt;\(0;\infty\)&lt;/span&gt;), as is the Poisson distribution from which expectations are drawn) into the scale of the linear predictor (which is &lt;span class=&#34;math inline&#34;&gt;\(-\infty;\infty\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;As implied in the name of this group of analyses, a Poisson rather than Gaussian (normal) distribution is used to represent the errors (residuals). Like count data (number of individuals, species etc), the Poisson distribution encapsulates positive integers and is bound by zero at one end. Consequently, the degree of variability is directly related the expected value (equivalent to the mean of a Gaussian distribution). Put differently, the variance is a function of the mean. Repeated observations from a Poisson distribution located close to zero will yield a much smaller spread of observations than will samples drawn from a Poisson distribution located a greater distance from zero. In the Poisson distribution, the variance has a 1:1 relationship with the mean. The canonical link function for the Poisson distribution is a log-link function.&lt;/p&gt;
&lt;p&gt;Whilst the expectation that the mean=variance (&lt;span class=&#34;math inline&#34;&gt;\(\mu=\sigma\)&lt;/span&gt;) is broadly compatible with actual count data (that variance increases at the same rate as the mean), under certain circumstances, this might not be the case. For example, when there are other unmeasured influences on the response variable, the distribution of counts might be somewhat clumped which can result in higher than expected variability (that is &lt;span class=&#34;math inline&#34;&gt;\(\sigma &amp;gt; \mu\)&lt;/span&gt;). The variance increases more rapidly than does the mean. This is referred to as &lt;strong&gt;overdispersion&lt;/strong&gt;. The degree to which the variability is greater than the mean (and thus the expected degree of variability) is called &lt;strong&gt;dispersion&lt;/strong&gt;. Effectively, the Poisson distribution has a dispersion parameter (or &lt;strong&gt;scaling factor&lt;/strong&gt;) of &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It turns out that overdispersion is very common for count data and it typically underestimates variability, standard errors and thus deflated p-values. There are a number of ways of overcoming this limitation, the effectiveness of which depend on the causes of overdispersion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quasi-Poisson models&lt;/strong&gt; - these introduce the dispersion parameter (&lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;) into the model. This approach does not utilize an underlying error distribution to calculate the maximum likelihood (there is no quasi-Poisson distribution). Instead, if the Newton-Ralphson iterative reweighting least squares algorithm is applied using a direct specification of the relationship between mean and variance (&lt;span class=&#34;math inline&#34;&gt;\(\text{var}(y)=\phi\mu\)&lt;/span&gt;, the estimates of the regression coefficients are identical to those of the maximum likelihood estimates from the Poisson model. This is analogous to fitting ordinary least squares on symmetrical, yet not normally distributed data - the parameter estimates are the same, however they won’t necessarily be as efficient. The standard errors of the coefficients are then calculated by multiplying the Poisson model coefficient standard errors by &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\phi}\)&lt;/span&gt;. Unfortunately, because the quasi-poisson model is not estimated via maximum likelihood, properties such as AIC and log-likelihood cannot be derived. Consequently, quasi-poisson and Poisson model fits cannot be compared via either AIC or likelihood ratio tests (nor can they be compared via deviance as uasi-poisson and Poisson models have the same residual deviance). That said, quasi-likelihood can be obtained by dividing the likelihood from the Poisson model by the dispersion (scale) factor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Negative binomial model&lt;/strong&gt; - technically, the negative binomial distribution is a probability distribution for the number of successes before a specified number of failures. However, the negative binomial can also be defined (parameterised) in terms of a mean (&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;) and scale factor (&lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt;),&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p(y_i) = \frac{\Gamma(y_i+\omega)}{\Gamma(\omega)y!} \times \frac{\mu^{y_i}_i\omega^\omega}{(\mu_i+\omega)^{\mu_i+\omega}},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the expectected value of the values &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; (the means) are (&lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;) and the variance is &lt;span class=&#34;math inline&#34;&gt;\(y_i=\frac{\mu_i+\mu^2_i}{\omega}\)&lt;/span&gt;. In this way, the negative binomial is a two-stage hierarchical process in which the response is modeled against a Poisson distribution whose expected count is in turn modeled by a Gamma distribution with a mean of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and constant scale parameter (&lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt;). Strictly, the negative binomial is not an exponential family distribution (unless &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; is fixed as a constant), and thus negative binomial models cannot be fit via the usual GLM iterative reweighting algorithm. Instead estimates of the regression parameters along with the scale factor (&lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt;) are obtained via maximum likelihood. The negative binomial model is useful for accommodating overdispersal when it is likely caused by clumping (due to the influence of other unmeasured factors) within the response.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zero-inflated Poisson model&lt;/strong&gt; - overdispersion can also be caused by the presence of a greater number of zero’s than would otherwise be expected for a Poisson distribution. There are potentially two sources of zero counts - genuine zeros and false zeros. Firstly, there may genuinely be no individuals present. This would be the number expected by a Poisson distribution. Secondly, individuals may have been present yet not detected or may not even been possible. These are false zero’s and lead to zero inflated data (data with more zeros than expected). For example, the number of joeys accompanying an adult koala could be zero because the koala has no offspring (true zero) or because the koala is male or infertile (both of which would be examples of false zeros). Similarly, zero counts of the number of individual in a transect are due either to the absence of individuals or the inability of the observer to detect them. Whilst in the former example, the latent variable representing false zeros (sex or infertility) can be identified and those individuals removed prior to analysis, this is not the case for the latter example. That is, we cannot easily partition which counts of zero are due to detection issues and which are a true indication of the natural state.&lt;/p&gt;
&lt;p&gt;Consistent with these two sources of zeros, zero-inflated models combine a binary logistic regression model (that models count membership according to a latent variable representing observations that can only be zeros - not detectable or male koalas) with a Poisson regression (that models count membership according to a latent variable representing observations whose values could be 0 or any positive integer - fertile female koalas).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poisson regression&lt;/h2&gt;
&lt;p&gt;The following equations are provided since in Bayesian modelling, it is occasionally necessary to directly define the log-likelihood calculations (particularly for zero-inflated models and other mixture models).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(y \mid \lambda) = \frac{\lambda^ye^{-\lambda}}{y!},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(E[Y]=Var(Y)=\lambda\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is the mean.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-generation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data generation&lt;/h2&gt;
&lt;p&gt;Lets say we wanted to model the abundance of an item (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) against a continuous predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; set.seed(8)
&amp;gt; #The number of samples
&amp;gt; n.x &amp;lt;- 20
&amp;gt; #Create x values that at uniformly distributed throughout the rate of 1 to 20
&amp;gt; x &amp;lt;- sort(runif(n = n.x, min = 1, max =20))
&amp;gt; mm &amp;lt;- model.matrix(~x)
&amp;gt; intercept &amp;lt;- 0.6
&amp;gt; slope=0.1
&amp;gt; #The linear predictor
&amp;gt; linpred &amp;lt;- mm %*% c(intercept,slope)
&amp;gt; #Predicted y values
&amp;gt; lambda &amp;lt;- exp(linpred)
&amp;gt; #Add some noise and make binomial
&amp;gt; y &amp;lt;- rpois(n=n.x, lambda=lambda)
&amp;gt; dat &amp;lt;- data.frame(y,x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these sort of data, we are primarily interested in investigating whether there is a relationship between the binary response variable and the linear predictor (linear combination of one or more continuous or categorical predictors).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;There are at least five main potential models we could consider fitting to these data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ordinary least squares regression (general linear model)&lt;/strong&gt; - assumes normality of residuals&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Poisson regression&lt;/strong&gt; - assumes mean=variance (dispersion&lt;span class=&#34;math inline&#34;&gt;\(=1\)&lt;/span&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Quasi-poisson regression&lt;/strong&gt; - a general solution to overdispersion. Assumes variance is a function of mean, dispersion estimated, however likelihood based statistics unavailable&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Negative binomial regression&lt;/strong&gt; - a specific solution to overdispersion caused by clumping (due to an unmeasured latent variable). Scaling factor (&lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt;) is estimated along with the regression parameters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Zero-inflation model&lt;/strong&gt; - a specific solution to overdispersion caused by excessive zeros (due to an unmeasured latent variable). Mixture of binomial and Poisson models.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When counts are all very large (not close to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;) and their ranges do not span orders of magnitude, they take on very Gaussian properties (symmetrical distribution and variance independent of the mean). Given that models based on the Gaussian distribution are more optimized and recognized than Generalized Linear Models, it can be prudent to adopt Gaussian models for such data. Hence it is a good idea to first explore whether a Poisson model is likely to be more appropriate than a standard Gaussian model. The potential for overdispersion can be explored by adding a rug to boxplot. The rug is simply tick marks on the inside of an axis at the position corresponding to an observation. As multiple identical values result in tick marks drawn over one another, it is typically a good idea to apply a slight amount of jitter (random displacement) to the values used by the rug.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; hist(dat$x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; boxplot(dat$y, horizontal=TRUE)
&amp;gt; rug(jitter(dat$y), side=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is definitely signs of non-normality that would warrant Poisson models. The rug applied to the boxplots does not indicate a series degree of clumping and there appears to be few zero. Thus overdispersion is unlikely to be an issue. Lets now explore linearity by creating a histogram of the predictor variable (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;) and a scatterplot of the relationship between the response (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) and the predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #now for the scatterplot
&amp;gt; plot(y~x, dat, log=&amp;quot;y&amp;quot;)
&amp;gt; with(dat, lines(lowess(y~x)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: the predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the scatterplot does not display major deviations from a straight line and thus linearity is satisfied. Violations of linearity could be addressed by either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;define a non-linear linear predictor (such as a polynomial, spline or other non-linear function).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;transform the scale of the predictor variables.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although we have already established that there are few zeros in the data (and thus overdispersion is unlikely to be an issue), we can also explore this by comparing the number of zeros in the data to the number of zeros that would be expected from a Poisson distribution with a mean equal to the mean count of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #proportion of 0&amp;#39;s in the data
&amp;gt; dat.tab&amp;lt;-table(dat$y==0)
&amp;gt; dat.tab/sum(dat.tab)

FALSE 
    1 
&amp;gt; 
&amp;gt; #proportion of 0&amp;#39;s expected from a Poisson distribution
&amp;gt; mu &amp;lt;- mean(dat$y)
&amp;gt; cnts &amp;lt;- rpois(1000, mu)
&amp;gt; dat.tab &amp;lt;- table(cnts == 0)
&amp;gt; dat.tab/sum(dat.tab)

FALSE  TRUE 
0.997 0.003 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above, the value under &lt;code&gt;FALSE&lt;/code&gt; is the proportion of non-zero values in the data and the value under &lt;code&gt;TRUE&lt;/code&gt; is the proportion of zeros in the data. In this example, there are no zeros in the observed data which corresponds closely to the very low proportion expected (&lt;span class=&#34;math inline&#34;&gt;\(0.003\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i \sim \text{Pois}(\lambda_i),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)=\eta_i\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\eta_i=\beta_0+\beta_1x_{i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0,\beta_1 \sim N(0, 10000)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; dat.list &amp;lt;- with(dat,list(Y=y, X=x,N=nrow(dat)))
&amp;gt; modelString=&amp;quot;
+ model {
+   for (i in 1:N) {
+      Y[i] ~ dpois(lambda[i])
+      log(lambda[i]) &amp;lt;- beta0 + beta1*X[i]
+   }
+   beta0 ~ dnorm(0,1.0E-06)
+   beta1 ~ dnorm(0,1.0E-06)
+ } 
+ &amp;quot;
&amp;gt; writeLines(modelString, con=&amp;#39;modelpois.txt&amp;#39;)
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; library(R2jags)
&amp;gt; dat.P.jags &amp;lt;- jags(data=dat.list,model.file=&amp;#39;modelpois.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 2
   Total graph size: 105

Initializing model&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-evaluation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model evaluation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(mcmcplots)
&amp;gt; denplot(dat.P.jags, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/mcmc_diag-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(dat.P.jags, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/mcmc_diag-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; raftery.diag(as.mcmc(dat.P.jags))
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta0    10       10830 3746         2.89      
 beta1    12       12612 3746         3.37      
 deviance 3        4410  3746         1.18      


[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta0    12       14878 3746         3.97      
 beta1    10       11942 3746         3.19      
 deviance 2        3995  3746         1.07      
&amp;gt; 
&amp;gt; autocorr.diag(as.mcmc(dat.P.jags))
             beta0         beta1     deviance
Lag 0   1.00000000  1.0000000000  1.000000000
Lag 1   0.83977964  0.8111423616  0.508803232
Lag 5   0.43884918  0.3859514845  0.118732714
Lag 10  0.22584100  0.1883831873  0.029775648
Lag 50 -0.01164622 -0.0003926876 -0.007507996&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One very important model validation procedure is to examine a plot of residuals against predicted or fitted values (the residual plot). Ideally, residual plots should show a random scatter of points without outliers. That is, there should be no patterns in the residuals. Patterns suggest inappropriate linear predictor (or scale) and/or inappropriate residual distribution/link function. The residuals used in such plots should be standardized (particularly if the model incorporated any variance-covariance structures - such as an autoregressive correlation structure) Pearsons’s residuals standardize residuals by division with the square-root of the variance. We can generate Pearson’s residuals within the &lt;code&gt;JAGS&lt;/code&gt; model. Alternatively, we could use the parameters to generate the residuals outside of &lt;code&gt;JAGS&lt;/code&gt;. Pearson’s residuals are calculated according to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \epsilon = \frac{y_i - \mu}{\sqrt{\text{var}(y)}},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the expected value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(=\lambda\)&lt;/span&gt; for Poisson) and var(&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) is the variance of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(=\lambda\)&lt;/span&gt; for Poisson).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #extract the samples for the two model parameters
&amp;gt; coefs &amp;lt;- dat.P.jags$BUGSoutput$sims.matrix[,1:2]
&amp;gt; Xmat &amp;lt;- model.matrix(~x, data=dat)
&amp;gt; #expected values on a log scale
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; #expected value on response scale
&amp;gt; lambda &amp;lt;- exp(eta)
&amp;gt; #Expected value and variance are both equal to lambda
&amp;gt; expY &amp;lt;- varY &amp;lt;- lambda
&amp;gt; #sweep across rows and then divide by lambda
&amp;gt; Resid &amp;lt;- -1*sweep(expY,2,dat$y,&amp;#39;-&amp;#39;)/sqrt(varY)
&amp;gt; #plot residuals vs expected values
&amp;gt; plot(apply(Resid,2,mean)~apply(eta,2,mean))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/model2_mcmc_diag-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Poisson distribution matching that estimated by the model. Essentially this is estimating how well the Poisson distribution, the log-link function and the linear model approximates the observed data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; SSres&amp;lt;-apply(Resid^2,1,sum)
&amp;gt; 
&amp;gt; #generate a matrix of draws from a poisson distribution
&amp;gt; # the matrix is the same dimensions as lambda and uses the probabilities of lambda
&amp;gt; YNew &amp;lt;- matrix(rpois(length(lambda),lambda=lambda),nrow=nrow(lambda))
&amp;gt; 
&amp;gt; Resid1&amp;lt;-(lambda - YNew)/sqrt(lambda)
&amp;gt; SSres.sim&amp;lt;-apply(Resid1^2,1,sum)
&amp;gt; mean(SSres.sim&amp;gt;SSres, na.rm = T)
[1] 0.4697&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;goodness-of-fit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goodness of fit&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; dat.list1 &amp;lt;- with(dat,list(Y=y, X=x,N=nrow(dat)))
&amp;gt; modelString=&amp;quot;
+ model {
+   for (i in 1:N) {
+     #likelihood function
+     Y[i] ~ dpois(lambda[i])
+     eta[i] &amp;lt;- beta0+beta1*X[i] #linear predictor
+     log(lambda[i]) &amp;lt;- eta[i]   #link function
+ 
+     #E(Y) and var(Y)
+     expY[i] &amp;lt;- lambda[i]
+     varY[i] &amp;lt;- lambda[i]
+ 
+     # Calculate RSS
+     Resid[i] &amp;lt;- (Y[i] - expY[i])/sqrt(varY[i])
+     RSS[i] &amp;lt;- pow(Resid[i],2)
+ 
+     #Simulate data from a Poisson distribution
+     Y1[i] ~ dpois(lambda[i])
+     #Calculate RSS for simulated data
+     Resid1[i] &amp;lt;- (Y1[i] - expY[i])/sqrt(varY[i])
+     RSS1[i] &amp;lt;-pow(Resid1[i],2) 
+   }
+   #Priors
+   beta0 ~ dnorm(0,1.0E-06)
+   beta1 ~ dnorm(0,1.0E-06)
+   #Bayesian P-value
+   Pvalue &amp;lt;- mean(sum(RSS1)&amp;gt;sum(RSS))
+ } 
+ &amp;quot;
&amp;gt; 
&amp;gt; writeLines(modelString, con=&amp;#39;modelpois_gof.txt&amp;#39;)
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;, &amp;#39;Pvalue&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.P.jags1 &amp;lt;- jags(data=dat.list,model.file=&amp;#39;modelpois_gof.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 22
   Total graph size: 272

Initializing model
&amp;gt; 
&amp;gt; print(dat.P.jags1)
Inference for Bugs model at &amp;quot;modelpois_gof.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff
Pvalue     0.478   0.500  0.000  0.000  0.000  1.000  1.000 1.001 10000
beta0      0.546   0.254  0.015  0.381  0.559  0.719  1.013 1.001 10000
beta1      0.112   0.018  0.077  0.099  0.111  0.124  0.149 1.001  3200
deviance  88.372   3.041 86.373 86.883 87.671 89.075 93.868 1.006  2000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 4.6 and DIC = 93.0
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: the Bayesian p-value is approximately &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;, suggesting that there is a good fit of the model to the data.&lt;/p&gt;
&lt;p&gt;Unfortunately, unlike with linear models (Gaussian family), the expected distribution of data (residuals) varies over the range of fitted values for numerous (often competing) ways that make diagnosing (and attributing causes thereof) miss-specified generalized linear models from standard residual plots very difficult. The use of standardized (Pearson) residuals or deviance residuals can partly address this issue, yet they still do not offer completely consistent diagnoses across all issues (miss-specified model, over-dispersion, zero-inflation). An alternative approach is to use simulated data from the model posteriors to calculate an empirical cumulative density function from which residuals are are generated as values corresponding to the observed data along the density function. Now we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Bernoulli distribution matching that estimated by the model. Essentially this is estimating how well the Bernoulli distribution and linear model approximates the observed data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #extract the samples for the two model parameters
&amp;gt; coefs &amp;lt;- dat.P.jags$BUGSoutput$sims.matrix[,1:2]
&amp;gt; Xmat &amp;lt;- model.matrix(~x, data=dat)
&amp;gt; #expected values on a log scale
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; #expected value on response scale
&amp;gt; lambda &amp;lt;- exp(eta)
&amp;gt; 
&amp;gt; simRes &amp;lt;- function(lambda, data,n=250, plot=T, family=&amp;#39;poisson&amp;#39;) {
+  require(gap)
+  N = nrow(data)
+  sim = switch(family,
+     &amp;#39;poisson&amp;#39; = matrix(rpois(n*N,apply(lambda,2,mean)),ncol=N, byrow=TRUE)
+  )
+  a = apply(sim + runif(n,-0.5,0.5),2,ecdf)
+  resid&amp;lt;-NULL
+  for (i in 1:nrow(data)) resid&amp;lt;-c(resid,a[[i]](data$y[i] + runif(1 ,-0.5,0.5)))
+  if (plot==T) {
+    par(mfrow=c(1,2))
+    gap::qqunif(resid,pch = 2, bty = &amp;quot;n&amp;quot;,
+    logscale = F, col = &amp;quot;black&amp;quot;, cex = 0.6, main = &amp;quot;QQ plot residuals&amp;quot;,
+    cex.main = 1, las=1)
+    plot(resid~apply(lambda,2,mean), xlab=&amp;#39;Predicted value&amp;#39;, ylab=&amp;#39;Standardized residual&amp;#39;, las=1)
+  }
+  resid
+ }
&amp;gt; 
&amp;gt; simRes(lambda,dat, family=&amp;#39;poisson&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/model2_mcmc_res2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; [1] 0.220 0.544 0.532 0.344 0.812 0.980 0.048 0.592 0.548 0.728 0.164 0.492
[13] 0.856 0.096 0.240 0.292 0.876 0.880 0.148 0.748&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The trend (black symbols) in the qq-plot does not appear to be overly non-linear (matching the ideal red line well), suggesting that the model is not overdispersed. The spread of standardized (simulated) residuals in the residual plot do not appear overly non-uniform. That is there is not trend in the residuals. Furthermore, there is not a concentration of points close to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; (which would imply overdispersion).&lt;/p&gt;
&lt;p&gt;Recall that the Poisson regression model assumes that variance=mean (var=μϕ where &lt;span class=&#34;math inline&#34;&gt;\(\phi=1\)&lt;/span&gt;) and thus dispersion (&lt;span class=&#34;math inline&#34;&gt;\(\phi=\frac{\text{var}}{\mu}=1)\)&lt;/span&gt;). However, we can also calculate approximately what the dispersion factor would be by using sum square of the residuals as a measure of variance and the model residual degrees of freedom as a measure of the mean (since the expected value of a Poisson distribution is the same as its degrees of freedom).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \phi = \frac{RSS}{df},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(df=n−k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the number of estimated model coefficients.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Resid &amp;lt;- -1*sweep(lambda,2,dat$y,&amp;#39;-&amp;#39;)/sqrt(lambda)
&amp;gt; RSS&amp;lt;-apply(Resid^2,1,sum)
&amp;gt; (df&amp;lt;-nrow(dat)-ncol(coefs))
[1] 18
&amp;gt; 
&amp;gt; Disp &amp;lt;- RSS/df
&amp;gt; data.frame(Median=median(Disp), Mean=mean(Disp), HPDinterval(as.mcmc(Disp)),
+            HPDinterval(as.mcmc(Disp),p=0.5))
       Median     Mean     lower    upper   lower.1  upper.1
var1 1.053527 1.110853 0.9299722 1.449502 0.9300381 1.053579&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can incorporate the dispersion statistic directly into &lt;code&gt;JAGS&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; dat.list &amp;lt;- with(dat,list(Y=y, X=x,N=nrow(dat)))
&amp;gt; modelString=&amp;quot;
+ model {
+   for (i in 1:N) {
+      Y[i] ~ dpois(lambda[i])
+      eta[i] &amp;lt;- beta0 + beta1*X[i]
+      log(lambda[i]) &amp;lt;- eta[i]
+      expY[i] &amp;lt;- lambda[i]
+      varY[i] &amp;lt;- lambda[i]
+    Resid[i] &amp;lt;- (Y[i] - expY[i])/sqrt(varY[i]) 
+   }
+   beta0 ~ dnorm(0,1.0E-06)
+   beta1 ~ dnorm(0,1.0E-06)
+   RSS &amp;lt;- sum(pow(Resid,2))
+   df &amp;lt;- N-2
+   phi &amp;lt;- RSS/df
+ } 
+ &amp;quot;
&amp;gt; 
&amp;gt; writeLines(modelString, con=&amp;#39;modelpois_disp.txt&amp;#39;)
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;,&amp;#39;phi&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.P.jags &amp;lt;- jags(data=dat.list,model.file=&amp;#39;modelpois_disp.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 2
   Total graph size: 171

Initializing model
&amp;gt; 
&amp;gt; print(dat.P.jags)
Inference for Bugs model at &amp;quot;modelpois_disp.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff
beta0      0.552   0.256  0.039  0.382  0.557  0.724  1.042 1.001 10000
beta1      0.111   0.019  0.074  0.099  0.112  0.124  0.147 1.001  2800
phi        1.105   0.246  0.934  0.977  1.048  1.169  1.581 1.001 10000
deviance  88.354   2.633 86.368 86.896 87.709 89.074 93.897 1.002  4300

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 3.5 and DIC = 91.8
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dispersion statistic is close to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and thus there is no evidence that the data were overdispersed. The Poisson distribution was therefore appropriate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-model-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring the model parameters&lt;/h2&gt;
&lt;p&gt;If there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(coda)
&amp;gt; print(dat.P.jags)
Inference for Bugs model at &amp;quot;modelpois_disp.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff
beta0      0.552   0.256  0.039  0.382  0.557  0.724  1.042 1.001 10000
beta1      0.111   0.019  0.074  0.099  0.112  0.124  0.147 1.001  2800
phi        1.105   0.246  0.934  0.977  1.048  1.169  1.581 1.001 10000
deviance  88.354   2.633 86.368 86.896 87.709 89.074 93.897 1.002  4300

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 3.5 and DIC = 91.8
DIC is an estimate of expected predictive error (lower deviance is better).
&amp;gt; 
&amp;gt; library(plyr)
&amp;gt; adply(dat.P.jags$BUGSoutput$sims.matrix[,1:2], 2, function(x) {
+   data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))
+ })
     X1    Median      Mean      lower     upper    lower.1   upper.1
1 beta0 0.5570252 0.5517525 0.03871735 1.0423628 0.39092213 0.7317579
2 beta1 0.1115363 0.1113176 0.07499903 0.1484004 0.09893134 0.1239861
&amp;gt; 
&amp;gt; #on original scale
&amp;gt; adply(exp(dat.P.jags$BUGSoutput$sims.matrix[,1:2]), 2, function(x) {
+   data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))
+ })
     X1   Median     Mean     lower    upper  lower.1  upper.1
1 beta0 1.745472 1.793464 0.9803783 2.734057 1.423789 2.013575
2 beta1 1.117994 1.117948 1.0778831 1.159977 1.101510 1.129458&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: We would reject the null hypothesis of no effect of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. An increase in x is associated with a significant linear increase (positive slope) in the abundance of y. Every &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; results in a log &lt;span class=&#34;math inline&#34;&gt;\(0.11\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. We usually express this in terms of abundance rather than log abundance, so every &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; results in a ($e^{ 0.11} = 1.12 $) &lt;span class=&#34;math inline&#34;&gt;\(1.12\)&lt;/span&gt; unit increase in the abundance of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explorations-of-the-trends&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Explorations of the trends&lt;/h2&gt;
&lt;p&gt;A measure of the strength of the relationship can be obtained according to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ R^2 = 1 - \frac{\text{RSS}_{model}}{\text{RSS}_{null}}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Xmat &amp;lt;- model.matrix(~x, data=dat)
&amp;gt; #expected values on a log scale
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; #expected value on response scale
&amp;gt; lambda &amp;lt;- exp(eta)
&amp;gt; #calculate the raw SS residuals
&amp;gt; SSres &amp;lt;- apply((-1*(sweep(lambda,2,dat$y,&amp;#39;-&amp;#39;)))^2,1,sum)
&amp;gt; SSres.null &amp;lt;- sum((dat$y - mean(dat$y))^2)
&amp;gt; #OR 
&amp;gt; SSres.null &amp;lt;- crossprod(dat$y - mean(dat$y))
&amp;gt; #calculate the model r2
&amp;gt; 1-mean(SSres)/SSres.null
          [,1]
[1,] 0.6569594&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(65\)&lt;/span&gt;% of the variation in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; abundance can be explained by its relationship with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We can also do it directly into &lt;code&gt;JAGS&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; dat.list &amp;lt;- with(dat,list(Y=y, X=x,N=nrow(dat)))
&amp;gt; modelString=&amp;quot;
+ model {
+   for (i in 1:N) {
+      Y[i] ~ dpois(lambda[i])
+      eta[i] &amp;lt;- beta0 + beta1*X[i]
+      log(lambda[i]) &amp;lt;- eta[i]
+      res[i] &amp;lt;- Y[i] - lambda[i]
+      resnull[i] &amp;lt;- Y[i] - meanY
+   }
+   meanY &amp;lt;- mean(Y)
+   beta0 ~ dnorm(0,1.0E-06)
+   beta1 ~ dnorm(0,1.0E-06)
+   RSS &amp;lt;- sum(res^2)
+   RSSnull &amp;lt;- sum(resnull^2)
+   r2 &amp;lt;- 1-RSS/RSSnull
+ } 
+ &amp;quot;
&amp;gt; 
&amp;gt; writeLines(modelString, con=&amp;#39;modelpois_disp_r2.txt&amp;#39;)
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;,&amp;#39;r2&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.P.jags &amp;lt;- jags(data=dat.list,model.file=&amp;#39;modelpois_disp_r2.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 2
   Total graph size: 150

Initializing model
&amp;gt; 
&amp;gt; print(dat.P.jags)
Inference for Bugs model at &amp;quot;modelpois_disp_r2.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff
beta0      0.547   0.257  0.024  0.379  0.556  0.721  1.032 1.001  3900
beta1      0.112   0.019  0.077  0.100  0.112  0.125  0.150 1.001  7000
r2         0.655   0.057  0.510  0.640  0.672  0.690  0.701 1.001 10000
deviance  88.383   2.776 86.372 86.904 87.733 89.122 93.692 1.003  6200

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 3.9 and DIC = 92.2
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we will create a summary plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; par(mar = c(4, 5, 0, 0))
&amp;gt; plot(y ~ x, data = dat, type = &amp;quot;n&amp;quot;, ann = F, axes = F)
&amp;gt; points(y ~ x, data = dat, pch = 16)
&amp;gt; xs &amp;lt;- seq(min(dat$x,na.rm=TRUE),max(dat$x,na.rm=TRUE), l = 1000)
&amp;gt; Xmat &amp;lt;- model.matrix(~xs)
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; ys &amp;lt;- exp(eta)
&amp;gt; library(plyr)
&amp;gt; library(coda)
&amp;gt; data.tab &amp;lt;- adply(ys,2,function(x) {
+   data.frame(Median=median(x), HPDinterval(as.mcmc(x)))
+ })
&amp;gt; data.tab &amp;lt;- cbind(x=xs,data.tab)
&amp;gt; points(Median ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;)
&amp;gt; lines(lower ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;, lty = 2)
&amp;gt; lines(upper ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;, lty = 2)
&amp;gt; 
&amp;gt; axis(1)
&amp;gt; mtext(&amp;quot;X&amp;quot;, 1, cex = 1.5, line = 3)
&amp;gt; axis(2, las = 2)
&amp;gt; mtext(&amp;quot;Abundance of Y&amp;quot;, 2, cex = 1.5, line = 3)
&amp;gt; box(bty = &amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/model_code_v2_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-log-likelihood-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Full log-likelihood function&lt;/h2&gt;
&lt;p&gt;Now lets try it by specifying log-likelihood and the zero trick. When applying this trick, we need to manually calculate the deviance as the inbuilt deviance will be based on the log-likelihood of estimating the zeros (as part of the zero trick) rather than the deviance of the intended model. The one advantage of the zero trick is that the Deviance and thus DIC, AIC provided by &lt;code&gt;R2jags&lt;/code&gt; will be incorrect. Hence, they too need to be manually defined within &lt;code&gt;JAGS&lt;/code&gt; I suspect that the AIC calculation I have used is incorrect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Xmat &amp;lt;- model.matrix(~x, dat)
&amp;gt; nX &amp;lt;- ncol(Xmat)
&amp;gt; dat.list2 &amp;lt;- with(dat,list(Y=y, X=Xmat,N=nrow(dat), mu=rep(0,nX),
+                   Sigma=diag(1.0E-06,nX), zeros=rep(0,nrow(dat)), C=10000))
&amp;gt; modelString=&amp;quot;
+ model {
+   for (i in 1:N) {
+      zeros[i] ~ dpois(zeros.lambda[i])
+      zeros.lambda[i] &amp;lt;- -ll[i] + C     
+      ll[i] &amp;lt;- Y[i]*log(lambda[i]) - lambda[i] - loggam(Y[i]+1)
+      eta[i] &amp;lt;- inprod(beta[], X[i,])
+      log(lambda[i]) &amp;lt;- eta[i]
+     llm[i] &amp;lt;- Y[i]*log(meanlambda) - meanlambda - loggam(Y[i]+1)
+   }
+   meanlambda &amp;lt;- mean(lambda)
+   beta ~ dmnorm(mu[],Sigma[,])
+   dev &amp;lt;- sum(-2*ll)
+   pD &amp;lt;- mean(dev)-sum(-2*llm)
+   AIC &amp;lt;- min(dev+(2*pD))
+ } 
+ &amp;quot;
&amp;gt; 
&amp;gt; writeLines(modelString, con=&amp;#39;modelpois_ll.txt&amp;#39;)
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta&amp;#39;,&amp;#39;dev&amp;#39;,&amp;#39;AIC&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.P.jags3 &amp;lt;- jags(data=dat.list2,model.file=&amp;#39;modelpois_ll.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 1
   Total graph size: 353

Initializing model
&amp;gt; 
&amp;gt; print(dat.P.jags3)
Inference for Bugs model at &amp;quot;modelpois_ll.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
            mu.vect sd.vect       2.5%        25%        50%        75%
AIC          13.728   4.725      9.624     10.548     11.952     15.158
beta[1]       0.481   0.259     -0.079      0.319      0.506      0.669
beta[2]       0.116   0.019      0.084      0.103      0.114      0.128
dev          88.382   2.009     86.361     86.883     87.731     89.265
deviance 400088.382   2.009 400086.361 400086.883 400087.731 400089.265
              97.5%  Rhat n.eff
AIC          26.878 1.016   180
beta[1]       0.922 1.037    49
beta[2]       0.155 1.029    60
dev          94.071 1.009   300
deviance 400094.071 1.000     1

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 2.0 and DIC = 400090.4
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;negative-binomial&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Negative binomial&lt;/h1&gt;
&lt;p&gt;The following equations are provided since in Bayesian modelling, it is occasionally necessary to directly define the log-likelihood calculations (particularly for zero-inflated models and other mixture models).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(y \mid r, p) = \frac{\Gamma(y+r)}{\Gamma(r)\Gamma(y+1)}p^r(1-p)^y,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the probability of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; successes until &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; failures. If, we make &lt;span class=&#34;math inline&#34;&gt;\(p=\frac{\text{size}}{\text{size}+\mu}\)&lt;/span&gt;, then we can define the function in terms of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \mu = \frac{r(1-p)}{p},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(E[Y]=\mu\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Var(Y)=\mu + \frac{\mu^2}{r}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;data-generation-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data generation&lt;/h2&gt;
&lt;p&gt;Lets say we wanted to model the abundance of an item (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) against a continuous predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; set.seed(37) #16 #35
&amp;gt; #The number of samples
&amp;gt; n.x &amp;lt;- 20
&amp;gt; #Create x values that at uniformly distributed throughout the rate of 1 to 20
&amp;gt; x &amp;lt;- sort(runif(n = n.x, min = 1, max =20))
&amp;gt; mm &amp;lt;- model.matrix(~x)
&amp;gt; intercept &amp;lt;- 0.6
&amp;gt; slope=0.1
&amp;gt; #The linear predictor
&amp;gt; linpred &amp;lt;- mm %*% c(intercept,slope)
&amp;gt; #Predicted y values
&amp;gt; lambda &amp;lt;- exp(linpred)
&amp;gt; #Add some noise and make binomial
&amp;gt; y &amp;lt;- rnbinom(n=n.x, mu=lambda, size=1)
&amp;gt; dat.nb &amp;lt;- data.frame(y,x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When counts are all very large (not close to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;) and their ranges do not span orders of magnitude, they take on very Gaussian properties (symmetrical distribution and variance independent of the mean). Given that models based on the Gaussian distribution are more optimized and recognized than Generalized Linear Models, it can be prudent to adopt Gaussian models for such data. Hence it is a good idea to first explore whether a Poisson or Negative Binomial model is likely to be more appropriate than a standard Gaussian model. Recall from Poisson regression, there are five main potential models that we could consider fitting to these data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; hist(dat$x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data1_negbin-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; #now for the scatterplot
&amp;gt; plot(y~x, dat.nb, log=&amp;quot;y&amp;quot;)
&amp;gt; with(dat.nb, lines(lowess(y~x)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data1_negbin-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: the predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the scatterplot does not display major deviations from a straight line and thus linearity is satisfied. Violations of linearity could be addressed by either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;define a non-linear linear predictor (such as a polynomial, spline or other non-linear function).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;transform the scale of the predictor variables.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although we have already established that there are few zeros in the data (and thus overdispersion is unlikely to be an issue), we can also explore this by comparing the number of zeros in the data to the number of zeros that would be expected from a Poisson distribution with a mean equal to the mean count of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #proportion of 0&amp;#39;s in the data
&amp;gt; dat.nb.tab&amp;lt;-table(dat.nb$y==0)
&amp;gt; dat.nb.tab/sum(dat.nb.tab)

FALSE  TRUE 
 0.95  0.05 
&amp;gt; 
&amp;gt; #proportion of 0&amp;#39;s expected from a Poisson distribution
&amp;gt; mu &amp;lt;- mean(dat.nb$y)
&amp;gt; cnts &amp;lt;- rpois(1000, mu)
&amp;gt; dat.nb.tabE &amp;lt;- table(cnts == 0)
&amp;gt; dat.nb.tabE/sum(dat.nb.tabE)

FALSE 
    1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above, the value under &lt;code&gt;FALSE&lt;/code&gt; is the proportion of non-zero values in the data and the value under &lt;code&gt;TRUE&lt;/code&gt; is the proportion of zeros in the data. In this example, the proportion of zeros observed is similar to the proportion expected. Indeed, there was only a single zero observed. Hence it is likely that if there is overdispersion it is unlikely to be due to excessive zeros.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i \sim \text{NegBin}(p_i,r),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(p_i=\frac{r}{r+\lambda_i}\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)=\beta_0+\beta_1x_{i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0,\beta_1 \sim N(0, 10000)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(r \sim \text{Unif}(0.001,1000)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; dat.nb.list &amp;lt;- with(dat.nb,list(Y=y, X=x,N=nrow(dat.nb)))
&amp;gt; modelString=&amp;quot;
+ model {
+   for (i in 1:N) {
+      Y[i] ~ dnegbin(p[i],size)
+      p[i] &amp;lt;- size/(size+lambda[i])
+      log(lambda[i]) &amp;lt;- beta0 + beta1*X[i]
+   }
+   beta0 ~ dnorm(0,1.0E-06)
+   beta1 ~ dnorm(0,1.0E-06)
+   size ~ dunif(0.001,1000)
+   theta &amp;lt;- pow(1/mean(p),2)
+   scaleparam &amp;lt;- mean((1-p)/p) 
+ } 
+ &amp;quot;
&amp;gt; writeLines(modelString, con=&amp;#39;modelnbin.txt&amp;#39;)
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;, &amp;#39;size&amp;#39;,&amp;#39;theta&amp;#39;,&amp;#39;scaleparam&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.NB.jags &amp;lt;- jags(data=dat.nb.list,model.file=&amp;#39;modelnbin.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 3
   Total graph size: 157

Initializing model&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-evaluation-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model evaluation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; denplot(dat.NB.jags, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;,&amp;quot;size&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/mcmc_diag_nbin-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(dat.NB.jags, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;,&amp;quot;size&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/mcmc_diag_nbin-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; raftery.diag(as.mcmc(dat.NB.jags))
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                  
            Burn-in  Total Lower bound  Dependence
            (M)      (N)   (Nmin)       factor (I)
 beta0      16       17518 3746         4.68      
 beta1      24       28713 3746         7.66      
 deviance   3        4198  3746         1.12      
 scaleparam 16       16290 3746         4.35      
 size       4        5038  3746         1.34      
 theta      16       16244 3746         4.34      


[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                  
            Burn-in  Total Lower bound  Dependence
            (M)      (N)   (Nmin)       factor (I)
 beta0      18       20025 3746         5.35      
 beta1      24       21072 3746         5.63      
 deviance   3        4267  3746         1.14      
 scaleparam 18       19920 3746         5.32      
 size       3        4375  3746         1.17      
 theta      20       20682 3746         5.52      
&amp;gt; 
&amp;gt; autocorr.diag(as.mcmc(dat.NB.jags))
              beta0        beta1     deviance  scaleparam        size
Lag 0   1.000000000  1.000000000  1.000000000  1.00000000 1.000000000
Lag 1   0.855250119  0.856542892  0.566377262  0.33360033 0.684520361
Lag 5   0.519024321  0.521535488  0.163024546  0.07618281 0.220180993
Lag 10  0.276801196  0.280283232  0.025179110  0.02814049 0.039259726
Lag 50 -0.008060569 -0.004454124 -0.003876422 -0.01103395 0.006904325
             theta
Lag 0   1.00000000
Lag 1   0.26024619
Lag 5   0.05872969
Lag 10  0.02940084
Lag 50 -0.01349378&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now explore the goodness of fit of the models via the residuals and deviance. We could calculate the Pearsons’s residuals within the &lt;code&gt;JAGS&lt;/code&gt; model. Alternatively, we could use the parameters to generate the residuals outside of &lt;code&gt;JAGS&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #extract the samples for the two model parameters
&amp;gt; coefs &amp;lt;- dat.NB.jags$BUGSoutput$sims.matrix[,1:2]
&amp;gt; size &amp;lt;- dat.NB.jags$BUGSoutput$sims.matrix[,&amp;#39;size&amp;#39;]
&amp;gt; Xmat &amp;lt;- model.matrix(~x, data=dat.nb)
&amp;gt; #expected values on a log scale
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; #expected value on response scale
&amp;gt; lambda &amp;lt;- exp(eta)
&amp;gt; varY &amp;lt;- lambda + (lambda^2)/size
&amp;gt; #sweep across rows and then divide by lambda
&amp;gt; Resid &amp;lt;- -1*sweep(lambda,2,dat.nb$y,&amp;#39;-&amp;#39;)/sqrt(varY)
&amp;gt; #plot residuals vs expected values
&amp;gt; plot(apply(Resid,2,mean)~apply(eta,2,mean))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/model2_mcmc_diag_nbin-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Negative binomial distribution matching that estimated by the model. Essentially this is estimating how well the Negative binomial distribution, the log-link function and the linear model approximates the observed data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; SSres&amp;lt;-apply(Resid^2,1,sum)
&amp;gt; 
&amp;gt; #generate a matrix of draws from a negative binomial distribution
&amp;gt; # the matrix is the same dimensions as pi and uses the probabilities of pi
&amp;gt; YNew &amp;lt;- matrix(rnbinom(length(lambda),mu=lambda, size=size),nrow=nrow(lambda))
&amp;gt; Resid1&amp;lt;-(lambda - YNew)/sqrt(varY)
&amp;gt; SSres.sim&amp;lt;-apply(Resid1^2,1,sum)
&amp;gt; mean(SSres.sim&amp;gt;SSres, na.rm = T)
[1] 0.4163&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: the Bayesian p-value is approximately &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;, suggesting that there is a good fit of the model to the data.&lt;/p&gt;
&lt;p&gt;Unfortunately, unlike with linear models (Gaussian family), the expected distribution of data (residuals) varies over the range of fitted values for numerous (often competing) ways that make diagnosing (and attributing causes thereof) miss-specified generalized linear models from standard residual plots very difficult. The use of standardized (Pearson) residuals or deviance residuals can partly address this issue, yet they still do not offer completely consistent diagnoses across all issues (miss-specified model, over-dispersion, zero-inflation). An alternative approach is to use simulated data from the model posteriors to calculate an empirical cumulative density function from which residuals are are generated as values corresponding to the observed data along the density function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #extract the samples for the two model parameters
&amp;gt; coefs &amp;lt;- dat.NB.jags$BUGSoutput$sims.matrix[,1:2]
&amp;gt; size &amp;lt;- dat.NB.jags$BUGSoutput$sims.matrix[,&amp;#39;size&amp;#39;]
&amp;gt; Xmat &amp;lt;- model.matrix(~x, data=dat.nb)
&amp;gt; #expected values on a log scale
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; #expected value on response scale
&amp;gt; lambda &amp;lt;- exp(eta)
&amp;gt; 
&amp;gt; simRes &amp;lt;- function(lambda, data,n=250, plot=T, family=&amp;#39;negbin&amp;#39;, size=NULL) {
+  require(gap)
+  N = nrow(data)
+  sim = switch(family,
+     &amp;#39;poisson&amp;#39; = matrix(rpois(n*N,apply(lambda,2,mean)),ncol=N, byrow=TRUE),
+     &amp;#39;negbin&amp;#39; = matrix(MASS:::rnegbin(n*N,apply(lambda,2,mean),size),ncol=N, byrow=TRUE)
+  )
+  a = apply(sim + runif(n,-0.5,0.5),2,ecdf)
+  resid&amp;lt;-NULL
+  for (i in 1:nrow(data)) resid&amp;lt;-c(resid,a[[i]](data$y[i] + runif(1 ,-0.5,0.5)))
+  if (plot==T) {
+    par(mfrow=c(1,2))
+    gap::qqunif(resid,pch = 2, bty = &amp;quot;n&amp;quot;,
+    logscale = F, col = &amp;quot;black&amp;quot;, cex = 0.6, main = &amp;quot;QQ plot residuals&amp;quot;,
+    cex.main = 1, las=1)
+    plot(resid~apply(lambda,2,mean), xlab=&amp;#39;Predicted value&amp;#39;, ylab=&amp;#39;Standardized residual&amp;#39;, las=1)
+  }
+  resid
+ }
&amp;gt; 
&amp;gt; simRes(lambda,dat.nb, family=&amp;#39;negbin&amp;#39;, size=mean(size))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/model2_mcmc_res2_nbin-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; [1] 0.368 0.944 0.456 0.788 0.148 0.928 0.136 0.704 0.164 0.800 0.500 0.464
[13] 0.100 0.216 0.680 0.212 0.000 0.676 0.924 0.852&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The trend (black symbols) in the qq-plot does not appear to be overly non-linear (matching the ideal red line well), suggesting that the model is not overdispersed. The spread of standardized (simulated) residuals in the residual plot do not appear overly non-uniform. That is there is not trend in the residuals. Furthermore, there is not a concentration of points close to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; (which would imply overdispersion).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-model-parameters-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring the model parameters&lt;/h2&gt;
&lt;p&gt;If there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics. As with most Bayesian models, it is best to base conclusions on medians rather than means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; print(dat.NB.jags)
Inference for Bugs model at &amp;quot;modelnbin.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
           mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
beta0        0.731   0.395  -0.023   0.470   0.717   0.984   1.534 1.001 10000
beta1        0.097   0.032   0.034   0.077   0.098   0.118   0.158 1.001 10000
scaleparam   2.787   1.756   0.704   1.670   2.412   3.444   7.089 1.001 10000
size         3.255   2.190   1.055   1.941   2.697   3.853   9.050 1.001 10000
theta       12.548  12.474   2.669   5.892   9.157  14.790  43.249 1.001 10000
deviance   113.053   2.691 110.093 111.115 112.352 114.190 120.305 1.002  2000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 3.6 and DIC = 116.7
DIC is an estimate of expected predictive error (lower deviance is better).
&amp;gt; 
&amp;gt; adply(dat.NB.jags$BUGSoutput$sims.matrix, 2, function(x) {
+   data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))
+ })
          X1       Median         Mean        lower       upper     lower.1
1      beta0   0.71693048   0.73121205  -0.05129743   1.5032427   0.4523583
2      beta1   0.09800852   0.09730699   0.03509028   0.1591976   0.0789372
3   deviance 112.35178835 113.05255254 109.86520971 118.4814291 110.0898498
4 scaleparam   2.41198253   2.78665197   0.33094006   5.9583607   1.2865037
5       size   2.69653197   3.25545915   0.68960555   7.2146030   1.4202953
6      theta   9.15704708  12.54776430   1.61632232  32.9116959   3.6489231
      upper.1
1   0.9610028
2   0.1201659
3 112.4668566
4   2.8677393
5   2.9988148
6  10.3646959
&amp;gt; 
&amp;gt; #on original scale
&amp;gt; adply(exp(dat.NB.jags$BUGSoutput$sims.matrix[,1:2]), 2, function(x) {
+   data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))
+ })
     X1   Median     Mean     lower    upper  lower.1  upper.1
1 beta0 2.048137 2.249960 0.8335273 4.249614 1.340384 2.309801
2 beta1 1.102972 1.102753 1.0357132 1.172570 1.080463 1.125944&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: We would reject the null hypothesis of no effect of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. An increase in x is associated with a significant linear increase (positive slope) in the abundance of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Every &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; results in a log &lt;span class=&#34;math inline&#34;&gt;\(0.09\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. We usually express this in terms of abundance rather than log abundance, so every &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; results in a ($e^{ 0.09} = 1.02 $) &lt;span class=&#34;math inline&#34;&gt;\(1.02\)&lt;/span&gt; unit increase in the abundance of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explorations-of-the-trends-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Explorations of the trends&lt;/h2&gt;
&lt;p&gt;A measure of the strength of the relationship can be obtained according to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ R^2 = 1 - \frac{\text{RSS}_{model}}{\text{RSS}_{null}}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Xmat &amp;lt;- model.matrix(~x, data=dat.nb)
&amp;gt; #expected values on a log scale
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; #expected value on response scale
&amp;gt; lambda &amp;lt;- exp(eta)
&amp;gt; #calculate the raw SS residuals
&amp;gt; SSres &amp;lt;- apply((-1*(sweep(lambda,2,dat.nb$y,&amp;#39;-&amp;#39;)))^2,1,sum)
&amp;gt; SSres.null &amp;lt;- sum((dat.nb$y - mean(dat.nb$y))^2)
&amp;gt; #OR 
&amp;gt; SSres.null &amp;lt;- crossprod(dat.nb$y - mean(dat.nb$y))
&amp;gt; #calculate the model r2
&amp;gt; 1-mean(SSres)/SSres.null
         [,1]
[1,] 0.270553&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(27\)&lt;/span&gt;% of the variation in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; abundance can be explained by its relationship with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We can also do it directly into &lt;code&gt;JAGS&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we will create a summary plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; par(mar = c(4, 5, 0, 0))
&amp;gt; plot(y ~ x, data = dat.nb, type = &amp;quot;n&amp;quot;, ann = F, axes = F)
&amp;gt; points(y ~ x, data = dat.nb, pch = 16)
&amp;gt; xs &amp;lt;- seq(min(dat.nb$x,na.rm=TRUE),max(dat.nb$x,na.rm=TRUE), l = 1000)
&amp;gt; Xmat &amp;lt;- model.matrix(~xs)
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; ys &amp;lt;- exp(eta)
&amp;gt; library(plyr)
&amp;gt; library(coda)
&amp;gt; data.tab &amp;lt;- adply(ys,2,function(x) {
+   data.frame(Median=median(x), HPDinterval(as.mcmc(x)))
+ })
&amp;gt; data.tab &amp;lt;- cbind(x=xs,data.tab)
&amp;gt; points(Median ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;)
&amp;gt; lines(lower ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;, lty = 2)
&amp;gt; lines(upper ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;, lty = 2)
&amp;gt; 
&amp;gt; axis(1)
&amp;gt; mtext(&amp;quot;X&amp;quot;, 1, cex = 1.5, line = 3)
&amp;gt; axis(2, las = 2)
&amp;gt; mtext(&amp;quot;Abundance of Y&amp;quot;, 2, cex = 1.5, line = 3)
&amp;gt; box(bty = &amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/model_code_v2_plot_nbin-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-log-likelihood-function-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Full log-likelihood function&lt;/h2&gt;
&lt;p&gt;Now lets try it by specifying log-likelihood and the zero trick. When applying this trick, we need to manually calculate the deviance as the inbuilt deviance will be based on the log-likelihood of estimating the zeros (as part of the zero trick) rather than the deviance of the intended model. The one advantage of the zero trick is that the Deviance and thus DIC, AIC provided by &lt;code&gt;R2jags&lt;/code&gt; will be incorrect. Hence, they too need to be manually defined within &lt;code&gt;JAGS&lt;/code&gt; I suspect that the AIC calculation I have used is incorrect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Xmat &amp;lt;- model.matrix(~x, dat.nb)
&amp;gt; nX &amp;lt;- ncol(Xmat)
&amp;gt; dat.nb.list2 &amp;lt;- with(dat.nb,list(Y=y, X=Xmat,N=nrow(dat.nb), mu=rep(0,nX),
+                   Sigma=diag(1.0E-06,nX), zeros=rep(0,nrow(dat.nb)), C=10000))
&amp;gt; modelString=&amp;quot;
+ model {
+   for (i in 1:N) {
+      zeros[i] ~ dpois(zeros.lambda[i])
+      zeros.lambda[i] &amp;lt;- -ll[i] + C     
+      ll[i] &amp;lt;- loggam(Y[i]+size) - loggam(Y[i]+1) - loggam(size) + size*(log(p[i]) - log(p[i]+1)) - 
+               Y[i]*log(p[i]+1)
+      p[i] &amp;lt;- size/lambda[i]
+      eta[i] &amp;lt;- inprod(beta[], X[i,])
+      log(lambda[i]) &amp;lt;- eta[i]
+   }
+   beta ~ dmnorm(mu[],Sigma[,])
+   size ~ dunif(0.001,1000)
+   dev &amp;lt;- sum(-2*ll)
+ } 
+ &amp;quot;
&amp;gt; 
&amp;gt; writeLines(modelString, con=&amp;#39;modelnbin_ll.txt&amp;#39;)
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta&amp;#39;,&amp;#39;dev&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.NB.jags3 &amp;lt;- jags(data=dat.nb.list2,model.file=&amp;#39;modelnbin_ll.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 2
   Total graph size: 453

Initializing model
&amp;gt; 
&amp;gt; print(dat.NB.jags3)
Inference for Bugs model at &amp;quot;modelnbin_ll.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
            mu.vect sd.vect       2.5%        25%        50%        75%
beta[1]       0.739   0.386      0.039      0.484      0.726      0.968
beta[2]       0.096   0.031      0.034      0.077      0.096      0.116
dev         112.830   2.548    110.074    111.037    112.105    113.842
deviance 400112.830   2.548 400110.074 400111.037 400112.105 400113.842
              97.5%  Rhat n.eff
beta[1]       1.536 1.015   160
beta[2]       0.153 1.010   230
dev         119.701 1.002  1200
deviance 400119.701 1.000     1

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 3.2 and DIC = 400116.1
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;zero-inflated-poisson&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Zero inflated Poisson&lt;/h1&gt;
&lt;p&gt;Zero-Inflation Poisson (ZIP) mixture model is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p(y_i \mid \theta, \lambda) = \begin{cases}
  \theta + (1-\theta) \times \text{Pois}(0 \mid \lambda) &amp;amp; \text{if } y_i = 0\\    
  (1-\theta) \times \text{Pois}(y_i \mid \lambda) &amp;amp; \text{if } y_i &amp;gt; 0,    
\end{cases}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the probability of false values (zeros). Hence there is essentially two models coupled together (a mixture model) to yield an overall probability:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;when an observed response is zero (&lt;span class=&#34;math inline&#34;&gt;\(y_i=0\)&lt;/span&gt;), it is the probability of getting a false value (zero) plus the probability of a true value multiplied probability of drawing a value of zero from a Poisson distribution of lambda.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;when an observed response is greater than &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, it is the probability of a true value multiplied probability of drawing that value from a Poisson distribution of lambda&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The above formulation indicates the same &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; for both the zeros and non-zeros components. In the model of zero values, we are essentially investigating whether the likelihood of false zeros is related to the linear predictor and then the greater than zero model investigates whether the counts are related to the linear predictor. However, we are typically less interested in modelling determinants of false zeros. Indeed, it is better that the likelihood of false zeros be unrelated to the linear predictor. For example, if excess (false zeros) are due to issues of detectability (individuals are present, just not detected), it is better that the detectability is not related to experimental treatments. Ideally, any detectability issues should be equal across all treatment levels. The expected value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and the variance in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for a ZIP model are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ E[y_i] = \lambda \times (1-\theta),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \text{Var}(y_i) = \lambda \times (1-\theta) \times (1+\theta \times \lambda^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;data-generation-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data generation&lt;/h2&gt;
&lt;p&gt;Lets say we wanted to model the abundance of an item (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) against a continuous predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; set.seed(9) #34.5  #4 #10 #16 #17 #26
&amp;gt; #The number of samples
&amp;gt; n.x &amp;lt;- 20
&amp;gt; #Create x values that at uniformly distributed throughout the rate of 1 to 20
&amp;gt; x &amp;lt;- sort(runif(n = n.x, min = 1, max =20))
&amp;gt; mm &amp;lt;- model.matrix(~x)
&amp;gt; intercept &amp;lt;- 0.6
&amp;gt; slope=0.1
&amp;gt; #The linear predictor
&amp;gt; linpred &amp;lt;- mm %*% c(intercept,slope)
&amp;gt; #Predicted y values
&amp;gt; lambda &amp;lt;- exp(linpred)
&amp;gt; #Add some noise and make binomial
&amp;gt; library(gamlss.dist)
&amp;gt; #fixed latent binomial
&amp;gt; y&amp;lt;- rZIP(n.x,lambda, 0.4)
&amp;gt; #latent binomial influenced by the linear predictor 
&amp;gt; #y&amp;lt;- rZIP(n.x,lambda, 1-exp(linpred)/(1+exp(linpred)))
&amp;gt; dat.zip &amp;lt;- data.frame(y,x)
&amp;gt; 
&amp;gt; summary(glm(y~x, dat.zip, family=&amp;quot;poisson&amp;quot;))

Call:
glm(formula = y ~ x, family = &amp;quot;poisson&amp;quot;, data = dat.zip)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-4.6803  -2.0343   0.2895   1.2767   2.1153  

Coefficients:
            Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)  0.30200    0.25247   1.196    0.232    
x            0.10691    0.01847   5.789 7.09e-09 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 111.495  on 19  degrees of freedom
Residual deviance:  79.118  on 18  degrees of freedom
AIC: 126.64

Number of Fisher Scoring iterations: 5
&amp;gt; 
&amp;gt; plot(glm(y~x, dat.zip, family=&amp;quot;poisson&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/generate_data_zip-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/generate_data_zip-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/generate_data_zip-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/generate_data_zip-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; library(pscl)
&amp;gt; summary(zeroinfl(y ~ x | 1, dist = &amp;quot;poisson&amp;quot;, data = dat.zip))

Call:
zeroinfl(formula = y ~ x | 1, data = dat.zip, dist = &amp;quot;poisson&amp;quot;)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-1.1625 -0.9549  0.1955  0.8125  1.4438 

Count model coefficients (poisson with log link):
            Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)  0.88696    0.28825   3.077  0.00209 ** 
x            0.09374    0.02106   4.450 8.58e-06 ***

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&amp;gt;|z|)
(Intercept)  -0.4581     0.4725   -0.97    0.332
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1 

Number of iterations in BFGS optimization: 8 
Log-likelihood: -38.58 on 3 Df
&amp;gt; 
&amp;gt; plot(resid(zeroinfl(y ~ x | 1, dist = &amp;quot;poisson&amp;quot;, data = dat.zip))~fitted(zeroinfl(y ~ x | 1, dist = &amp;quot;poisson&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/generate_data_zip-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; library(gamlss)
&amp;gt; summary(gamlss(y~x,data=dat.zip, family=ZIP))
GAMLSS-RS iteration 1: Global Deviance = 77.8434 
GAMLSS-RS iteration 2: Global Deviance = 77.1603 
GAMLSS-RS iteration 3: Global Deviance = 77.1598 
******************************************************************
Family:  c(&amp;quot;ZIP&amp;quot;, &amp;quot;Poisson Zero Inflated&amp;quot;) 

Call:  gamlss(formula = y ~ x, family = ZIP, data = dat.zip) 

Fitting method: RS() 

------------------------------------------------------------------
Mu link function:  log
Mu Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)  0.88620    0.28819   3.075 0.006862 ** 
x            0.09387    0.02105   4.458 0.000345 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

------------------------------------------------------------------
Sigma link function:  logit
Sigma Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)
(Intercept)  -0.4582     0.4725   -0.97    0.346

------------------------------------------------------------------
No. of observations in the fit:  20 
Degrees of Freedom for the fit:  3
      Residual Deg. of Freedom:  17 
                      at cycle:  3 
 
Global Deviance:     77.15981 
            AIC:     83.15981 
            SBC:     86.14701 
******************************************************************
&amp;gt; 
&amp;gt; predict(gamlss(y~x,data=dat.zip, family=ZIP), se.fit=TRUE, what=&amp;quot;mu&amp;quot;)
GAMLSS-RS iteration 1: Global Deviance = 77.8434 
GAMLSS-RS iteration 2: Global Deviance = 77.1603 
GAMLSS-RS iteration 3: Global Deviance = 77.1598 
$fit
        1         2         3         4         5         6         7         8 
0.9952647 1.0233409 1.1897115 1.2189891 1.3490911 1.3644351 1.3748867 1.5164069 
        9        10        11        12        13        14        15        16 
1.6184170 1.6379917 1.6760055 1.6962694 1.7705249 1.8559090 1.8578379 1.8718850 
       17        18        19        20 
2.1712345 2.5536059 2.7205304 2.7472964 

$se.fit
        1         2         3         4         5         6         7         8 
0.3826655 0.3724115 0.3131865 0.3031078 0.2601025 0.2552658 0.2520053 0.2112310 
        9        10        11        12        13        14        15        16 
0.1872286 0.1833232 0.1765049 0.1733169 0.1646100 0.1610460 0.1610499 0.1611915 
       17        18        19        20 
0.2055555 0.3248709 0.3848647 0.3947072 &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;Check the distribution of the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; abundances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; hist(dat.zip$y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data1_zip-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; boxplot(dat.zip$y, horizontal=TRUE)
&amp;gt; rug(jitter(dat.zip$y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data1_zip-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is definitely signs of non-normality that would warrant Poisson models. Further to that, there appears to be a large number of zeros that are likely to be the cause of overdispersion A zero-inflated Poisson model is likely to be one of the most effective for modeling these data. Lets now explore linearity by creating a histogram of the predictor variable (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;). Note, it is difficult to directly assess issues of linearity. Indeed, a scatterplot with lowess smoother will be largely influenced by the presence of zeros. One possible way of doing so is to explore the trend in the non-zero data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; hist(dat.zip$x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data2_zip-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; #now for the scatterplot
&amp;gt; plot(y~x, dat.zip)
&amp;gt; with(subset(dat.zip,y&amp;gt;0), lines(lowess(y~x)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data2_zip-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: the predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the non-zero data cloud does not display major deviations from a straight line and thus linearity is likely to be satisfied. Violations of linearity (whilst difficult to be certain about due to the unknown influence of the zeros) could be addressed by either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;define a non-linear linear predictor (such as a polynomial, spline or other non-linear function).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;transform the scale of the predictor variables.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although we have already established that there are few zeros in the data (and thus overdispersion is unlikely to be an issue), we can also explore this by comparing the number of zeros in the data to the number of zeros that would be expected from a Poisson distribution with a mean equal to the mean count of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #proportion of 0&amp;#39;s in the data
&amp;gt; dat.zip.tab&amp;lt;-table(dat.zip$y==0)
&amp;gt; dat.zip.tab/sum(dat.zip.tab)

FALSE  TRUE 
  0.6   0.4 
&amp;gt; 
&amp;gt; #proportion of 0&amp;#39;s expected from a Poisson distribution
&amp;gt; mu &amp;lt;- mean(dat.zip$y)
&amp;gt; cnts &amp;lt;- rpois(1000, mu)
&amp;gt; dat.zip.tabE &amp;lt;- table(cnts == 0)
&amp;gt; dat.zip.tabE/sum(dat.zip.tabE)

FALSE  TRUE 
0.982 0.018 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above, the value under &lt;code&gt;FALSE&lt;/code&gt; is the proportion of non-zero values in the data and the value under TRUE is the proportion of zeros in the data. In this example, the proportion of zeros observed (&lt;span class=&#34;math inline&#34;&gt;\(45\)&lt;/span&gt;%) far exceeds that that would have been expected (&lt;span class=&#34;math inline&#34;&gt;\(7.9\)&lt;/span&gt;%). Hence it is highly likely that any models will be zero-inflated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i \sim \text{ZIP}(\lambda_i, \theta),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{logit}(\theta) = \gamma_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)=\eta_i\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\eta_i=\beta_0+\beta_1x_{i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0,\beta_1,\gamma_0 \sim N(0, 10000)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; dat.zip.list &amp;lt;- with(dat.zip,list(Y=y, X=x,N=nrow(dat.nb), z=ifelse(y==0,0,1)))
&amp;gt; modelString=&amp;quot;
+ model {
+   for (i in 1:N) {
+      z[i] ~ dbern(one.minus.theta)
+      Y[i] ~ dpois(lambda[i])
+      lambda[i] &amp;lt;- z[i]*eta[i]
+      log(eta[i]) &amp;lt;- beta0 + beta1*X[i]
+   }
+   one.minus.theta &amp;lt;- 1-theta
+   logit(theta) &amp;lt;- gamma0
+   beta0 ~ dnorm(0,1.0E-06)
+   beta1 ~ dnorm(0,1.0E-06)
+   gamma0 ~ dnorm(0,1.0E-06)
+ } 
+ &amp;quot;
&amp;gt; writeLines(modelString, con=&amp;#39;modelzip.txt&amp;#39;)
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;, &amp;#39;gamma0&amp;#39;,&amp;#39;theta&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.zip.jags &amp;lt;- jags(data=dat.zip.list,model.file=&amp;#39;modelzip.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 40
   Unobserved stochastic nodes: 3
   Total graph size: 149

Initializing model&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-evaluation-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model evaluation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; denplot(dat.zip.jags, parms = c(&amp;#39;beta&amp;#39;, &amp;#39;gamma0&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/mcmc_diag_zip-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(dat.zip.jags, parms = c(&amp;#39;beta&amp;#39;, &amp;#39;gamma0&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/mcmc_diag_zip-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; raftery.diag(as.mcmc(dat.zip.jags))
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta0    20       20276 3746         5.41      
 beta1    22       24038 3746         6.42      
 deviance 4        4636  3746         1.24      
 gamma0   5        5908  3746         1.58      
 theta    5        5908  3746         1.58      


[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta0    20       21336 3746         5.70      
 beta1    20       22636 3746         6.04      
 deviance 3        4267  3746         1.14      
 gamma0   5        6078  3746         1.62      
 theta    5        6078  3746         1.62      
&amp;gt; 
&amp;gt; autocorr.diag(as.mcmc(dat.zip.jags))
             beta0       beta1    deviance      gamma0       theta
Lag 0   1.00000000  1.00000000  1.00000000 1.000000000 1.000000000
Lag 1   0.88627108  0.88426590  0.51799594 0.232408997 0.227686735
Lag 5   0.58998005  0.59775827  0.19471855 0.002321179 0.001571686
Lag 10  0.35846288  0.35888205  0.06697926 0.017561785 0.015598223
Lag 50 -0.01753582 -0.01936659 -0.01212528 0.022040872 0.021016755&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;goodness-of-fit-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goodness of fit&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #extract the samples for the two model parameters
&amp;gt; coefs &amp;lt;- dat.zip.jags$BUGSoutput$sims.matrix[,1:2]
&amp;gt; theta &amp;lt;- dat.zip.jags$BUGSoutput$sims.matrix[,&amp;#39;theta&amp;#39;]
&amp;gt; Xmat &amp;lt;- model.matrix(~x, data=dat.zip)
&amp;gt; #expected values on a log scale
&amp;gt; lambda&amp;lt;-coefs %*% t(Xmat)
&amp;gt; #expected value on response scale
&amp;gt; eta &amp;lt;- exp(lambda)
&amp;gt; expY &amp;lt;- sweep(eta,1,(1-theta),&amp;quot;*&amp;quot;)
&amp;gt; varY &amp;lt;- eta+sweep(eta^2,1,theta,&amp;quot;*&amp;quot;)
&amp;gt; varY &amp;lt;- sweep(varY,1,(1-theta),&amp;#39;*&amp;#39;)
&amp;gt; #sweep across rows and then divide by lambda
&amp;gt; Resid &amp;lt;- -1*sweep(expY,2,dat.zip$y,&amp;#39;-&amp;#39;)/sqrt(varY)
&amp;gt; #plot residuals vs expected values
&amp;gt; plot(apply(Resid,2,mean)~apply(eta,2,mean))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/model2_mcmc_gof_zip-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Poisson distribution matching that estimated by the model. Essentially this is estimating how well the Poisson distribution, the log-link function and the linear model approximates the observed data. When doing so, we need to consider the expected value and variance of the zero-inflated poisson.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; SSres&amp;lt;-apply(Resid^2,1,sum, na.rm=T)
&amp;gt; 
&amp;gt; #generate a matrix of draws from a zero-inflated poisson (ZIP) distribution
&amp;gt; # the matrix is the same dimensions as lambda
&amp;gt; library(gamlss.dist)
&amp;gt; #YNew &amp;lt;- matrix(rZIP(length(lambda),eta, theta),nrow=nrow(lambda))
&amp;gt; lambda &amp;lt;- sweep(eta,1,ifelse(dat.zip$y==0,0,1),&amp;#39;*&amp;#39;)
&amp;gt; YNew &amp;lt;- matrix(rpois(length(lambda),lambda),nrow=nrow(lambda))
&amp;gt; Resid1&amp;lt;-(expY - YNew)/sqrt(varY)
&amp;gt; SSres.sim&amp;lt;-apply(Resid1^2,1,sum)
&amp;gt; mean(SSres.sim&amp;gt;SSres, na.rm = T)
[1] 0.5619&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since it is difficult to diagnose many issues from the typical residuals we will now explore simulated residuals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #extract the samples for the two model parameters
&amp;gt; coefs &amp;lt;- dat.zip.jags$BUGSoutput$sims.matrix[,1:2]
&amp;gt; theta &amp;lt;- dat.zip.jags$BUGSoutput$sims.matrix[,&amp;#39;theta&amp;#39;]
&amp;gt; Xmat &amp;lt;- model.matrix(~x, data=dat.zip)
&amp;gt; #expected values on a log scale
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; #expected value on response scale
&amp;gt; lambda &amp;lt;- exp(eta)
&amp;gt; 
&amp;gt; simRes &amp;lt;- function(lambda, data,n=250, plot=T, family=&amp;#39;negbin&amp;#39;, size=NULL,theta=NULL) {
+  require(gap)
+  N = nrow(data)
+  sim = switch(family,
+     &amp;#39;poisson&amp;#39; = matrix(rpois(n*N,apply(lambda,2,mean)),ncol=N, byrow=TRUE),
+     &amp;#39;negbin&amp;#39; = matrix(MASS:::rnegbin(n*N,apply(lambda,2,mean),size),ncol=N, byrow=TRUE),
+         &amp;#39;zip&amp;#39; = matrix(gamlss.dist:::rZIP(n*N,apply(lambda,2,mean),theta),ncol=N, byrow=TRUE)
+  )
+  a = apply(sim + runif(n,-0.5,0.5),2,ecdf)
+  resid&amp;lt;-NULL
+  for (i in 1:nrow(data)) resid&amp;lt;-c(resid,a[[i]](data$y[i] + runif(1 ,-0.5,0.5)))
+  if (plot==T) {
+    par(mfrow=c(1,2))
+    gap::qqunif(resid,pch = 2, bty = &amp;quot;n&amp;quot;,
+    logscale = F, col = &amp;quot;black&amp;quot;, cex = 0.6, main = &amp;quot;QQ plot residuals&amp;quot;,
+    cex.main = 1, las=1)
+    plot(resid~apply(lambda,2,mean), xlab=&amp;#39;Predicted value&amp;#39;, ylab=&amp;#39;Standardized residual&amp;#39;, las=1)
+  }
+  resid
+ }
&amp;gt; 
&amp;gt; simRes(lambda,dat.zip, family=&amp;#39;zip&amp;#39;,theta=theta)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/model2_mcmc_res3_zip-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; [1] 0.718 0.212 0.106 0.050 0.476 0.778 0.248 0.060 0.878 0.704 0.090 0.890
[13] 0.416 0.764 0.282 0.752 0.602 0.848 0.154 0.656&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The trend (black symbols) in the qq-plot does not appear to be overly non-linear (matching the ideal red line well), suggesting that the model is not overdispersed. The spread of standardized (simulated) residuals in the residual plot do not appear overly non-uniform. That is there is not trend in the residuals. Furthermore, there is not a concentration of points close to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; (which would imply overdispersion). Hence, once zero-inflation is accounted for, the model does not display overdispersion. Although there is a slight hint of non-linearity in that the residuals are high for low and high fitted values and lower in the middle, this might well be an artifact of the small data set size. By change, most of the observed values in the middle range of the predictor were zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-model-parameters-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring the model parameters&lt;/h2&gt;
&lt;p&gt;If there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics. As with most Bayesian models, it is best to base conclusions on medians rather than means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; print(dat.zip.jags)
Inference for Bugs model at &amp;quot;modelzip.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff
beta0      0.930   0.282  0.365  0.742  0.933  1.128  1.468 1.003   860
beta1      0.090   0.021  0.049  0.076  0.090  0.104  0.132 1.002  1400
gamma0    -0.420   0.458 -1.349 -0.722 -0.417 -0.110  0.459 1.001 10000
theta      0.401   0.105  0.206  0.327  0.397  0.472  0.613 1.001  7600
deviance  80.674   2.501 77.856 78.867 80.008 81.801 87.064 1.001 10000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 3.1 and DIC = 83.8
DIC is an estimate of expected predictive error (lower deviance is better).
&amp;gt; 
&amp;gt; adply(dat.zip.jags$BUGSoutput$sims.matrix, 2, function(x) {
+   data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))
+ })
        X1      Median        Mean       lower      upper    lower.1
1    beta0  0.93334635  0.92997411  0.37821529  1.4774189  0.7530788
2    beta1  0.09005537  0.09005981  0.04864163  0.1313802  0.0749821
3 deviance 80.00841187 80.67383245 77.63946567 85.5798539 77.8273778
4   gamma0 -0.41667356 -0.41996110 -1.34903991  0.4586909 -0.7013225
5    theta  0.39731301  0.40136809  0.19516803  0.6012233  0.3173026
      upper.1
1  1.13629442
2  0.10333030
3 80.10544007
4 -0.09258569
5  0.46170971
&amp;gt; 
&amp;gt; #on original scale
&amp;gt; adply(exp(dat.zip.jags$BUGSoutput$sims.matrix[,1:2]), 2, function(x) {
+   data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))
+ })
     X1   Median     Mean    lower    upper  lower.1  upper.1
1 beta0 2.543005 2.636434 1.280362 4.056990 1.911083 2.853498
2 beta1 1.094235 1.094483 1.049844 1.140401 1.077865 1.108858&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: We would reject the null hypothesis of no effect of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. An increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is associated with a significant linear increase (positive slope) in the abundance of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Every &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; results in a log &lt;span class=&#34;math inline&#34;&gt;\(0.09\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. We usually express this in terms of abundance rather than log abundance, so every &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; results in a (&lt;span class=&#34;math inline&#34;&gt;\(e^{0.09}=1.1\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(1.1\)&lt;/span&gt; unit increase in the abundance of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explorations-of-the-trends-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Explorations of the trends&lt;/h2&gt;
&lt;p&gt;A measure of the strength of the relationship can be obtained according to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ R^2 = 1 - \frac{\text{RSS}_{model}}{\text{RSS}_{null}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, we could use McFadden’s psuedo&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ R^2 = 1- \frac{LL(Model_{full})}{LL(Model_{reduced}}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Xmat &amp;lt;- model.matrix(~x, dat=dat.zip)
&amp;gt; #expected values on a log scale
&amp;gt; neta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; #expected value on response scale
&amp;gt; eta &amp;lt;- exp(neta)
&amp;gt; lambda &amp;lt;- sweep(eta,2,ifelse(dat.zip$y==0,0,1),&amp;#39;*&amp;#39;)
&amp;gt; theta &amp;lt;- dat.zip.jags$BUGSoutput$sims.matrix[,&amp;#39;theta&amp;#39;]
&amp;gt; expY &amp;lt;- sweep(lambda,2,1-theta,&amp;#39;*&amp;#39;)
&amp;gt; #calculate the raw SS residuals
&amp;gt; SSres &amp;lt;- apply((-1*(sweep(expY,2,dat.zip$y,&amp;#39;-&amp;#39;)))^2,1,sum)
&amp;gt; mean(SSres)
[1] 168.3814
&amp;gt; 
&amp;gt; SSres.null &amp;lt;- sum((dat.zip$y - mean(dat.zip$y))^2)
&amp;gt; #calculate the model r2
&amp;gt; 1-mean(SSres)/SSres.null
[1] 0.5977029&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(50\)&lt;/span&gt;% of the variation in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; abundance can be explained by its relationship with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Finally, we will create a summary plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; par(mar = c(4, 5, 0, 0))
&amp;gt; plot(y ~ x, data = dat.zip, type = &amp;quot;n&amp;quot;, ann = F, axes = F)
&amp;gt; points(y ~ x, data = dat.zip, pch = 16)
&amp;gt; xs &amp;lt;- seq(min(dat.zip$x,na.rm=TRUE),max(dat.zip$x,na.rm=TRUE), l = 1000)
&amp;gt; Xmat &amp;lt;- model.matrix(~xs)
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; ys &amp;lt;- exp(eta)
&amp;gt; library(plyr)
&amp;gt; library(coda)
&amp;gt; data.tab &amp;lt;- adply(ys,2,function(x) {
+   data.frame(Median=median(x), HPDinterval(as.mcmc(x)))
+ })
&amp;gt; data.tab &amp;lt;- cbind(x=xs,data.tab)
&amp;gt; points(Median ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;)
&amp;gt; lines(lower ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;, lty = 2)
&amp;gt; lines(upper ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;, lty = 2)
&amp;gt; 
&amp;gt; axis(1)
&amp;gt; mtext(&amp;quot;X&amp;quot;, 1, cex = 1.5, line = 3)
&amp;gt; axis(2, las = 2)
&amp;gt; mtext(&amp;quot;Abundance of Y&amp;quot;, 2, cex = 1.5, line = 3)
&amp;gt; box(bty = &amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/model_code_v2_plot_zip-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-log-likelihood-function-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Full log-likelihood function&lt;/h2&gt;
&lt;p&gt;Now lets try it by specifying log-likelihood and the zero trick. When applying this trick, we need to manually calculate the deviance as the inbuilt deviance will be based on the log-likelihood of estimating the zeros (as part of the zero trick) rather than the deviance of the intended model. The one advantage of the zero trick is that the Deviance and thus DIC, AIC provided by &lt;code&gt;R2jags&lt;/code&gt; will be incorrect. Hence, they too need to be manually defined within &lt;code&gt;JAGS&lt;/code&gt; I suspect that the AIC calculation I have used is incorrect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Xmat &amp;lt;- model.matrix(~x, dat.zip)
&amp;gt; nX &amp;lt;- ncol(Xmat)
&amp;gt; dat.zip.list2 &amp;lt;- with(dat.zip,list(Y=y, X=Xmat,N=nrow(dat.zip), mu=rep(0,nX),
+                   Sigma=diag(1.0E-06,nX), zeros=rep(0,nrow(dat)), C=10000))
&amp;gt; modelString=&amp;quot;
+ model {
+   for (i in 1:N) {
+      zeros[i] ~ dpois(zeros.lambda[i])
+      zeros.lambda[i] &amp;lt;- -ll[i] + C     
+      ll[i] &amp;lt;- Y[i]*log(lambda[i]) - lambda[i] - loggam(Y[i]+1)
+      eta[i] &amp;lt;- inprod(beta[], X[i,])
+      log(lambda[i]) &amp;lt;- eta[i]
+     llm[i] &amp;lt;- Y[i]*log(meanlambda) - meanlambda - loggam(Y[i]+1)
+   }
+   meanlambda &amp;lt;- mean(lambda)
+   beta ~ dmnorm(mu[],Sigma[,])
+   dev &amp;lt;- sum(-2*ll)
+   pD &amp;lt;- mean(dev)-sum(-2*llm)
+   AIC &amp;lt;- min(dev+(2*pD))
+ } 
+ &amp;quot;
&amp;gt; 
&amp;gt; writeLines(modelString, con=&amp;#39;modelzip_ll.txt&amp;#39;)
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta&amp;#39;,&amp;#39;dev&amp;#39;,&amp;#39;AIC&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.ZIP.jags3  &amp;lt;- jags(data=dat.zip.list2,model.file=&amp;#39;modelzip_ll.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 1
   Total graph size: 328

Initializing model
&amp;gt; 
&amp;gt; print(dat.ZIP.jags3 )
Inference for Bugs model at &amp;quot;modelzip_ll.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
            mu.vect sd.vect       2.5%        25%        50%        75%
AIC          61.488   3.844     57.991     58.846     60.144     62.785
beta[1]       0.329   0.225     -0.122      0.176      0.331      0.475
beta[2]       0.104   0.017      0.070      0.093      0.104      0.116
dev         124.472   1.801    122.700    123.170    123.871    125.221
deviance 400124.472   1.801 400122.700 400123.170 400123.871 400125.221
              97.5%  Rhat n.eff
AIC          72.257 1.089    35
beta[1]       0.785 1.071    67
beta[2]       0.137 1.051    69
dev         129.054 1.042    53
deviance 400129.054 1.000     1

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 1.6 and DIC = 400126.1
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;zero-inflated-negative-binomial&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Zero inflated Negative Binomial&lt;/h1&gt;
&lt;div id=&#34;data-generation-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data generation&lt;/h2&gt;
&lt;p&gt;Lets say we wanted to model the abundance of an item (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) against a continuous predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; set.seed(37) #34.5  #4 #10 #16 #17 #26
&amp;gt; #The number of samples
&amp;gt; n.x &amp;lt;- 20
&amp;gt; #Create x values that at uniformly distributed throughout the rate of 1 to 20
&amp;gt; x &amp;lt;- sort(runif(n = n.x, min = 1, max =20))
&amp;gt; mm &amp;lt;- model.matrix(~x)
&amp;gt; intercept &amp;lt;- 0.6
&amp;gt; slope=0.1
&amp;gt; #The linear predictor
&amp;gt; linpred &amp;lt;- mm %*% c(intercept,slope)
&amp;gt; #Predicted y values
&amp;gt; lambda &amp;lt;- exp(linpred)
&amp;gt; #Add some noise and make binomial
&amp;gt; library(gamlss.dist)
&amp;gt; #fixed latent binomial
&amp;gt; y&amp;lt;- rZINBI(n.x,lambda, 0.4)
&amp;gt; #latent binomial influenced by the linear predictor 
&amp;gt; #y&amp;lt;- rZINB(n.x,lambda, 1-exp(linpred)/(1+exp(linpred)))
&amp;gt; dat.zinb &amp;lt;- data.frame(y,x)
&amp;gt; 
&amp;gt; summary(dat.glm.nb&amp;lt;-glm.nb(y~x, dat.zinb))

Call:
glm.nb(formula = y ~ x, data = dat.zinb, init.theta = 0.4646673144, 
    link = log)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3578  -1.3455  -0.5069   0.3790   1.1809  

Coefficients:
            Estimate Std. Error z value Pr(&amp;gt;|z|)
(Intercept) 0.914191   0.796804   1.147    0.251
x           0.009149   0.067713   0.135    0.893

(Dispersion parameter for Negative Binomial(0.4647) family taken to be 1)

    Null deviance: 20.303  on 19  degrees of freedom
Residual deviance: 20.282  on 18  degrees of freedom
AIC: 90.365

Number of Fisher Scoring iterations: 1

              Theta:  0.465 
          Std. Err.:  0.218 

 2 x log-likelihood:  -84.365 
&amp;gt; 
&amp;gt; plot(glm.nb(y~x, dat.zinb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/generate_data_zinb-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/generate_data_zinb-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/generate_data_zinb-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/generate_data_zinb-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; library(pscl)
&amp;gt; summary(dat.zeroinfl&amp;lt;-zeroinfl(y ~ x | 1, dist = &amp;quot;negbin&amp;quot;, data = dat.zinb))

Call:
zeroinfl(formula = y ~ x | 1, data = dat.zinb, dist = &amp;quot;negbin&amp;quot;)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.9609 -0.9268 -0.4446  1.0425  1.7556 

Count model coefficients (negbin with log link):
            Estimate Std. Error z value Pr(&amp;gt;|z|)   
(Intercept)  0.92733    0.32507   2.853  0.00433 **
x            0.06870    0.02755   2.494  0.01263 * 
Log(theta)   3.36066    3.59739   0.934  0.35020   

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&amp;gt;|z|)
(Intercept)  -0.2250     0.4559  -0.494    0.622
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1 

Theta = 28.8082 
Number of iterations in BFGS optimization: 17 
Log-likelihood: -38.54 on 4 Df
&amp;gt; 
&amp;gt; plot(resid(zeroinfl(y ~ x | 1, dist = &amp;quot;negbin&amp;quot;, data = dat.zinb))~fitted(zeroinfl(y ~ x | 1, dist = &amp;quot;negbin&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/generate_data_zinb-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; vuong(dat.glm.nb, dat.zeroinfl)
Vuong Non-Nested Hypothesis Test-Statistic: 
(test-statistic is asymptotically distributed N(0,1) under the
 null that the models are indistinguishible)
-------------------------------------------------------------
              Vuong z-statistic             H_A p-value
Raw                  -1.2809521 model2 &amp;gt; model1 0.10011
AIC-corrected        -0.9296587 model2 &amp;gt; model1 0.17627
BIC-corrected        -0.7547616 model2 &amp;gt; model1 0.22520
&amp;gt; 
&amp;gt; library(gamlss)
&amp;gt; summary(gamlss(y~x, data=dat.zinb, family=&amp;#39;ZINBI&amp;#39;))
GAMLSS-RS iteration 1: Global Deviance = 81.436 
GAMLSS-RS iteration 2: Global Deviance = 78.1917 
GAMLSS-RS iteration 3: Global Deviance = 77.0798 
GAMLSS-RS iteration 4: Global Deviance = 77.0726 
GAMLSS-RS iteration 5: Global Deviance = 77.0725 
******************************************************************
Family:  c(&amp;quot;ZINBI&amp;quot;, &amp;quot;Zero inflated negative binomial type I&amp;quot;) 

Call:  gamlss(formula = y ~ x, family = &amp;quot;ZINBI&amp;quot;, data = dat.zinb) 

Fitting method: RS() 

------------------------------------------------------------------
Mu link function:  log
Mu Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)  
(Intercept)  0.92653    0.32502   2.851   0.0116 *
x            0.06880    0.02753   2.499   0.0237 *
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

------------------------------------------------------------------
Sigma link function:  log
Sigma Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)
(Intercept)   -3.363      3.603  -0.933    0.365

------------------------------------------------------------------
Nu link function:  logit 
Nu Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)
(Intercept)  -0.2250     0.4559  -0.494    0.628

------------------------------------------------------------------
No. of observations in the fit:  20 
Degrees of Freedom for the fit:  4
      Residual Deg. of Freedom:  16 
                      at cycle:  5 
 
Global Deviance:     77.0725 
            AIC:     85.0725 
            SBC:     89.05543 
******************************************************************
&amp;gt; 
&amp;gt; summary(gamlss(y~x, nu.fo=y~x,data=dat.zinb, family=&amp;#39;ZINBI&amp;#39;))
GAMLSS-RS iteration 1: Global Deviance = 78.2478 
GAMLSS-RS iteration 2: Global Deviance = 74.2622 
GAMLSS-RS iteration 3: Global Deviance = 73.8329 
GAMLSS-RS iteration 4: Global Deviance = 73.8305 
GAMLSS-RS iteration 5: Global Deviance = 73.8305 
******************************************************************
Family:  c(&amp;quot;ZINBI&amp;quot;, &amp;quot;Zero inflated negative binomial type I&amp;quot;) 

Call:  gamlss(formula = y ~ x, nu.formula = y ~ x, family = &amp;quot;ZINBI&amp;quot;,  
    data = dat.zinb) 

Fitting method: RS() 

------------------------------------------------------------------
Mu link function:  log
Mu Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)  
(Intercept)  0.84246    0.35267   2.389   0.0305 *
x            0.07481    0.02933   2.550   0.0222 *
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

------------------------------------------------------------------
Sigma link function:  log
Sigma Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)
(Intercept)   -2.982      2.844  -1.048    0.311

------------------------------------------------------------------
Nu link function:  logit 
Nu Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)
(Intercept)  -2.4988     1.8283  -1.367    0.192
x             0.1996     0.1417   1.408    0.179

------------------------------------------------------------------
No. of observations in the fit:  20 
Degrees of Freedom for the fit:  5
      Residual Deg. of Freedom:  15 
                      at cycle:  5 
 
Global Deviance:     73.83046 
            AIC:     83.83046 
            SBC:     88.80912 
******************************************************************&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;Check the distribution of the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; abundances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; hist(dat.zinb$y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data1_zinb-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; boxplot(dat.zinb$y, horizontal=TRUE)
&amp;gt; rug(jitter(dat.zinb$y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data1_zinb-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is definitely signs of non-normality that would warrant Poisson or negative binomial models. Further to that, there appears to be a large number of zeros and a possible clumpiness that are likely to be the cause of overdispersion A zero-inflated negative binomial model is likely to be one of the most effective for modeling these data. Lets now explore linearity by creating a histogram of the predictor variable (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;). Note, it is difficult to directly assess issues of linearity. Indeed, a scatterplot with lowess smoother will be largely influenced by the presence of zeros. One possible way of doing so is to explore the trend in the non-zero data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; hist(dat.zinb$x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data2_zinb-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; #now for the scatterplot
&amp;gt; plot(y~x, dat.zinb, log=&amp;quot;y&amp;quot;)
&amp;gt; with(subset(dat.zinb,y&amp;gt;0), lines(lowess(y~x)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/expl_data2_zinb-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: the predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the non-zero data cloud does not display major deviations from a straight line and thus linearity is likely to be satisfied. Violations of linearity (whilst difficult to be certain about due to the unknown influence of the zeros) could be addressed by either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;define a non-linear linear predictor (such as a polynomial, spline or other non-linear function).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;transform the scale of the predictor variables.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although we have already established that there are few zeros in the data (and thus overdispersion is unlikely to be an issue), we can also explore this by comparing the number of zeros in the data to the number of zeros that would be expected from a Poisson distribution with a mean equal to the mean count of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #proportion of 0&amp;#39;s in the data
&amp;gt; dat.zinb.tab&amp;lt;-table(dat.zinb$y==0)
&amp;gt; dat.zinb.tab/sum(dat.zinb.tab)

FALSE  TRUE 
 0.55  0.45 
&amp;gt; 
&amp;gt; #proportion of 0&amp;#39;s expected from a Poisson distribution
&amp;gt; mu &amp;lt;- mean(dat.zinb$y)
&amp;gt; v &amp;lt;- var(dat.zinb$y)
&amp;gt; size &amp;lt;- mu + (mu^2)/v
&amp;gt; cnts &amp;lt;- rnbinom(1000, mu=mu, size=size)
&amp;gt; dat.zinb.tabE &amp;lt;- table(cnts == 0)
&amp;gt; dat.zinb.tabE/sum(dat.zinb.tabE)

FALSE  TRUE 
0.861 0.139 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above, the value under &lt;code&gt;FALSE&lt;/code&gt; is the proportion of non-zero values in the data and the value under TRUE is the proportion of zeros in the data. In this example, the proportion of zeros observed (&lt;span class=&#34;math inline&#34;&gt;\(45\)&lt;/span&gt;%) far exceeds that that would have been expected (&lt;span class=&#34;math inline&#34;&gt;\(14\)&lt;/span&gt;%). Hence it is highly likely that any models will be zero-inflated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i \sim \text{ZINB}(\lambda_i, \theta),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{logit}(\theta) = \gamma_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\log(\lambda_i)=\eta_i\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\eta_i=\beta_0+\beta_1x_{i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0,\beta_1,\gamma_0 \sim N(0, 10000)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; dat.zinb.list &amp;lt;- with(dat.zinb,list(Y=y, X=x,N=nrow(dat.zinb),z=ifelse(y==0,0,1)))
&amp;gt; modelString=&amp;quot;
+ model {
+   for (i in 1:N) {
+      z[i] ~ dbern(psi.min)
+      Y[i] ~ dnegbin(p[i],size)
+      p[i] &amp;lt;- size/(size+mu.eff[i])
+      mu.eff[i] &amp;lt;- z[i]*mu[i]
+      eta[i] &amp;lt;- beta0 + beta1*X[i]
+      log(mu[i]) &amp;lt;- eta[i]
+   }
+   gamma ~ dnorm(0,0.001)
+   psi.min &amp;lt;- min(0.9999, max(0.00001, (1-psi)))
+   logit(psi) &amp;lt;- max(-20, min(20, gamma))
+   size ~ dunif(0.001, 5)
+   theta &amp;lt;- pow(1/mean(p),2)
+   beta0 ~ dnorm(0,1.0E-06)
+   beta1 ~ dnorm(0,1.0E-06)
+ } 
+ &amp;quot;
&amp;gt; writeLines(modelString, con=&amp;#39;modelzinb.txt&amp;#39;)
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;, &amp;#39;size&amp;#39;, &amp;#39;theta&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.zinb.jags &amp;lt;- jags(data=dat.zinb.list,model.file=&amp;#39;modelzinb.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 40
   Unobserved stochastic nodes: 4
   Total graph size: 205

Initializing model
&amp;gt; 
&amp;gt; print(dat.zinb.jags)
Inference for Bugs model at &amp;quot;modelzinb.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff
beta0      0.971   0.460  0.055  0.678  0.963  1.273  1.868 1.007   250
beta1      0.067   0.042 -0.016  0.039  0.066  0.094  0.151 1.008   200
size       3.501   1.015  1.389  2.763  3.644  4.351  4.935 1.001 10000
theta      2.200   0.367  1.721  1.937  2.115  2.371  3.145 1.001  3300
deviance  82.769   2.843 79.139 80.663 82.139 84.254 89.891 1.001 10000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 4.0 and DIC = 86.8
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-evaluation-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model evaluation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; denplot(dat.zinb.jags, parms = c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;, &amp;#39;size&amp;#39;, &amp;#39;theta&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/mcmc_diag_zinb-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(dat.zinb.jags, parms = c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;, &amp;#39;size&amp;#39;, &amp;#39;theta&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/mcmc_diag_zinb-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; raftery.diag(as.mcmc(dat.zinb.jags))
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta0    15       16236 3746         4.33      
 beta1    14       15725 3746         4.20      
 deviance 3        4484  3746         1.20      
 size     5        5771  3746         1.54      
 theta    3        4338  3746         1.16      


[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta0    27       27564 3746         7.36      
 beta1    18       21057 3746         5.62      
 deviance 3        4410  3746         1.18      
 size     5        5771  3746         1.54      
 theta    2        3995  3746         1.07      
&amp;gt; 
&amp;gt; autocorr.diag(as.mcmc(dat.zinb.jags))
             beta0       beta1    deviance        size       theta
Lag 0   1.00000000  1.00000000  1.00000000 1.000000000 1.000000000
Lag 1   0.82187294  0.82300377  0.55172222 0.391115803 0.387289605
Lag 5   0.44679310  0.44494849  0.13559995 0.045297725 0.067379632
Lag 10  0.19928140  0.20123773  0.05371302 0.008341721 0.013304093
Lag 50 -0.04037202 -0.04473554 -0.02496182 0.011474420 0.007333003&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;goodness-of-fit-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goodness of fit&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #extract the samples for the two model parameters
&amp;gt; coefs &amp;lt;- dat.zinb.jags$BUGSoutput$sims.matrix[,1:2]
&amp;gt; theta &amp;lt;- dat.zinb.jags$BUGSoutput$sims.matrix[,&amp;#39;theta&amp;#39;]
&amp;gt; Xmat &amp;lt;- model.matrix(~x, data=dat.zinb)
&amp;gt; #expected values on a log scale
&amp;gt; lambda&amp;lt;-coefs %*% t(Xmat)
&amp;gt; #expected value on response scale
&amp;gt; eta &amp;lt;- exp(lambda)
&amp;gt; expY &amp;lt;- sweep(eta,1,(1-theta),&amp;quot;*&amp;quot;)
&amp;gt; varY &amp;lt;- eta+sweep(eta^2,1,theta,&amp;quot;*&amp;quot;)
&amp;gt; head(varY)
             1         2        3        4        5        6        7        8
[1,] 10.844323 13.189501 15.47499 15.73287 18.21519 26.87133 28.14742 29.27065
[2,] 71.832694 61.952112 54.97632 54.30484 48.72535 36.70113 35.49495 34.51074
[3,] 24.764991 24.273552 23.88302 23.84316 23.49392 22.60135 22.49799 22.41131
[4,]  6.397443  8.786249 11.40150 11.71375 14.89149 28.26188 30.51610 32.55772
[5,] 27.048585 28.561484 29.85015 29.98628 31.21706 34.70423 35.14294 35.51685
[6,] 32.911549 36.163316 39.03708 39.34619 42.18864 50.70280 51.82155 52.78337
            9       10       11       12        13        14        15
[1,] 32.48606 39.45733 45.43874 50.02645  59.31437  71.66019  73.00520
[2,] 32.02933 27.89750 25.25717 23.61192  20.97369  18.40930  18.17586
[3,] 22.18262 21.76433 21.46712 21.26761  20.92017  20.54281  20.50616
[4,] 38.69312 53.42044 67.53693 79.24793 105.19992 144.10581 148.63604
[5,] 36.53055 38.49254 39.97741 41.01961  42.92731  45.14296  45.36660
[6,] 55.42928 60.70818 64.84042 67.81058  73.39529  80.11924  80.81199
            16        17        18        19        20
[1,]  89.37583  90.29710  93.03697  99.45044 121.73297
[2,]  15.83105  15.72115  15.40547  14.72560  12.85289
[3,]  20.11263  20.09293  20.03565  19.90863  19.52937
[4,] 208.15726 211.74132 222.54395 248.66128 348.12988
[5,]  47.86864  47.99890  48.38049  49.24196  51.94528
[6,]  88.73695  89.15824  90.39740  93.22189 102.32692
&amp;gt; 
&amp;gt; #varY &amp;lt;- sweep(varY,1,(1-theta),&amp;#39;*&amp;#39;)
&amp;gt; #sweep across rows and then divide by lambda
&amp;gt; Resid &amp;lt;- -1*sweep(expY,2,dat.zinb$y,&amp;#39;-&amp;#39;)/sqrt(varY)
&amp;gt; #plot residuals vs expected values
&amp;gt; plot(apply(Resid,2,mean)~apply(eta,2,mean))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm2-jags/2020-02-01-glm2-jags_files/figure-html/model2_mcmc_gof_zinb-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Poisson distribution matching that estimated by the model. Essentially this is estimating how well the Poisson distribution, the log-link function and the linear model approximates the observed data. When doing so, we need to consider the expected value and variance of the zero-inflated poisson.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; SSres&amp;lt;-apply(Resid^2,1,sum, na.rm=T)
&amp;gt; 
&amp;gt; #generate a matrix of draws from a zero-inflated poisson (ZINB) distribution
&amp;gt; # the matrix is the same dimensions as lambda
&amp;gt; library(gamlss.dist)
&amp;gt; #YNew &amp;lt;- matrix(rZINB(length(lambda),eta, theta),nrow=nrow(lambda))
&amp;gt; lambda &amp;lt;- sweep(eta,1,ifelse(dat.zinb$y==0,0,1),&amp;#39;*&amp;#39;)
&amp;gt; YNew &amp;lt;- matrix(rpois(length(lambda),lambda),nrow=nrow(lambda))
&amp;gt; Resid1&amp;lt;-(expY - YNew)/sqrt(varY)
&amp;gt; SSres.sim&amp;lt;-apply(Resid1^2,1,sum)
&amp;gt; mean(SSres.sim&amp;gt;SSres, na.rm = T)
[1] 0.5212&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-model-parameters-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring the model parameters&lt;/h2&gt;
&lt;p&gt;If there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics. As with most Bayesian models, it is best to base conclusions on medians rather than means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; print(dat.zinb.jags)
Inference for Bugs model at &amp;quot;modelzinb.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff
beta0      0.971   0.460  0.055  0.678  0.963  1.273  1.868 1.007   250
beta1      0.067   0.042 -0.016  0.039  0.066  0.094  0.151 1.008   200
size       3.501   1.015  1.389  2.763  3.644  4.351  4.935 1.001 10000
theta      2.200   0.367  1.721  1.937  2.115  2.371  3.145 1.001  3300
deviance  82.769   2.843 79.139 80.663 82.139 84.254 89.891 1.001 10000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 4.0 and DIC = 86.8
DIC is an estimate of expected predictive error (lower deviance is better).
&amp;gt; 
&amp;gt; adply(dat.zinb.jags$BUGSoutput$sims.matrix, 2, function(x) {
+   data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))
+ })
        X1      Median        Mean       lower      upper     lower.1
1    beta0  0.96339931  0.97060322  0.05196655  1.8646388  0.68771701
2    beta1  0.06565837  0.06658472 -0.01850221  0.1478933  0.03570031
3 deviance 82.13938661 82.76912313 78.75619568 88.5891138 79.69050804
4     size  3.64385931  3.50054311  1.63847682  4.9995959  3.62583688
5    theta  2.11463918  2.19954948  1.65289052  2.9565781  1.83698278
      upper.1
1  1.28169341
2  0.09027889
3 82.76458253
4  4.98121591
5  2.20696839
&amp;gt; 
&amp;gt; #on original scale
&amp;gt; adply(exp(dat.zinb.jags$BUGSoutput$sims.matrix[,1:2]), 2, function(x) {
+   data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))
+ })
     X1   Median     Mean     lower    upper  lower.1  upper.1
1 beta0 2.620590 2.935935 0.7910564 5.701997 1.628127 3.096623
2 beta1 1.067862 1.069796 0.9816679 1.159389 1.036345 1.094479&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: We would reject the null hypothesis of no effect of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. An increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is associated with a significant linear increase (positive slope) in the abundance of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Every &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; results in a log &lt;span class=&#34;math inline&#34;&gt;\(0.06\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. We usually express this in terms of abundance rather than log abundance, so every &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; results in a (&lt;span class=&#34;math inline&#34;&gt;\(e^{0.06}=1.07\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(1.07\)&lt;/span&gt; unit increase in the abundance of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-plummer2004jags&#34;&gt;
&lt;p&gt;Plummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-su2015package&#34;&gt;
&lt;p&gt;Su, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” &lt;em&gt;R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Generalised Linear Models - JAGS</title>
      <link>/jags/glm-jags/glm-jags/</link>
      <pubDate>Thu, 13 Feb 2020 21:13:14 -0500</pubDate>
      
      <guid>/jags/glm-jags/glm-jags/</guid>
      <description>


&lt;p&gt;This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. &lt;code&gt;BUGS&lt;/code&gt; (Bayesian inference Using &lt;em&gt;Gibbs Sampling&lt;/em&gt;) is an algorithm and supporting language (resembling &lt;code&gt;R&lt;/code&gt;) dedicated to performing the Gibbs sampling implementation of &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; (MCMC) method. Dialects of the &lt;code&gt;BUGS&lt;/code&gt; language are implemented within three main projects:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OpenBUGS&lt;/strong&gt; - written in component pascal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;JAGS&lt;/strong&gt; - (Just Another Gibbs Sampler) - written in &lt;code&gt;C++&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;STAN&lt;/strong&gt; - a dedicated Bayesian modelling framework written in &lt;code&gt;C++&lt;/code&gt; and implementing &lt;em&gt;Hamiltonian&lt;/em&gt; MCMC samplers.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of &lt;code&gt;R&lt;/code&gt;, and thus, they are best accessed from within &lt;code&gt;R&lt;/code&gt; itself. As such there are multiple packages devoted to interfacing with the various software implementations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2OpenBUGS&lt;/em&gt; - interfaces with &lt;code&gt;OpenBUGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2jags&lt;/em&gt; - interfaces with &lt;code&gt;JAGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;rstan&lt;/em&gt; - interfaces with &lt;code&gt;STAN&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial will demonstrate how to fit models in &lt;code&gt;JAGS&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Plummer (2004)&lt;/span&gt;) using the package &lt;code&gt;R2jags&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Su et al. (2015)&lt;/span&gt;) as interface, which also requires to load some other packages.&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Before discussing generalised linear models, we will first revise a couple of fundamental aspects of general linear models and in particular, how they restrict the usefulness of these models in clinical applications. General linear models provide a set of well adopted and recognised procedures for relating response variables to a linear combination of one or more continuous or categorical predictors (hence the “general”). Nevertheless, the reliability and applicability of such models are restricted by the degree to which the residuals conform to normality and the mean and variance are independent of one another. The general linear model essentially comprises three components.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ E[Y] = \beta_0 + \beta_1x_1 + \ldots + \beta_px_p + \epsilon.\]&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The Random (Stochastic) component&lt;/strong&gt; that specifies the conditional distribution (Normal or Gaussian distribution) of the response variable. Whilst the mean of the normal distribution is assumed to vary as a function of the linear predictors (Systematic component - the regression equation), the variance is assumed to remain constant. Denoted &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; in the above equation, the random component is more formally defined as &lt;span class=&#34;math inline&#34;&gt;\(Y_i \sim N(0, \sigma^2)\)&lt;/span&gt;. That is, each value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (the response) is assumed to be drawn from a normal distribution with different means (&lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;) yet fixed variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The Systematic component&lt;/strong&gt; that represents the linear combination of predictors (which can be categorical, continuous, polynomial or other contrasts) for a linear predictor. The linear predictor describes (predict) the “expected” mean and variability of the response(s) (which are assumed to follow normal distribution(s)).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The Link function&lt;/strong&gt; which links the expected values of the response (Random component) to the linear combination of predictors (systematic component). For the normal (Gaussian) distribution, the link function is a the “identity” link (&lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;). That is:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \mu_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_px_{ip}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are many real situations for which the assumptions imposed by the normal distribution are unlikely to be satisfied. For example, if the measured response to a predictor treatment (such as nest parasite load) can only be binary (such as abandoned or not), then the differences between the observed and expected values (residuals) are unlikely to follow a normal distribution. Instead, in this case, they should follow a binomial distribution.&lt;/p&gt;
&lt;p&gt;Often response variables have a restricted range. For example a species may be either present or not present and thus the response is restricted to either &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (present) or absent (&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;). Values less than &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; or greater than &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; are not logical. Similarly, the abundance of a species in a quadrat is bounded by a minimum value of zero - it is not possible to have fewer than zero individuals. Proportional abundances are also restricted to between &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(100\)&lt;/span&gt;). The normal distribution however, is valid for the range between positive and negative infinity (ie not restricted) and thus expected values of the linear predictor can be outside of the restricted range that naturally operates on the response variable. Hence, the normal distribution might not always represent a sensible probability model as it can predict values outside the logical range of the data. Furthermore, the as a result of these range restrictions, variance can be tied to the mean in that expected probabilities towards the extremes of the restricted range tend to have lower variability (as the lower or upper bounds of the probabilities are trunctated).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-types&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data types&lt;/h2&gt;
&lt;p&gt;Response data can generally be classified into one of four levels&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Nominal&lt;/strong&gt; - responses are those that represent un-ordered categories For example, we could record the ‘preferred’ food choice of an animal as either “Fruit”, “Meat”, “Seeds” or “Leaves”. The spacing between categories is undetermined and responses are restricted to those options.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ordinal&lt;/strong&gt; - responses are those that represent categories with sensible orders, yet undetermined spacing between categories. Likert scale questionnaire responses to questions such as “Rate the quality of your experience… on a scale of &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;” are a classic example. Categorized levels of a response (“High”, “Medium”,“Low”) would also be another example of an ordinal variable&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Interval&lt;/strong&gt; - responses are those for which both the order and scale (spacing) are meaningful, yet multiplication is meaningless due to the arbitrary scale of the data (where zero does not refer to nothing). Temperature in degrees C is a good example of such a response (consider whether &lt;span class=&#34;math inline&#34;&gt;\(-28\)&lt;/span&gt; degrees &lt;span class=&#34;math inline&#34;&gt;\(^\star-1 = 28\)&lt;/span&gt; degrees has a sensible interpretation).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ratio&lt;/strong&gt; - responses are those for which order, scale and zero are meaningful. For example a measurement scale such as length in millimeters or mass in grams.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;glms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GLMs&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Generalized linear models&lt;/strong&gt; (GLM’s) extend the application range of linear modelling by accommodating non-stable variances as well as alternative exponential residual distributions (such as the binomial and Poisson distributions). GLMs have the same three components as general linear models (of which the systematic component is identical), yet a broader range of Random components are accommodated and thus alternative Link functions must also be possible.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Random component defines the exponential distribution (Gaussian, Poisson, binomial, gamma, and inverse Gaussian distributions) from which the responses are assumed to be drawn. These distributions are characterised by some function of the mean (canonical or location parameter) and a function of the variance (dispersion parameter). Note that for binomial and Poisson distributions, the dispersion parameter is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, whereas for the Guassian (normal) distribution the dispersion parameter is the error variance and is assumed to be independent of the mean. The negative binomial distribution can also be treated as an exponential distribution if the dispersion parameter is fixed as a constant.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Systematic component again defines the linear combination of predictors&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Link function, &lt;span class=&#34;math inline&#34;&gt;\(g(\mu)\)&lt;/span&gt; links the systematic and random components. Although there are many commonly employed link functions, typically the exact form of the link function depends on the nature of the random response distribution. Some of the canonical (natural choice) link functions and distribution pairings that are suitable for different forms of generalized linear models are listed in the following table. The only real restriction on a link function is that it must preserve the order of values such that larger values are always larger than smaller values (be monotonic) and must yield derivatives that are legal throughout the entire range of the data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;link-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Link functions&lt;/h2&gt;
&lt;p&gt;In contrast to fitting linear models to transformations of the raw data, the link functions transform the curve predicted by the systematic component into a scale approximating that of the response.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Logit&lt;/strong&gt;. Log odds-ratio The slope parameter represents the rate of change in log odds-ratio per unit increase in a predictor.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Probit&lt;/strong&gt;. The probit transformation is the inverse cumulative distribution for the standard normal distribution and is useful when the response is likely to be a categorization of an otherwise continuous scale. So whilst measurements might be recorded on a categorical scale (either for convenience or because that is how they manifest), these measurements are a proxy for an underlying variable (latent variable) that is actually continuous. So if the purpose of the linear modeling is to predict the underlying latent variable, then probit regression is likely to be appropriate. The slope parameter represents the rate of change in response probability per unit increase in a predictor.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complementary log-log&lt;/strong&gt;. The log-log transformation is useful for extremely asymmetrical distributions (notably survival analyses).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimation&lt;/h2&gt;
&lt;p&gt;The generalized nature of GLM’s makes them incompatible with ordinary least squares model fitting procedures. Instead, parameter estimates and model fitting are typically achieved by maximum likelihood methods based on an iterative re-weighting algorithm (such as the Newton-Raphson algorithm). Essentially, the Newton-Raphson algorithm (also known as a scoring algorithm) fits a linear model to an adjusted response variable (transformed via the link function) using a set of weights and then iteratively re-fits the model with new sets of weights recalculated according to the fit of the previous iteration. For canonical link-distribution pairs (see the table above), the Newton-Raphson algorithm usually converges (arrives at a common outcome or equilibrium) very efficiently and reliably. The Newton-Raphson algorithm facilitates a unifying model fitting procedure across the family of exponential probability distributions thereby providing a means by which binary and count data can be incorporated into the suit of regular linear model designs. In fact, linear regression (including ANOVA, ANCOVA and other general linear models) can be considered a special form of GLM that features a normal distribution and identity link function and for which the maximum likelihood procedure has an exact solution. Notably, when variance is stable, both maximum likelihood and ordinary least squares yield very similar parameter estimates.&lt;/p&gt;
&lt;p&gt;Typical distributions used for GLMs include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gaussian&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Binomial&lt;/strong&gt;. Represents the number of successes out of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent trials each with a set probability (typically &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Poisson&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Negative Binomial&lt;/strong&gt;. Represents the number of failures out of a sequence of n independent trials before a success is obtained each with a set probability. Alternatively, a negative binomial can be defined in terms of its mean (&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;) and dispersion parameter. The dispersion parameter can be used to adjust the variances independent of the mean and is therefore useful as an alternative to the Poisson distribution when there is evidence of overdispersion (dispersion parameter &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;1\)&lt;/span&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;dispersion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dispersion&lt;/h2&gt;
&lt;p&gt;The variance of binomial or Poisson distributions is assumed to be related to the sample size and mean respectively, and thus, there is not a variance parameter in their definitions. In fact, the variance (or dispersion) parameter is fixed to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. As a result, logistic/probit regression as well as Poisson regression and log-linear modelling assume that sample variances conform to the respective distribution definitions. However, it is common for individual sampling units (e.g. individuals) to co-vary such that other, unmeasured influences, increase (or less commonly, decrease) variability. For example, although a population sex ratio might be 1:1, male to female ratios within a clutch might be highly skewed towards one or other sex. Positive correlations cause greater variance (overdispersion) and result in deflated standard errors (and thus exaggerated levels of precision and higher Type I errors). Additionally, count data (for example number of fish per transect) can be overdispersed as a result of an unexpectedly high number of zero’s (zero inflated). In this case, the zeros arise for two reasons.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Genuine zero values - zero fish counted because there were non present.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;False zeros - there were fish present, yet not detected (and thus not recorded).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The dispersion parameter (degree of variance inflation or over-dispersion) can be estimated by dividing either the Pearsons &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; or the Deviance by the degrees of freedom, where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of observations in p parameters). As a general rule, dispersion parameters approaching &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;) indicate possible violations of this assumption (although large overdispersion parameters can also be the result of a poorly specified model or outliers). Where over (or under) dispersion is suspected to be an issue, the following options are available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;use &lt;strong&gt;quasibinomial&lt;/strong&gt; and &lt;strong&gt;quasipoisson&lt;/strong&gt; families can be used as alternatives to model the dispersion. These quasi-likelihood models derive the dispersion parameter (function of the variance) from the observed data and are useful when overdispersion is suspected to be caused by positive correlations or other unobserved sources of variance. Rather than assuming that the variance is fixed, quasi- models assume that variance is a linear (multiplicative) function of the mean. Test statistics from such models should be based on F-tests rather than chi-squared tests.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for count data, use a &lt;strong&gt;negative binomial&lt;/strong&gt; as an alternative to a Poisson distribution. The negative binomial distribution also estimates the dispersion parameter and assumes that the variance is a quadratic function of the mean.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;use &lt;strong&gt;zero-inflated binomial&lt;/strong&gt; (ZIB) and &lt;strong&gt;zero-inflated poisson&lt;/strong&gt; (ZIP) when overdispersion is suspected to be caused by excessive numbers of zeros.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;binary-data---logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Binary data - logistic regression&lt;/h2&gt;
&lt;p&gt;Logistic regression is a form of GLM that employs the logit-binomial link distribution canonical pairing to model the effects of one or more continuous or categorical (with dummy coding) predictor variables on a binary (dead/alive, presence/absence, etc) response variable. For example, we could investigate the relationship between salinity levels (salt concentration) and mortality of frogs. Similarly, we could model the presence of a species of bird as a function of habitat patch size, or nest predation (predated or not) as a function of the distance from vegetative cover. Consider the fictitious data presented in the following figure. Clearly, a regular simple linear model is inappropriate for modelling the probability of presence. Note that at very low and high levels of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, the predicted probabilities (probabilities or proportions of the population) are less than zero and greater than one respectively - logically impossible outcomes. Note also, that the residuals cannot be drawn from a normal distribution, since for any value of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, there are only two possible outcomes (&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The logistic model (Figure c above) relating the probability (&lt;span class=&#34;math inline&#34;&gt;\(\pi(x)\)&lt;/span&gt;) that the response (&lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;) equals one (present) for a given level of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; (patch size) is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \pi(x) = \frac{e^{\beta_0 + \beta_1x}}{1+e^{\beta_0+\beta_1x}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Appropriately, since &lt;span class=&#34;math inline&#34;&gt;\(e^{\beta_0+\beta_1x}\)&lt;/span&gt; (the “natural constant” raised to a simple linear model) must evaluate to between 0 and infinity, the logistic model must asymptote towards (and is thus bounded by) zero and one. Alternatively (as described briefly above), the logit link function can be used to transform &lt;span class=&#34;math inline&#34;&gt;\(\pi(x)\)&lt;/span&gt; such that the logistic model is expressed as the log odds (probability of one state relative to the alternative) against a familiar linear combination of the explanatory variables (as is linear regression).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ ln \left(  \frac{\pi(x)}{1-\pi(x)} \right) = \beta_0 + \beta_1x_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Although the &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-intercept) parameter is interpreted similar to that of linear regression (albeit of little clinical interest), this is not the case for the slope parameter (&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;). Rather than representing the rate of change in the response for a given change in the predictor, in logistic regression, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; represents the rate of change in the odds ratio (ratio of odds of an event at two different levels of a predictor) for a given unit change in the predictor. The exponentiated slope represents the odds ratio (&lt;span class=&#34;math inline&#34;&gt;\(\theta=e^{\beta_1}\)&lt;/span&gt;), the proportional rate at which the predicted odds change for a given unit change of the predictor.&lt;/p&gt;
&lt;div id=&#34;null-hypotheses&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Null hypotheses&lt;/h3&gt;
&lt;p&gt;As with linear regression, a separate &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; is tested for each of the estimated model parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0:\beta_1=0\)&lt;/span&gt; (the population slope - proportional rate of change in odds ratio). This test examines whether the log odds of an occurrence are independent of the predictor variable and thus whether or not there is likely to be a relationship between the response and predictor.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0:\beta_0=0\)&lt;/span&gt; (the population intercept equals zero). As stated previously, this is typically of little clinical interest.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similar to linear regression, there are two ways of testing the main null hypotheses:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Parameter estimation approach. Maximum likelihood estimates of the parameters and their asymptoticd standard errors (&lt;span class=&#34;math inline&#34;&gt;\(S_{b1}\)&lt;/span&gt;) are used to calculate the Wald &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-ratio) statistic &lt;span class=&#34;math inline&#34;&gt;\(W=\frac{b_1}{S_{b1}}\)&lt;/span&gt;, which approximately follows a standard &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; distribution when the null hypothesis is true. The reliability of Wald tests diminishes substantially with small sample sizes. For such cases, the second option is therefore more appropriate.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;(log)-likelihood ratio tests approach. This approach essentially involves comparing the fit of models with (full) and without (reduced) the term of interest:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \text{logit}(\pi) = \beta_0 + \beta_1x_1 \;\;\; (\text{full model})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \text{logit}(\pi) = \beta_0 \;\;\; (\text{reduced model})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The fit of any given model is measured via log-likelihood and the differences between the fit of two models is described by a likelihood ratio statistic (G2 &lt;span class=&#34;math inline&#34;&gt;\(= 2\)&lt;/span&gt;(log-likelihood reduced model - log-likelihood full model)). The G2 quantity is also known as deviance and is analogous to the residual sums of squares in a linear model. When the null hypothesis is true, the G2 statistic approximately follows a &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution with one degree of freedom. An analogue of the linear model &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; measure can be calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ r^2 = 1- \frac{G^2_0}{G^2_1},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(G^2_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(G^2_1\)&lt;/span&gt; are the deviances due to the intercept and slope terms respectively. Analogous to the ANOVA table that partitions the total variation into components explained by each of the model terms (and the unexplained error), it is possible to construct a analysis of deviance table that partitions the deviance into components explained by each of the model terms.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;count-data---poisson-and-log-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Count data - Poisson and log-linear models&lt;/h2&gt;
&lt;p&gt;Another form of data for which scale transformations are often unsuitable or unsuccessful are count data. Count data tend to follow a Poisson distribution (see here) and consequently, the mean and variance are usually related. Generalized linear models provide appropriate means to model count data according to two design contexts:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;as an alternative to linear regression for modeling count data against a linear combination of continuous and/or categorical predictor variables (&lt;strong&gt;Poisson regression&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;as an alternative to contingency tables in which the associations between categorical variables are explored (&lt;strong&gt;log-linear modelling&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Poisson regression&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Poisson regression model is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \log(\mu)=\beta_0 + \beta_1x_1+ \ldots + \beta_px_p,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\log(\mu)\)&lt;/span&gt; is the link function used to link the mean of the Poisson response variable to the linear combination of predictor variables. Poisson regression otherwise shares null hypotheses, parameter estimation, model fitting and selection with logistic regression.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Log-linear modelling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Contingency tables were introduced along with caveats regarding the reliability and interoperability of such analyses (particularly when expected proportions are small or for multi-way tables). In contrast to logistic and Poisson regression, all variables in a log-linear model do not empirically distinguish between response and predictor variables. Nevertheless, as in contingency tables, causality can be implied when logical and justified by interpretation. The saturated (or full) log-linear model resembles a multiway ANOVA model. The full and reduced log-linear models for a two factor design are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \log(f_{ij}) = \mu + \gamma^A_i + \gamma^B_j + \gamma^{AB}_{ij} \;\;\; (\text{full model}),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \log(f_{ij}) = \mu + \gamma^A_i + \gamma^B_j \;\;\; (\text{reduced model})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\log(f_{ij}\)&lt;/span&gt; is the log link function, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the mean of the (log) of expected frequencies (&lt;span class=&#34;math inline&#34;&gt;\(f_{ij}\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(\gamma^A_i\)&lt;/span&gt; is the effect of the ith category of the variable (A), &lt;span class=&#34;math inline&#34;&gt;\(\gamma^B_j\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th category of B and &lt;span class=&#34;math inline&#34;&gt;\(\gamma^{AB}_{ij}\)&lt;/span&gt; is the interactive effect of each category combination on the (log) expected frequencies. Reduced models differ from full models in the absence of all higher order interaction terms. Comparing the fit of full and reduced models therefore provides a means of assessing the effect of the interaction. Whilst two-way tables contain only a single interaction term (and thus a single full and reduced model), multiway tables have multiple interactions. For example, a three-way table has a three way interaction (ABC) as well as three two-way interactions (AB, AC, BC). Consequently, there are numerous full and reduced models, each appropriate for different interaction terms. The following table indicates the association between null hypothesis and fitted models.&lt;/p&gt;
&lt;div id=&#34;null-hypothese&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Null hypothese&lt;/h3&gt;
&lt;p&gt;Consistent with contingency table analysis, log-linear models test the null hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;) that the categorical variables are independent of (not associated with) one another. Such null hypotheses are tested by comparing the fit (deviance, G2) of full and reduced models. The G2 is compared to a &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution with degrees of freedom equal to the difference in degrees of freedom of the full and reduced models. Thereafter, odds ratios are useful for interpreting any lack of independence. For multi-way tables, there are multiple full and reduced models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete dependence&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(H_0: ABC = 0\)&lt;/span&gt;. No three way interaction. Either no association (conditional independence) between each pair of variables, or else the patterns of associations (conditional dependencies) are the same for each level of the third. If this null hypothesis is rejected (&lt;span class=&#34;math inline&#34;&gt;\(ABC \neq 0\)&lt;/span&gt;), the causes of lack of independence can be explored by examining the residuals or odds ratios. Alternatively, main effects tests (testing the effects of two-way interactions separately at each level of the third) can be performed. If the three-way interaction is not rejected (no three-way association), lower order interactions can be explored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Conditional independence/dependence&lt;/strong&gt;: if the three-way interaction is not rejected (no three-way association), lower order interactions can be explored.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: AB=0\)&lt;/span&gt; - A and B conditionally independent (not associated) within each level of C.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: AC=0\)&lt;/span&gt; - A and C conditionally independent (not associated) within each level of B.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: BC=0\)&lt;/span&gt; - B and C conditionally independent (not associated) within each level of A.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Marginal independence&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: AB=0\)&lt;/span&gt; - no association between A and B pooling over C.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: AC=0\)&lt;/span&gt; - no association between A and C pooling over B.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: BC=0\)&lt;/span&gt; - no association between B and C pooling over A.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complete independence&lt;/strong&gt;: If none of the two-way interactions are rejected (no two-way associations), complete independence (all two-way interactions equal zero) can be explored.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: AB=AC=BC=0\)&lt;/span&gt; - Each of the variables are completely independent of all the other variables.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Analysis of designs with more than three factors proceed similarly, starting with tests of higher order interactions and progressing to lower order interactions only in the absence of higher order interactions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assumptions&lt;/h2&gt;
&lt;p&gt;Compared to general linear models, the requirements of generalised linear models are less stringent. In particular, neither normality nor homoscedasticity are assumed. Nevertheless, to maximize the reliability of null hypotheses tests, the following assumptions do apply:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;all observations should be &lt;strong&gt;independent&lt;/strong&gt; to ensure that the samples provide an unbiased estimate of the intended population.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;it is important to establish that no observations are overly influential. Most linear model &lt;strong&gt;influence&lt;/strong&gt; (and outlier) diagnostics extend to generalized linear models and are taken from the final iteration of the weighted least squares algorithm. Useful diagnoses include:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Residuals&lt;/em&gt; - there are numerous forms of residuals that have been defined for generalized linear models, each essentially being a variant on the difference between observed and predicted (influence in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-space) theme. Note that the residuals from logistic regression are difficult to interpret.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Leverage&lt;/em&gt; - a measure of outlyingness and influence in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-space.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Dfbeta&lt;/em&gt; - an analogue of Cook’s D statistic which provides a standardized measure of the overall influence of observations on the parameter estimates and model fit.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;although &lt;strong&gt;linearity&lt;/strong&gt; between the response and predictors is not assumed, the relationship between each of the predictors and the link function is assumed to be linear. This linearity can be examined via the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;goodness-of-fit&lt;/em&gt;. For log-linear models, &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; contingency tables can be performed, however due to the low reliability of such tests with small sample sizes, this is not an option for logistic regression with continuous predictor(s) (since each combination is typically unique and thus the expected values are always &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Hosmer-Lemeshow&lt;/em&gt; (&lt;span class=&#34;math inline&#34;&gt;\(\hat{C}\)&lt;/span&gt;). Data are aggregated into &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; groups or bins (either by cutting the data according to the predictor range or equal frequencies in each group) such that goodness-of-fit test is more reliable. Nevertheless, the Hosmer-Lemeshow statistic has low power and relies on the somewhat arbitrary bin sizes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;le Cessie-van Houwelingen-Copas omnibus test&lt;/em&gt;. This is a goodness-of-fit test for binary data based on the smoothing of residuals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;component + residual&lt;/em&gt; (partial residual) plots. Non-linearity is diagnosed as a substantial deviation from a linear trend.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Non-linearity can be dealt with either by transformation (of the predictor variable(s), fitting polynomial terms or via splines/generalised additive modelling (GAM) depending on the degree and nature of the non-linearity.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(over or under) &lt;strong&gt;dispersion&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-generation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data generation&lt;/h1&gt;
&lt;p&gt;Logistic regression is a type of generalised linear model (GLM) that models a binary response against a linear predictor via a specific link function. The linear predictor is the typically a linear combination of effects parameters (e.g. &lt;span class=&#34;math inline&#34;&gt;\(\beta_0+\beta_1x_1\)&lt;/span&gt;). The role of the link function is to transform the expected values of the response &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; (which is on the scale of (&lt;span class=&#34;math inline&#34;&gt;\(0,1\)&lt;/span&gt;), as is the binomial distribution from which expectations are drawn) into the scale of the linear predictor (which is &lt;span class=&#34;math inline&#34;&gt;\(-\infty;\infty\)&lt;/span&gt;). GLM’s transform the expected values (via a link) whereas LM’s transform the observed data. Thus while GLM’s operate on the scale of the original data and yet also on a scale appropriate of the residuals, LM’s do neither. There are many ways (transformations) that can map values on the (&lt;span class=&#34;math inline&#34;&gt;\(0,1\)&lt;/span&gt;) scale into values on the (&lt;span class=&#34;math inline&#34;&gt;\(-\infty;\infty\)&lt;/span&gt;) scale, however, the three most common are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;logit: &lt;span class=&#34;math inline&#34;&gt;\(\log\left(\frac{\pi}{1-\pi}\right)\)&lt;/span&gt; - log odds ratio.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;probit: &lt;span class=&#34;math inline&#34;&gt;\(\phi^{-1}(\pi)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\phi^{-1}\)&lt;/span&gt; is an inverse normal cumulative density function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;complimentary log-log: &lt;span class=&#34;math inline&#34;&gt;\(\log(−\log(1−\pi))\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lets say we wanted to model the presence/absence of an item (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) against a continuous predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;) As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; set.seed(865)
&amp;gt; #The number of samples
&amp;gt; n.x &amp;lt;- 20
&amp;gt; #Create x values that at uniformly distributed throughout the rate of 1 to 20
&amp;gt; x &amp;lt;- sort(runif(n = n.x, min = 1, max =20))
&amp;gt; #The slope is the rate of change in log odds ratio for each unit change in x
&amp;gt; # the smaller the slope, the slower the change (more variability in data too)
&amp;gt; slope=0.5
&amp;gt; #Inflection point is where the slope of the line is greatest
&amp;gt; #this is also the LD50 point
&amp;gt; inflect &amp;lt;- 10
&amp;gt; #Intercept (no interpretation)
&amp;gt; intercept &amp;lt;- -1*(slope*inflect)
&amp;gt; #The linear predictor
&amp;gt; linpred &amp;lt;- intercept+slope*x
&amp;gt; #Predicted y values
&amp;gt; y.pred &amp;lt;- exp(linpred)/(1+exp(linpred))
&amp;gt; #Add some noise and make binomial
&amp;gt; n.y &amp;lt;-rbinom(n=n.x,20,p=0.9)
&amp;gt; y&amp;lt;- rbinom(n = n.x,size=1, prob = y.pred)
&amp;gt; dat &amp;lt;- data.frame(y,x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these sort of data, we are primarily interested in investigating whether there is a relationship between the binary response variable and the linear predictor (linear combination of one or more continuous or categorical predictors).&lt;/p&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;So lets explore linearity by creating a histogram of the predictor variable (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;) and a scatterplot of the relationship between the response (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) and the predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; hist(dat$x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/expl_data1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; #now for the scatterplot
&amp;gt; plot(y~x, dat)
&amp;gt; with(dat, lines(lowess(y~x)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/expl_data1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: the predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the scatterplot does not display major deviations from a standard sigmoidal curve and thus linearity is satisfied. Violations of linearity could be addressed by either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;define a non-linear linear predictor (such as a polynomial, spline or other non-linear function).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;transform the scale of the predictor variables.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Effects model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Note that in order to prevent arithmetic overflows (particularly with the clog-log model, I am going to constrain the estimated linear predictor to between &lt;span class=&#34;math inline&#34;&gt;\(-20\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(20\)&lt;/span&gt;. Values outside of this on a inverse-log scale are extremely small and huge respectively.
I will demonstrate logistic regression with a range of possible link functions (each of which yield different parameter interpretations). Consider first the logit function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y \sim \text{Bern}(\pi),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{logit}(\pi)=\beta_0+\beta_1x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0,\beta_1 \sim N(0, 10000)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; modelString=&amp;quot;
+ model{
+   for (i in 1:N) {
+     y[i] ~ dbern(p[i])
+     logit(p[i]) &amp;lt;- max(-20,min(20,beta0+beta1*x[i]))
+   }
+   beta0 ~ dnorm(0,1.0E-06)
+   beta1 ~ dnorm(0,1.0E-06)
+ }
+ &amp;quot;
&amp;gt; writeLines(modelString, con=&amp;#39;modellogit.txt&amp;#39;)
&amp;gt; 
&amp;gt; dat.list &amp;lt;- with(dat, list(y=y, x=x, N=nrow(dat)))
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; library(R2jags)
&amp;gt; dat.logit.jags &amp;lt;- jags(data=dat.list,model.file=&amp;#39;modellogit.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 2
   Total graph size: 147

Initializing model&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we consider the probit function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y \sim \text{Bern}(\pi),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{probit}(\pi)=\beta_0+\beta_1x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0,\beta_1 \sim N(0, 10000)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; modelString2=&amp;quot;
+ model{
+   for (i in 1:N) {
+     y[i] ~ dbern(p[i])
+     probit(p[i]) &amp;lt;- max(-20,min(20,beta0+beta1*x[i]))
+   }
+   beta0 ~ dnorm(0,1.0E-06)
+   beta1 ~ dnorm(0,1.0E-06)
+ }
+ &amp;quot;
&amp;gt; writeLines(modelString2, con=&amp;#39;modelprobit.txt&amp;#39;)
&amp;gt; 
&amp;gt; dat.list &amp;lt;- with(dat, list(y=y, x=x, N=nrow(dat)))
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.probit.jags &amp;lt;- jags(data=dat.list,model.file=&amp;#39;modelprobit.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 2
   Total graph size: 147

Initializing model&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the complementary log-log&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y \sim \text{Bern}(\pi),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{probit}(\pi)=\beta_0+\beta_1x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0,\beta_1 \sim N(0, 10000)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; modelString3=&amp;quot;
+ model{
+   for (i in 1:N) {
+     y[i] ~ dbern(p[i])
+     cloglog(p[i]) &amp;lt;- max(-20,min(20,beta0+beta1*x[i]))
+   }
+   beta0 ~ dnorm(0,1.0E-06)
+   beta1 ~ dnorm(0,1.0E-06)
+ }
+ &amp;quot;
&amp;gt; writeLines(modelString3, con=&amp;#39;modelcloglog.txt&amp;#39;)
&amp;gt; 
&amp;gt; dat.list &amp;lt;- with(dat, list(y=y, x=x, N=nrow(dat)))
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.cloglog.jags &amp;lt;- jags(data=dat.list,model.file=&amp;#39;modelcloglog.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 2
   Total graph size: 147

Initializing model&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prior to exploring the model parameters, it is prudent to confirm that the model did indeed fit the assumptions and was an appropriate fit to the data as well as that the MCMC sampling chain was adequately mixed and the retained samples independent. Whilst I will only demonstrate this for the logit model, the procedure would be identical for exploring the probit and clog-log models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-evaluation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model evaluation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(mcmcplots)
&amp;gt; denplot(dat.logit.jags, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/mcmc_diag-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(dat.logit.jags, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/mcmc_diag-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; raftery.diag(as.mcmc(dat.logit.jags))
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta0    50       54338 3746         14.50     
 beta1    36       39555 3746         10.60     
 deviance 4        4955  3746          1.32     


[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta0    30       31743 3746          8.47     
 beta1    40       52860 3746         14.10     
 deviance 8        10336 3746          2.76     
&amp;gt; 
&amp;gt; autocorr.diag(as.mcmc(dat.logit.jags))
           beta0     beta1  deviance
Lag 0  1.0000000 1.0000000 1.0000000
Lag 1  0.9816715 0.9811729 0.5841946
Lag 5  0.9190319 0.9197111 0.4477029
Lag 10 0.8458674 0.8477906 0.3948904
Lag 50 0.4300407 0.4306464 0.2065881&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that the level of auto-correlation at the nominated lag of &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; is extremely high. Ideally, the level of auto-correlation should be less than &lt;span class=&#34;math inline&#34;&gt;\(0.1\)&lt;/span&gt;. To achieve this, we need a lag of &lt;span class=&#34;math inline&#34;&gt;\(1000\)&lt;/span&gt;. Consequently, we will resample at a lag of &lt;span class=&#34;math inline&#34;&gt;\(1000\)&lt;/span&gt; and obviously we are going to need more iterations to ensure that we retain a large enough sample from which to derive estimates. In order to support a thinning rate of &lt;span class=&#34;math inline&#34;&gt;\(1000\)&lt;/span&gt;, the number of iterations is going to need to be very high. Hence, the following might take considerable time to run.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; dat.logit.jags &amp;lt;- jags(data=dat.list,model.file=&amp;#39;modellogit.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=100)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 2
   Total graph size: 147

Initializing model
&amp;gt; 
&amp;gt; print(dat.logit.jags)
Inference for Bugs model at &amp;quot;modellogit.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded), n.thin = 100
 n.sims = 100 iterations saved
         mu.vect sd.vect    2.5%     25%     50%    75%  97.5%  Rhat n.eff
beta0     -16.51   9.652 -40.133 -20.118 -14.170 -9.587 -5.463 1.040    58
beta1       1.66   0.973   0.458   0.979   1.427  2.032  3.926 1.026   100
deviance    9.94   2.764   7.457   8.161   8.942 10.678 16.848 1.024   100

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 3.8 and DIC = 13.8
DIC is an estimate of expected predictive error (lower deviance is better).
&amp;gt; 
&amp;gt; autocorr.diag(as.mcmc(dat.logit.jags))
              beta0      beta1   deviance
Lag 0     1.0000000  1.0000000  1.0000000
Lag 100   0.4435502  0.4390086  0.1529258
Lag 500   0.1102886  0.1246140  0.1950554
Lag 1000 -0.1091505 -0.1008427 -0.1582021&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: the samples are now less auto-correlated and the chains are arguably mixed better. We now explore the goodness of fit of the models via the residuals and deviance. We could calculate the Pearsons’s residuals within the &lt;code&gt;JAGS&lt;/code&gt; model. Alternatively, we could use the parameters to generate the residuals outside of &lt;code&gt;JAGS&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(boot)
&amp;gt; coefs &amp;lt;- dat.logit.jags$BUGSoutput$sims.matrix[,1:2]
&amp;gt; Xmat &amp;lt;- model.matrix(~x, data=dat)
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; pi &amp;lt;- inv.logit(eta)
&amp;gt; #sweep across rows and then divide by pi
&amp;gt; Resid &amp;lt;- -1*sweep(pi,2,dat$y,&amp;#39;-&amp;#39;)/sqrt(pi*(1-pi))
&amp;gt; plot(apply(Resid,2,mean)~apply(eta,2,mean))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/model2_mcmc_res-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Bernoulli distribution matching that estimated by the model. Essentially this is estimating how well the Bernoulli distribution and linear model approximates the observed data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; SSres&amp;lt;-apply(Resid^2,1,sum)
&amp;gt; 
&amp;gt; #generate a matrix of draws from a binomial distribution
&amp;gt; # the matrix is the same dimensions as pi and uses the probabilities of pi
&amp;gt; YNew &amp;lt;- matrix(rbinom(length(pi),prob=pi,size=1),nrow=nrow(pi))
&amp;gt; 
&amp;gt; Resid1&amp;lt;-(pi - YNew)/sqrt(pi*(1-pi))
&amp;gt; SSres.sim&amp;lt;-apply(Resid1^2,1,sum)
&amp;gt; mean(SSres.sim&amp;gt;SSres, na.rm = T)
[1] 0.21875&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, we could generate the new samples and calculate the sums squares of residuals etc all within &lt;code&gt;JAGS&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; dat.list &amp;lt;- with(dat, list(y=y, x=x, N=nrow(dat)))
&amp;gt; modelString=&amp;quot;
+ model{
+   for (i in 1:N) {
+     y[i] ~ dbern(p[i])
+     logit(p[i]) &amp;lt;- max(-20,min(20,eta[i]))
+     eta[i] &amp;lt;- beta0+beta1*x[i]
+     YNew[i] ~dbern(p[i])
+     varY[i] &amp;lt;- p[i]*(1-p[i])
+     PRes[i] &amp;lt;- (y[i] - p[i]) / sqrt(varY[i])
+     PResNew[i] &amp;lt;- (YNew[i] - p[i]) / sqrt(varY[i])
+     D[i] &amp;lt;- pow(PRes[i],2)
+     DNew[i] &amp;lt;- pow(PResNew[i],2)
+   }
+   Fit &amp;lt;- sum(D[1:N])
+   FitNew &amp;lt;-sum(DNew[1:N]) 
+   beta0 ~ dnorm(0,1.0E-06)
+   beta1 ~ dnorm(0,1.0E-06)
+   pvalue &amp;lt;- mean(FitNew&amp;gt;Fit)
+ }
+ &amp;quot;
&amp;gt; writeLines(modelString, con=&amp;#39;modellogit_v2.txt&amp;#39;)
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;,&amp;#39;Fit&amp;#39;,&amp;#39;FitNew&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.logit.jags1 &amp;lt;- jags(data=dat.list,model.file=&amp;#39;modellogit_v2.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 20
   Unobserved stochastic nodes: 22
   Total graph size: 343

Initializing model
&amp;gt; 
&amp;gt; print(dat.logit.jags1)
Inference for Bugs model at &amp;quot;modellogit_v2.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
Fit       38.470 339.283   6.122   8.237  12.604  24.120 186.621 1.013   540
FitNew    15.837 230.080   0.395   2.025   3.780   8.372  63.411 1.001  3800
beta0    -15.507   7.665 -35.657 -19.433 -13.831 -10.040  -4.873 1.024   660
beta1      1.570   0.791   0.501   0.991   1.390   1.979   3.656 1.017 10000
deviance   9.678   2.175   7.482   8.070   9.018  10.581  15.412 1.013   220

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 2.4 and DIC = 12.0
DIC is an estimate of expected predictive error (lower deviance is better).
&amp;gt; 
&amp;gt; out &amp;lt;- dat.logit.jags1$BUGSoutput
&amp;gt; mean(out$sims.list$FitNew &amp;gt; out$sims.list$Fit)
[1] 0.1947&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: although the Bayesian p-value is quite a bit lower than &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;, suggesting that there is more variability in the data than should be expected from this simple logistic regression model, this value is not any closer to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; (a value that would indicate that the model does not fit the data at all well. Thus we might conclude that whilst not ideal, the model is adequate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-model-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring the model parameters&lt;/h2&gt;
&lt;p&gt;If there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(coda)
&amp;gt; print(dat.logit.jags)
Inference for Bugs model at &amp;quot;modellogit.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded), n.thin = 100
 n.sims = 100 iterations saved
         mu.vect sd.vect    2.5%     25%     50%    75%  97.5%  Rhat n.eff
beta0     -16.51   9.652 -40.133 -20.118 -14.170 -9.587 -5.463 1.040    58
beta1       1.66   0.973   0.458   0.979   1.427  2.032  3.926 1.026   100
deviance    9.94   2.764   7.457   8.161   8.942 10.678 16.848 1.024   100

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 3.8 and DIC = 13.8
DIC is an estimate of expected predictive error (lower deviance is better).
&amp;gt; 
&amp;gt; library(plyr)
&amp;gt; adply(dat.logit.jags$BUGSoutput$sims.matrix[,1:2], 2, function(x) {
+   data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))
+ })
     X1     Median       Mean       lower     upper    lower.1   upper.1
1 beta0 -14.169526 -16.510277 -38.4322729 -2.190571 -15.809670 -6.767604
2 beta1   1.427161   1.660376   0.3019023  3.728819   0.866335  1.791501&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: We would reject the null hypothesis (p&lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;0.05\)&lt;/span&gt;). An increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is associated with a significant linear increase (positive slope) in log odds of y success. Every &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; results in a &lt;span class=&#34;math inline&#34;&gt;\(0.86\)&lt;/span&gt; unit increase in log odds-ratio. We usually express this in terms of odds-ratio rather than log odds-ratio, so every &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; results in a (&lt;span class=&#34;math inline&#34;&gt;\(e^{0.86}=2.36\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(2.36\)&lt;/span&gt; unit increase in odds-ratio.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explorations-of-the-trends&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Explorations of the trends&lt;/h2&gt;
&lt;p&gt;We might also be interested in the LD50 - the value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; where the probability switches from favoring &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; to favoring &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. LD50 is calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ LD50 = - \frac{\text{intercept}}{\text{slope}}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; summary(as.mcmc(-coefs[,1]/coefs[,2]))

Iterations = 1:100
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 100 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

          Mean             SD       Naive SE Time-series SE 
       9.92488        0.84980        0.08498        0.06916 

2. Quantiles for each variable:

  2.5%    25%    50%    75%  97.5% 
 7.737  9.460  9.894 10.448 11.538 
&amp;gt; 
&amp;gt; #OR
&amp;gt; LD50 &amp;lt;- -coefs[,1]/coefs[,2]
&amp;gt; data.frame(Median=median(LD50), Mean=mean(LD50), HPDinterval(as.mcmc(LD50)), HPDinterval(as.mcmc(LD50),p=0.5))
       Median     Mean    lower    upper  lower.1  upper.1
var1 9.894002 9.924877 7.930942 11.59808 9.547373 10.50285&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: the LD50 is &lt;span class=&#34;math inline&#34;&gt;\(10.5\)&lt;/span&gt;. Finally, we will create a summary plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; par(mar = c(4, 5, 0, 0))
&amp;gt; plot(y ~ x, data = dat, type = &amp;quot;n&amp;quot;, ann = F, axes = F)
&amp;gt; points(y ~ x, data = dat, pch = 16)
&amp;gt; xs &amp;lt;- seq(0, 20, l = 1000)
&amp;gt; 
&amp;gt; Xmat &amp;lt;- model.matrix(~xs)
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; ys &amp;lt;- inv.logit(eta)
&amp;gt; library(plyr)
&amp;gt; data.tab &amp;lt;- adply(ys,2,function(x) {
+   data.frame(Median=median(x), HPDinterval(as.mcmc(x)))
+ })
&amp;gt; data.tab &amp;lt;- cbind(x=xs,data.tab)
&amp;gt; 
&amp;gt; points(Median ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;)
&amp;gt; lines(lower ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;, lty = 2)
&amp;gt; lines(upper ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;, lty = 2)
&amp;gt; axis(1)
&amp;gt; mtext(&amp;quot;X&amp;quot;, 1, cex = 1.5, line = 3)
&amp;gt; axis(2, las = 2)
&amp;gt; mtext(&amp;quot;Y&amp;quot;, 2, cex = 1.5, line = 3)
&amp;gt; box(bty = &amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/model_code_v2_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;grouped-binary-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Grouped binary data&lt;/h1&gt;
&lt;p&gt;In the previous demonstration, the response variable represented the state of a single item per level of the predictor variable (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;). That single item could be observed having a value of either &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. Another common situation is to observe the number of items in one of two states (typically dead or alive) for each level of a treatment. For example, you could tally up the number of germinated and non-germinated seeds out of a bank of &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; seeds at each of &lt;span class=&#34;math inline&#34;&gt;\(8\)&lt;/span&gt; temperature or nutrient levels. Recall that the binomial distribution represents the density (probability) of all possible successes (germinations) out of a total of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; items (seeds). Hence the binomial distribution is also a suitable error distribution for such grouped binary data. For this demonstration, we will model the number of successes against a uniformly distributed predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;). The number of trials in each group (level of the predictor) will vary slightly (yet randomly) so as to mimick complications that inevadably occur in real experiments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; set.seed(876)
&amp;gt; #The number of levels of x
&amp;gt; n.x &amp;lt;- 10
&amp;gt; #Create x values that at uniformly distributed throughout the rate of 10 to 20
&amp;gt; x &amp;lt;- sort(runif(n = n.x, min = 10, max =20))
&amp;gt; #The slope is the rate of change in log odds ratio for each unit change in x
&amp;gt; # the smaller the slope, the slower the change (more variability in data too)
&amp;gt; slope=-.25
&amp;gt; #Inflection point is where the slope of the line is greatest
&amp;gt; #this is also the LD50 point
&amp;gt; inflect &amp;lt;- 15
&amp;gt; #Intercept (no interpretation)
&amp;gt; intercept &amp;lt;- -1*(slope*inflect)
&amp;gt; #The linear predictor
&amp;gt; linpred &amp;lt;- intercept+slope*x
&amp;gt; #Predicted y values
&amp;gt; y.pred &amp;lt;- exp(linpred)/(1+exp(linpred))
&amp;gt; #Add some noise and make binary (0&amp;#39;s and 1&amp;#39;s)
&amp;gt; n.trial &amp;lt;- rbinom(n=n.x,20, prob=0.9)
&amp;gt; success &amp;lt;- rbinom(n = n.x, size = n.trial,prob = y.pred)
&amp;gt; failure &amp;lt;- n.trial - success
&amp;gt; dat &amp;lt;- data.frame(success,failure,x)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exploratory-data-analysis-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;So lets explore linearity by creating a histogram of the predictor variable (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;) and a scatterplot of the relationship between the either the number of successes (success) or the number of (failures) and the predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;). Note, that this will not account for the differences in trial size per group and so a scatterplot of the relationship between the number of successes (success) or the number of (failures) divided by the total number of trials against the predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;) might be more appropriate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; hist(dat$x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/expl_data1_gbin-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; #now for the scatterplot
&amp;gt; plot(success~x, dat)
&amp;gt; with(dat, lines(lowess(success~x)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/expl_data1_gbin-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; #scatterplot standardised for trial size
&amp;gt; plot(success/(success+failure)~x, dat)
&amp;gt; with(dat, lines(lowess(success/(success+failure)~x)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/expl_data1_gbin-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: the predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;) does not display any skewness (although it is not all that uniform - random data) or other issues that might lead to non-linearity. The lowess smoother on either scatterplot does not display major deviations from a standard sigmoidal curve and thus linearity is likely to be satisfied. Violations of linearity could be addressed by either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;define a non-linear linear predictor (such as a polynomial, spline or other non-linear function).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;transform the scale of the predictor variables.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;Clearly the number of successes is also dependent on the number of trials. Larger numbers of trials might be expected to yeild higher numbers of successes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; dat.list &amp;lt;- with(dat, list(success=success, total=success+failure, x=x, N=nrow(dat)))
&amp;gt; modelString=&amp;quot;
+ model{
+   for (i in 1:N) {
+     success[i] ~ dbin(p[i],total[i])
+     logit(p[i]) &amp;lt;- max(-20,min(20,beta0+beta1*x[i]))
+   }
+   beta0 ~ dnorm(0,1.0E-06)
+   beta1 ~ dnorm(0,1.0E-06)
+ }
+ &amp;quot;
&amp;gt; writeLines(modelString, con=&amp;#39;modelgbin.txt&amp;#39;)
&amp;gt; 
&amp;gt; params &amp;lt;- c(&amp;#39;beta0&amp;#39;,&amp;#39;beta1&amp;#39;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 20000
&amp;gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)
&amp;gt; 
&amp;gt; dat.logit.jags &amp;lt;- jags(data=dat.list,model.file=&amp;#39;modelgbin.txt&amp;#39;, param=params,
+                    n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 10
   Unobserved stochastic nodes: 2
   Total graph size: 87

Initializing model&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with the logistic regression presented earlier, we could alternatively use probit or clog-log link functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-evaluation-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model evaluation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; denplot(dat.logit.jags, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/mcmc_diag_gbin-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(dat.logit.jags, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/mcmc_diag_gbin-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; raftery.diag(as.mcmc(dat.logit.jags))
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta0    46       50468 3746         13.50     
 beta1    90       98698 3746         26.30     
 deviance 6        8920  3746          2.38     


[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                 
          Burn-in  Total  Lower bound  Dependence
          (M)      (N)    (Nmin)       factor (I)
 beta0    84       103188 3746         27.50     
 beta1    52       58312  3746         15.60     
 deviance 8        9488   3746          2.53     
&amp;gt; 
&amp;gt; autocorr.diag(as.mcmc(dat.logit.jags))
           beta0     beta1   deviance
Lag 0  1.0000000 1.0000000 1.00000000
Lag 1  0.9830416 0.9831425 0.56062724
Lag 5  0.9248140 0.9256704 0.42678260
Lag 10 0.8543024 0.8555131 0.36633408
Lag 50 0.4631353 0.4636323 0.07250394&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets explore the diagnostics - particularly the residuals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; inv.logit &amp;lt;- binomial()$linkinv
&amp;gt; #Calculate residuals
&amp;gt; coefs &amp;lt;- dat.logit.jags$BUGSoutput$sims.matrix[,1:2]
&amp;gt; Xmat &amp;lt;- model.matrix(~x, data=dat)
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; pi &amp;lt;- inv.logit(eta)
&amp;gt; #sweep across rows and then divide by pi
&amp;gt; Resid &amp;lt;- -1*sweep(pi,2,dat$success/(dat$success+dat$failure),&amp;#39;-&amp;#39;)/sqrt(pi*(1-pi))
&amp;gt; plot(apply(Resid,2,mean)~apply(eta,2,mean))
&amp;gt; lines(lowess(apply(Resid,2,mean)~apply(eta,2,mean)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/model2_mcmc_res_gbin-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: there is no obvious patterns in the residuals, or at least there are no obvious trends remaining that would be indicative of non-linearity.&lt;/p&gt;
&lt;p&gt;Now we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Bernoulli distribution matching that estimated by the model. Essentially this is estimating how well the Bernoulli distribution and linear model approximates the observed data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; SSres&amp;lt;-apply(Resid^2,1,sum)
&amp;gt; 
&amp;gt; #generate a matrix of draws from a binomial distribution
&amp;gt; #the matrix is the same dimensions as pi and uses the probabilities of pi
&amp;gt; YNew &amp;lt;- matrix(rbinom(length(pi),prob=pi,size=(dat$success+dat$failure)),nrow=nrow(pi))
&amp;gt; Resid1 &amp;lt;- 1*(pi-YNew/(dat$success+dat$failure))/sqrt(pi*(1-pi))
&amp;gt; SSres.sim&amp;lt;-apply(Resid1^2,1,sum)
&amp;gt; mean(SSres.sim&amp;gt;SSres, na.rm=T)
[1] 0.4559&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: this Bayesian p-value is reasonably close to &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;. Therefore we would conclude that there was no strong evidence for a lack of fit of the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explorations-of-the-trends-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Explorations of the trends&lt;/h2&gt;
&lt;p&gt;We might also be interested in the LD50 - the value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; where the probability switches from favoring &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; to favoring &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. LD50 is calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ LD50 = - \frac{\text{intercept}}{\text{slope}}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; summary(as.mcmc(-coefs[,1]/coefs[,2]))

Iterations = 1:10000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 10000 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

          Mean             SD       Naive SE Time-series SE 
      12.80838        6.30455        0.06305        0.05732 

2. Quantiles for each variable:

 2.5%   25%   50%   75% 97.5% 
10.09 12.45 13.08 13.58 14.41 
&amp;gt; 
&amp;gt; #OR
&amp;gt; LD50 &amp;lt;- -coefs[,1]/coefs[,2]
&amp;gt; data.frame(Median=median(LD50), Mean=mean(LD50), HPDinterval(as.mcmc(LD50)), HPDinterval(as.mcmc(LD50),p=0.5))
       Median     Mean    lower    upper  lower.1  upper.1
var1 13.08204 12.80838 10.76017 14.74202 12.71997 13.79013&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: the LD50 is &lt;span class=&#34;math inline&#34;&gt;\(13.1\)&lt;/span&gt;. Finally, we will create a summary plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; par(mar = c(4, 5, 0, 0))
&amp;gt; plot(success/(success+failure) ~ x, data = dat, type = &amp;quot;n&amp;quot;, ann = F, axes = F)
&amp;gt; points(success/(success+failure) ~ x, data = dat, pch = 16)
&amp;gt; xs &amp;lt;- seq(min(dat$x, na.rm=TRUE),max(dat$x, na.rm=TRUE), l = 1000)
&amp;gt; 
&amp;gt; Xmat &amp;lt;- model.matrix(~xs)
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; ys &amp;lt;- inv.logit(eta)
&amp;gt; data.tab &amp;lt;- adply(ys,2,function(x) {
+   data.frame(Median=median(x), HPDinterval(as.mcmc(x)))
+ })
&amp;gt; data.tab &amp;lt;- cbind(x=xs,data.tab)
&amp;gt; 
&amp;gt; points(Median ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;)
&amp;gt; with(data.tab,polygon(c(x,rev(x)),c(lower,rev(upper)), col=&amp;quot;#0000ff60&amp;quot;, border=NA))
&amp;gt; #lines(lower ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;, lty = 2)
&amp;gt; #lines(upper ~ x, data=data.tab,col = &amp;quot;black&amp;quot;, type = &amp;quot;l&amp;quot;, lty = 2)
&amp;gt; axis(1)
&amp;gt; mtext(&amp;quot;X&amp;quot;, 1, cex = 1.5, line = 3)
&amp;gt; axis(2, las = 2)
&amp;gt; mtext(&amp;quot;Probability of success&amp;quot;, 2, cex = 1.5, line = 3)
&amp;gt; box(bty = &amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/model_code_v2_plot_gbin-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; #or via ggplot
&amp;gt; 
&amp;gt; xs &amp;lt;- seq(min(dat$x, na.rm=TRUE),max(dat$x, na.rm=TRUE), l = 1000)
&amp;gt; Xmat &amp;lt;- model.matrix(~xs)
&amp;gt; eta&amp;lt;-coefs %*% t(Xmat)
&amp;gt; library(boot)
&amp;gt; ys &amp;lt;- inv.logit(eta)
&amp;gt; library(plyr)
&amp;gt; data.tab &amp;lt;- adply(ys,2,function(x) {
+   data.frame(Median=median(x), HPDinterval(as.mcmc(x)))
+ })
&amp;gt; data.tab &amp;lt;- cbind(x=xs,data.tab)
&amp;gt; 
&amp;gt; library(ggplot2)
&amp;gt; library(grid)
&amp;gt; dat$p &amp;lt;- with(dat, success/(success+failure))
&amp;gt; p1 &amp;lt;- ggplot(data.tab,aes(y=Median, x=x)) + geom_point(data=dat,aes(y=p, x=x),color=&amp;quot;gray40&amp;quot;)+
+              geom_smooth(aes(ymin=lower, ymax=upper), stat=&amp;quot;identity&amp;quot;)+
+                          scale_x_continuous(&amp;quot;X&amp;quot;)+scale_y_continuous(&amp;quot;Probability of success&amp;quot;)
&amp;gt; p1+theme(panel.grid.major=element_blank(),
+          panel.grid.minor=element_blank(),
+          panel.border=element_blank(),
+          panel.background=element_blank(),
+                  axis.title.y=element_text(size=15,vjust=0,angle=90),
+                  axis.title.x=element_text(size=15,vjust=-1),
+                  axis.text.y=element_text(size=12),
+                  axis.text.x=element_text(size=12),
+                  axis.line=element_line(),
+                  plot.margin=unit(c(0.5,0.5,2,2), &amp;quot;lines&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/glm-jags/2020-02-01-glm-jags_files/figure-html/model_code_v2_plot_gbin-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-plummer2004jags&#34;&gt;
&lt;p&gt;Plummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-su2015package&#34;&gt;
&lt;p&gt;Su, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” &lt;em&gt;R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
