<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>STAN on Andrea Gabrio</title>
    <link>/tags/stan/</link>
    <description>Recent content in STAN on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 02 Feb 2020 21:13:14 -0500</lastBuildDate>
    
	    <atom:link href="/tags/stan/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Simple Linear Regression - STAN</title>
      <link>/stan/simple-linear-regression-stan/simple-linear-regression-stan/</link>
      <pubDate>Sun, 02 Feb 2020 21:13:14 -0500</pubDate>
      
      <guid>/stan/simple-linear-regression-stan/simple-linear-regression-stan/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. &lt;code&gt;BUGS&lt;/code&gt; (Bayesian inference Using &lt;em&gt;Gibbs Sampling&lt;/em&gt;) is an algorithm and supporting language (resembling &lt;code&gt;R&lt;/code&gt;) dedicated to performing the Gibbs sampling implementation of &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; (MCMC) method. Dialects of the &lt;code&gt;BUGS&lt;/code&gt; language are implemented within three main projects:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OpenBUGS&lt;/strong&gt; - written in component pascal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;JAGS&lt;/strong&gt; - (Just Another Gibbs Sampler) - written in &lt;code&gt;C++&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;STAN&lt;/strong&gt; - a dedicated Bayesian modelling framework written in &lt;code&gt;C++&lt;/code&gt; and implementing &lt;em&gt;Hamiltonian&lt;/em&gt; MCMC samplers.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of &lt;code&gt;R&lt;/code&gt;, and thus, they are best accessed from within &lt;code&gt;R&lt;/code&gt; itself. As such there are multiple packages devoted to interfacing with the various software implementations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2OpenBUGS&lt;/em&gt; - interfaces with &lt;code&gt;OpenBUGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2jags&lt;/em&gt; - interfaces with &lt;code&gt;JAGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;rstan&lt;/em&gt; - interfaces with &lt;code&gt;STAN&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial will demonstrate how to fit models in &lt;code&gt;STAN&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Gelman, Lee, and Guo (2015)&lt;/span&gt;) using the package &lt;code&gt;rstan&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Stan Development Team (2018)&lt;/span&gt;) as interface, which also requires to load some other packages.&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Many clinicians get a little twitchy and nervous around mathematical and statistical formulae and nomenclature. Whilst it is possible to perform basic statistics without too much regard for the actual equation (model) being employed, as the complexity of the analysis increases, the need to understand the underlying model becomes increasingly important. Moreover, model specification in &lt;code&gt;BUGS/JAGS/STAN&lt;/code&gt; (the language used to program Bayesian modelling) aligns very closely to the underlying formulae. Hence a good understanding of the underlying model is vital to be able to create a sensible Bayesian model. Consequently, I will always present the linear model formulae along with the analysis.&lt;/p&gt;
&lt;p&gt;To introduce the philosophical and mathematical differences between classical (frequentist) and Bayesian statistics, based on previous works, we present a provocative yet compelling trend analysis of two hypothetical populations (A vs B). The temporal trend of population A shows very little variability from a very subtle linear decline (&lt;span class=&#34;math inline&#34;&gt;\(n=10\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{slope}=-0.10\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{p-value}=0.048\)&lt;/span&gt;). By contrast, the B population appears to decline more dramatically, yet has substantially more variability (&lt;span class=&#34;math inline&#34;&gt;\(n=10\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{slope}=-10.23\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{p-value}=0.058\)&lt;/span&gt;). From a traditional frequentist perspective, we would conclude that there is a “significant” relationship in Population A (&lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;0.05\)&lt;/span&gt;), yet not in Population B (&lt;span class=&#34;math inline&#34;&gt;\(p&amp;gt;0.05\)&lt;/span&gt;). However, if we consider a third population C which is exactly the same as populstion B but with a higher number of observations, then we may end up with a completely different conclusion compared with that based on population B (&lt;span class=&#34;math inline&#34;&gt;\(n=100\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{slope}=-10.47\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{p-value}&amp;lt;0.001\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The above illustrates a couple of things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;statistical significance does not necessarily translate into clinical importance. Indeed, population B is declining at nearly &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; times the rate of population A. That sounds rather important, yet on the basis of the hypothesis test, we would dismiss the decline in population B.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;that a p-value is just the probability of detecting an effect or relationship - what is the probability that the sample size is large enough to pick up a difference.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let us now look at it from a Bayesian perspective, with a focus on population A and B. We would conclude that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the mean (plus or minus CI) slopes for Population A and B are &lt;span class=&#34;math inline&#34;&gt;\(-0.1 (-0.21,0)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-10.08 (-20.32,0.57)\)&lt;/span&gt; respectively&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the Bayesian approach allows us to query the posterior distribution is many other ways in order to ask sensible clinical questions. For example, we might consider that a rate of change of &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;% or greater represents an important biological impact. For population A and B, the probability that the rate is &lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;% or greater is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0.85\)&lt;/span&gt; respectively.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear regression&lt;/h2&gt;
&lt;p&gt;Simple linear regression is a linear modelling process that models a continuous response against a single continuous predictor. The linear model is expressed as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i = \beta_0 + \beta_1x_i + \epsilon_i, \;\;\; \epsilon_i \sim \text{Normal}(0,\sigma),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is the response variable for each of the &lt;span class=&#34;math inline&#34;&gt;\(i=1\ldots,n\)&lt;/span&gt; observations, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the intercept (value when &lt;span class=&#34;math inline&#34;&gt;\(x=0\)&lt;/span&gt;), &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the slope (rate of change in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; per unit change in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;), &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is the predictor variable, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; is the residual value (difference between the observed value and the value expected by the model). The parameters of the trendline &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol \beta=(\beta_0,\beta_1)\)&lt;/span&gt; are determined by &lt;em&gt;Ordinary Least Squares&lt;/em&gt; (OLS) in which the sum of the squared residuals is minimized. A non-zero population slope is indicative of a relationship.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-generation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data generation&lt;/h1&gt;
&lt;p&gt;Lets say we had set up an experiment in which we applied a continuous treatment (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;) ranging in magnitude from &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(16\)&lt;/span&gt; to a total of &lt;span class=&#34;math inline&#34;&gt;\(16\)&lt;/span&gt; sampling units (&lt;span class=&#34;math inline&#34;&gt;\(n=16\)&lt;/span&gt;) and then measured a response (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) from each unit. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; set.seed(123)
&amp;gt; n &amp;lt;- 16
&amp;gt; a &amp;lt;- 40  #intercept
&amp;gt; b &amp;lt;- -1.5  #slope
&amp;gt; sigma2 &amp;lt;- 25  #residual variance (sd=5)
&amp;gt; x &amp;lt;- 1:n  #values of the year covariate
&amp;gt; eps &amp;lt;- rnorm(n, mean = 0, sd = sqrt(sigma2))  #residuals
&amp;gt; y &amp;lt;- a + b * x + eps  #response variable
&amp;gt; # OR
&amp;gt; y &amp;lt;- (model.matrix(~x) %*% c(a, b)) + eps
&amp;gt; data &amp;lt;- data.frame(y, x)  #dataset
&amp;gt; head(data)  #print out the first six rows of the data set
         y x
1 35.69762 1
2 35.84911 2
3 43.29354 3
4 34.35254 4
5 33.14644 5
6 39.57532 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these sort of data, we are primarily interested in investigating whether there is a relationship between the continuous response variable and the linear predictor (single continuous predictor).&lt;/p&gt;
&lt;div id=&#34;centering-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Centering the data&lt;/h2&gt;
&lt;p&gt;When a linear model contains a covariate (continuous predictor variable) in addition to another predictor (continuous or categorical), it is nearly always advisable that the continuous predictor variables are centered prior to the analysis. Centering is a process by which the mean of a variable is subtracted from each of the values such that the scale of the variable is shifted so as to be centered around &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. Hence the mean of the new centered variable will be &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, yet it will retain the same variance.&lt;/p&gt;
&lt;p&gt;There are multiple reasons for this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;It provides some clinical meaning to the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-intercept. Recall that the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-intercept is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is equal to zero. If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is centered, then the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-intercept represents the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at the mid-point of the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; range. The &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-intercept of an uncentered &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; typically represents a unreal value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (as an &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; is often beyond the reasonable range of values).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In multiplicative models (in which predictors and their interactions are included), main effects and interaction terms built from centered predictors will not be correlated to one another.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For more complex models, centering the covariates can increase the likelihood that the modelling engine converges (arrives at a numerically stable and reliable outcome).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note, centering will not effect the slope estimates. In &lt;code&gt;R&lt;/code&gt;, centering is easily achieved with the &lt;code&gt;scale&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data &amp;lt;- within(data, {
+     cx &amp;lt;- as.numeric(scale(x, scale = FALSE))
+ })
&amp;gt; head(data)
         y x   cx
1 35.69762 1 -7.5
2 35.84911 2 -6.5
3 43.29354 3 -5.5
4 34.35254 4 -4.5
5 33.14644 5 -3.5
6 39.57532 6 -2.5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exploratory data analysis&lt;/h1&gt;
&lt;div id=&#34;normality&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normality&lt;/h2&gt;
&lt;p&gt;Estimation and inference testing in linear regression assumes that the response is normally distributed in each of the populations. In this case, the populations are all possible measurements that could be collected at each level of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; - hence there are &lt;span class=&#34;math inline&#34;&gt;\(16\)&lt;/span&gt; populations. Typically however, we only collect a single observation from each population (as is also the case here). How then can be evaluate whether each of these populations are likely to have been normal? For a given response, the population distributions should follow much the same distribution shapes. Therefore provided the single samples from each population are unbiased representations of those populations, a boxplot of all observations should reflect the population distributions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;homogeneity-of-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Homogeneity of variance&lt;/h2&gt;
&lt;p&gt;Simple linear regression also assumes that each of the populations are equally varied. Actually, it is prospect of a relationship between the mean and variance of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-values across x-values that is of the greatest concern. Strictly the assumption is that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; values at each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; value are equally varied and that there is no relationship between mean and variance. However, as we only have a single &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-value for each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-value, it is difficult to directly determine whether the assumption of &lt;em&gt;homogeneity of variance&lt;/em&gt; is likely to have been violated (mean of one value is meaningless and variability can’t be assessed from a single value). If we then plot the residuals (difference between observed values and those predicted by the trendline) against the predict values and observe a definite presence of a pattern, then it is indicative of issues with the assumption of homogeneity of variance.&lt;/p&gt;
&lt;p&gt;Hence looking at the spread of values around a trendline on a scatterplot of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; against &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is a useful way of identifying gross violations of homogeneity of variance. Residual plots provide an even better diagnostic. The presence of a &lt;em&gt;wedge shape&lt;/em&gt; is indicative that the population mean and variance are related.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linearity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linearity&lt;/h2&gt;
&lt;p&gt;Linear regression fits a straight (linear) line through the data. Therefore, prior to fitting such a model, it is necessary to establish whether this really is the most sensible way of describing the relationship. That is, does the relationship appear to be linearly related or could some other non-linear function describe the relationship better. Scatterplots and residual plots are useful diagnostics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model assumptions&lt;/h2&gt;
&lt;p&gt;The typical assumptions which need to be checked when fitting a standard linear regression model are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;All of the observations are independent - this must be addressed at the design and collection stages&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The response variable (and thus the residuals) should be normally distributed&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The response variable should be equally varied (variance should not be related to mean as these are supposed to be estimated separately)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The relationship between the linear predictor (right hand side of the regression formula) and the link function should be linear. A scatterplot with smoother can be useful for identifying possible non-linearity.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So lets explore normality, homogeneity of variances and linearity by constructing a scatterplot of the relationship between the response (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) and the predictor (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;). We will also include a range of smoothers (linear and lowess) and marginal boxplots on the scatterplot to assist in exploring linearity and normality respectively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; # scatterplot
&amp;gt; library(car)
&amp;gt; scatterplot(y ~ x, data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/simple-linear-regression-stan/2020-02-01-simple-linear-regression-stan_files/figure-html/scatter-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;There is no evidence that the response variable is non-normal. The spread of values around the trendline seems fairly even (hence it there is no evidence of non-homogeneity). The data seems well represented by the linear trendline. Furthermore, the lowess smoother does not appear to have a consistent shift trajectory. Obvious violations could be addressed either by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Consider a non-linear linear predictor (such as a polynomial, spline or other non-linear function)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Transform the scale of the response variables (e.g. to address normality)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting&lt;/h1&gt;
&lt;p&gt;Whilst Gibbs sampling provides an elegantly simple MCMC sampling routine, very complex hierarchical models can take enormous numbers of iterations (often prohibitory large) to converge on a stable posterior distribution. To address this, Andrew Gelman (and other collaborators) have implemented a variation on &lt;em&gt;Hamiltonian Monte Carlo&lt;/em&gt; (HMC): a sampler that selects subsequent samples in a way that reduces the correlation between samples, thereby speeding up convergence) called the &lt;em&gt;No-U-Turn&lt;/em&gt; (NUTS) sampler. All of these developments are brought together into a tool called &lt;em&gt;Stan&lt;/em&gt;. By design (to appeal to the vast &lt;code&gt;BUGS/JAGS&lt;/code&gt; users), &lt;code&gt;STAN&lt;/code&gt; models are defined in a manner reminiscent of &lt;code&gt;BUGS/JAGS&lt;/code&gt;. &lt;code&gt;STAN&lt;/code&gt; first converts these models into &lt;code&gt;C++&lt;/code&gt; code which is then compiled to allow very rapid computation. Consistent with this, the model must be accompanied by variable declarations for all inputs and parameters.&lt;/p&gt;
&lt;p&gt;Note the following important characteristics of a &lt;code&gt;STAN&lt;/code&gt; code:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;code&gt;STAN&lt;/code&gt; model file comprises a number of blocks (not all of which are compulsory).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;STAN&lt;/code&gt; language is an intermediary between (&lt;code&gt;R/BUGS&lt;/code&gt; and &lt;code&gt;c++&lt;/code&gt;) and requires all types (integers, vectors, matrices etc) to be declared prior to use and it uses &lt;code&gt;c++&lt;/code&gt; commenting (&lt;code&gt;//&lt;/code&gt; and &lt;code&gt;/* */&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Code order is important, objects must be declared before they are used. When a type is declared in one block, it is available in subsequent blocks.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;
                      data {
                      // declare the input data / parameters
                      }
                      transformed data {
                      // optional - for transforming/scaling input data
                      }
                      parameters {
                      // define model parameters
                      }
                      transformed parameters {
                      // optional - for deriving additional non-model parameters
                      //            note however, as they are part of the sampling chain
                      //            transformed parameters slow sampling down.
                      }
                      model {
                      // specifying priors and likelihood as well as the linear predictor
                      }
                      generated quantities {
                      // optional - derivatives (posteriors) of the samples
                      }
            &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The minimum model in &lt;code&gt;STAN&lt;/code&gt; required to fit the above simple regression follows. Note the following modifications from the model defined in &lt;code&gt;JAGS&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The normal distribution is defined by standard deviation rather than precision&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Rather than using a uniform prior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, I am using a half-Cauchy&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We now translate the likelihood model into &lt;code&gt;STAN&lt;/code&gt; code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; modelString = &amp;quot;
+   data {
+   int&amp;lt;lower=0&amp;gt; n;
+   vector [n] y;
+   vector [n] x;
+   }
+   parameters {
+   real beta0;
+   real beta;
+   real&amp;lt;lower=0&amp;gt; sigma;
+   }
+   model {
+   vector [n] mu;
+   #Priors
+   beta0 ~ normal(0,10000);
+   beta ~ normal(0,10000);
+   sigma ~ cauchy(0,5);
+  
+   mu = beta0+beta*x;
+   
+   #Likelihood
+   y~normal(mu,sigma);
+   }
+   
+   &amp;quot;
&amp;gt; ## write the model to a stan file 
&amp;gt; writeLines(modelString, con = &amp;quot;linregModel.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The No-U-Turn sampler operates much more efficiently if all predictors are centered. Although it is possible to pre-center all predictors that are passed to &lt;code&gt;STAN&lt;/code&gt;, it is then often necessary to later convert back to the original scale for graphing and further analyses. Since centering is a routine procedure, arguably it should be built into the &lt;code&gt;STAN&lt;/code&gt; we generate. Furthermore, we should also include the back-scaling as well. In this version, the data are to be supplied as a model matrix (so as to leverage various vectorized and matrix multiplier routines). The transformed data block is used to center the non-intercept columns of the predictor model matrix. The model is fit on centered data thereby generating a slope and intercept. This intercept parameter is also expressed back on the non-centered scale (generated properties block).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; modelStringv2 = &amp;quot;
+ data { 
+   int&amp;lt;lower=1&amp;gt; n;   // total number of observations 
+   vector[n] Y;      // response variable 
+   int&amp;lt;lower=1&amp;gt; nX;  // number of effects 
+   matrix[n, nX] X;   // model matrix 
+   } 
+   transformed data { 
+   matrix[n, nX - 1] Xc;  // centered version of X 
+   vector[nX - 1] means_X;  // column means of X before centering 
+   
+   for (i in 2:nX) { 
+   means_X[i - 1] = mean(X[, i]); 
+   Xc[, i - 1] = X[, i] - means_X[i - 1]; 
+   } 
+   }  
+   parameters { 
+   vector[nX-1] beta;  // population-level effects 
+   real cbeta0;  // center-scale intercept 
+   real&amp;lt;lower=0&amp;gt; sigma;  // residual SD 
+   } 
+   transformed parameters { 
+   } 
+   model { 
+   vector[n] mu; 
+   mu = Xc * beta + cbeta0; 
+   // prior specifications 
+   beta ~ normal(0, 100); 
+   cbeta0 ~ normal(0, 100); 
+   sigma ~ cauchy(0, 5); 
+   // likelihood contribution 
+   Y ~ normal(mu, sigma); 
+   } 
+   generated quantities { 
+   real beta0;  // population-level intercept 
+   beta0 = cbeta0 - dot_product(means_X, beta); 
+   }
+   
+   &amp;quot;
&amp;gt; ## write the model to a stan file 
&amp;gt; writeLines(modelStringv2, con = &amp;quot;linregModelv2.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Arrange the data as a list (as required by &lt;code&gt;JAGS&lt;/code&gt;). As input, &lt;code&gt;JAGS&lt;/code&gt; will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Xmat &amp;lt;- model.matrix(~x, data = data)
&amp;gt; data.list &amp;lt;- with(data, list(Y = y, X = Xmat, nX = ncol(Xmat), n = nrow(data)))
&amp;gt; data.list
$Y
 [1] 35.69762 35.84911 43.29354 34.35254 33.14644 39.57532 31.80458 21.67469
 [9] 23.06574 22.77169 29.62041 23.79907 22.50386 19.55341 14.72079 24.93457

$X
   (Intercept)  x
1            1  1
2            1  2
3            1  3
4            1  4
5            1  5
6            1  6
7            1  7
8            1  8
9            1  9
10           1 10
11           1 11
12           1 12
13           1 13
14           1 14
15           1 15
16           1 16
attr(,&amp;quot;assign&amp;quot;)
[1] 0 1

$nX
[1] 2

$n
[1] 16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the initial values for the chain. Reasonable starting points can be gleaned from the data themselves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; inits &amp;lt;- rep(list(list(beta0 = mean(data$y), beta1 = diff(tapply(data$y,
+     data$x, mean)), sigma = sd(data$y))), 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the nodes (parameters and derivatives) to monitor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params &amp;lt;- c(&amp;quot;beta&amp;quot;,&amp;quot;beta0&amp;quot;, &amp;quot;cbeta0&amp;quot;, &amp;quot;sigma&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the chain parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; nChains = 2
&amp;gt; burnInSteps = 1000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 3000  #across all chains
&amp;gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&amp;gt; nIter
[1] 2500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Start the &lt;code&gt;STAN&lt;/code&gt; model (check the model, load data into the model, specify the number of chains and compile the model). Load the &lt;code&gt;rstan&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When using the &lt;code&gt;stan&lt;/code&gt; function (&lt;code&gt;rstan&lt;/code&gt; package), it is not necessary to provide initial values. However, if they are to be supplied, the inital values must be provided as a list of the same length as the number of chains.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data.rstan &amp;lt;- stan(data = data.list, file = &amp;quot;linregModelv2.stan&amp;quot;, 
+     chains = nChains, iter = nIter, warmup = burnInSteps,
+     thin = thinSteps, save_dso = TRUE)

SAMPLING FOR MODEL &amp;#39;linregModelv2&amp;#39; NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2500 [  0%]  (Warmup)
Chain 1: Iteration:  250 / 2500 [ 10%]  (Warmup)
Chain 1: Iteration:  500 / 2500 [ 20%]  (Warmup)
Chain 1: Iteration:  750 / 2500 [ 30%]  (Warmup)
Chain 1: Iteration: 1000 / 2500 [ 40%]  (Warmup)
Chain 1: Iteration: 1001 / 2500 [ 40%]  (Sampling)
Chain 1: Iteration: 1250 / 2500 [ 50%]  (Sampling)
Chain 1: Iteration: 1500 / 2500 [ 60%]  (Sampling)
Chain 1: Iteration: 1750 / 2500 [ 70%]  (Sampling)
Chain 1: Iteration: 2000 / 2500 [ 80%]  (Sampling)
Chain 1: Iteration: 2250 / 2500 [ 90%]  (Sampling)
Chain 1: Iteration: 2500 / 2500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.028 seconds (Warm-up)
Chain 1:                0.035 seconds (Sampling)
Chain 1:                0.063 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL &amp;#39;linregModelv2&amp;#39; NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2500 [  0%]  (Warmup)
Chain 2: Iteration:  250 / 2500 [ 10%]  (Warmup)
Chain 2: Iteration:  500 / 2500 [ 20%]  (Warmup)
Chain 2: Iteration:  750 / 2500 [ 30%]  (Warmup)
Chain 2: Iteration: 1000 / 2500 [ 40%]  (Warmup)
Chain 2: Iteration: 1001 / 2500 [ 40%]  (Sampling)
Chain 2: Iteration: 1250 / 2500 [ 50%]  (Sampling)
Chain 2: Iteration: 1500 / 2500 [ 60%]  (Sampling)
Chain 2: Iteration: 1750 / 2500 [ 70%]  (Sampling)
Chain 2: Iteration: 2000 / 2500 [ 80%]  (Sampling)
Chain 2: Iteration: 2250 / 2500 [ 90%]  (Sampling)
Chain 2: Iteration: 2500 / 2500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.028 seconds (Warm-up)
Chain 2:                0.035 seconds (Sampling)
Chain 2:                0.063 seconds (Total)
Chain 2: &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mcmc-diagnostics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;MCMC diagnostics&lt;/h1&gt;
&lt;p&gt;In addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Traceplots&lt;/em&gt; for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Autocorrelation&lt;/em&gt; plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;). A lag of &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Potential scale reduction factor&lt;/em&gt; (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than &lt;span class=&#34;math inline&#34;&gt;\(1.05\)&lt;/span&gt;. If there are values of &lt;span class=&#34;math inline&#34;&gt;\(1.05\)&lt;/span&gt; or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Prior to examining the summaries, we should have explored the convergence diagnostics. We use the package &lt;code&gt;mcmcplots&lt;/code&gt; to obtain density and trace plots for the effects model as an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(mcmcplots)
&amp;gt; s = as.array(data.rstan)
&amp;gt; mcmc &amp;lt;- do.call(mcmc.list, plyr:::alply(s[, , -(length(s[1, 1, ]))], 2, as.mcmc))
&amp;gt; denplot(mcmc, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta&amp;quot;,&amp;quot;cbeta0&amp;quot;,&amp;quot;sigma&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/simple-linear-regression-stan/2020-02-01-simple-linear-regression-stan_files/figure-html/mcmc_diag-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(mcmc, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta&amp;quot;,&amp;quot;cbeta0&amp;quot;,&amp;quot;sigma&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/simple-linear-regression-stan/2020-02-01-simple-linear-regression-stan_files/figure-html/mcmc_diag-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. We can also look at just the density plot computed from the &lt;code&gt;bayesplot&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(bayesplot)
&amp;gt; mcmc_dens(as.array(data.rstan))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/simple-linear-regression-stan/2020-02-01-simple-linear-regression-stan_files/figure-html/mcmc_diag2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Density plots sugggest mean or median would be appropriate to describe the fixed posteriors and median is appropriate for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; posterior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-validation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model validation&lt;/h1&gt;
&lt;p&gt;Model validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. Ideally, a good model should also be able to predict the data used to fit the model.&lt;/p&gt;
&lt;p&gt;Although residuals can be computed directly within &lt;code&gt;rstan&lt;/code&gt;, we can calculate them manually from the posteriors to be consistent across other approaches.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(ggplot2)
&amp;gt; mcmc = as.matrix(data.rstan)[, c(&amp;quot;beta0&amp;quot;, &amp;quot;beta[1]&amp;quot;)]
&amp;gt; # generate a model matrix
&amp;gt; newdata = data.frame(x = data$x)
&amp;gt; Xmat = model.matrix(~x, newdata)
&amp;gt; ## get median parameter estimates
&amp;gt; coefs = apply(mcmc, 2, median)
&amp;gt; fit = as.vector(coefs %*% t(Xmat))
&amp;gt; resid = data$y - fit
&amp;gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/simple-linear-regression-stan/2020-02-01-simple-linear-regression-stan_files/figure-html/mcmc_residuals-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Residuals against predictors&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc = as.matrix(data.rstan)[, c(&amp;quot;beta0&amp;quot;, &amp;quot;beta[1]&amp;quot;)]
&amp;gt; # generate a model matrix
&amp;gt; newdata = data.frame(x = data$x)
&amp;gt; Xmat = model.matrix(~x, newdata)
&amp;gt; ## get median parameter estimates
&amp;gt; coefs = apply(mcmc, 2, median)
&amp;gt; fit = as.vector(coefs %*% t(Xmat))
&amp;gt; resid = data$y - fit
&amp;gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data$x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/simple-linear-regression-stan/2020-02-01-simple-linear-regression-stan_files/figure-html/mcmc_residuals2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And now for studentized residuals&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc = as.matrix(data.rstan)[, c(&amp;quot;beta0&amp;quot;, &amp;quot;beta[1]&amp;quot;)]
&amp;gt; # generate a model matrix
&amp;gt; newdata = data.frame(x = data$x)
&amp;gt; Xmat = model.matrix(~x, newdata)
&amp;gt; ## get median parameter estimates
&amp;gt; coefs = apply(mcmc, 2, median)
&amp;gt; fit = as.vector(coefs %*% t(Xmat))
&amp;gt; resid = data$y - fit
&amp;gt; sresid = resid/sd(resid)
&amp;gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/simple-linear-regression-stan/2020-02-01-simple-linear-regression-stan_files/figure-html/mcmc_residuals3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For this simple model, the studentized residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc = as.matrix(data.rstan)
&amp;gt; # generate a model matrix
&amp;gt; Xmat = model.matrix(~x, data)
&amp;gt; ## get median parameter estimates
&amp;gt; coefs = mcmc[, c(&amp;quot;beta0&amp;quot;, &amp;quot;beta[1]&amp;quot;)]
&amp;gt; fit = coefs %*% t(Xmat)
&amp;gt; ## draw samples from this model
&amp;gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i, ], mcmc[i,
+     &amp;quot;sigma&amp;quot;]))
&amp;gt; ggplot() + geom_density(data = NULL, aes(x = as.vector(yRep), fill = &amp;quot;Model&amp;quot;),
+     alpha = 0.5) + geom_density(data = data, aes(x = y, fill = &amp;quot;Obs&amp;quot;),
+     alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/simple-linear-regression-stan/2020-02-01-simple-linear-regression-stan_files/figure-html/mcmc_rep-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-estimates&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Parameter estimates&lt;/h1&gt;
&lt;p&gt;Although all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and &lt;span class=&#34;math inline&#34;&gt;\(95\)&lt;/span&gt;% credibility intervals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; summary(data.rstan)
$summary
              mean     se_mean        sd       2.5%        25%        50%
beta[1]  -1.386241 0.005409925 0.2628701  -1.928032  -1.556143  -1.390692
cbeta0   28.526455 0.025241285 1.2563021  26.053580  27.679148  28.517154
sigma     4.938850 0.022620333 0.9747690   3.433777   4.255653   4.800261
beta0    40.309502 0.051054981 2.5104697  35.305689  38.660487  40.321813
lp__    -32.392865 0.038016453 1.2930877 -35.617366 -32.976932 -32.063282
               75%       97.5%    n_eff      Rhat
beta[1]  -1.216324  -0.8695824 2361.021 1.0010917
cbeta0   29.358476  31.0250363 2477.224 1.0000293
sigma     5.472441   7.1969472 1856.972 1.0020160
beta0    41.901624  45.2722391 2417.874 0.9999759
lp__    -31.431890 -30.9164925 1156.945 1.0014468

$c_summary
, , chains = chain:1

         stats
parameter       mean        sd       2.5%        25%        50%        75%
  beta[1]  -1.395354 0.2663176  -1.938168  -1.570786  -1.397259  -1.224694
  cbeta0   28.499907 1.2671111  25.965301  27.665662  28.499904  29.339459
  sigma     4.974297 1.0354718   3.403207   4.248213   4.814835   5.545663
  beta0    40.360414 2.4961956  35.294178  38.744797  40.418903  41.921210
  lp__    -32.448925 1.3633114 -35.893989 -33.121738 -32.073215 -31.438407
         stats
parameter       97.5%
  beta[1]  -0.8701744
  cbeta0   31.0552336
  sigma     7.3827875
  beta0    45.2489270
  lp__    -30.9157752

, , chains = chain:2

         stats
parameter       mean        sd       2.5%        25%        50%        75%
  beta[1]  -1.377128 0.2591452  -1.911379  -1.544377  -1.385973  -1.203797
  cbeta0   28.553004 1.2452556  26.163699  27.717229  28.540569  29.378535
  sigma     4.903404 0.9089921   3.457094   4.261745   4.785809   5.423194
  beta0    40.258590 2.5244684  35.319104  38.595264  40.250020  41.864964
  lp__    -32.336805 1.2167003 -35.405972 -32.883087 -32.042686 -31.424106
         stats
parameter       97.5%
  beta[1]  -0.8699495
  cbeta0   31.0050930
  sigma     7.0778361
  beta0    45.2803661
  lp__    -30.9173256
&amp;gt; 
&amp;gt; # OR
&amp;gt; library(broom)
&amp;gt; tidyMCMC(data.rstan, conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;, rhat = TRUE, ess = TRUE)
# A tibble: 4 x 7
  term    estimate std.error conf.low conf.high  rhat   ess
  &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
1 beta[1]    -1.39     0.263    -1.90    -0.856 1.00   2361
2 cbeta0     28.5      1.26     26.2     31.1   1.00   2477
3 sigma       4.94     0.975     3.31     6.96  1.00   1857
4 beta0      40.3      2.51     35.3     45.2   1.000  2418&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A one unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is associated with a &lt;span class=&#34;math inline&#34;&gt;\(-1.39\)&lt;/span&gt; change in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. That is, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; declines at a rate of &lt;span class=&#34;math inline&#34;&gt;\(-1.39\)&lt;/span&gt; per unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(95\)&lt;/span&gt;% confidence interval for the slope does not overlap with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; implying a significant effect of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.&lt;/p&gt;
&lt;p&gt;Also note that since our &lt;code&gt;STAN&lt;/code&gt; model incorporated predictor centering, we have estimates of the intercept based on both centered (&lt;code&gt;cbeta0&lt;/code&gt;) and uncentered data (&lt;code&gt;beta0&lt;/code&gt;). Since the intercept from uncentered data is beyond the domain of our sampling data it has very little interpretability. However, the intercept based on centered data can be interpreted as the estimate of the response at the mean predictor (in this case &lt;span class=&#34;math inline&#34;&gt;\(28.5\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmcpvalue &amp;lt;- function(samp) {
+     ## elementary version that creates an empirical p-value for the
+     ## hypothesis that the columns of samp have mean zero versus a general
+     ## multivariate distribution with elliptical contours.
+ 
+     ## differences from the mean standardized by the observed
+     ## variance-covariance factor
+ 
+     ## Note, I put in the bit for single terms
+     if (length(dim(samp)) == 0) {
+         std &amp;lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),
+             transpose = TRUE)
+         sqdist &amp;lt;- colSums(std * std)
+         sum(sqdist[-1] &amp;gt; sqdist[1])/length(samp)
+     } else {
+         std &amp;lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),
+             transpose = TRUE)
+         sqdist &amp;lt;- colSums(std * std)
+         sum(sqdist[-1] &amp;gt; sqdist[1])/nrow(samp)
+     }
+ 
+ }
&amp;gt; ## since values are less than zero
&amp;gt; mcmcpvalue(as.matrix(data.rstan)[, c(&amp;quot;beta[1]&amp;quot;)])
[1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With a p-value of essentially &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;graphical-summaries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Graphical summaries&lt;/h1&gt;
&lt;p&gt;A nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group. We do this by loading functions in the package &lt;code&gt;dplyr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(dplyr)
&amp;gt; mcmc = as.matrix(data.rstan)
&amp;gt; ## Calculate the fitted values
&amp;gt; newdata = data.frame(x = seq(min(data$x, na.rm = TRUE), max(data$x, na.rm = TRUE),
+     len = 1000))
&amp;gt; Xmat = model.matrix(~x, newdata)
&amp;gt; coefs = mcmc[, c(&amp;quot;beta0&amp;quot;, &amp;quot;beta[1]&amp;quot;)]
&amp;gt; fit = coefs %*% t(Xmat)
&amp;gt; newdata = newdata %&amp;gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;))
&amp;gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_line() + geom_ribbon(aes(ymin = conf.low,
+     ymax = conf.high), fill = &amp;quot;blue&amp;quot;, alpha = 0.3) + scale_y_continuous(&amp;quot;Y&amp;quot;) +
+     scale_x_continuous(&amp;quot;X&amp;quot;) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/simple-linear-regression-stan/2020-02-01-simple-linear-regression-stan_files/figure-html/mcmc_post1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data, aes(y = y,
+     x = x), color = &amp;quot;gray&amp;quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low,
+     ymax = conf.high), fill = &amp;quot;blue&amp;quot;, alpha = 0.3) + scale_y_continuous(&amp;quot;Y&amp;quot;) +
+     scale_x_continuous(&amp;quot;X&amp;quot;) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/simple-linear-regression-stan/2020-02-01-simple-linear-regression-stan_files/figure-html/mcmc_post2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since &lt;span class=&#34;math inline&#34;&gt;\(\text{resid}=\text{obs}−\text{fitted}\)&lt;/span&gt; and the fitted values depend only on the single predictor we are interested in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; ## Calculate partial residuals fitted values
&amp;gt; fdata = rdata = data
&amp;gt; fMat = rMat = model.matrix(~x, fdata)
&amp;gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))
&amp;gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))
&amp;gt; rdata = rdata %&amp;gt;% mutate(partial.resid = resid + fit)
&amp;gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),
+     color = &amp;quot;gray&amp;quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low, ymax = conf.high),
+     fill = &amp;quot;blue&amp;quot;, alpha = 0.3) + scale_y_continuous(&amp;quot;Y&amp;quot;) + scale_x_continuous(&amp;quot;X&amp;quot;) +
+     theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/simple-linear-regression-stan/2020-02-01-simple-linear-regression-stan_files/figure-html/mcmc_post3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;effect-sizes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Effect sizes&lt;/h1&gt;
&lt;p&gt;Lets explore a range of effect sizes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Raw effect size&lt;/em&gt; between the largest and smallest &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Cohen’s D&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Percentage change&lt;/em&gt; between the largest and smallest &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Fractional change&lt;/em&gt; between the largest and smallest &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Probability&lt;/em&gt; that a change in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is associated with greater than a &lt;span class=&#34;math inline&#34;&gt;\(25\)&lt;/span&gt;% decline in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Clearly, in order to explore this inference, we must first express the change in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as a percentage. This in turn requires us to calculate start and end points from which to calculate the magnitude of the effect (amount of decline in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) as well as the percentage decline. Hence, we start by predicting the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; at the lowest and highest values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc = as.matrix(data.rstan)
&amp;gt; newdata = data.frame(x = c(min(data$x, na.rm = TRUE), max(data$x, na.rm = TRUE)))
&amp;gt; Xmat = model.matrix(~x, newdata)
&amp;gt; coefs = mcmc[, c(&amp;quot;beta0&amp;quot;, &amp;quot;beta[1]&amp;quot;)]
&amp;gt; fit = coefs %*% t(Xmat)
&amp;gt; ## Raw effect size
&amp;gt; (RES = tidyMCMC(as.mcmc(fit[, 2] - fit[, 1]), conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;))
# A tibble: 1 x 5
  term  estimate std.error conf.low conf.high
  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 var1     -20.8      3.94    -28.5     -12.8
&amp;gt; ## Cohen&amp;#39;s D
&amp;gt; cohenD = (fit[, 2] - fit[, 1])/mcmc[, &amp;quot;sigma&amp;quot;]
&amp;gt; (cohenDES = tidyMCMC(as.mcmc(cohenD), conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;))
# A tibble: 1 x 5
  term  estimate std.error conf.low conf.high
  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 var1     -4.37      1.14    -6.68     -2.27
&amp;gt; # Percentage change (relative to Group A)
&amp;gt; ESp = 100 * (fit[, 2] - fit[, 1])/fit[, 1]
&amp;gt; (PES = tidyMCMC(as.mcmc(ESp), conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;))
# A tibble: 1 x 5
  term  estimate std.error conf.low conf.high
  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 var1     -53.1      7.81    -68.5     -37.5
&amp;gt; # Probability that the effect is greater than 25% (a decline of &amp;gt;25%)
&amp;gt; sum(-1 * ESp &amp;gt; 25)/length(ESp)
[1] 0.998
&amp;gt; ## fractional change
&amp;gt; fit = fit[fit[, 2] &amp;gt; 0, ]
&amp;gt; (FES = tidyMCMC(as.mcmc(fit[, 2]/fit[, 1]), conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;))
# A tibble: 1 x 5
  term  estimate std.error conf.low conf.high
  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 var1     0.469    0.0781    0.315     0.625&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;On average, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; declines by &lt;span class=&#34;math inline&#34;&gt;\(-20.8\)&lt;/span&gt; over the observed range of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We are &lt;span class=&#34;math inline&#34;&gt;\(95\)&lt;/span&gt;% confident that the decline is between &lt;span class=&#34;math inline&#34;&gt;\(-28.5\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-12.8\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Cohen’s D associated with a change over the observed range of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(-4.37\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On average, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; declines by &lt;span class=&#34;math inline&#34;&gt;\(-53.1\)&lt;/span&gt;% over the observed range of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We are &lt;span class=&#34;math inline&#34;&gt;\(95\)&lt;/span&gt;% confident that the decline is between &lt;span class=&#34;math inline&#34;&gt;\(-68.5\)&lt;/span&gt;% and &lt;span class=&#34;math inline&#34;&gt;\(-37.5\)&lt;/span&gt;%.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The probability that &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; declines by more than &lt;span class=&#34;math inline&#34;&gt;\(25\)&lt;/span&gt;% over the observed range of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0.998\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On average, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; declines by a factor of &lt;span class=&#34;math inline&#34;&gt;\(0.469\)&lt;/span&gt;% over the observed range of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We are &lt;span class=&#34;math inline&#34;&gt;\(95\)&lt;/span&gt;% confident that the decline is between a factor of &lt;span class=&#34;math inline&#34;&gt;\(0.315\)&lt;/span&gt;% and &lt;span class=&#34;math inline&#34;&gt;\(0.625\)&lt;/span&gt;%.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;finite-population-standard-deviations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Finite population standard deviations&lt;/h1&gt;
&lt;p&gt;Variance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).&lt;/p&gt;
&lt;p&gt;Finite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (&lt;span class=&#34;citation&#34;&gt;Gelman and others (2005)&lt;/span&gt;). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2 x 5
  term     estimate std.error conf.low conf.high
  &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 sd.x         6.60     1.25      4.07      9.04
2 sd.resid     4.70     0.243     4.54      5.18
# A tibble: 2 x 5
  term     estimate std.error conf.low conf.high
  &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 sd.x         59.3      5.30     47.8      63.9
2 sd.resid     40.7      5.30     36.1      52.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/simple-linear-regression-stan/2020-02-01-simple-linear-regression-stan_files/figure-html/effects_modelv4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Approximately &lt;span class=&#34;math inline&#34;&gt;\(59.3\)&lt;/span&gt;% of the total finite population standard deviation is due to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-squared&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R squared&lt;/h1&gt;
&lt;p&gt;In a frequentist context, the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; greater than &lt;span class=&#34;math inline&#34;&gt;\(100\)&lt;/span&gt;%. &lt;span class=&#34;citation&#34;&gt;Gelman et al. (2019)&lt;/span&gt; proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.&lt;/p&gt;
&lt;p&gt;So in the standard regression model notation of:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i \sim \text{Normal}(\boldsymbol X \boldsymbol \beta, \sigma),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; could be formulated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ R^2 = \frac{\sigma^2_f}{\sigma^2_f + \sigma^2_e},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_f=\text{var}(\boldsymbol X \boldsymbol \beta)\)&lt;/span&gt;, and for normal models &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_e=\text{var}(y-\boldsymbol X \boldsymbol \beta)\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc &amp;lt;- as.matrix(data.rstan)
&amp;gt; Xmat = model.matrix(~x, data)
&amp;gt; coefs = mcmc[, c(&amp;quot;beta0&amp;quot;, &amp;quot;beta[1]&amp;quot;)]
&amp;gt; fit = coefs %*% t(Xmat)
&amp;gt; resid = sweep(fit, 2, data$y, &amp;quot;-&amp;quot;)
&amp;gt; var_f = apply(fit, 1, var)
&amp;gt; var_e = apply(resid, 1, var)
&amp;gt; R2 = var_f/(var_f + var_e)
&amp;gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;)
# A tibble: 1 x 5
  term  estimate std.error conf.low conf.high
  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 var1     0.652    0.0982    0.456     0.758
&amp;gt; 
&amp;gt; # for comparison with frequentist
&amp;gt; summary(lm(y ~ x, data))

Call:
lm(formula = y ~ x, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.5427 -3.3510 -0.3309  2.0411  7.5791 

Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)  40.3328     2.4619  16.382 1.58e-10 ***
x            -1.3894     0.2546  -5.457 8.45e-05 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 4.695 on 14 degrees of freedom
Multiple R-squared:  0.6802,    Adjusted R-squared:  0.6574 
F-statistic: 29.78 on 1 and 14 DF,  p-value: 8.448e-05&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references hanging-indent&#34;&gt;
&lt;div id=&#34;ref-gelman2019r&#34;&gt;
&lt;p&gt;Gelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” &lt;em&gt;The American Statistician&lt;/em&gt; 73 (3): 307–9.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2015stan&#34;&gt;
&lt;p&gt;Gelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” &lt;em&gt;Journal of Educational and Behavioral Statistics&lt;/em&gt; 40 (5): 530–43.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2005analysis&#34;&gt;
&lt;p&gt;Gelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” &lt;em&gt;The Annals of Statistics&lt;/em&gt; 33 (1): 1–53.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rstanpackage&#34;&gt;
&lt;p&gt;Stan Development Team. 2018. “RStan: The R Interface to Stan.” &lt;a href=&#34;http://mc-stan.org/&#34;&gt;http://mc-stan.org/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Comparing Two Populations - STAN</title>
      <link>/stan/comparing-two-populations-stan/comparing-two-populations-stan/</link>
      <pubDate>Sat, 01 Feb 2020 21:13:14 -0500</pubDate>
      
      <guid>/stan/comparing-two-populations-stan/comparing-two-populations-stan/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This tutorial will focus on the use of Bayesian estimation to explore differences between two populations. &lt;code&gt;BUGS&lt;/code&gt; (Bayesian inference Using &lt;em&gt;Gibbs Sampling&lt;/em&gt;) is an algorithm and supporting language (resembling &lt;code&gt;R&lt;/code&gt;) dedicated to performing the Gibbs sampling implementation of &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; (MCMC) method. Dialects of the &lt;code&gt;BUGS&lt;/code&gt; language are implemented within three main projects:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OpenBUGS&lt;/strong&gt; - written in component pascal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;JAGS&lt;/strong&gt; - (Just Another Gibbs Sampler) - written in &lt;code&gt;C++&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;STAN&lt;/strong&gt; - a dedicated Bayesian modelling framework written in &lt;code&gt;C++&lt;/code&gt; and implementing &lt;em&gt;Hamiltonian&lt;/em&gt; MCMC samplers.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within &lt;code&gt;R&lt;/code&gt; itself. As such there are multiple packages devoted to interfacing with the various software implementations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2OpenBUGS&lt;/em&gt; - interfaces with &lt;code&gt;OpenBUGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2jags&lt;/em&gt; - interfaces with &lt;code&gt;JAGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;rstan&lt;/em&gt; - interfaces with &lt;code&gt;STAN&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;BUGS/JAGS/STAN&lt;/code&gt; languages and algorithms are very powerful and flexible. However, the cost of this power and flexibility is complexity and the need for a firm understanding of the model you wish to fit as well as the priors to be used. The algorithms requires the following inputs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Within the model:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The likelihood function relating the response to the predictors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The definition of the priors.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Chain properties:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The number of chains.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The length of chains (number of iterations).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The burn-in length (number of initial iterations to ignore).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The thinning rate (number of iterations to count on before storing a sample).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The initial estimates to start an MCMC chain. If there are multiple chains, these starting values can differ between chains.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The list of model parameters and derivatives to monitor (and return the posterior distributions of)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial will demonstrate how to fit models in &lt;code&gt;STAN&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Gelman, Lee, and Guo (2015)&lt;/span&gt;) using the package &lt;code&gt;rstan&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Stan Development Team (2018)&lt;/span&gt;) as interface, which also requires to load some other packages.&lt;/p&gt;
&lt;div id=&#34;data-generation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data generation&lt;/h1&gt;
&lt;p&gt;We will start by generating a random data set. Note, I am creating two versions of the predictor variable (a numeric version and a factorial version).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; set.seed(123)
&amp;gt; nA &amp;lt;- 60  #sample size from Population A
&amp;gt; nB &amp;lt;- 40  #sample size from Population B
&amp;gt; muA &amp;lt;- 105  #population mean of Population A
&amp;gt; muB &amp;lt;- 77.5  #population mean of Population B
&amp;gt; sigma &amp;lt;- 3  #standard deviation of both populations (equally varied)
&amp;gt; yA &amp;lt;- rnorm(nA, muA, sigma)  #Population A sample
&amp;gt; yB &amp;lt;- rnorm(nB, muB, sigma)  #Population B sample
&amp;gt; y &amp;lt;- c(yA, yB)
&amp;gt; x &amp;lt;- factor(rep(c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;), c(nA, nB)))  #categorical listing of the populations
&amp;gt; xn &amp;lt;- as.numeric(x)  #numerical version of the population category for means parameterization. # Should not start at 0.
&amp;gt; data &amp;lt;- data.frame(y, x, xn)  # dataset&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let inspect the first few rows of the dataset using the command &lt;code&gt;head&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; head(data)
         y x xn
1 103.3186 A  1
2 104.3095 A  1
3 109.6761 A  1
4 105.2115 A  1
5 105.3879 A  1
6 110.1452 A  1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also perform some exploratory data analysis - in this case, a boxplot of the response for each level of the predictor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; boxplot(y ~ x, data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/boxplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-one-sample-t-test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The One Sample t-test&lt;/h1&gt;
&lt;p&gt;A &lt;em&gt;t-test&lt;/em&gt; is essentially just a simple regression model in which the categorical predictor is represented by a binary variable in which one level is coded as &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and the other &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. For the model itself, the observed response &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; are assumed to be drawn from a normal distribution with a given mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. The expected values are themselves determined by the linear predictor &lt;span class=&#34;math inline&#34;&gt;\(\mu_i=\beta_0+\beta_1x_i\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; represents the mean of the first treatment group and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; represents the difference between the mean of the first group and the mean of the second group (the effect of interest).&lt;/p&gt;
&lt;p&gt;MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying “uninformative” priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (&lt;span class=&#34;math inline&#34;&gt;\(1000\)&lt;/span&gt;) for both the intercept and the treatment effect and a wide half-cauchy (scale=&lt;span class=&#34;math inline&#34;&gt;\(25\)&lt;/span&gt;) for the standard deviation (&lt;span class=&#34;citation&#34;&gt;Gelman and others (2006)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i \sim \text{Normal}(\mu_i, \sigma),  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu_i=\beta_0+\beta_1x_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Priors are defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \beta_j \sim \text{Normal}(0,1000),  \;\;\; \text{and} \;\;\; \sigma \sim \text{Cauchy}(0,25),  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(j=0,1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;fitting-the-model-in-stan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting the model in STAN&lt;/h2&gt;
&lt;p&gt;Broadly, there are two ways of parameterising (expressing the unknown (to be estimated) components of a model) a model. Either we can estimate the means of each group (&lt;em&gt;Means parameterisation&lt;/em&gt;) or we can estimate the mean of one group and the difference between this group and the other group(s) (&lt;em&gt;Effects parameterisation&lt;/em&gt;). The latter is commonly used for frequentist null hypothesis testing as its parameters are more consistent with the null hypothesis of interest (that the difference between the two groups equals zero).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Effects parameterisation&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i = \beta_0 + \beta_{j}x_i + \epsilon_i, \;\;\; \text{with} \;\;\; \epsilon_i \sim \text{Normal}(0,\sigma).  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Each &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is modelled by an intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; (mean of group A) plus a difference parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; (difference between mean of group A and group B) multiplied by an indicator of which group the observation came from (&lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;), plus a residual drawn from a normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Actually, there are as many &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; parameters as there are groups but one of them (typically the first) is set to be equal to zero (to avoid over-parameterization). Expected values of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of effect parameters and whose variance is defined by the degree of variability in this mean. The parameters are: &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Means parameterisation&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i = \beta_{j} + \epsilon_i, \;\;\; \text{with} \;\;\; \epsilon_i \sim \text{Normal}(0,\sigma).  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Each &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is modelled as the mean &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; of each group (&lt;span class=&#34;math inline&#34;&gt;\(j=1,2\)&lt;/span&gt;) plus a residual drawn from a normal distribution with a mean of zero and a standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Actually, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol \beta\)&lt;/span&gt; is a set of &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; coefficients corresponding to the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; dummy coded factor levels. Expected values of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of means parameters and whose variance is defined by the degree of variability in this mean. The parameters are: &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Whilst the &lt;code&gt;STAN&lt;/code&gt; language broadly resembles &lt;code&gt;BUGS/JAGS&lt;/code&gt;, there are numerous important differences. Some of these differences are to support translation to &lt;code&gt;c++&lt;/code&gt; for compilation (such as declaring variables). Others reflect leveraging of vectorization to speed up run time. Here are some important notes about &lt;code&gt;STAN&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;All variables must be declared&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Variables declared in the parameters block will be collected&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Anything in the transformed block will be collected as samples. Also, checks will be made every loop&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now I will demonstrate fitting the models with &lt;code&gt;STAN&lt;/code&gt;. Note, I am using the &lt;code&gt;refresh=0&lt;/code&gt; option so as to suppress the larger regular output in the interest of keeping output to what is necessary for this tutorial. When running outside of a tutorial context, the regular verbose output is useful as it provides a way to gauge progress.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Effects Parameterisation&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; stanString = &amp;quot; 
+ data {
+  int n;
+  vector [n] y;
+  vector [n] x;
+  }
+  parameters {
+  real &amp;lt;lower=0, upper=100&amp;gt; sigma;
+  real beta0;
+  real beta;
+  }
+  transformed parameters {
+  }
+  model {
+  vector [n] mu;
+  
+  //Priors
+  beta0 ~ normal(0,1000);
+  beta ~ normal(0,1000);
+  sigma ~ cauchy(0,25);
+  
+  mu = beta0 + beta*x;
+  //Likelihood
+  y ~ normal(mu, sigma);
+  }
+  generated quantities {
+  vector [2] Group_means;
+  real CohensD;
+  //Other Derived parameters 
+  //# Group means (note, beta is a vector)
+  Group_means[1] = beta0;
+  Group_means[2] = beta0+beta;
+  
+  CohensD = beta /sigma;  
+  }
+  
+  &amp;quot;
&amp;gt; ## write the model to a text file
&amp;gt; writeLines(stanString, con = &amp;quot;ttestModel.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Means Parameterisation&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; stanString.means = &amp;quot;  
+  data {
+  int n;
+  int nX;
+  vector [n] y;
+  matrix [n,nX] x;
+  }
+  parameters {
+  real &amp;lt;lower=0, upper=100&amp;gt; sigma;
+  vector [nX] beta;
+  }
+  transformed parameters {
+  }
+  model {
+  vector [n] mu;
+  
+  //Priors
+  beta ~ normal(0,1000);
+  sigma ~ cauchy(0,25);
+  
+  mu = x*beta;
+  //Likelihood
+  y ~ normal(mu, sigma);
+  }
+  generated quantities {
+  vector [2] Group_means;
+  real CohensD;
+  
+  //Other Derived parameters 
+  Group_means[1] = beta[1];
+  Group_means[2] = beta[1]+beta[2];
+  
+  CohensD = beta[2] /sigma;  
+  }
+  
+  &amp;quot;
&amp;gt; ## write the model to a text file
&amp;gt; writeLines(stanString.means, con = &amp;quot;ttestModelMeans.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Arrange the data as a list (as required by &lt;code&gt;STAN&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data.list &amp;lt;- with(data, list(y = y, x = (xn - 1), n = nrow(data)))
&amp;gt; X &amp;lt;- model.matrix(~x, data)
&amp;gt; data.list.means = with(data, list(y = y, x = X, n = nrow(data), nX = ncol(X)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the initial values for the chain. Reasonable starting points can be gleaned from the data themselves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; inits &amp;lt;- list(beta0 = mean(data$y), beta = c(NA, diff(tapply(data$y,
+     data$x, mean))), sigma = sd(data$y/2))
&amp;gt; inits.means &amp;lt;- list(beta = tapply(data$y, data$x, mean), sigma = sd(data$y/2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the nodes (parameters and derivatives) to monitor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params &amp;lt;- c(&amp;quot;beta0&amp;quot;, &amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;Group_means&amp;quot;, &amp;quot;CohensD&amp;quot;)
&amp;gt; params.means &amp;lt;- c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;Group_means&amp;quot;,&amp;quot;CohensD&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the chain parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; burnInSteps = 500  # the number of initial samples to discard
&amp;gt; nChains = 2  # the number of independed sampling chains to perform 
&amp;gt; thinSteps = 1  # the thinning rate
&amp;gt; nIter = 2000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Start the &lt;code&gt;STAN&lt;/code&gt; model (check the model, load data into the model, specify the number of chains and compile the model). Load the &lt;code&gt;rstan&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When using the &lt;code&gt;stan&lt;/code&gt; function (&lt;code&gt;rtsan&lt;/code&gt; package), it is not necessary to provide initial values. However, if they are to be supplied, the inital values must be provided as a list of the same length as the number of chains.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Effects Parameterisation&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data.stan = stan(file = &amp;quot;ttestModel.stan&amp;quot;, 
+   data = data.list, 
+   pars = params,
+   iter = nIter,
+   warmup = burnInSteps, 
+   chains = nChains, 
+   thin = thinSteps, 
+   init = &amp;quot;random&amp;quot;, #or inits=list(inits,inits)
+   refresh = 0)
&amp;gt; 
&amp;gt; #print results
&amp;gt; print(data.stan)
Inference for Stan model: ttestModel.
2 chains, each with iter=2000; warmup=500; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=3000.

                  mean se_mean   sd    2.5%     25%     50%     75%   97.5%
beta0           105.20    0.01 0.36  104.47  104.95  105.20  105.44  105.91
beta            -27.32    0.01 0.57  -28.43  -27.72  -27.33  -26.93  -26.22
sigma             2.79    0.00 0.21    2.41    2.64    2.77    2.92    3.23
Group_means[1]  105.20    0.01 0.36  104.47  104.95  105.20  105.44  105.91
Group_means[2]   77.88    0.01 0.45   77.01   77.59   77.87   78.18   78.76
CohensD          -9.85    0.02 0.75  -11.36  -10.35   -9.86   -9.35   -8.36
lp__           -150.78    0.04 1.25 -154.05 -151.31 -150.44 -149.88 -149.34
               n_eff Rhat
beta0           1802    1
beta            1731    1
sigma           2187    1
Group_means[1]  1802    1
Group_means[2]  2826    1
CohensD         2238    1
lp__            1272    1

Samples were drawn using NUTS(diag_e) at Mon Feb 10 14:10:29 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Means Parameterisation&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data.stan.means = stan(file = &amp;quot;ttestModelMeans.stan&amp;quot;, 
+   data = data.list.means, 
+   pars = params.means,
+   iter = nIter,
+   warmup = burnInSteps, 
+   chains = nChains, 
+   thin = thinSteps, 
+   init = &amp;quot;random&amp;quot;, #or inits=list(inits.means,inits.means)
+   refresh = 0)
&amp;gt; 
&amp;gt; #print results
&amp;gt; print(data.stan.means)
Inference for Stan model: ttestModelMeans.
2 chains, each with iter=2000; warmup=500; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=3000.

                  mean se_mean   sd    2.5%     25%     50%     75%   97.5%
beta[1]         105.21    0.01 0.37  104.51  104.96  105.20  105.44  105.92
beta[2]         -27.33    0.01 0.58  -28.47  -27.71  -27.31  -26.93  -26.23
sigma             2.78    0.00 0.20    2.43    2.64    2.76    2.90    3.22
Group_means[1]  105.21    0.01 0.37  104.51  104.96  105.20  105.44  105.92
Group_means[2]   77.88    0.01 0.44   77.02   77.59   77.88   78.17   78.77
CohensD          -9.88    0.02 0.74  -11.35  -10.40   -9.89   -9.40   -8.40
lp__           -150.74    0.03 1.26 -153.85 -151.33 -150.42 -149.83 -149.33
               n_eff Rhat
beta[1]         1439    1
beta[2]         1654    1
sigma           1955    1
Group_means[1]  1439    1
Group_means[2]  3595    1
CohensD         2056    1
lp__            1397    1

Samples were drawn using NUTS(diag_e) at Mon Feb 10 14:11:08 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If &lt;code&gt;inits=&#34;random&#34;&lt;/code&gt; the &lt;code&gt;stan&lt;/code&gt; function will randomly generate initial values between &lt;span class=&#34;math inline&#34;&gt;\(-2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; on the &lt;em&gt;unconstrained support&lt;/em&gt;. The optional additional parameter &lt;code&gt;init_r&lt;/code&gt; can be set to some value other than &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; to change the range of the randomly generated inits. Other available options include: set &lt;code&gt;inits=&#34;0&#34;&lt;/code&gt; to initialize all parameters to zero on the unconstrained support; set inital values by providing a list equal in length to the number of chains; set initial values by providing a function that returns a list for specifying the initial values of parameters for a chain.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In addition to the mean and quantiles of each of the sample nodes, the &lt;code&gt;stan&lt;/code&gt; function will calculate.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The &lt;em&gt;effective sample size&lt;/em&gt; for each sample - if &lt;code&gt;n.eff&lt;/code&gt; for a node is substantially less than the number of iterations, then it suggests poor mixing.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;em&gt;Potential scale reduction factor&lt;/em&gt; or &lt;code&gt;Rhat&lt;/code&gt; values for each sample - these are a convergence diagnostic (values of &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; indicate full convergence, values greater than &lt;span class=&#34;math inline&#34;&gt;\(1.01\)&lt;/span&gt; are indicative of non-convergence.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The total number samples collected is &lt;span class=&#34;math inline&#34;&gt;\(3000\)&lt;/span&gt;. That is, there are &lt;span class=&#34;math inline&#34;&gt;\(3000\)&lt;/span&gt; samples collected from the multidimensional posterior distribution and thus, &lt;span class=&#34;math inline&#34;&gt;\(3000\)&lt;/span&gt; samples collected from the posterior distributions of each parameter. The effective number of samples column indicates the number of independent samples represented in the total. It is clear that for all parameters the chains were well mixed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mcmc-diagnostics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;MCMC diagnostics&lt;/h1&gt;
&lt;p&gt;Again, prior to examining the summaries, we should have explored the convergence diagnostics. There are numerous ways of working with &lt;code&gt;STAN&lt;/code&gt; model fits (for exploring diagnostics and summarisation).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;extract the mcmc samples and convert them into a mcmc.list to leverage the various &lt;code&gt;mcmcplots&lt;/code&gt; routines&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;use the numerous routines that come with the &lt;code&gt;rstan&lt;/code&gt; package&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;use the routines that come with the &lt;code&gt;bayesplot&lt;/code&gt; package&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will explore all of these.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;mcmcplots&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, we need to convert the &lt;code&gt;rtsan&lt;/code&gt; object into an &lt;code&gt;mcmc.list&lt;/code&gt; object to apply the functions in the &lt;code&gt;mcmcplots&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(mcmcplots)
&amp;gt; s = as.array(data.stan.means)
&amp;gt; mcmc &amp;lt;- do.call(mcmc.list, plyr:::alply(s[, , -(length(s[1, 1, ]))], 2, as.mcmc))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we look at density and trace plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; denplot(mcmc, parms = c(&amp;quot;Group_means&amp;quot;, &amp;quot;CohensD&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(mcmc, parms = c(&amp;quot;Group_means&amp;quot;, &amp;quot;CohensD&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;rstan&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MCMC diagnostic measures that can be directly applied to &lt;code&gt;rstan&lt;/code&gt; objects via the &lt;code&gt;rstan&lt;/code&gt; package include: traceplots, autocorrelation, effective sample size and Rhat diagnostics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #traceplots
&amp;gt; stan_trace(data.stan.means, pars = c(&amp;quot;Group_means&amp;quot;, &amp;quot;CohensD&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; #autocorrelation
&amp;gt; stan_ac(data.stan.means, pars = c(&amp;quot;Group_means&amp;quot;, &amp;quot;CohensD&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; #rhat
&amp;gt; stan_rhat(data.stan.means, pars = c(&amp;quot;Group_means&amp;quot;, &amp;quot;CohensD&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; #ess
&amp;gt; stan_ess(data.stan.means, pars = c(&amp;quot;Group_means&amp;quot;, &amp;quot;CohensD&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv3-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Rhat values are a measure of sampling efficiency/effectiveness. Ideally, all values should be less than &lt;span class=&#34;math inline&#34;&gt;\(1.05\)&lt;/span&gt;. If there are values of 1.05 or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentiall slower than it could have been, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ESS indicates the number samples (or proportion of samples that the sampling algorithm) deamed effective. The sampler rejects samples on the basis of certain criterion and when it does so, the previous sample value is used. Hence while the MCMC sampling chain may contain &lt;span class=&#34;math inline&#34;&gt;\(1000\)&lt;/span&gt; samples, if there are only &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; effective samples (&lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;%), the estimated properties are not likely to be reliable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;bayesplot&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another alternative is to use the package &lt;code&gt;bayesplot&lt;/code&gt;, which provides a range of standardised diagnostic measures for assessing MCMC convergence and issues, which can be directly applied to the &lt;code&gt;rstan&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(bayesplot)
&amp;gt; 
&amp;gt; #density and trace plots
&amp;gt; mcmc_combo(as.array(data.stan.means), regex_pars = &amp;quot;Group_means|CohensD&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-validation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model validation&lt;/h1&gt;
&lt;p&gt;Residuals are not computed directly within &lt;code&gt;rstan&lt;/code&gt;. However, we can calculate them manually form the posteriors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(ggplot2)
&amp;gt; mcmc = as.matrix(data.stan.means)[, c(&amp;quot;beta[1]&amp;quot;, &amp;quot;beta[2]&amp;quot;)]
&amp;gt; # generate a model matrix
&amp;gt; newdata = data.frame(x = data$x)
&amp;gt; Xmat = model.matrix(~x, newdata)
&amp;gt; ## get median parameter estimates
&amp;gt; coefs = apply(mcmc, 2, median)
&amp;gt; fit = as.vector(coefs %*% t(Xmat))
&amp;gt; resid = data$y - fit
&amp;gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_residuals-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is no evidence that the mcmc chain did not converge on a stable posterior distribution. We are now in a position to examine the summaries of the parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-estimates&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Parameter estimates&lt;/h1&gt;
&lt;p&gt;A quick look at posterior summaries can be obtained through the command &lt;code&gt;summary&lt;/code&gt; which can be directly applied to our &lt;code&gt;rstan&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; summary(data.stan.means)
$summary
                      mean     se_mean        sd        2.5%        25%
beta[1]         105.205981 0.009650332 0.3660680  104.512893  104.95847
beta[2]         -27.327670 0.014175541 0.5765494  -28.471465  -27.70858
sigma             2.779295 0.004564259 0.2017951    2.425126    2.63877
Group_means[1]  105.205981 0.009650332 0.3660680  104.512893  104.95847
Group_means[2]   77.878310 0.007293288 0.4372737   77.017179   77.58974
CohensD          -9.883908 0.016318680 0.7398548  -11.345145  -10.39596
lp__           -150.744310 0.033765022 1.2622380 -153.847632 -151.32845
                       50%         75%       97.5%    n_eff      Rhat
beta[1]         105.197887  105.442341  105.923970 1438.928 1.0006369
beta[2]         -27.313058  -26.929462  -26.228003 1654.222 0.9996207
sigma             2.761057    2.904130    3.220382 1954.702 1.0008448
Group_means[1]  105.197887  105.442341  105.923970 1438.928 1.0006369
Group_means[2]   77.881198   78.173471   78.765424 3594.677 0.9997923
CohensD          -9.893648   -9.396558   -8.403284 2055.526 1.0013095
lp__           -150.420841 -149.826519 -149.327836 1397.489 1.0006469

$c_summary
, , chains = chain:1

                stats
parameter               mean        sd        2.5%         25%         50%
  beta[1]         105.194598 0.3722763  104.485138  104.943830  105.189222
  beta[2]         -27.316749 0.5909082  -28.503315  -27.700926  -27.303076
  sigma             2.787113 0.2017944    2.439039    2.649487    2.769964
  Group_means[1]  105.194598 0.3722763  104.485138  104.943830  105.189222
  Group_means[2]   77.877849 0.4452879   76.953676   77.589838   77.884335
  CohensD          -9.851471 0.7306980  -11.291742  -10.351622   -9.856804
  lp__           -150.774304 1.3031195 -154.143552 -151.358639 -150.446011
                stats
parameter                75%       97.5%
  beta[1]         105.430335  105.928130
  beta[2]         -26.900706  -26.189346
  sigma             2.905763    3.220038
  Group_means[1]  105.430335  105.928130
  Group_means[2]   78.167639   78.777570
  CohensD          -9.358039   -8.394201
  lp__           -149.844014 -149.328052

, , chains = chain:2

                stats
parameter               mean        sd        2.5%         25%         50%
  beta[1]         105.217363 0.3595164  104.544008  104.970466  105.208509
  beta[2]         -27.338592 0.5618086  -28.444722  -27.716894  -27.323423
  sigma             2.771476 0.2015598    2.417028    2.631247    2.750654
  Group_means[1]  105.217363 0.3595164  104.544008  104.970466  105.208509
  Group_means[2]   77.878771 0.4292579   77.031912   77.589743   77.878030
  CohensD          -9.916344 0.7477366  -11.431004  -10.435551   -9.924630
  lp__           -150.714316 1.2196850 -153.673281 -151.305580 -150.383196
                stats
parameter                75%       97.5%
  beta[1]         105.450568  105.916257
  beta[2]         -26.963106  -26.265061
  sigma             2.898644    3.219905
  Group_means[1]  105.450568  105.916257
  Group_means[2]   78.179664   78.753253
  CohensD          -9.430001   -8.422613
  lp__           -149.795340 -149.327597&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Group A is typically &lt;span class=&#34;math inline&#34;&gt;\(27.3\)&lt;/span&gt; units greater than Group B. The &lt;span class=&#34;math inline&#34;&gt;\(95\)&lt;/span&gt;% confidence interval for the difference between Group A and B does not overlap with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; implying a significant difference between the two groups.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;graphical-summaries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Graphical summaries&lt;/h1&gt;
&lt;p&gt;A nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group. We do this by loading functions in the package &lt;code&gt;broom&lt;/code&gt; and &lt;code&gt;dplyr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(broom)
&amp;gt; library(dplyr)
&amp;gt; mcmc = as.matrix(data.stan.means)
&amp;gt; ## Calculate the fitted values
&amp;gt; newdata = data.frame(x = levels(data$x))
&amp;gt; Xmat = model.matrix(~x, newdata)
&amp;gt; coefs = mcmc[, c(&amp;quot;beta[1]&amp;quot;, &amp;quot;beta[2]&amp;quot;)]
&amp;gt; fit = coefs %*% t(Xmat)
&amp;gt; newdata = newdata %&amp;gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;))
&amp;gt; newdata
  x  estimate std.error  conf.low conf.high
1 A 105.20598 0.3660680 104.52503 105.93588
2 B  77.87831 0.4372737  76.99792  78.74455
&amp;gt; 
&amp;gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_pointrange(aes(ymin = conf.low,
+     ymax = conf.high)) + scale_y_continuous(&amp;quot;Y&amp;quot;) + scale_x_discrete(&amp;quot;X&amp;quot;) +
+     theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_post1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data, aes(y = y,
+     x = x), color = &amp;quot;gray&amp;quot;) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +
+     scale_y_continuous(&amp;quot;Y&amp;quot;) + scale_x_discrete(&amp;quot;X&amp;quot;) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_post2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since &lt;span class=&#34;math inline&#34;&gt;\(\text{resid}=\text{obs}−\text{fitted}\)&lt;/span&gt; and the fitted values depend only on the single predictor we are interested in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; ## Calculate partial residuals fitted values
&amp;gt; fdata = rdata = data
&amp;gt; fMat = rMat = model.matrix(~x, fdata)
&amp;gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))
&amp;gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))
&amp;gt; rdata = rdata %&amp;gt;% mutate(partial.resid = resid + fit)
&amp;gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),
+     color = &amp;quot;gray&amp;quot;) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +
+     scale_y_continuous(&amp;quot;Y&amp;quot;) + scale_x_discrete(&amp;quot;X&amp;quot;) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_post3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;effect-sizes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Effect sizes&lt;/h1&gt;
&lt;p&gt;We can compute summaries for our effect size of interest (e.g. Cohen’s or the percentage ES) by post-processing our posterior distributions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc = as.matrix(data.stan.means)
&amp;gt; ## Cohen&amp;#39;s D
&amp;gt; cohenD = mcmc[, &amp;quot;beta[2]&amp;quot;]/mcmc[, &amp;quot;sigma&amp;quot;]
&amp;gt; tidyMCMC(as.mcmc(cohenD), conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;)
# A tibble: 1 x 5
  term  estimate std.error conf.low conf.high
  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 var1     -9.88     0.740    -11.3     -8.38
&amp;gt; 
&amp;gt; # Percentage change (relative to Group A)
&amp;gt; ES = 100 * mcmc[, &amp;quot;beta[2]&amp;quot;]/mcmc[, &amp;quot;beta[1]&amp;quot;]
&amp;gt; 
&amp;gt; # Probability that the effect is greater than 10% (a decline of &amp;gt;10%)
&amp;gt; sum(-1 * ES &amp;gt; 10)/length(ES)
[1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;probability-statements&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Probability statements&lt;/h1&gt;
&lt;p&gt;Any sort of probability statements of interest about our effect size can be computed in a relatively easy way by playing around with the posteriors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc = as.matrix(data.stan.means)
&amp;gt; 
&amp;gt; # Percentage change (relative to Group A)
&amp;gt; ES = 100 * mcmc[, &amp;quot;beta[2]&amp;quot;]/mcmc[, &amp;quot;beta[1]&amp;quot;]
&amp;gt; hist(ES)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/prob_stat-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; # Probability that the effect is greater than 10% (a decline of &amp;gt;10%)
&amp;gt; sum(-1 * ES &amp;gt; 10)/length(ES)
[1] 1
&amp;gt; 
&amp;gt; # Probability that the effect is greater than 25% (a decline of &amp;gt;25%)
&amp;gt; sum(-1 * ES &amp;gt; 25)/length(ES)
[1] 0.978&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;finite-population-standard-deviations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Finite population standard deviations&lt;/h1&gt;
&lt;p&gt;Estimates for the variability associated with between and within group differences can also be easily obtained.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2 x 5
  term     estimate std.error conf.low conf.high
  &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 sd.x        19.3     0.408     18.5      20.1 
2 sd.resid     2.75    0.0207     2.74      2.79
# A tibble: 2 x 5
  term     estimate std.error conf.low conf.high
  &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 sd.x         87.5     0.238     87.1      87.8
2 sd.resid     12.5     0.238     12.2      12.9&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unequally-varied-populations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Unequally varied populations&lt;/h1&gt;
&lt;p&gt;We can also generate data assuming two populations with different variances, e.g. between male and female subgroups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; set.seed(123)
&amp;gt; n1 &amp;lt;- 60  #sample size from population 1
&amp;gt; n2 &amp;lt;- 40  #sample size from population 2
&amp;gt; mu1 &amp;lt;- 105  #population mean of population 1
&amp;gt; mu2 &amp;lt;- 77.5  #population mean of population 2
&amp;gt; sigma1 &amp;lt;- 3  #standard deviation of population 1
&amp;gt; sigma2 &amp;lt;- 2  #standard deviation of population 2
&amp;gt; n &amp;lt;- n1 + n2  #total sample size
&amp;gt; y1 &amp;lt;- rnorm(n1, mu1, sigma1)  #population 1 sample
&amp;gt; y2 &amp;lt;- rnorm(n2, mu2, sigma2)  #population 2 sample
&amp;gt; y &amp;lt;- c(y1, y2)
&amp;gt; x &amp;lt;- factor(rep(c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;), c(n1, n2)))  #categorical listing of the populations
&amp;gt; xn &amp;lt;- rep(c(0, 1), c(n1, n2))  #numerical version of the population category
&amp;gt; data2 &amp;lt;- data.frame(y, x, xn)  # dataset
&amp;gt; head(data2)  #print out the first six rows of the data set
         y x xn
1 103.3186 A  0
2 104.3095 A  0
3 109.6761 A  0
4 105.2115 A  0
5 105.3879 A  0
6 110.1452 A  0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Start by defining the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i = \beta_0 + \beta_1x_i + \epsilon, \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_1 \sim \text{Normal}(0,\sigma_1)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(x_1=0\)&lt;/span&gt; (females), and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_2 \sim \text{Normal}(0,\sigma_2)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(x_2=1\)&lt;/span&gt; (males). In &lt;code&gt;STAN&lt;/code&gt; code, the model becomes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; stanStringv3 = &amp;quot; 
+  data {
+  int n;
+  vector [n] y;
+  vector [n] x;
+  int&amp;lt;lower=1,upper=2&amp;gt; xn[n];
+  }
+  parameters {
+  vector &amp;lt;lower=0, upper=100&amp;gt;[2] sigma;
+  real beta0;
+  real beta;
+  }
+  transformed parameters {
+  }
+  model {
+  vector [n] mu;
+  //Priors
+  beta0 ~ normal(0,1000);
+  beta ~ normal(0,1000);
+  sigma ~ cauchy(0,25);
+ 
+  mu = beta0 + beta*x;
+  //Likelihood
+  for (i in 1:n) y[i] ~ normal(mu[i], sigma[xn[i]]);
+  }
+  generated quantities {
+  vector [2] Group_means;
+  real CohensD;
+  real CLES;
+ 
+  Group_means[1] = beta0;
+  Group_means[2] = beta0+beta;
+  CohensD = beta /(sum(sigma)/2);
+  CLES = normal_cdf(beta /sum(sigma),0,1);  
+  }
+  
+  &amp;quot;
&amp;gt; 
&amp;gt; ## write the model to a text file 
&amp;gt; writeLines(stanStringv3,con=&amp;quot;ttestModelv3.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We specify priors directly on &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_2\)&lt;/span&gt; using Cauchy distributions with a scale of &lt;span class=&#34;math inline&#34;&gt;\(25\)&lt;/span&gt;. Next, arrange the data as a list (as required by &lt;code&gt;STAN&lt;/code&gt;) and define the MCMC parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data2.list &amp;lt;- with(data, list(y = y, x = (xn - 1), xn = xn, n = nrow(data)))
&amp;gt; paramsv3 &amp;lt;- c(&amp;quot;beta0&amp;quot;,&amp;quot;beta&amp;quot;,&amp;quot;sigma&amp;quot;,&amp;quot;Group_means&amp;quot;,&amp;quot;CohensD&amp;quot;, &amp;quot;CLES&amp;quot;)
&amp;gt; burnInSteps = 500
&amp;gt; nChains = 2
&amp;gt; thinSteps = 1
&amp;gt; nIter = 2000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, fit the model in &lt;code&gt;STAN&lt;/code&gt; and print the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data.stanv3 = stan(file = &amp;quot;ttestModelv3.stan&amp;quot;, 
+   data = data2.list, 
+   pars = paramsv3,
+   iter = nIter,
+   warmup = burnInSteps, 
+   chains = nChains, 
+   thin = thinSteps, 
+   init = &amp;quot;random&amp;quot;, #or inits=list(inits,inits)
+   refresh = 0)
&amp;gt; 
&amp;gt; #print results
&amp;gt; print(data.stanv3)
Inference for Stan model: ttestModelv3.
2 chains, each with iter=2000; warmup=500; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=3000.

                  mean se_mean   sd    2.5%     25%     50%     75%   97.5%
beta0           105.21    0.01 0.36  104.51  104.97  105.21  105.44  105.92
beta            -27.34    0.01 0.57  -28.45  -27.71  -27.35  -26.96  -26.21
sigma[1]          2.79    0.01 0.27    2.31    2.60    2.77    2.97    3.38
sigma[2]          2.88    0.01 0.34    2.31    2.63    2.84    3.07    3.65
Group_means[1]  105.21    0.01 0.36  104.51  104.97  105.21  105.44  105.92
Group_means[2]   77.86    0.01 0.44   77.00   77.57   77.86   78.15   78.75
CohensD          -9.70    0.02 0.76  -11.23  -10.23   -9.69   -9.17   -8.26
CLES              0.00    0.00 0.00    0.00    0.00    0.00    0.00    0.00
lp__           -150.30    0.04 1.42 -153.88 -151.02 -149.99 -149.25 -148.53
               n_eff Rhat
beta0           2426    1
beta            2359    1
sigma[1]        2166    1
sigma[2]        2547    1
Group_means[1]  2426    1
Group_means[2]  3478    1
CohensD         2468    1
CLES            1875    1
lp__            1277    1

Samples were drawn using NUTS(diag_e) at Mon Feb 10 14:11:55 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references hanging-indent&#34;&gt;
&lt;div id=&#34;ref-gelman2015stan&#34;&gt;
&lt;p&gt;Gelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” &lt;em&gt;Journal of Educational and Behavioral Statistics&lt;/em&gt; 40 (5): 530–43.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2006prior&#34;&gt;
&lt;p&gt;Gelman, Andrew, and others. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).” &lt;em&gt;Bayesian Analysis&lt;/em&gt; 1 (3): 515–34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rstanpackage&#34;&gt;
&lt;p&gt;Stan Development Team. 2018. “RStan: The R Interface to Stan.” &lt;a href=&#34;http://mc-stan.org/&#34;&gt;http://mc-stan.org/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Super basic introduction to STAN</title>
      <link>/stan/basic-introduction-to-stan/super-basic-introduction-to-stan/</link>
      <pubDate>Wed, 03 Jul 2019 21:13:14 -0500</pubDate>
      
      <guid>/stan/basic-introduction-to-stan/super-basic-introduction-to-stan/</guid>
      <description>


&lt;p&gt;The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using &lt;code&gt;STAN&lt;/code&gt; via &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Prerequisites:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latest version of &lt;code&gt;R&lt;/code&gt;, which can be downloaded and installed for Windows, Mac or Linux OS from the &lt;a href=&#34;https://www.r-project.org/%7D&#34;&gt;CRAN&lt;/a&gt; website&lt;/li&gt;
&lt;li&gt;I also &lt;strong&gt;strongly&lt;/strong&gt; recommend to download and install &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;Rstudio&lt;/a&gt;, an integrated development environment which provides an “user-friendly” interaction with &lt;code&gt;R&lt;/code&gt; (e.g. many drop-down menus, tabs, customisation options)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Preliminaries&lt;/h1&gt;
&lt;div id=&#34;what-is-stan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is STAN?&lt;/h2&gt;
&lt;p&gt;Stan provides full Bayesian inference for continuous-variable models through Markov Chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling&lt;/p&gt;
&lt;p&gt;&lt;code&gt;STAN&lt;/code&gt; is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (&lt;span class=&#34;citation&#34;&gt;Gelman, Lee, and Guo (2015)&lt;/span&gt;). &lt;code&gt;STAN&lt;/code&gt; is a free software and a probabilistic programming language for specifying statistical models using a specific class of MCMC algorithms known as &lt;strong&gt;H&lt;/strong&gt;amiltonian &lt;strong&gt;M&lt;/strong&gt;onte &lt;strong&gt;C&lt;/strong&gt;arlo methods (HMC). The latest version of &lt;code&gt;STAN&lt;/code&gt; can be dowloaded from the web &lt;a href=&#34;https://mc-stan.org/users/interfaces/&#34;&gt;repository&lt;/a&gt; and is available for different OS. There are different &lt;code&gt;R&lt;/code&gt; packages which function as frontends for &lt;code&gt;STAN&lt;/code&gt;. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the &lt;code&gt;rstan&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Stan Development Team (2018)&lt;/span&gt;) and show how to fit &lt;code&gt;STAN&lt;/code&gt; models using this package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-stan-and-rstan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing STAN and rstan&lt;/h2&gt;
&lt;p&gt;Unlike other Bayesian software, such as &lt;code&gt;JAGS&lt;/code&gt; or &lt;code&gt;OpenBUGS&lt;/code&gt;, it is not required to separately install the program and the corresponding frontend &lt;code&gt;R&lt;/code&gt; package. Indeed, installing the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;rstan&lt;/code&gt; will automatically install &lt;code&gt;STAN&lt;/code&gt; on your machine. However, you will also need to make sure to having installed on your pc a &lt;code&gt;C++&lt;/code&gt; compiler which is used by &lt;code&gt;rstan&lt;/code&gt; to fit the models. Under a Windows OS, for example, this can be done by installing &lt;code&gt;Rtools&lt;/code&gt;, a collection of resources for building packages for &lt;code&gt;R&lt;/code&gt;, which is freely available from the web &lt;a href=&#34;https://cran.r-project.org/bin/windows/Rtools/&#34;&gt;repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next, install the package &lt;code&gt;rstan&lt;/code&gt; from within &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;Rstudio&lt;/code&gt;, via the package installer or by typing in the command line&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;rstan&amp;quot;, dependencies = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;dependencies = TRUE&lt;/code&gt; option will automatically install all the packages on which the functions in the &lt;code&gt;rstan&lt;/code&gt; package rely.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic model&lt;/h1&gt;
&lt;div id=&#34;simulate-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulate data&lt;/h2&gt;
&lt;p&gt;For an example dataset, I simulate my own data in &lt;code&gt;R&lt;/code&gt;. I create a continuous outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as a function of one predictor &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and a disturbance term &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficients, i.e.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \beta_1 x + \epsilon  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;R&lt;/code&gt; commands which I use to simulate the data are the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; n_sim=100; set.seed(123)
&amp;gt; x=rnorm(n_sim, mean = 5, sd = 2)
&amp;gt; epsilon=rnorm(n_sim, mean = 0, sd = 1)
&amp;gt; beta0=1.5
&amp;gt; beta1=1.2
&amp;gt; y=beta0 + beta1 * x + epsilon
&amp;gt; n_sim=as.integer(n_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I define all the data for &lt;code&gt;STAN&lt;/code&gt; in a list object&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; datalist=list(&amp;quot;y&amp;quot;=y,&amp;quot;x&amp;quot;=x,&amp;quot;n_sim&amp;quot;=n_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model file&lt;/h2&gt;
&lt;p&gt;Now, I write the model for &lt;code&gt;STAN&lt;/code&gt; and save it as a stan file named &lt;code&gt;&#34;basic.mod.stan&#34;&lt;/code&gt; in the current working directory&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod= &amp;quot;
+ data {
+ int&amp;lt;lower=0&amp;gt; n_sim;
+ vector[n_sim] y;
+ vector[n_sim] x;
+ }
+ parameters {
+ real beta0;
+ real beta1;
+ real&amp;lt;lower=0&amp;gt; sigma;
+ }
+ transformed parameters {
+ vector[n_sim] mu;
+ mu=beta0 + beta1*x;
+ } 
+ model {
+ sigma~uniform(0,100);
+ beta0~normal(0,1000);
+ beta1~normal(0,1000);
+ y~normal(mu,sigma);
+ }
+ 
+ &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;STAN&lt;/code&gt; models are written using an imperative programming language, which means that the order in which you write the elements in your model file matters, i.e. you first need to define your variables (e.g. integers, vectors, matrices, etc.), the constraints which define the range of values your variable can take (e.g. only positive values for standard deviations), and finally define the relationship among the variables (e.g. one is a liner function of another).&lt;/p&gt;
&lt;p&gt;A Stan model is defined by six program blocks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data (required). The &lt;em&gt;data block&lt;/em&gt; reads external information – e.g. data vectors, matrices, integers, etc.&lt;/li&gt;
&lt;li&gt;Transformed data (optional). The &lt;em&gt;transformed data block&lt;/em&gt; allows for preprocessing of the data – e.g. transformation or rescaling of the data.&lt;/li&gt;
&lt;li&gt;Parameters (required). The &lt;em&gt;parameters block&lt;/em&gt; defines the sampling space – e.g. parameters to which prior distributions must be assigned.&lt;/li&gt;
&lt;li&gt;Transformed parameters (optional). The &lt;em&gt;transformed parameters block&lt;/em&gt; allows for parameter processing before the posterior is computed – e.g. tranformation or rescaling of the parameters.&lt;/li&gt;
&lt;li&gt;Model (required). In the &lt;em&gt;model block&lt;/em&gt; we define our posterior distributions – e.g. choice of distributions for all variables.&lt;/li&gt;
&lt;li&gt;Generated quantities (optional). The &lt;em&gt;generated quantities block&lt;/em&gt; allows for postprocessing – e.g. backtranformation of the parameters using the posterior samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this introduction I consider a very simple model which only requires the specification of four blocks in the &lt;code&gt;STAN&lt;/code&gt; model. In the data block, I first define the size of the sample &lt;code&gt;n_sim&lt;/code&gt; as a positive integer number using the expression &lt;code&gt;int&amp;lt;lower=0&amp;gt; n_sim&lt;/code&gt;; then I declare the two variables &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; as reals (or vectors) with length equal to N. In the parameters block, I define the coefficients for the linear regression &lt;code&gt;beta0&lt;/code&gt; and &lt;code&gt;beta1&lt;/code&gt; (as two real numbers) and the standard deviation parameter &lt;code&gt;sigma&lt;/code&gt; (as a positive real number). In the transformed parameters block, I define the conditional mean &lt;code&gt;mu&lt;/code&gt; (a real vector of length &lt;code&gt;N&lt;/code&gt;) as a linear function of the intercept &lt;code&gt;beta0&lt;/code&gt;, the slope &lt;code&gt;beta1&lt;/code&gt;, and the covariate &lt;code&gt;x&lt;/code&gt;. Finally, in the model block, I assign weakly informative priors to the regression coefficients and the standard deviation parameters, and I model the outcome data &lt;code&gt;y&lt;/code&gt; using a normal distribution indexed by the conditional mean &lt;code&gt;mu&lt;/code&gt; and the standard deviation &lt;code&gt;sigma&lt;/code&gt; parameters. In many cases, &lt;code&gt;STAN&lt;/code&gt; uses sampling statements which can be vectorised, i.e. you do not need to use for loop statements.&lt;/p&gt;
&lt;p&gt;To write and save the model as the text file “basic.mod.stan” in the current working directory, I use the &lt;code&gt;writeLines&lt;/code&gt; function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; writeLines(basic.mod, &amp;quot;basic.mod.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pre-processing&lt;/h2&gt;
&lt;p&gt;Define the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in &lt;code&gt;STAN&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params=c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;)
&amp;gt; inits=function(){list(&amp;quot;beta0&amp;quot;=rnorm(1), &amp;quot;beta1&amp;quot;=rnorm(1))}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters in the object &lt;code&gt;inits&lt;/code&gt; which is then passed to the &lt;code&gt;stan&lt;/code&gt; function in &lt;code&gt;rstan&lt;/code&gt;. However, for more complex models, this may not be immediate and a lot of trial and error may be required. However, &lt;code&gt;STAN&lt;/code&gt; can automatically select the initial values for all parameters randomly. This can be achieved by setting &lt;code&gt;inits=&#34;random&#34;&lt;/code&gt;, which is then passed to the &lt;code&gt;stan&lt;/code&gt; function in &lt;code&gt;rstan&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before using &lt;code&gt;rstan&lt;/code&gt; for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(rstan)
&amp;gt; set.seed(123)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit the model&lt;/h2&gt;
&lt;p&gt;Now, we can fit the model in &lt;code&gt;STAN&lt;/code&gt; using the &lt;code&gt;stan&lt;/code&gt; function in the &lt;code&gt;rstan&lt;/code&gt; package and save it in the object &lt;code&gt;basic.mod&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod&amp;lt;-stan(data = datalist, pars = params, iter = 9000, 
+   warmup = 1000, init = inits, chains = 2, file = &amp;quot;basic.mod.stan&amp;quot;)

SAMPLING FOR MODEL &amp;#39;basic&amp;#39; NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 9000 [  0%]  (Warmup)
Chain 1: Iteration:  900 / 9000 [ 10%]  (Warmup)
Chain 1: Iteration: 1001 / 9000 [ 11%]  (Sampling)
Chain 1: Iteration: 1900 / 9000 [ 21%]  (Sampling)
Chain 1: Iteration: 2800 / 9000 [ 31%]  (Sampling)
Chain 1: Iteration: 3700 / 9000 [ 41%]  (Sampling)
Chain 1: Iteration: 4600 / 9000 [ 51%]  (Sampling)
Chain 1: Iteration: 5500 / 9000 [ 61%]  (Sampling)
Chain 1: Iteration: 6400 / 9000 [ 71%]  (Sampling)
Chain 1: Iteration: 7300 / 9000 [ 81%]  (Sampling)
Chain 1: Iteration: 8200 / 9000 [ 91%]  (Sampling)
Chain 1: Iteration: 9000 / 9000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.078 seconds (Warm-up)
Chain 1:                0.593 seconds (Sampling)
Chain 1:                0.671 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL &amp;#39;basic&amp;#39; NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 9000 [  0%]  (Warmup)
Chain 2: Iteration:  900 / 9000 [ 10%]  (Warmup)
Chain 2: Iteration: 1001 / 9000 [ 11%]  (Sampling)
Chain 2: Iteration: 1900 / 9000 [ 21%]  (Sampling)
Chain 2: Iteration: 2800 / 9000 [ 31%]  (Sampling)
Chain 2: Iteration: 3700 / 9000 [ 41%]  (Sampling)
Chain 2: Iteration: 4600 / 9000 [ 51%]  (Sampling)
Chain 2: Iteration: 5500 / 9000 [ 61%]  (Sampling)
Chain 2: Iteration: 6400 / 9000 [ 71%]  (Sampling)
Chain 2: Iteration: 7300 / 9000 [ 81%]  (Sampling)
Chain 2: Iteration: 8200 / 9000 [ 91%]  (Sampling)
Chain 2: Iteration: 9000 / 9000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.078 seconds (Warm-up)
Chain 2:                0.594 seconds (Sampling)
Chain 2:                0.672 seconds (Total)
Chain 2: &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Different packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the &lt;code&gt;bayesplot&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Gabry and Mahr (2017)&lt;/span&gt;) to obtain graphical diagnostics and results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;bayesplot&amp;quot;)
&amp;gt; library(bayesplot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, density and trace plots can be obtained by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc_combo(as.array(basic.mod),regex_pars=&amp;quot;beta0|beta1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/basic-introduction-to-stan/2018-07-06-super-basic-introduction-to-stan_files/figure-html/diagnostic3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;This tutorial was simply a brief introduction on how simple linear regression models can be fitted using the Bayesian software &lt;code&gt;STAN&lt;/code&gt; via the &lt;code&gt;rstan&lt;/code&gt; package. Although this may seem a complex procedure compared with simply fitting a linear model under the frequentist framework, however, the real advantages of Bayesian methods become evident when the complexity of the analysis is increased (which is often the case in real applications). Indeed, the flexibility in Bayesian modelling allows to account for increasingly complex models in a relatively easy way. In addition, Bayesian methods are ideal when the interest is in taking into account the potential impact that different sources of uncertainty may have on the final results, as they allow the natural propagation of uncertainty throughout each quantity in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-gabry2017bayesplot&#34;&gt;
&lt;p&gt;Gabry, J, and T Mahr. 2017. “Bayesplot: Plotting for Bayesian Models.” &lt;em&gt;R Package Version&lt;/em&gt; 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2015stan&#34;&gt;
&lt;p&gt;Gelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” &lt;em&gt;Journal of Educational and Behavioral Statistics&lt;/em&gt; 40 (5): 530–43.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rstanpackage&#34;&gt;
&lt;p&gt;Stan Development Team. 2018. “RStan: The R Interface to Stan.” &lt;a href=&#34;http://mc-stan.org/&#34;&gt;http://mc-stan.org/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
