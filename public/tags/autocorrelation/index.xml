<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>autocorrelation on Andrea Gabrio</title>
    <link>/tags/autocorrelation/</link>
    <description>Recent content in autocorrelation on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 08 Feb 2020 21:13:14 -0500</lastBuildDate>
    
	    <atom:link href="/tags/autocorrelation/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Temporal Autocorrelation - JAGS</title>
      <link>/jags/autocorrelation-jags/autocorrelation-jags/</link>
      <pubDate>Sat, 08 Feb 2020 21:13:14 -0500</pubDate>
      
      <guid>/jags/autocorrelation-jags/autocorrelation-jags/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. &lt;code&gt;BUGS&lt;/code&gt; (Bayesian inference Using &lt;em&gt;Gibbs Sampling&lt;/em&gt;) is an algorithm and supporting language (resembling &lt;code&gt;R&lt;/code&gt;) dedicated to performing the Gibbs sampling implementation of &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; (MCMC) method. Dialects of the &lt;code&gt;BUGS&lt;/code&gt; language are implemented within three main projects:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OpenBUGS&lt;/strong&gt; - written in component pascal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;JAGS&lt;/strong&gt; - (Just Another Gibbs Sampler) - written in &lt;code&gt;C++&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;STAN&lt;/strong&gt; - a dedicated Bayesian modelling framework written in &lt;code&gt;C++&lt;/code&gt; and implementing &lt;em&gt;Hamiltonian&lt;/em&gt; MCMC samplers.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of &lt;code&gt;R&lt;/code&gt;, and thus, they are best accessed from within &lt;code&gt;R&lt;/code&gt; itself. As such there are multiple packages devoted to interfacing with the various software implementations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2OpenBUGS&lt;/em&gt; - interfaces with &lt;code&gt;OpenBUGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2jags&lt;/em&gt; - interfaces with &lt;code&gt;JAGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;rstan&lt;/em&gt; - interfaces with &lt;code&gt;STAN&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial will demonstrate how to fit models in &lt;code&gt;JAGS&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Plummer (2004)&lt;/span&gt;) using the package &lt;code&gt;R2jags&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Su et al. (2015)&lt;/span&gt;) as interface, which also requires to load some other packages.&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Up until now (in the proceeding tutorials), the focus has been on models that adhere to specific assumptions about the underlying populations (and data). Indeed, both before and immediately after fitting these models, I have stressed the importance of evaluating and validating the proposed and fitted models to ensure reliability of the models. It is now worth us revisiting those fundamental assumptions as well as exploring the options that are available when the populations (data) do not conform. Let’s explore a simple linear regression model to see how each of the assumptions relate to the model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i = \beta_0 + \beta_1x_i + \epsilon_i \;\;\; \text{with} \;\;\; \epsilon_i \sim \text{Normal}(0, \sigma^2).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above simple statistical model models the &lt;strong&gt;linear relationship&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; against &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. The residuals (&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;) are assumed to be &lt;strong&gt;normally distributed&lt;/strong&gt; with a mean of zero and a constant (yet unknown) variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, &lt;strong&gt;homogeneity of variance&lt;/strong&gt;). The residuals (and thus observations) are also assumed to all be &lt;strong&gt;independent&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Homogeneity of variance and independence are encapsulated within the single symbol for variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;). In assuming equal variances and independence, we are actually making an assumption about the variance-covariance structure of the populations (and thus residuals). Specifically, we assume that all populations are equally varied and thus can be represented well by a single variance term (all diagonal values in a &lt;span class=&#34;math inline&#34;&gt;\(N\times N\)&lt;/span&gt; covariance matrix are the same, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;) and the covariances between each population are zero (off diagonals). In simple regression, each observation (data point) represents a single observation drawn (sampled) from an entire population of possible observations. The above covariance structure thus assumes that the covariance between each population (observation) is zero - that is, each observation is completely independent of each other observation. Whilst it is mathematically convenient when data conform to these conditions (normality, homogeneity of variance, independence and linearity), data often violate one or more of these assumptions. In the following, I want to discuss and explore the causes and options for dealing with non-compliance to each of these conditions. By gaining a better understanding of how the various model fitting engines perform their task, we are better equipped to accommodate aspects of the data that don’t otherwise conform to the simple regression assumptions. In this tutorial we specifically focus on the topic of heterogeneity of the variance.&lt;/p&gt;
&lt;p&gt;In order that the estimated parameters represent the underlying populations in an unbiased manner, the residuals (and thus each each observation) must be independent. However, what if we were sampling a population over time and we were interested in investigating how changes in a response relate to changes in a predictor (such as rainfall). For any response that does not “reset” itself on a regular basis, the state of the population (the value of its response) at a given time is likely to be at least partly dependent on the state of the population at the sampling time before. We can further generalise the above into:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i \sim Dist(\mu_i),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu_i=\boldsymbol X \boldsymbol \beta + \boldsymbol Z \boldsymbol \gamma\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol \beta\)&lt;/span&gt; representing the &lt;em&gt;fixed data structure&lt;/em&gt; and &lt;em&gt;fixed effects&lt;/em&gt;, respectively, while with &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol Z\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol \gamma\)&lt;/span&gt; represent the &lt;em&gt;varying data structure&lt;/em&gt; and &lt;em&gt;varying effects&lt;/em&gt;, respectively. In simple regression, there are no “varying” effects, and thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \boldsymbol \gamma \sim MVN(\boldsymbol 0, \boldsymbol \Sigma),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol \Sigma\)&lt;/span&gt; is a variance-covariance matrix of the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \boldsymbol \Sigma =  \frac{\sigma^2}{1-\rho^2}
  \begin{bmatrix}
   1 &amp;amp; \rho^{\phi_{1,2}} &amp;amp; \ldots &amp;amp; \rho^{\phi_{1,n}} \\
   \rho^{\phi_{2,1}} &amp;amp; 1 &amp;amp; \ldots &amp;amp; \vdots\\
   \vdots &amp;amp; \ldots &amp;amp; 1 &amp;amp; \vdots\\
   \rho^{\phi_{n,1}} &amp;amp; \ldots &amp;amp; \ldots &amp;amp; 1
   \end{bmatrix}. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice that this introduces a very large number of additional parameters that require estimating: &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; (error variance), &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; (base autocorrelation) and each of the individual covariances (&lt;span class=&#34;math inline&#34;&gt;\(\rho^{\phi_{n,n}}\)&lt;/span&gt;). Hence, there are always going to be more parameters to estimate than there are date avaiable to use to estimate these paramters. We typically make one of a number of alternative assumptions so as to make this task more manageable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When we assume that all residuals are independent (regular regression), i.e. &lt;span class=&#34;math inline&#34;&gt;\(\rho=0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol \Sigma\)&lt;/span&gt; is essentially equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 \boldsymbol I\)&lt;/span&gt; and we simply use:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \boldsymbol \gamma \sim N( 0,\sigma^2).\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We could assume there is a reasonably simple pattern of correlation that declines over time. The simplest of these is a &lt;em&gt;first order autoregressive&lt;/em&gt; (AR1) structure in which exponent on the correlation declines linearly according to the time lag (&lt;span class=&#34;math inline&#34;&gt;\(\mid t - s\mid\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \boldsymbol \Sigma =  \frac{\sigma^2}{1-\rho^2}
  \begin{bmatrix}
   1 &amp;amp; \rho &amp;amp; \ldots &amp;amp; \rho^{\mid t-s \mid} \\
   \rho &amp;amp; 1 &amp;amp; \ldots &amp;amp; \vdots\\
   \vdots &amp;amp; \ldots &amp;amp; 1 &amp;amp; \vdots\\
   \rho^{\mid t-s \mid } &amp;amp; \ldots &amp;amp; \ldots &amp;amp; 1
   \end{bmatrix}. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note, in making this assumption, we are also assuming that the degree of correlation is dependent only on the lag and not on when the lag occurs (stationarity). That is all lag 1 residual pairs will have the same degree of correlation, all the lag &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; pairs will have the same correlation and so on.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;first-order-autocorrelation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;First order autocorrelation&lt;/h1&gt;
&lt;p&gt;Consider an example, in which the number of individuals at time &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; will be partly dependent on the number of individuals present at time &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. Clearly then, the observations (and thus residuals) are not fully independent - there is an auto-regressive correlation dependency structure. We could accommodate this lack of independence by fitting a model that incorporates a AR1 variance-covariance structure. Alternatively, we fit the following model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_{it} \sim Dist(\mu_{it}),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu_{it}=\boldsymbol X \boldsymbol \beta + \rho \epsilon_{i,t-1} + \gamma_{it},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and where &lt;span class=&#34;math inline&#34;&gt;\(\gamma \sim N(0, \sigma^2)\)&lt;/span&gt;. In this version of the model, we are stating that the expected value of an observation is equal to the regular linear predictor plus the autocorrelation parameter (&lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;) multipled by the residual associated with the previous observation plus the regular independently distributed noise (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;). Such a model is substantially faster to fit, although along with stationarity assumes in estimating the autocorrelation parameter, only the smallest lags are used. To see this in action, we will first generate some temporally auto-correlated data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; set.seed(126)
&amp;gt; n = 50
&amp;gt; a &amp;lt;- 20  #intercept
&amp;gt; b &amp;lt;- 0.2  #slope
&amp;gt; x &amp;lt;- round(runif(n, 1, n), 1)  #values of the year covariate
&amp;gt; year &amp;lt;- 1:n
&amp;gt; sigma &amp;lt;- 20
&amp;gt; rho &amp;lt;- 0.8
&amp;gt; 
&amp;gt; library(nlme)
&amp;gt; ## define a constructor for a first-order
&amp;gt; ## correlation structure
&amp;gt; ar1 &amp;lt;- corAR1(form = ~year, value = rho)
&amp;gt; ## initialize this constructor against our data
&amp;gt; AR1 &amp;lt;- Initialize(ar1, data = data.frame(year))
&amp;gt; ## generate a correlation matrix
&amp;gt; V &amp;lt;- corMatrix(AR1)
&amp;gt; ## Cholesky factorization of V
&amp;gt; Cv &amp;lt;- chol(V)
&amp;gt; ## simulate AR1 errors
&amp;gt; e &amp;lt;- t(Cv) %*% rnorm(n, 0, sigma)  # cov(e) = V * sig^2
&amp;gt; ## generate response
&amp;gt; y &amp;lt;- a + b * x + e
&amp;gt; data.temporalCor = data.frame(y = y, x = x, year = year)
&amp;gt; write.table(data.temporalCor, file = &amp;quot;data.temporalCor.csv&amp;quot;,
+     sep = &amp;quot;,&amp;quot;, quote = F, row.names = FALSE)
&amp;gt; 
&amp;gt; pairs(data.temporalCor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/generate_data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will now proceed to analyse these data via both of the above techniques for &lt;code&gt;JAGS&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;incorporating AR1 residual autocorrelation structure&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;incorporating lagged residuals into the model&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;incorporating-lagged-residuals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Incorporating lagged residuals&lt;/h1&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;We proceed to code the model into &lt;code&gt;JAGS&lt;/code&gt; (remember that in this software normal distribution are parameterised in terms of precisions &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; rather than variances, where &lt;span class=&#34;math inline&#34;&gt;\(\tau=\frac{1}{\sigma^2}\)&lt;/span&gt;). Define the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; modelString = &amp;quot; 
+   model {
+   #Likelihood
+   for (i in 1:n) {
+   fit[i] &amp;lt;- inprod(beta[],X[i,])
+   y[i] ~ dnorm(mu[i],tau.cor)
+   }
+   e[1] &amp;lt;- (y[1] - fit[1])
+   mu[1] &amp;lt;- fit[1]
+   for (i in 2:n) {
+   e[i] &amp;lt;- (y[i] - fit[i]) #- phi*e[i-1]
+   mu[i] &amp;lt;- fit[i] + phi * e[i-1]
+   }
+   #Priors
+   phi ~ dunif(-1,1)
+   for (i in 1:nX) {
+   beta[i] ~ dnorm(0,1.0E-6)
+   }
+   sigma &amp;lt;- z/sqrt(chSq)    # prior for sigma; cauchy = normal/sqrt(chi^2)
+   z ~ dnorm(0, 0.04)I(0,)
+   chSq ~ dgamma(0.5, 0.5)  # chi^2 with 1 d.f.
+   tau &amp;lt;- pow(sigma, -2)
+   tau.cor &amp;lt;- tau #* (1- phi*phi)
+   }
+   &amp;quot;
&amp;gt; 
&amp;gt; ## write the model to a text file
&amp;gt; writeLines(modelString, con = &amp;quot;tempModel.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Arrange the data as a list (as required by &lt;code&gt;JAGS&lt;/code&gt;). As input, &lt;code&gt;JAGS&lt;/code&gt; will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Xmat = model.matrix(~x, data.temporalCor)
&amp;gt; data.temporalCor.list &amp;lt;- with(data.temporalCor, list(y = y, X = Xmat,
+     n = nrow(data.temporalCor), nX = ncol(Xmat)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the nodes (parameters and derivatives) to monitor and the chain parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params &amp;lt;- c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 10000  #across all chains
&amp;gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&amp;gt; nIter
[1] 10000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Start the &lt;code&gt;JAGS&lt;/code&gt; model (check the model, load data into the model, specify the number of chains and compile the model). Load the &lt;code&gt;R2jags&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(R2jags)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now run the &lt;code&gt;JAGS&lt;/code&gt; code via the &lt;code&gt;R2jags&lt;/code&gt; interface.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data.temporalCor.r2jags &amp;lt;- jags(data = data.temporalCor.list, inits = NULL, parameters.to.save = params,
+     model.file = &amp;quot;tempModel.txt&amp;quot;, n.chains = nChains, n.iter = nIter,
+     n.burnin = burnInSteps, n.thin = thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 50
   Unobserved stochastic nodes: 5
   Total graph size: 413

Initializing model
&amp;gt; 
&amp;gt; print(data.temporalCor.r2jags)
Inference for Bugs model at &amp;quot;tempModel.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
beta[1]   30.841  11.858   8.852  22.556  30.505  38.559  55.177 1.001 10000
beta[2]    0.225   0.100   0.028   0.159   0.225   0.292   0.422 1.001  3800
phi        0.913   0.054   0.793   0.879   0.919   0.954   0.994 1.001  3400
sigma     12.133   1.253   9.967  11.253  12.034  12.902  14.828 1.001  7300
deviance 391.602   2.641 388.354 389.656 390.985 392.927 398.180 1.001  9200

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 3.5 and DIC = 395.1
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mcmc-diagnostics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MCMC diagnostics&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(mcmcplots)
&amp;gt; denplot(data.temporalCor.r2jags, parms = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/mcmc_diag-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(data.temporalCor.r2jags, parms = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/mcmc_diag-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data.mcmc = as.mcmc(data.temporalCor.r2jags)
&amp;gt; #Raftery diagnostic
&amp;gt; raftery.diag(data.mcmc)
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta[1]  2        3930  3746         1.05      
 beta[2]  2        3866  3746         1.03      
 deviance 2        3866  3746         1.03      
 phi      7        7397  3746         1.97      
 sigma    4        4636  3746         1.24      


[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta[1]  3        4062  3746         1.080     
 beta[2]  2        3620  3746         0.966     
 deviance 2        3803  3746         1.020     
 phi      6        6878  3746         1.840     
 sigma    4        4713  3746         1.260     &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #Autocorrelation diagnostic
&amp;gt; autocorr.diag(data.mcmc)
           beta[1]      beta[2]     deviance          phi        sigma
Lag 0  1.000000000  1.000000000  1.000000000  1.000000000  1.000000000
Lag 1  0.174857318 -0.006205038  0.164212015  0.398270011  0.166634323
Lag 5  0.017823932  0.002140092 -0.016470982  0.017851360  0.011892997
Lag 10 0.004107514  0.010910488  0.020001216 -0.005693854  0.007020861
Lag 50 0.002176470  0.016102607  0.008360988  0.002061169 -0.007663541&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All diagnostics seem fine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model validation&lt;/h2&gt;
&lt;p&gt;Whenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by &lt;code&gt;sigma&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Res_i = Y_i - \mu_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Res_{i+1} = Res_{i+1} - \rho Res_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Res_i = \frac{Res_i}{\sigma} \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc = data.temporalCor.r2jags$BUGSoutput$sims.matrix
&amp;gt; # generate a model matrix
&amp;gt; newdata = data.temporalCor
&amp;gt; Xmat = model.matrix(~x, newdata)
&amp;gt; ## get median parameter estimates
&amp;gt; wch = grep(&amp;quot;beta&amp;quot;, colnames(mcmc))
&amp;gt; coefs = mcmc[, wch]
&amp;gt; fit = coefs %*% t(Xmat)
&amp;gt; resid = -1 * sweep(fit, 2, data.temporalCor$y, &amp;quot;-&amp;quot;)
&amp;gt; n = ncol(resid)
&amp;gt; resid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, &amp;quot;phi&amp;quot;])
&amp;gt; resid = apply(resid, 2, median)/median(mcmc[, &amp;quot;sigma&amp;quot;])
&amp;gt; fit = apply(fit, 2, median)
&amp;gt; 
&amp;gt; library(ggplot2)
&amp;gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/mcmc_residuals-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/mcmc_residuals-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; ggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +
+     geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/mcmc_residuals-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; plot(acf(resid, lag = 40))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/mcmc_residuals-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;No obvious autocorrelation or other issues with residuals remaining.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-estimates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameter estimates&lt;/h2&gt;
&lt;p&gt;Explore parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(broom)
&amp;gt; tidyMCMC(as.mcmc(data.temporalCor.r2jags), conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;)
# A tibble: 5 x 5
  term     estimate std.error conf.low conf.high
  &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 beta[1]    30.8     11.9      7.36      53.5  
2 beta[2]     0.225    0.100    0.0321     0.425
3 deviance  392.       2.64   388.       397.   
4 phi         0.913    0.0537   0.813      1.000
5 sigma      12.1      1.25     9.91      14.7  &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;incorporating-ar1-residual-autocorrelation-structure&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Incorporating AR1 residual autocorrelation structure&lt;/h1&gt;
&lt;div id=&#34;model-fitting-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;We proceed to code the model into &lt;code&gt;JAGS&lt;/code&gt; (remember that in this software normal distribution are parameterised in terms of precisions &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; rather than variances, where &lt;span class=&#34;math inline&#34;&gt;\(\tau=\frac{1}{\sigma^2}\)&lt;/span&gt;). Define the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; modelString2 = &amp;quot;
+   model {
+   #Likelihood
+   for (i in 1:n) {
+   mu[i] &amp;lt;- inprod(beta[],X[i,])
+   }
+   y[1:n] ~ dmnorm(mu[1:n],Omega)
+   for (i in 1:n) {
+   for (j in 1:n) {
+   Sigma[i,j] &amp;lt;- sigma2*(equals(i,j) + (1-equals(i,j))*pow(phi,abs(i-j))) 
+   }
+   }
+   Omega &amp;lt;- inverse(Sigma)
+   
+   #Priors
+   phi ~ dunif(-1,1)
+   for (i in 1:nX) {
+   beta[i] ~ dnorm(0,1.0E-6)
+   }
+   sigma &amp;lt;- z/sqrt(chSq)    # prior for sigma; cauchy = normal/sqrt(chi^2)
+   z ~ dnorm(0, 0.04)I(0,)
+   chSq ~ dgamma(0.5, 0.5)  # chi^2 with 1 d.f.
+   sigma2 = pow(sigma,2)
+   #tau.cor &amp;lt;- tau #* (1- phi*phi)
+   }
+   &amp;quot;
&amp;gt; 
&amp;gt; ## write the model to a text file
&amp;gt; writeLines(modelString2, con = &amp;quot;tempModel2.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Arrange the data as a list (as required by &lt;code&gt;JAGS&lt;/code&gt;). As input, &lt;code&gt;JAGS&lt;/code&gt; will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Xmat = model.matrix(~x, data.temporalCor)
&amp;gt; data.temporalCor.list &amp;lt;- with(data.temporalCor, list(y = y, X = Xmat,
+     n = nrow(data.temporalCor), nX = ncol(Xmat)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the nodes (parameters and derivatives) to monitor and the chain parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params &amp;lt;- c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 5000
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 10000  #across all chains
&amp;gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&amp;gt; nIter
[1] 10000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now run the &lt;code&gt;JAGS&lt;/code&gt; code via the &lt;code&gt;R2jags&lt;/code&gt; interface.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data.temporalCor2.r2jags &amp;lt;- jags(data = data.temporalCor.list, inits = NULL, parameters.to.save = params,
+     model.file = &amp;quot;tempModel2.txt&amp;quot;, n.chains = nChains, n.iter = nIter,
+     n.burnin = burnInSteps, n.thin = thinSteps)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 1
   Unobserved stochastic nodes: 5
   Total graph size: 5566

Initializing model
&amp;gt; 
&amp;gt; print(data.temporalCor2.r2jags)
Inference for Bugs model at &amp;quot;tempModel2.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 5000 discarded)
 n.sims = 10000 iterations saved
         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
beta[1]   19.926  24.597 -19.141   9.722  18.990  29.365  64.348 1.014 10000
beta[2]    0.225   0.100   0.028   0.159   0.227   0.291   0.421 1.001 10000
phi        0.890   0.055   0.773   0.854   0.895   0.930   0.980 1.011   160
sigma     30.352  15.780  18.171  22.799  26.810  32.951  61.419 1.010   410
deviance 392.642   2.706 389.232 390.628 392.029 394.019 399.490 1.001  2900

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 3.7 and DIC = 396.3
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mcmc-diagnostics-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MCMC diagnostics&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; denplot(data.temporalCor2.r2jags, parms = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/mcmc_diag_ex2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(data.temporalCor2.r2jags, parms = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/mcmc_diag_ex2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data.mcmc = as.mcmc(data.temporalCor2.r2jags)
&amp;gt; #Raftery diagnostic
&amp;gt; raftery.diag(data.mcmc)
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta[1]  15       14982 3746         4.00      
 beta[2]  2        3866  3746         1.03      
 deviance 2        3995  3746         1.07      
 phi      9        9308  3746         2.48      
 sigma    8        10294 3746         2.75      


[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 
                                                
          Burn-in  Total Lower bound  Dependence
          (M)      (N)   (Nmin)       factor (I)
 beta[1]  4        4955  3746         1.320     
 beta[2]  2        3620  3746         0.966     
 deviance 2        3930  3746         1.050     
 phi      12       12162 3746         3.250     
 sigma    8        10644 3746         2.840     &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #Autocorrelation diagnostic
&amp;gt; autocorr.diag(data.mcmc)
            beta[1]      beta[2]   deviance       phi      sigma
Lag 0   1.000000000  1.000000000 1.00000000 1.0000000 1.00000000
Lag 1   0.023745389 -0.007088969 0.19477040 0.8775299 0.95206712
Lag 5   0.019171996  0.008569178 0.08589717 0.5774327 0.80961727
Lag 10 -0.009155805  0.008682983 0.06468974 0.3677587 0.64495814
Lag 50  0.012167974  0.014954099 0.01686647 0.0317406 0.04466731&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All diagnostics seem fine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-validation-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model validation&lt;/h2&gt;
&lt;p&gt;Whenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by &lt;code&gt;sigma&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Res_i = Y_i - \mu_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Res_{i+1} = Res_{i+1} - \rho Res_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Res_i = \frac{Res_i}{\sigma} \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc = data.temporalCor2.r2jags$BUGSoutput$sims.matrix
&amp;gt; # generate a model matrix
&amp;gt; newdata = data.temporalCor
&amp;gt; Xmat = model.matrix(~x, newdata)
&amp;gt; ## get median parameter estimates
&amp;gt; wch = grep(&amp;quot;beta&amp;quot;, colnames(mcmc))
&amp;gt; coefs = mcmc[, wch]
&amp;gt; fit = coefs %*% t(Xmat)
&amp;gt; resid = -1 * sweep(fit, 2, data.temporalCor$y, &amp;quot;-&amp;quot;)
&amp;gt; n = ncol(resid)
&amp;gt; resid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, &amp;quot;phi&amp;quot;])
&amp;gt; resid = apply(resid, 2, median)/median(mcmc[, &amp;quot;sigma&amp;quot;])
&amp;gt; fit = apply(fit, 2, median)
&amp;gt; 
&amp;gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/mcmc_residuals_ex2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/mcmc_residuals_ex2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; ggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +
+     geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/mcmc_residuals_ex2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; plot(acf(resid, lag = 40))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/autocorrelation-jags/2020-02-01-autocorrelation-jags_files/figure-html/mcmc_residuals_ex2-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;No obvious autocorrelation or other issues with residuals remaining&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-estimates-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameter estimates&lt;/h2&gt;
&lt;p&gt;Explore parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; tidyMCMC(as.mcmc(data.temporalCor2.r2jags), conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;)
# A tibble: 5 x 5
  term     estimate std.error conf.low conf.high
  &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 beta[1]    19.9     24.6    -16.6       66.3  
2 beta[2]     0.225    0.0997   0.0313     0.423
3 deviance  393.       2.71   389.       398.   
4 phi         0.890    0.0546   0.780      0.984
5 sigma      30.4     15.8     16.2       51.2  &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references hanging-indent&#34;&gt;
&lt;div id=&#34;ref-plummer2004jags&#34;&gt;
&lt;p&gt;Plummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-su2015package&#34;&gt;
&lt;p&gt;Su, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” &lt;em&gt;R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Temporal Autocorrelation - STAN</title>
      <link>/stan/autocorrelation-stan/autocorrelation-stan/</link>
      <pubDate>Sat, 08 Feb 2020 21:13:14 -0500</pubDate>
      
      <guid>/stan/autocorrelation-stan/autocorrelation-stan/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. &lt;code&gt;BUGS&lt;/code&gt; (Bayesian inference Using &lt;em&gt;Gibbs Sampling&lt;/em&gt;) is an algorithm and supporting language (resembling &lt;code&gt;R&lt;/code&gt;) dedicated to performing the Gibbs sampling implementation of &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; (MCMC) method. Dialects of the &lt;code&gt;BUGS&lt;/code&gt; language are implemented within three main projects:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OpenBUGS&lt;/strong&gt; - written in component pascal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;JAGS&lt;/strong&gt; - (Just Another Gibbs Sampler) - written in &lt;code&gt;C++&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;STAN&lt;/strong&gt; - a dedicated Bayesian modelling framework written in &lt;code&gt;C++&lt;/code&gt; and implementing &lt;em&gt;Hamiltonian&lt;/em&gt; MCMC samplers.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of &lt;code&gt;R&lt;/code&gt;, and thus, they are best accessed from within &lt;code&gt;R&lt;/code&gt; itself. As such there are multiple packages devoted to interfacing with the various software implementations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2OpenBUGS&lt;/em&gt; - interfaces with &lt;code&gt;OpenBUGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;R2jags&lt;/em&gt; - interfaces with &lt;code&gt;JAGS&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;rstan&lt;/em&gt; - interfaces with &lt;code&gt;STAN&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial will demonstrate how to fit models in &lt;code&gt;STAN&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Gelman, Lee, and Guo (2015)&lt;/span&gt;) using the package &lt;code&gt;rstan&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Stan Development Team (2018)&lt;/span&gt;) as interface, which also requires to load some other packages.&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Up until now (in the proceeding tutorials), the focus has been on models that adhere to specific assumptions about the underlying populations (and data). Indeed, both before and immediately after fitting these models, I have stressed the importance of evaluating and validating the proposed and fitted models to ensure reliability of the models. It is now worth us revisiting those fundamental assumptions as well as exploring the options that are available when the populations (data) do not conform. Let’s explore a simple linear regression model to see how each of the assumptions relate to the model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i = \beta_0 + \beta_1x_i + \epsilon_i \;\;\; \text{with} \;\;\; \epsilon_i \sim \text{Normal}(0, \sigma^2).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above simple statistical model models the &lt;strong&gt;linear relationship&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; against &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. The residuals (&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;) are assumed to be &lt;strong&gt;normally distributed&lt;/strong&gt; with a mean of zero and a constant (yet unknown) variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, &lt;strong&gt;homogeneity of variance&lt;/strong&gt;). The residuals (and thus observations) are also assumed to all be &lt;strong&gt;independent&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Homogeneity of variance and independence are encapsulated within the single symbol for variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;). In assuming equal variances and independence, we are actually making an assumption about the variance-covariance structure of the populations (and thus residuals). Specifically, we assume that all populations are equally varied and thus can be represented well by a single variance term (all diagonal values in a &lt;span class=&#34;math inline&#34;&gt;\(N\times N\)&lt;/span&gt; covariance matrix are the same, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;) and the covariances between each population are zero (off diagonals). In simple regression, each observation (data point) represents a single observation drawn (sampled) from an entire population of possible observations. The above covariance structure thus assumes that the covariance between each population (observation) is zero - that is, each observation is completely independent of each other observation. Whilst it is mathematically convenient when data conform to these conditions (normality, homogeneity of variance, independence and linearity), data often violate one or more of these assumptions. In the following, I want to discuss and explore the causes and options for dealing with non-compliance to each of these conditions. By gaining a better understanding of how the various model fitting engines perform their task, we are better equipped to accommodate aspects of the data that don’t otherwise conform to the simple regression assumptions. In this tutorial we specifically focus on the topic of heterogeneity of the variance.&lt;/p&gt;
&lt;p&gt;In order that the estimated parameters represent the underlying populations in an unbiased manner, the residuals (and thus each each observation) must be independent. However, what if we were sampling a population over time and we were interested in investigating how changes in a response relate to changes in a predictor (such as rainfall). For any response that does not “reset” itself on a regular basis, the state of the population (the value of its response) at a given time is likely to be at least partly dependent on the state of the population at the sampling time before. We can further generalise the above into:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_i \sim Dist(\mu_i),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu_i=\boldsymbol X \boldsymbol \beta + \boldsymbol Z \boldsymbol \gamma\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol \beta\)&lt;/span&gt; representing the &lt;em&gt;fixed data structure&lt;/em&gt; and &lt;em&gt;fixed effects&lt;/em&gt;, respectively, while with &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol Z\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol \gamma\)&lt;/span&gt; represent the &lt;em&gt;varying data structure&lt;/em&gt; and &lt;em&gt;varying effects&lt;/em&gt;, respectively. In simple regression, there are no “varying” effects, and thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \boldsymbol \gamma \sim MVN(\boldsymbol 0, \boldsymbol \Sigma),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol \Sigma\)&lt;/span&gt; is a variance-covariance matrix of the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \boldsymbol \Sigma =  \frac{\sigma^2}{1-\rho^2}
  \begin{bmatrix}
   1 &amp;amp; \rho^{\phi_{1,2}} &amp;amp; \ldots &amp;amp; \rho^{\phi_{1,n}} \\
   \rho^{\phi_{2,1}} &amp;amp; 1 &amp;amp; \ldots &amp;amp; \vdots\\
   \vdots &amp;amp; \ldots &amp;amp; 1 &amp;amp; \vdots\\
   \rho^{\phi_{n,1}} &amp;amp; \ldots &amp;amp; \ldots &amp;amp; 1
   \end{bmatrix}. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice that this introduces a very large number of additional parameters that require estimating: &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; (error variance), &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; (base autocorrelation) and each of the individual covariances (&lt;span class=&#34;math inline&#34;&gt;\(\rho^{\phi_{n,n}}\)&lt;/span&gt;). Hence, there are always going to be more parameters to estimate than there are date avaiable to use to estimate these paramters. We typically make one of a number of alternative assumptions so as to make this task more manageable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When we assume that all residuals are independent (regular regression), i.e. &lt;span class=&#34;math inline&#34;&gt;\(\rho=0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol \Sigma\)&lt;/span&gt; is essentially equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 \boldsymbol I\)&lt;/span&gt; and we simply use:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \boldsymbol \gamma \sim N( 0,\sigma^2).\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We could assume there is a reasonably simple pattern of correlation that declines over time. The simplest of these is a &lt;em&gt;first order autoregressive&lt;/em&gt; (AR1) structure in which exponent on the correlation declines linearly according to the time lag (&lt;span class=&#34;math inline&#34;&gt;\(\mid t - s\mid\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \boldsymbol \Sigma =  \frac{\sigma^2}{1-\rho^2}
  \begin{bmatrix}
   1 &amp;amp; \rho &amp;amp; \ldots &amp;amp; \rho^{\mid t-s \mid} \\
   \rho &amp;amp; 1 &amp;amp; \ldots &amp;amp; \vdots\\
   \vdots &amp;amp; \ldots &amp;amp; 1 &amp;amp; \vdots\\
   \rho^{\mid t-s \mid } &amp;amp; \ldots &amp;amp; \ldots &amp;amp; 1
   \end{bmatrix}. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note, in making this assumption, we are also assuming that the degree of correlation is dependent only on the lag and not on when the lag occurs (stationarity). That is all lag 1 residual pairs will have the same degree of correlation, all the lag &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; pairs will have the same correlation and so on.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;first-order-autocorrelation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;First order autocorrelation&lt;/h1&gt;
&lt;p&gt;Consider an example, in which the number of individuals at time &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; will be partly dependent on the number of individuals present at time &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. Clearly then, the observations (and thus residuals) are not fully independent - there is an auto-regressive correlation dependency structure. We could accommodate this lack of independence by fitting a model that incorporates a AR1 variance-covariance structure. Alternatively, we fit the following model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_{it} \sim Dist(\mu_{it}),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu_{it}=\boldsymbol X \boldsymbol \beta + \rho \epsilon_{i,t-1} + \gamma_{it},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and where &lt;span class=&#34;math inline&#34;&gt;\(\gamma \sim N(0, \sigma^2)\)&lt;/span&gt;. In this version of the model, we are stating that the expected value of an observation is equal to the regular linear predictor plus the autocorrelation parameter (&lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;) multipled by the residual associated with the previous observation plus the regular independently distributed noise (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;). Such a model is substantially faster to fit, although along with stationarity assumes in estimating the autocorrelation parameter, only the smallest lags are used. To see this in action, we will first generate some temporally auto-correlated data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; set.seed(126)
&amp;gt; n = 50
&amp;gt; a &amp;lt;- 20  #intercept
&amp;gt; b &amp;lt;- 0.2  #slope
&amp;gt; x &amp;lt;- round(runif(n, 1, n), 1)  #values of the year covariate
&amp;gt; year &amp;lt;- 1:n
&amp;gt; sigma &amp;lt;- 20
&amp;gt; rho &amp;lt;- 0.8
&amp;gt; 
&amp;gt; library(nlme)
&amp;gt; ## define a constructor for a first-order
&amp;gt; ## correlation structure
&amp;gt; ar1 &amp;lt;- corAR1(form = ~year, value = rho)
&amp;gt; ## initialize this constructor against our data
&amp;gt; AR1 &amp;lt;- Initialize(ar1, data = data.frame(year))
&amp;gt; ## generate a correlation matrix
&amp;gt; V &amp;lt;- corMatrix(AR1)
&amp;gt; ## Cholesky factorization of V
&amp;gt; Cv &amp;lt;- chol(V)
&amp;gt; ## simulate AR1 errors
&amp;gt; e &amp;lt;- t(Cv) %*% rnorm(n, 0, sigma)  # cov(e) = V * sig^2
&amp;gt; ## generate response
&amp;gt; y &amp;lt;- a + b * x + e
&amp;gt; data.temporalCor = data.frame(y = y, x = x, year = year)
&amp;gt; write.table(data.temporalCor, file = &amp;quot;data.temporalCor.csv&amp;quot;,
+     sep = &amp;quot;,&amp;quot;, quote = F, row.names = FALSE)
&amp;gt; 
&amp;gt; pairs(data.temporalCor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/generate_data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will now proceed to analyse these data via both of the above techniques for &lt;code&gt;JAGS&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;incorporating AR1 residual autocorrelation structure&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;incorporating lagged residuals into the model&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;incorporating-lagged-residuals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Incorporating lagged residuals&lt;/h1&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;We proceed to code the model into &lt;code&gt;JAGS&lt;/code&gt; (remember that in this software normal distribution are parameterised in terms of precisions &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; rather than variances, where &lt;span class=&#34;math inline&#34;&gt;\(\tau=\frac{1}{\sigma^2}\)&lt;/span&gt;). Define the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; stanString = &amp;quot;
+   data {
+   int&amp;lt;lower=1&amp;gt; n;
+   vector [n] y;
+   int&amp;lt;lower=1&amp;gt; nX;
+   matrix[n,nX] X;
+   }
+   transformed data {
+   }
+   parameters {
+   vector[nX] beta;
+   real&amp;lt;lower=0&amp;gt; sigma;
+   real&amp;lt;lower=-1,upper=1&amp;gt; phi;
+   }
+   transformed parameters {
+   vector[n] mu;
+   vector[n] epsilon;
+   mu = X*beta;
+   epsilon[1] = y[1] - mu[1];
+   for (i in 2:n) {
+   epsilon[i] = (y[i] - mu[i]);
+   mu[i] = mu[i] + phi*epsilon[i-1];
+   }
+   }
+   model {
+   phi ~ uniform(-1,1);
+   beta ~ normal(0,100);
+   sigma ~ cauchy(0,5);
+   y ~ normal(mu, sigma);
+   }
+   generated quantities {
+   }
+   
+   &amp;quot;
&amp;gt; 
&amp;gt; ## write the model to a text file
&amp;gt; writeLines(stanString, con = &amp;quot;tempModel.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Arrange the data as a list (as required by &lt;code&gt;STAN&lt;/code&gt;). As input, &lt;code&gt;STAN&lt;/code&gt; will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Xmat = model.matrix(~x, data.temporalCor)
&amp;gt; data.temporalCor.list &amp;lt;- with(data.temporalCor, list(y = y, X = Xmat,
+     n = nrow(data.temporalCor), nX = ncol(Xmat)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the nodes (parameters and derivatives) to monitor and chain parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params &amp;lt;- c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 500
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 2000  #across all chains
&amp;gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&amp;gt; nIter
[1] 1500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now compile and run the Stan code via the &lt;code&gt;rstan&lt;/code&gt; interface.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;During the warmup stage, the No-U-Turn sampler (NUTS) attempts to determine the optimum stepsize - the stepsize that achieves the target acceptance rate (&lt;span class=&#34;math inline&#34;&gt;\(0.8\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(80\)&lt;/span&gt;% by default) without divergence (occurs when the stepsize is too large relative to the curvature of the log posterior and results in approximations that are likely to diverge and be biased) - and without hitting the maximum treedepth (&lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt;). At each iteration of the NUTS algorithm, the number of leapfrog steps doubles (as it increases the treedepth) and only terminates when either the NUTS criterion are satisfied or the tree depth reaches the maximum (&lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; by default).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data.temporalCor.rstan &amp;lt;- stan(data = data.temporalCor.list, file = &amp;quot;tempModel.stan&amp;quot;, chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)

SAMPLING FOR MODEL &amp;#39;tempModel&amp;#39; NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)
Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)
Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)
Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)
Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)
Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)
Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)
Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)
Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)
Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)
Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)
Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.087 seconds (Warm-up)
Chain 1:                0.052 seconds (Sampling)
Chain 1:                0.139 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL &amp;#39;tempModel&amp;#39; NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)
Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)
Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)
Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)
Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)
Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)
Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)
Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)
Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)
Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)
Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)
Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.084 seconds (Warm-up)
Chain 2:                0.055 seconds (Sampling)
Chain 2:                0.139 seconds (Total)
Chain 2: 
&amp;gt; 
&amp;gt; print(data.temporalCor.rstan, par = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;))
Inference for Stan model: tempModel.
2 chains, each with iter=1500; warmup=500; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=2000.

         mean se_mean    sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
beta[1] 30.80    0.30 11.73  9.28 22.78 30.20 38.06 55.53  1518    1
beta[2]  0.22    0.00  0.10  0.02  0.16  0.22  0.29  0.43  1362    1
sigma   12.06    0.03  1.20 10.04 11.22 11.96 12.83 14.61  1245    1
phi      0.92    0.00  0.05  0.80  0.88  0.92  0.96  1.00   898    1

Samples were drawn using NUTS(diag_e) at Tue Feb 18 16:51:59 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mcmc-diagnostics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MCMC diagnostics&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(mcmcplots)
&amp;gt; mcmc = As.mcmc.list(data.temporalCor.rstan)
&amp;gt; denplot(mcmc, parms = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_diag-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(mcmc, parms = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_diag-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #Raftery diagnostic
&amp;gt; raftery.diag(mcmc)
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 

You need a sample size of at least 3746 with these values of q, r and s

[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 

You need a sample size of at least 3746 with these values of q, r and s&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #Autocorrelation diagnostic
&amp;gt; autocorr.diag(mcmc)
           beta[1]     beta[2]        sigma        phi        lp__
Lag 0   1.00000000 1.000000000  1.000000000 1.00000000  1.00000000
Lag 1   0.11945325 0.130376690  0.227794064 0.17925033  0.53885687
Lag 5   0.03437963 0.007224984 -0.042874656 0.07298004  0.09845135
Lag 10  0.03127466 0.025645224  0.006317540 0.01226778 -0.02012550
Lag 50 -0.05003287 0.024817286 -0.001190987 0.01445287 -0.02401952&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; stan_ac(data.temporalCor.rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_diag4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; stan_rhat(data.temporalCor.rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_diag4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; stan_ess(data.temporalCor.rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_diag4-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;All diagnostics seem fine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model validation&lt;/h2&gt;
&lt;p&gt;Whenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by &lt;code&gt;sigma&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Res_i = Y_i - \mu_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Res_{i+1} = Res_{i+1} - \rho Res_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Res_i = \frac{Res_i}{\sigma} \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc = as.matrix(data.temporalCor.rstan)
&amp;gt; wch = grep(&amp;quot;beta&amp;quot;, colnames(mcmc))
&amp;gt; # generate a model matrix
&amp;gt; newdata = data.frame(x = data.temporalCor$x)
&amp;gt; Xmat = model.matrix(~x, newdata)
&amp;gt; ## get median parameter estimates
&amp;gt; coefs = mcmc[, wch]
&amp;gt; fit = coefs %*% t(Xmat)
&amp;gt; resid = -1 * sweep(fit, 2, data.temporalCor$y, &amp;quot;-&amp;quot;)
&amp;gt; n = ncol(resid)
&amp;gt; resid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, &amp;quot;phi&amp;quot;])
&amp;gt; resid = apply(resid, 2, median)/median(mcmc[, &amp;quot;sigma&amp;quot;])
&amp;gt; fit = apply(fit, 2, median)
&amp;gt; 
&amp;gt; library(ggplot2)
&amp;gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_residuals-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_residuals-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; ggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +
+     geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = &amp;quot;dashed&amp;quot;) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_residuals-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; plot(acf(resid, lag = 40))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_residuals-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; fit = coefs %*% t(Xmat)
&amp;gt; ## draw samples from this model
&amp;gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data.temporalCor),
+     fit[i, ], mcmc[i, &amp;quot;sigma&amp;quot;]))
&amp;gt; ggplot() + geom_density(data = NULL, aes(x = as.vector(yRep),
+     fill = &amp;quot;Model&amp;quot;), alpha = 0.5) + geom_density(data = data.temporalCor,
+     aes(x = y, fill = &amp;quot;Obs&amp;quot;), alpha = 0.5) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_residuals-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;No obvious autocorrelation or other issues with residuals remaining.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-estimates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameter estimates&lt;/h2&gt;
&lt;p&gt;Explore parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(broom)
&amp;gt; tidyMCMC(data.temporalCor.rstan, par = c(&amp;quot;beta&amp;quot;, &amp;quot;phi&amp;quot;, &amp;quot;sigma&amp;quot;),
+     conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;, rhat = TRUE,
+     ess = TRUE)
# A tibble: 4 x 7
  term    estimate std.error conf.low conf.high  rhat   ess
  &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
1 beta[1]   30.8     11.7      8.41      54.2   1.00   1518
2 beta[2]    0.223    0.101    0.0145     0.417 1.000  1362
3 phi        0.915    0.0519   0.825      1.000 1.00    898
4 sigma     12.1      1.20    10.1       14.6   1.000  1245&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;incorporating-ar1-residual-autocorrelation-structure&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Incorporating AR1 residual autocorrelation structure&lt;/h1&gt;
&lt;div id=&#34;model-fitting-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;We proceed to code the model into &lt;code&gt;JAGS&lt;/code&gt; (remember that in this software normal distribution are parameterised in terms of precisions &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; rather than variances, where &lt;span class=&#34;math inline&#34;&gt;\(\tau=\frac{1}{\sigma^2}\)&lt;/span&gt;). Define the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; stanString = &amp;quot;
+ functions { 
+   matrix cov_matrix_ar1(real ar, real sigma, int nrows) { 
+   matrix[nrows, nrows] mat; 
+   vector[nrows - 1] gamma; 
+   mat = diag_matrix(rep_vector(1, nrows)); 
+       for (i in 2:nrows) { 
+           gamma[i - 1] = pow(ar, i - 1); 
+               for (j in 1:(i - 1)) { 
+                       mat[i, j] = gamma[i - j]; 
+                       mat[j, i] = gamma[i - j]; 
+                   } 
+               } 
+               return sigma^2 / (1 - ar^2) * mat; 
+       }
+ } 
+     
+       data { 
+            int&amp;lt;lower=1&amp;gt; n;  // total number of observations 
+                vector[n] y;  // response variable
+                int&amp;lt;lower=1&amp;gt; nX;
+                    matrix[n,nX] X;
+          } 
+            transformed data {
+               vector[n] se2 = rep_vector(0, n); 
+            } 
+            parameters { 
+               vector[nX] beta;
+                   real&amp;lt;lower=0&amp;gt; sigma;  // residual SD 
+                   real &amp;lt;lower=-1,upper=1&amp;gt; phi;  // autoregressive effects 
+               } 
+               transformed parameters { 
+               } 
+               model {
+                   matrix[n, n] res_cov_matrix;
+                   matrix[n, n] Sigma; 
+                   vector[n] mu = X*beta;
+                   res_cov_matrix = cov_matrix_ar1(phi, sigma, n);
+                   Sigma = res_cov_matrix + diag_matrix(se2);
+                   Sigma = cholesky_decompose(Sigma); 
+ 
+                   // priors including all constants
+                   beta ~ student_t(3,30,30);
+                   sigma ~ cauchy(0,5);
+                   y ~ multi_normal_cholesky(mu,Sigma);
+               } 
+               generated quantities { 
+               }
+   
+   &amp;quot;
&amp;gt; 
&amp;gt; ## write the model to a text file
&amp;gt; writeLines(stanString, con = &amp;quot;tempModel2.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Arrange the data as a list (as required by &lt;code&gt;STAN&lt;/code&gt;). As input, &lt;code&gt;STAN&lt;/code&gt; will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; Xmat = model.matrix(~x, data.temporalCor)
&amp;gt; data.temporalCor.list &amp;lt;- with(data.temporalCor, list(y = y, X = Xmat,
+     n = nrow(data.temporalCor), nX = ncol(Xmat)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the nodes (parameters and derivatives) to monitor and chain parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params &amp;lt;- c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;)
&amp;gt; nChains = 2
&amp;gt; burnInSteps = 500
&amp;gt; thinSteps = 1
&amp;gt; numSavedSteps = 2000  #across all chains
&amp;gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&amp;gt; nIter
[1] 1500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now compile and run the Stan code via the &lt;code&gt;rstan&lt;/code&gt; interface.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; data.temporalCor2.rstan &amp;lt;- stan(data = data.temporalCor.list, file = &amp;quot;tempModel2.stan&amp;quot;, chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)

SAMPLING FOR MODEL &amp;#39;tempModel2&amp;#39; NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)
Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)
Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)
Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)
Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)
Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)
Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)
Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)
Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)
Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)
Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)
Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 2.143 seconds (Warm-up)
Chain 1:                1.194 seconds (Sampling)
Chain 1:                3.337 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL &amp;#39;tempModel2&amp;#39; NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.001 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)
Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)
Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)
Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)
Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)
Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)
Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)
Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)
Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)
Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)
Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)
Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 2.478 seconds (Warm-up)
Chain 2:                1.287 seconds (Sampling)
Chain 2:                3.765 seconds (Total)
Chain 2: 
&amp;gt; 
&amp;gt; print(data.temporalCor2.rstan, par = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;))
Inference for Stan model: tempModel2.
2 chains, each with iter=1500; warmup=500; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=2000.

         mean se_mean    sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
beta[1] 21.73    0.53 16.53 -7.44 12.46 20.90 29.75 55.90   957    1
beta[2]  0.23    0.00  0.10  0.02  0.16  0.23  0.30  0.42  1523    1
sigma   12.02    0.03  1.23  9.93 11.13 11.95 12.80 14.60  1552    1
phi      0.89    0.00  0.06  0.77  0.86  0.90  0.93  0.99   781    1

Samples were drawn using NUTS(diag_e) at Tue Feb 18 16:52:55 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mcmc-diagnostics-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MCMC diagnostics&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc = As.mcmc.list(data.temporalCor2.rstan)
&amp;gt; denplot(mcmc, parms = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_diag_ex2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(mcmc, parms = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;phi&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_diag_ex2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #Raftery diagnostic
&amp;gt; raftery.diag(mcmc)
[[1]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 

You need a sample size of at least 3746 with these values of q, r and s

[[2]]

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 

You need a sample size of at least 3746 with these values of q, r and s&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; #Autocorrelation diagnostic
&amp;gt; autocorr.diag(mcmc)
            beta[1]      beta[2]        sigma          phi         lp__
Lag 0   1.000000000  1.000000000  1.000000000  1.000000000  1.000000000
Lag 1   0.248972079  0.105529288  0.061621949  0.168454454  0.557890281
Lag 5  -0.001409062  0.006290841  0.018746956  0.075009305  0.115634020
Lag 10  0.036198498  0.007579889 -0.001415388  0.013955579 -0.033295103
Lag 50  0.033443833 -0.040041892 -0.005562613 -0.004995361  0.004248614&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; stan_ac(data.temporalCor2.rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_diag4_ex2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; stan_rhat(data.temporalCor2.rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_diag4_ex2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; stan_ess(data.temporalCor2.rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_diag4_ex2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;All diagnostics seem fine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-validation-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model validation&lt;/h2&gt;
&lt;p&gt;Whenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by &lt;code&gt;sigma&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Res_i = Y_i - \mu_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Res_{i+1} = Res_{i+1} - \rho Res_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Res_i = \frac{Res_i}{\sigma} \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc = as.matrix(data.temporalCor2.rstan)
&amp;gt; wch = grep(&amp;quot;beta&amp;quot;, colnames(mcmc))
&amp;gt; # generate a model matrix
&amp;gt; newdata = data.frame(x = data.temporalCor$x)
&amp;gt; Xmat = model.matrix(~x, newdata)
&amp;gt; ## get median parameter estimates
&amp;gt; coefs = mcmc[, wch]
&amp;gt; fit = coefs %*% t(Xmat)
&amp;gt; resid = -1 * sweep(fit, 2, data.temporalCor$y, &amp;quot;-&amp;quot;)
&amp;gt; n = ncol(resid)
&amp;gt; resid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, &amp;quot;phi&amp;quot;])
&amp;gt; resid = apply(resid, 2, median)/median(mcmc[, &amp;quot;sigma&amp;quot;])
&amp;gt; fit = apply(fit, 2, median)
&amp;gt; 
&amp;gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_residuals_ex2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_residuals_ex2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; ggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +
+     geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = &amp;quot;dashed&amp;quot;) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_residuals_ex2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; plot(acf(resid, lag = 40))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_residuals_ex2-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; 
&amp;gt; fit = coefs %*% t(Xmat)
&amp;gt; ## draw samples from this model
&amp;gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data.temporalCor),
+     fit[i, ], mcmc[i, &amp;quot;sigma&amp;quot;]))
&amp;gt; ggplot() + geom_density(data = NULL, aes(x = as.vector(yRep),
+     fill = &amp;quot;Model&amp;quot;), alpha = 0.5) + geom_density(data = data.temporalCor,
+     aes(x = y, fill = &amp;quot;Obs&amp;quot;), alpha = 0.5) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/autocorrelation-stan/2020-02-01-autocorrelation-stan_files/figure-html/mcmc_residuals_ex2-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;No obvious autocorrelation or other issues with residuals remaining.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-estimates-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameter estimates&lt;/h2&gt;
&lt;p&gt;Explore parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; tidyMCMC(data.temporalCor2.rstan, par = c(&amp;quot;beta&amp;quot;, &amp;quot;phi&amp;quot;, &amp;quot;sigma&amp;quot;),
+     conf.int = TRUE, conf.method = &amp;quot;HPDinterval&amp;quot;, rhat = TRUE,
+     ess = TRUE)
# A tibble: 4 x 7
  term    estimate std.error conf.low conf.high  rhat   ess
  &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
1 beta[1]   21.7     16.5     -8.48      54.2   1.00    957
2 beta[2]    0.227    0.103    0.0405     0.439 1.00   1523
3 phi        0.893    0.0566   0.790      0.995 0.999   781
4 sigma     12.0      1.23     9.78      14.4   0.999  1552&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references hanging-indent&#34;&gt;
&lt;div id=&#34;ref-gelman2015stan&#34;&gt;
&lt;p&gt;Gelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” &lt;em&gt;Journal of Educational and Behavioral Statistics&lt;/em&gt; 40 (5): 530–43.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rstanpackage&#34;&gt;
&lt;p&gt;Stan Development Team. 2018. “RStan: The R Interface to Stan.” &lt;a href=&#34;http://mc-stan.org/&#34;&gt;http://mc-stan.org/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
