<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Frequentist statistics on Andrea Gabrio</title>
    <link>/tags/frequentist-statistics/</link>
    <description>Recent content in Frequentist statistics on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>`{year}`</copyright>
    <lastBuildDate>Fri, 07 Aug 2020 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/frequentist-statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>What is Bayesian inference?</title>
      <link>/post/update-july/</link>
      <pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-july/</guid>
      <description>


&lt;p&gt;What is probability ? The answer to this question is generally acknowledged to be the one that respects the so called Kolmogorov axioms which can be brutally simplified to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Probabilities are non-negative&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Probabilities sum to one&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The joint probability of disjoint events is the sum of the probabilities of the events&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One of the ways in which Bayesian statistics differs from classical statistics is in the &lt;strong&gt;interpretation&lt;/strong&gt; of probability. Differences in interpretation continue to be controversial, are critical to the distinction between Bayesian and non-Bayesian statistics.&lt;/p&gt;
&lt;p&gt;In classical statistics probability is often understood as a &lt;em&gt;property of the phenomenon being studied&lt;/em&gt;: for instance, the probability that a tossed coin will come up heads is a characteristic of the coin. Thus, by tossing the coin many times under more or less identical conditions, and noting the result of each toss, we can estimate the probability of a head, with the precision of the estimate monotonically increasing with the number of tosses. In this view, probability is the limit of a long-run, relative frequency; i.e. if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is an event of interest (e.g. the coin lands heads up) then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \text{Pr}(A) = \lim_{n\rightarrow\infty}\frac{m}{n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is the probabilty of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is the number of times we observe the event &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of repetitions. Given this definition of probability, we can understand why classicial statistics is sometimes referred to as &lt;em&gt;frequentist&lt;/em&gt; and &lt;em&gt;objectivist&lt;/em&gt;. However, historians of science stress that at least two notions of probability were under development from the late &lt;span class=&#34;math inline&#34;&gt;\(1600\)&lt;/span&gt;s onwards: the objectivist view described above, and a subjectivist view. With regard to the latter, we can consider different ‘degrees’ of belief to interpret probability, ‘from the very neighborhourhood of certainty and demonstration, quite down to improbability and unlikeliness, even to the confines of impossibility’. For Locke, ‘Probability is likeliness to be true’, a definition in which (repeated) games of chance play no part. For Bernoulli, ‘Probability is degree of certainty and differs from absolute certainty as the part differs from the whole’, it being unequivocal that the ‘certainty’ referred to is a state of mind, but, critically, (1) varied from person to person (depending on one’s knowledge and experience) and (2) was quantifiable. Ramsey and de Finetti, working independently, showed that subjective probability is not just any set of subjective beliefs, but beliefs that conform to the axioms of probability. The Ramsey-de Finetti Theorem states that if &lt;span class=&#34;math inline&#34;&gt;\(p_1, p_2, \ldots\)&lt;/span&gt; are a set of betting quotients on hypotheses &lt;span class=&#34;math inline&#34;&gt;\(h_1, h_2,\ldots\)&lt;/span&gt; , then if the &lt;span class=&#34;math inline&#34;&gt;\(p_j\)&lt;/span&gt; do not satisfy the probability axioms, there exists a betting strategy and a set of stakes such that whoever follows this betting strategy will lose a finite sum whatever the truth values of the hypotheses turn out to be. In de Finetti’s terminology, subjective probabilities that fail to conform to the axioms of probability are incoherent or inconsistent. Thus, subjective probabilities are whatever a particular person believes, provided they satisfy the axioms of probability. Thus, if I do not update my subjective beliefs in light of new information (data) in a manner consistent with the probability axioms, and you can convince me to gamble with you, you have the opportunity to take advantage of my irrationality, and are guaranteed to profit at my expense. That is, while probability may be subjective, Bayes Rule governs how rational people should update subjective beliefs.&lt;/p&gt;
&lt;div id=&#34;subjective-probability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Subjective probability&lt;/h2&gt;
&lt;p&gt;Bayesian probability statements are thus about states of mind over states of the world, and not about states of the world per se. Indeed, whatever one believes about determinism or chance in social processes, the meaningful uncertainty is that which resides in our brains, upon which we will base decisions and actions. This is why, in one of the more memorable and strongest statements of the subjectivist position, de Finetti writes &lt;strong&gt;probability does not exist&lt;/strong&gt;: “The abandonment of superstitious beliefs about … Fairies and Witches was an essential step along the road to scientific thinking. Probability, too, if regarded as something endowed with some kind of objective existence, is not less a misleading misconception, an illusory attempt to exteriorize or materialize our true probabilistic beliefs. In investigating the reasonableness of our own modes of thought and behaviour under uncertainty, all we require, and all that we are reasonably entitled to, is consistency among these beliefs, and their reasonable relation to any kind of relevant objective data”.&lt;/p&gt;
&lt;p&gt;The use of subjective probability also means that Bayesians can report probabilities without a “practically unlimited” sequence of observations. What is the frequentist probability of the truth of the proposition “Jackson was the eighth president”? Since there is only one relevant experiment for this problem, the frequentist probability is either zero (if Jackson was not the eighth president) or one (if Jackson was the eighth president). Non-trivial frequentist probabilities, it seems, are reserved for phenomena that are standardized and repeatable. Bayes Theorem itself is uncontroversial: it is merely an accounting identity that follows from the axioms of probability discussed above, plus the following additional definition.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Conditional probability&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; be events with &lt;span class=&#34;math inline&#34;&gt;\(P(B)&amp;gt;0\)&lt;/span&gt;. Then the conditional probability of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(A\mid B) = \frac{P(A \cap B)}{P(B)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The following two useful results are also implied by the probability axioms, plus the definition of conditional probability&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Multiplication rule&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(A \cap B) = P(A\mid B)P(B) = P(B\mid A)P(A)\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Law of total probability&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(B) = P(A\cap B)+ P\overline{(A\cap B)} = P(B\mid A)P(A) + P(B \mid \overline{A})P(\overline{A})\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayes-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayes theorem&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Bayes Theorem&lt;/em&gt; can now be stated, following immediately from the definition of conditional probability. If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are events with &lt;span class=&#34;math inline&#34;&gt;\(P(B)&amp;gt;0\)&lt;/span&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we consider the event &lt;span class=&#34;math inline&#34;&gt;\(A=H\)&lt;/span&gt; to be an hypothesis and the event &lt;span class=&#34;math inline&#34;&gt;\(B=E\)&lt;/span&gt; to be observing some evidence, then &lt;span class=&#34;math inline&#34;&gt;\(Pr(H\mid E)\)&lt;/span&gt; is the probability of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; after obtaining &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\text{Pr}(H)\)&lt;/span&gt; is the prior probability of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; before considering &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;. The conditional probability on the left-hand side of the theorem, &lt;span class=&#34;math inline&#34;&gt;\(\text{Pr}(H\mid E)\)&lt;/span&gt;, is usually referred to as the posterior probability of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;. Bayes Theorem thus supplies a solution to the general problem of inference or induction, providing a mechanism for learning about the plausibility of a hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; from data &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In most analyses in the social sciences, we want to learn about a continuous parameter, rather than the discrete parameters considered in the discussion thus far. Examples include the mean of a continuous variable, a proportion (a continuous parameter on the unit interval), a correlation, or a regression coefficient. In general, let the unknown parameter be &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and denote the data available for analysis as &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y = (y_1, \ldots , y_n)\)&lt;/span&gt;. In the case of continuous parameters, beliefs about the parameter are represented as probability density
functions or pdfs; we denote the prior pdf as &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; and the posterior pdf as &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid \boldsymbol y)\)&lt;/span&gt;. Then, Bayes Theorem for a continuous parameter is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p(\theta \mid \boldsymbol y) = \frac{p(\boldsymbol y \mid \theta) p(\theta)}{\int p(\boldsymbol y \mid \theta) p(\theta) d\theta}\]&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;which is often approximated by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p(\theta \mid \boldsymbol y) \propto p(\boldsymbol y \mid \theta) p(\theta) \]&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;where the proportionality constant is &lt;span class=&#34;math inline&#34;&gt;\(\left[ \int p(\boldsymbol y \mid \theta) p(\theta) d\theta \right]^{-1}\)&lt;/span&gt; which ensures that the posterior density integrates to one, as a proper probability density. The first term on the right hand side of the Equation is the &lt;em&gt;likelihood function&lt;/em&gt;, the probability density of the data &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y\)&lt;/span&gt;, considered as a function of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. This formulation of Bayes Rule highlights a particularly elegant feature of the Bayesian approach, showing how the likelihood function &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol y|\theta)\)&lt;/span&gt; can be “inverted” to generate a
probability statement about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, given data &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y\)&lt;/span&gt;. Thus, from a Bayesian perspective, likelihood based analyses of data assume prior ignorance, although seldom is this assumption made explicit, even if it were plausible. In other cases, when working with the so-called conjugate priors in the exponential family, the mean of the posterior distribution is a precision-weighted average of the prior and the likelihood. Suppose a prior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; belongs to a class of parametric of densities, &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;. More specifically, the prior density is said to be conjugate with respect to a likelihood &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol y\mid \theta)\)&lt;/span&gt; if the posterior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid \boldsymbol y )\)&lt;/span&gt; is also in &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Bayesian statistical inference is equivalent to combining information, marrying the information in the prior with the information in the data, with the relative contributions of prior and data to the posterior being proportional to their respective precisions. That is, Bayesian analysis with conjugate priors over a parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is equivalent to taking a precision-weighted average of prior information about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and the information in the data about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Thus:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Thus, when prior beliefs about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; are ‘vague’, ‘diffuse’, or, in the limit, uninformative, the posterior density will be dominated by the likelihood (i.e. the data contains much more information than the prior about the parameters);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When prior information is available, the posterior incorporates it, and rationally, in the sense of being consistent with the laws of probability via Bayes Theorem. In fact, when prior beliefs are quite precise relative to the data, it is possible that the likelihood is largely ignored, and the posterior distribution will look almost exactly like the prior&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note also that via Bayes Rule, if a particular region of the parameter space has zero prior probability, then it also has zero posterior probability. This feature of Bayesian updating
has been dubbed &lt;em&gt;Cromwell’s Rule&lt;/em&gt; by Lindley. The point here is that posterior distributions can sometimes look quite unusual, depending on the form of the prior and the likelihood for a particular problem. The fact that a posterior distribution may have a peculiar shape is of no great concern in a Bayesian analysis: provided one is updating prior beliefs via Bayes Rule, all is well. Unusual looking posterior distributions might suggest that one’s prior distribution was poorly specified, but, as a general rule, one should be extremely wary of engaging this kind of procedure. Bayes Rule is a procedure for generating posterior distributions over parameters in light of data. Although one can always re-run a Bayesian analysis with different priors (and indeed, this is usually a good idea), Bayesian procedures should not be used to hunt for priors that generate the most pleasing looking posterior distribution given a particular data set and likelihood. Indeed, such a practice would amount to an inversion of the Bayesian approach: i.e. if the researcher has strong ideas as to what values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; are more likely than others, aside from the information in the data, then that auxiliary information should be considered a prior, with Bayes Rule providing a procedure for rationally combining that auxiliary information with the information in the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-updating-of-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian updating of information&lt;/h2&gt;
&lt;p&gt;Bayesian procedures are often equivalent to combining the information in one set of data with another set of data. In fact, if prior beliefs represent the result of a previous data analysis (or perhaps many previous data analyses), then Bayesian analysis is equivalent to &lt;em&gt;pooling information&lt;/em&gt;. This is a particularly compelling feature of Bayesian analysis, and one that takes on special significance when working with cojugate priors. In these cases, Bayesian procedures accumulate information in the sense that the posterior distribution is more precise than either the prior distribution or the likelihood alone. Further, as the amount of data increases, say through repeated applications of the data generation process, the posterior precision will continue to increase, eventually overwhelming any non-degenerate prior; the upshot is that analysts with different (non-degenerate) prior beliefs over a parameter will eventually find their beliefs coinciding, provided they (1) see enough data and (2) update their beliefs using Bayes Theorem. In this way Bayesian analysis has been proclaimed as a model for scientific practice acknowledging that while reasonable people may differ (at least prior to seeing data), our views will tend to converge as scientific knowledge accumulates, provided we update our views rationally, consistent with the laws of probability.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters-as-random-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameters as random variables&lt;/h2&gt;
&lt;p&gt;One of the critical ways in which Bayesian statistical inference differs from frequentist
inference is that the result of a Bayesian analysis, the posterior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid \boldsymbol y)\)&lt;/span&gt; is just that, a probability density. Given a subjectivist interpretation of probabilty that most Bayesians adopt, the ‘randomness’ summarized by the posterior density is a reflection of the researcher’s uncertainty over &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, conditional on having observed data. Contrast the frequentist approach, in which &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is not random, but a fixed (but unknown) property of a population from which we randomly sample data &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y\)&lt;/span&gt;. Repeated applications of the sampling process, if undertaken, would yield different y, and different sample based estimates of θ, denoted &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta} = \hat{\theta}(\boldsymbol y)\)&lt;/span&gt;, this notation reminding us that estimates of parameters are functions of data. In the frequentist scheme, the &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}(\boldsymbol y)\)&lt;/span&gt; vary randomly across data sets (or would, if repeated sampling was undertaken), while the parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is a constant feature of the population from which data sets are drawn. The distribution of values of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}(\boldsymbol y)\)&lt;/span&gt; that would result from repeated application of the sampling process is called the sampling distribution, and is the basis of inference in the frequentist approach; the standard deviation of the sampling distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is the standard error of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt;, which plays a key role in frequentist inference. The Bayesian approach does not rely on how &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; might vary over repeated applications of random sampling. Instead, Bayesian procedures center on a simple question: “what should I believe about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in light of the data available for analysis, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y\)&lt;/span&gt; ?”&lt;/p&gt;
&lt;p&gt;The critical point to grasp is that in the Bayesian approach, the roles of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; are reversed relative to their roles in classical, frequentist inference: &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is random, in the sense that the researcher is uncertain about its value, while &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is fixed, a feature of the data at hand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;So, we have seen a couple of interesting things about Bayesian statistics which people may not be aware of. First, Bayesian statistics is a scientific approach in that it provides a rational way to update subjective beliefs based on the available evidence through Bayes theorem which conforms the rules of probability. This ensures the scientific credibility of the posterior results while also providing a way to solve the inductive problem of learning from the data and update our belief about a parameter/hypothesis. Second, in contrast to the classical approach, Bayesian statistics do not rely on asymptotic results of a series of repeateable events in order to hold and therefore can be used to answer questions which do not have any meaning in the context of repeated events. Finally, Bayesian statistics sees any unknown quantity (e.g. parameters) as random variables and attach to them a probability distribution expressing the uncertainty around the estimates. Since the entire posterior distribution is derived based on Bayes theorem, this ensures correct propagation of uncertainty from the data and prior and does not require the additional step of classical statistics of deriving uncertainty measures in an “artifical way” or relying on asymptotic results.&lt;/p&gt;
&lt;p&gt;I hope this was a bit interesting for those who would like to get more familiar with the Bayesian philosophy and its underlying implications in terms of statistical assumptions and methods. Of course, being a Bayesian, this is the best way to go for me when doing an analysis and I would love to see more people embracing the Bayesian way as a new way of thinking statistics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/qav3a2OPBdZoQ/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why be Bayesian?</title>
      <link>/post/update-july/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-july/</guid>
      <description>


&lt;p&gt;Many times I have been asked by co-workers and people around me who are a bit familiar with statistics why I choose to be Bayesian and whether I feel confident in using this approach for my data analysis rather than the most widely accepted frequentist methods, at least in my research area. Well, I am sure there are many valid arguments I could use to reply to this question but if I have to summarise my answer in two words I would say: &lt;strong&gt;why not&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;Now, a bit more into the details for those who were not extremely annoyed by my previous sentence. So, I truly believe that the Bayesian approach can be considered as a complement rather than a substitute to the frequentist paradigm. The main reason is relate to its much stronger links with probability theory compared with the classical approach in that not only are sampling distributions required for summaries of data, but also a wide range of distributions are used to represent prior opinion about proportions, event rates, and other unknown quantities. In a nutshell, the key difference between the two approaches is how they confront the concept of &lt;strong&gt;probability&lt;/strong&gt; of a certain event. In fact, although there is general consensus about the &lt;em&gt;rules&lt;/em&gt; of probability, that there is no universal &lt;em&gt;concept&lt;/em&gt; of probability, and two quite different definitions come from the frequentist and Bayesian approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The most widely known definition is: the proportion of times a will occur in an infinitely long series of repeated identical situations. This is known as the &lt;strong&gt;frequentist&lt;/strong&gt; perspective, as it rests on the &lt;em&gt;frequency&lt;/em&gt; with which specific events occur.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In contrast, the &lt;strong&gt;Bayesian&lt;/strong&gt; approach rests on an essentially subjective interpretation of probability, which is allowed to express generic uncertainty or &lt;em&gt;degree of belief&lt;/em&gt; about any unknown but potentially observable quantity, whether or not it is one of a number of repeatable experiments.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Rather than debating on philosophical debates about the foundations of statistics I prefer to focus on those aspects which I believe make the Bayesian approach, if not more intuitive than the frequentist counterpart, at least more attractive. Be worn I am not trying to start a war as I think both approaches could be used without the need to completely discard the other. The simple fact of being able to choose between two methods, rather than restricting themselves to a single option, seems a good enough reason for me to advocate the use of &lt;strong&gt;both&lt;/strong&gt; approaches. I terms of your own knowledge, experience and skills, You do not gain anything by saying “I will never be Bayesian” or “I will never be a frequentist”. On the contrary, by opening your mind and explore the use of one or the other method you will be able to have more options at your disposal that you can use to tackle the different problems you will face in your analyses.&lt;/p&gt;
&lt;p&gt;For the purpose of this post I just want to highlight some aspects which make the Bayesian approach particularly useful and, in some cases, even arguably preferable than the frequentist approach. Note that I am well aware there could be cases where the opposite holds and this is precisely why I believe it is important that statisticians should become familiar with both methods. By doing so they will be able to overcome the limitations/concerns associated with one method for a specific problem at hand using the instruments made available from the other method. Since I am a Bayesian, here I want to report the reasons and situations in which the Bayesian approach could provide a powerful tool.&lt;/p&gt;
&lt;p&gt;Let us start with a quick recap of the basic principle behind Bayesian methods. Bayesian statistical analysis relies on &lt;strong&gt;Bayes’s Theorem&lt;/strong&gt;, which tells us how to update prior
beliefs about parameters and hypotheses in light of data, to yield posterior beliefs. The theorem itself is utterly uncontroversial and follows directly from the conventional definition of conditional probability. If &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is some object of interest, but subject to uncertainty, e.g. a parameter, a hypothesis, a model, a data point, then Bayes Theorem tells us how to rationally
revise prior beliefs about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;, in light of the data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, to yield posterior beliefs &lt;span class=&#34;math inline&#34;&gt;\(p(y \mid \theta)\)&lt;/span&gt;. In this way Bayes Theorem provides a solution to the general problem of &lt;em&gt;induction&lt;/em&gt;, while in the specific case of statistical inference, Bayes Theorem provides a solution to problem of &lt;em&gt;how to learn from data&lt;/em&gt;. Thus, in a general sense, Bayesian statistical analysis is remarkably simple and even elegant, relying on this same simple recipe in each and every application.&lt;/p&gt;
&lt;p&gt;As I see it, there are a few major reasons why statisticians should consider learning about the Bayesian approach to statistical inference, and in the social sciences in particular:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Bayesian inference is simple and direct&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The result of a Bayesian analysis is a posterior probability statement, ‘posterior’ in the literal sense, in that such a statement characterizes beliefs after looking at data. Examples include: the posterior probability that a regression coefficient is positive, negative or lies in a particular interval; the posterior probability that a subject belongs to a particular latent class; the posterior probabilities that a particular statistical model is true model among a
family of statistical models.&lt;/p&gt;
&lt;p&gt;Note that the posterior probability statements produced by a Bayesian analysis are probability
statements over the quantities or objects of direct substantive interest to the researcher (e.g. parameters, hypotheses, models, predictions from models). Bayesian procedures condition on the data at hand to produce posterior probability statements about parameters and hypotheses. Frequentist procedures do just the reverse: one conditions on a null hypothesis to assess the plausibility of the data one observes (and more ‘extreme’ data sets that one did not observe but we might have had we done additional sampling), with another step of reasoning required to either reject or fail to reject the null hypothesis. Thus, compared to frequentist procedures, Bayesian procedures are simple and straightforward, at least conceptually.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical modeling&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The prior density also provides a way for model expansion when we work with data sets that pool data over multiple units and/or time periods. Data sets of this sort abound in the social sciences. Individuals live in different locations, with environmental factors that are constant for anyone within that location, but vary across locations. key question in research of this type is how the causal structure that operates at one level of analysis (e.g. individuals) varies across a ‘higher’ level of analysis (e.g. localities or time periods). The Bayesian approach to statistical inference is extremely well-suited to answering this question. Recall that in the Bayesian approach parameters are always random variables, typically (and most basically) in the sense that the researcher is unsure as to their value, but can characterize that uncertainty in the form of a prior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can replace the prior with a stochastic model formalizing the researcher’s assumptions about the way that parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; might vary across groups &lt;span class=&#34;math inline&#34;&gt;\(j = 1,..., J\)&lt;/span&gt; , perhaps as a function of observable characteristics of the groups; e.g., &lt;span class=&#34;math inline&#34;&gt;\(\theta_j \sim f (z_j, \gamma )\)&lt;/span&gt;, where now &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is a set of unknown hyperparameters. That is, the model is now comprised of a nested hierarchy of stochastic relations: the data from unit &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;, are modeled as a function of covariates and parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; , while cross-unit heterogeneity in the &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; is modeled as function of unit-specific covariates &lt;span class=&#34;math inline&#34;&gt;\(z_j\)&lt;/span&gt; and hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;. Models of this sort are known to Bayesians as &lt;em&gt;hierarchical models&lt;/em&gt;, but go by many different names in different parts of the social sciences depending on the specific form of the model and the estimation strategy being used (e.g. ‘random’ or ‘varying’ coefficients models, ‘multilevel’ or ‘mixed’ models). Compared with the frequentist counterpart, thanks to the use of &lt;em&gt;Markov chain Monte Carlo&lt;/em&gt; (MCMC) methods, Bayesian computation for these models has also become rather simple. Indeed, MCMC algorithms have proven themselves amazingly powerful and flexible, and have brought wide classes of models and data sets out of the ‘too hard’ basket. Other modelling examples include data sets with lots of missing data, or models with lots of parameters, model with latent variables, mixture
models, and flexible semi-and non-parametric models.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Statistically significant?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Frequentist inference asks assuming hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; is true, how often would we obtain a result at least as extreme as the result actually obtained?’, where ‘extreme’ is relative to the hypothesis being tested. If results such as the one obtained are sufficiently rare under hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; (e.g. generate a sufficiently small p value), then we conclude that &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; is incorrect, rejecting it in favor of some alternative hypothesis. Indeed, we teach our students to say that when the preceding conditions hold, we have a &lt;em&gt;statistically significant&lt;/em&gt; result. My experience is that in substituting this phrase for the much longer textbook definition, people quickly forget the frequentist underpinnings of what it is they are really asserting, and, hence seldom question whether the appeal to the long-run, repeated sampling properties of a statistical procedure is logical or realistic.&lt;/p&gt;
&lt;p&gt;In the Bayesian approach we condition on the data at hand to assess the plausibility of a hypothesis (via Bayes Rule), while the frequentist approach conditions on a hypothesis to assess the plausibility of the data (or more extreme data sets), with another step of reasoning required to either reject or fail to reject hypotheses. The frequentist p-value is the relative frequency of obtaining a result at least as extreme as the result actually obtained, assuming hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; to be true, where the sampling distribution of the result tells us how to assess relative frequencies of possible different results, under &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;. But what about cases where repeated sampling makes no sense, even as a thought experiment?&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Intervals&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Recall that in the frequentist approach, parameters are fixed characteristics of populations, so &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; either lies in the interval or it doesn’t. The correct interpretation of a frequentist confidence interval concerns the repeated sampling characteristics of a sample statistic. In the case of a &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; confidence interval, the correct frequentist interpretation is that &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; confidence intervals one would draw in repeated samples will include &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. Now, is the &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; confidence interval that one constructs from the data set at hand one of the lucky &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; that actually contains &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, or not? No ones knows.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Rational subjectivity&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, aside from acknowledging the subjectivity inherent to the general scientific exercise, the Bayesian approach rests on a subjective notion of probability, but demands that subjective
beliefs conform to the laws of probability. Put differently, in the Bayesian approach, the subjectivity of scientists is acknowledged, but simultaneously insists that subjectivity be
rational, in the sense that when confronted with evidence, subjective beliefs are updated rationally, in accord with the axioms of probability. Again, it is in this sense that Bayesian procedures offer a more direct path to inference; as I put it earlier, the Bayesian approach lets researchers mean what they say and say what they mean. For instance, the statement, having looked at the data, I am &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; sure that &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is included in an interval is a natural product of a Bayesian analysis, a characterization of the researcher’s beliefs about a parameter in formal, probabilistic terms, rather than a statement about the repeated sampling properties of a statistical procedure.&lt;/p&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The mathematics and computation underlying Bayesian analysis has been dramatically simplified
via a suite of MCMC algorithms. The combination of the popularization of MCMC and vast
increases in the computing power available to social scientists means that Bayesian analysis
is now well and truly part of the mainstream of quantitative social science. Despite these important pragmatic reasons for adopting the Bayesian approach, it is important to remember that MCMC algorithms are Bayesian algorithms: they are tools that simplify the computation of posterior densities. So, before we can fully and sensibly exploit the power of MCMC algorithms, it is important that we understand the foundations of Bayesian inference.&lt;/p&gt;
&lt;p&gt;This time I went overboard with the discussion but I thought it could be interesting to clarify here the key points, in my opinion, which make the Bayesian approach not only valid and efficient, but even a powerful tool that, once grasped the underlying phylosophy, can be used to overcome the difficulties of standard methods, especially when dealing with complex analyses.&lt;/p&gt;
&lt;p&gt;So what are you waiting for? do not sit in your frequentist comfort zone but expand your statistical knowledge! Evolve!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/u1k1kpDZSw5sA/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The P value fallacy</title>
      <link>/post/p-value-fallacy/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/p-value-fallacy/</guid>
      <description>&lt;p&gt;Today, I would like to briefly comment an interesting research article written by &lt;a href=&#34;https://jhu.pure.elsevier.com/en/publications/toward-evidence-based-medical-statistics-1-the-p-value-fallacy-4&#34;&gt;Goodman&lt;/a&gt;, who provided a clear and exemplary discussion about the typical incorrect interpretation of a standard frequentist analysis in the field of medical research. I will now briefly summarise the main argument of the paper and then add some personal comments.&lt;/p&gt;
&lt;p&gt;Essentially, the article describes the characteristics of the dominant school of medical statistics and highlights the logical fallacy at the heart of the typical frequentist analysis in clinical studies. This is based on a &lt;em&gt;deductive&lt;/em&gt; inferential approach, which starts with a given hypothesis and makes conclusions under the assumption that the hypothesis is true. This is in contrast with a &lt;em&gt;inductive&lt;/em&gt; approach, which uses the observed evidence to evaluate what hypothesis is most tenable. The two most popular methods of the frequentist paradigm are the &lt;em&gt;P value&lt;/em&gt; proposed by &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_6&#34;&gt;Fisher&lt;/a&gt; and the &lt;em&gt;hypothesis testing&lt;/em&gt; developed by &lt;a href=&#34;https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1933.0009?casa_token=sbSkualIaPYAAAAA%3ACxPsFTFEUK7vaxMPi5dJwUr4HoUWjrkxNh7Hl2q0owjtcU2wJHnakG-Xug7y95v1Tyqbbc8Mymaq_Q&amp;amp;&#34;&gt;Neyman and Pearson&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The P value is defined as the probability, under the assumption of no effect (null hypothesis), of obtaining a result equal to or more extreme than what was actually observed. Fisher proposed it as an informal index to be used as a measure of discrepancy between the data and the null hypothesis and therefore should not be interpreted as a formal inferential method. For example, since the P value can only be calculated on the assumption that the null hypothesis is true, it cannot be a direct measure of the probability that the null hypothesis is false. However, the main criticism to the P value is perhaps that it does not take into account the size of the observed effect, i.e. a small effect in a study with a large sample size can have the same P value as a large effect in a small study.&lt;/p&gt;
&lt;p&gt;Hypothesis testing was proposed by Neyman and Pearson as an alternative approach to the P value, which assumes the existence of a null hypothesis (e.g. no effect) and an alternative hypothesis (e.g. nonzero effect). The outcome of the test is then simply to reject one hypothesis in favour of the other, solely based on the data. This exposes the researcher to two types of errors: type I error or false-positive ($\alpha$) and type II error or false-negative ($\beta$) result. Rather than focussing on single experiments, like the P value, hypothesis testing is effectively based on a deductive approach to minimise the errors over a large number of experiments. However, the price to pay to obtain this &lt;em&gt;objectivity&lt;/em&gt; is the impossibility to make any inferential statement about a single experiment. The procedure only guarantees that in the long run, i.e. after considering many experiments, we shall not often be wrong.&lt;/p&gt;
&lt;p&gt;Over time a combination between the P value and hypothesis testing was developed under the assumption that the two approaches can be complementary. The idea was that the P value could be used to measure evidence in a single experiment while not violating the long run logic of hypothesis testing. The combined method is characterized by setting $\alpha$ and power $\beta$ before the experiment, then calculating a P value and rejecting the null hypothesis if the P value is less than the preset type I error rate. This means that the P value is considered a false-positive error rate specific to the data and also a measure of evidence against the null hypothesis. The &lt;strong&gt;P value fallacy&lt;/strong&gt; is born from this statement, which assumes that an event can be seen simultaneously from a long run perspective (where the observed results are put together with other results that might have occurred in hypothetical repetitions of the experiment) and from a short run perspective (where the observed results are interpreted only with respect to the single experiment). However, these views are not reconcilable since a result cannot be at the same time an interchangeable (long-run) and unique (short-run) member of a group of results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/JszzkKOlV6gTK/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I personally find this discussion fascinating and I believe that it is important to recognise the inconsistencies between the two alternative approaches to inference. The original authors of the two paradigms were well aware of the implications of their methods and never supported the combination of these. However, the combined approach has somehow become widely accepted in practice while its internal inconsistencies and conceptual limitations are hardly recognised.&lt;/p&gt;
&lt;p&gt;I feel that, since the two methods are perceived as &amp;ldquo;objective&amp;rdquo;, it is generally accepted that, if combined, they can produce reliable conclusions. This, however, is not necessarily true. Accepting at face value the significance result as a binary indicator of whether or not a relation is real is dangeroues and potentially misleading. This practice wants to show that conclusions are being drawn directly from the data, without any external influence, because direct inference from data to hypothesis is thought to result in mistaken conclusions only rarely and is therefore regarded as &amp;ldquo;scientific&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;This misguided approach has led to a much stronger emphasis towards the quantitative results alone (without any external input). In contrast, I believe that such perspective has the serious drawback of ignoring potentially useful information which is available (e.g. relevant medical knowledge or historical data) and which should be included in the analysis. Of course, I am aware of the potential issues that may arise from the selection and incorporation of external evidence, but I believe this should not be considered as &amp;ldquo;less reliable&amp;rdquo; or &amp;ldquo;more prone to mistakes&amp;rdquo; compared with the evidence from the available data. It is important that an agreement is reached about the selection of the type of evidence and methods to be used to perform the analysis solely based on their relevance with respect to the context analysed.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
