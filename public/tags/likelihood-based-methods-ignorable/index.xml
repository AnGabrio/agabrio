<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Likelihood Based Methods Ignorable on Andrea Gabrio</title>
    <link>/tags/likelihood-based-methods-ignorable/</link>
    <description>Recent content in Likelihood Based Methods Ignorable on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/likelihood-based-methods-ignorable/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Bayesian Iterative Simulation Methods</title>
      <link>/missmethods/bayesian-methods/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/bayesian-methods/</guid>
      <description>


&lt;p&gt;A useful alternative approach to &lt;em&gt;Maximum Likelihood&lt;/em&gt;(ML) methods, particularly when the sample size is small, is to include a reasonable prior distribution for the parameters and compute the posterior distribution of the parameters of interest. The posterior distribution for a model with ignorable missingness is&lt;/p&gt;
&lt;p&gt;\[
p(\theta \mid Y_0, M) \equiv p(\theta \mid Y_0) \propto p(\theta)f(Y_0 \mid \theta),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; is the prior and &lt;span class=&#34;math inline&#34;&gt;\(f(Y_0 \mid \theta)\)&lt;/span&gt; is the density of the observed data &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt;. Simulation from the posterior without iteration can be accomplished if the likelihood can be factored into complete data components, while for general patterns of missing data, Bayesian simulation requires iteration.&lt;/p&gt;
&lt;div id=&#34;data-augmentation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Augmentation&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Data Augmentation&lt;/em&gt;(&lt;span class=&#34;citation&#34;&gt;Tanner and Wong (1987)&lt;/span&gt;), or DA, is an iterative method of simulating the posteiror distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that combines features of the &lt;em&gt;Expecation Maximisation&lt;/em&gt;(EM) algorithm and &lt;em&gt;Multiple Imputation&lt;/em&gt;(MI). Starting with an initial draw &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; from an approximation to the posterior, then given the value &lt;span class=&#34;math inline&#34;&gt;\(\theta_t\)&lt;/span&gt; at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Draw &lt;span class=&#34;math inline&#34;&gt;\(Y_{1,t+1}\)&lt;/span&gt; with density &lt;span class=&#34;math inline&#34;&gt;\(p(Y_1 \mid Y_0, \theta_t)\)&lt;/span&gt; (I step).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Draw &lt;span class=&#34;math inline&#34;&gt;\(\theta_{t+1}\)&lt;/span&gt; with density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid Y_0, Y_{1,t+1})\)&lt;/span&gt; (P step).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The procedure is motivated by the fact that the distributions in these two steps are often much easier to draw from than either of the posteriors &lt;span class=&#34;math inline&#34;&gt;\(p(Y_1 \mid Y_0)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid Y_0)\)&lt;/span&gt;, or the joint posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\theta, Y_1 \mid Y_0)\)&lt;/span&gt;. The procedure can be shown to eventually yield a draw from the joint posterior of &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt;, in the sense that as &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; tends to infinity this sequence converges to a draw from the joint distribution.&lt;/p&gt;
&lt;div id=&#34;bivariate-normal-data-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bivariate Normal Data Example&lt;/h3&gt;
&lt;p&gt;Suppose having a sample &lt;span class=&#34;math inline&#34;&gt;\(y_i=(y_{1i},y_{2i})\)&lt;/span&gt; from a Bivariate Normal distribution for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; units, with mean vector &lt;span class=&#34;math inline&#34;&gt;\(\mu=(\mu_1,\mu_2)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(2\times2\)&lt;/span&gt; covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;. Assume that one group of units has &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; observed and &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt; missing, while a second group of units has both variables observed and a third group of units has &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; missing and &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt; observed. Under DA methods, each iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; consists of an I step and a P step. In the first, missing data are replaced with draws from its conditional distribution given the observed data and current values of the parameters (rather then its conditional mean as in the EM algorithm). Because units are conditionally independent given the parameters, each missing &lt;span class=&#34;math inline&#34;&gt;\(y_{2i}\)&lt;/span&gt; is drawn independently as&lt;/p&gt;
&lt;p&gt;\[
y_{2i,t+1} \sim N\left(\beta_{20t} + \beta_{21t}y_{1i}, \sigma^2_{2t} \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_{20t},\beta_{21t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{2t}\)&lt;/span&gt; are the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-th iterates of the regression parameters of &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt;. Analogously, each missing &lt;span class=&#34;math inline&#34;&gt;\(y_{1i}\)&lt;/span&gt; is drawn independently as&lt;/p&gt;
&lt;p&gt;\[
y_{1i,t+1} \sim N\left(\beta_{10t} + \beta_{11t}y_{2i}, \sigma^2_{1t} \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_{10t},\beta_{11t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{1t}\)&lt;/span&gt; are the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-th iterates of the regression parameters of &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt;. In the second step, these drawn values are treated as if they were the observed values and one draw of the bivariate Normal parameters is made from the complete data posterior. In the limit, the draws are from the joint posterior of the missing values and the parameters. Thus, a run of DA generates both a draw from the posterior predictive distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; and a draw from the posterior of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, and the procedure can be run &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; times to obtain &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; iid draws from the joint posterior of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt;. Unlike the EM, estimates of the sampling covariance matrix from the filled-in data can be computed without any corrections to the estimated variances because draws from the posterior predictive distribution of the missing values are imputed in the I step of DA, rather than the conditional means as in the E step of EM. The loss of efficiency from imputing draws is limited when the posterior mean from DA is computed over many draws from the posterior.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-gibbs-sampler&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Gibbs’ Sampler&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;Gibbs’s sampler&lt;/em&gt; is an iterative simulation method that is designed to yield draws from the joint posterior distribution in the case of a general pattern of missingness and provides a Bayesian analogous to the &lt;em&gt;Expectation Conditonal Maximisation &lt;/em&gt;(ECM) algorithm for ML estimation. The Gibbs’ sampler eventually generates a draw from the distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x_1,\ldots,x_J)\)&lt;/span&gt; of a set of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; random variables &lt;span class=&#34;math inline&#34;&gt;\(X_1,\ldots,X_J\)&lt;/span&gt; in settings where draws from the joint distribution are hard to compute but draws from the conditional distributions &lt;span class=&#34;math inline&#34;&gt;\(p(x_j \mid x_1,\ldots,x_{j-1},x_{j+1},\ldots, x_J)\)&lt;/span&gt; are relatively easy to compute. Initial values &lt;span class=&#34;math inline&#34;&gt;\(x_{10},\ldots,x_{J0}\)&lt;/span&gt; are chosen in some way and then, given current values of &lt;span class=&#34;math inline&#34;&gt;\(x_{1t},\ldots,x_{Jt}\)&lt;/span&gt; at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, new values are found by drawing from the following sequence of conditional distributions:&lt;/p&gt;
&lt;p&gt;\[
x_{1t+1} \sim p\left(x_1 \mid x_{2t},\ldots,x_{Jt} \right),
\]&lt;/p&gt;
&lt;p&gt;\[
x_{2t+1} \sim p\left(x_2 \mid x_{1t+1},\ldots,x_{Jt} \right),
\]&lt;/p&gt;
&lt;p&gt;up to&lt;/p&gt;
&lt;p&gt;\[
x_{Jt+1} \sim p\left(x_J \mid x_{2t+1},\ldots,x_{J-1t+1} \right).
\]&lt;/p&gt;
&lt;p&gt;It can be shown that, under general conditions, the sequence of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; iterates converges to a draw from the joint posterior of the variables. When &lt;span class=&#34;math inline&#34;&gt;\(J=2\)&lt;/span&gt;, the Gibbs’ sampler is the same as DA if &lt;span class=&#34;math inline&#34;&gt;\(x_1=Y_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2=\theta\)&lt;/span&gt; and the distributions condition on &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt;. We can then obtain a draw from the joint posterior of &lt;span class=&#34;math inline&#34;&gt;\(Y_1,\theta \mid Y_0\)&lt;/span&gt; by applying the Gibbs’ sampler, where at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-th imputed data set:&lt;/p&gt;
&lt;p&gt;\[
Y^d_{1t+1} \sim p\left(Y_1 \mid Y_0, \theta^d_{t}\right) \;\;\; \text{and} \;\;\; \theta^d_{t+1} \sim p\left(\theta \mid Y^d_{1t+1}, Y_0\right),
\]&lt;/p&gt;
&lt;p&gt;such that one run of the sampler converges to a draw from the posterior predictive distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; and a draw from the posterior of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. The sampler can be run independently &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; times to generate &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; iid draws from the approximate joint posterior of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt;. The values of &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; are multiple imputations of the missing values, drawn from their posterior predictive distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assessing-convergence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assessing Convergence&lt;/h2&gt;
&lt;p&gt;Assessing convergence of the sequence of draws to the target distribution is more difficult than assessing convergence of an EM-type algorithm because there is no single target quantity to monitor like the maximum value of the likelihood. Methods have been proposed to assess convergence of a single sequence (&lt;span class=&#34;citation&#34;&gt;Geyer (1992)&lt;/span&gt;), but a more reliable approach is to simulate &lt;span class=&#34;math inline&#34;&gt;\(D&amp;gt;1\)&lt;/span&gt; sequences with starting values dispersed throughout the parameter space, and the convergence of all quantities of interest can then be monitored by comparing variation between and within simulated sequences, until the “within” variation roughly equals the “between” variation. The idea is that when the distribution of each simulated sequence is close enough to the distribution of all the sequences mixed together, they can all be approximating the target distribution. &lt;span class=&#34;citation&#34;&gt;Gelman and Rubin (1992)&lt;/span&gt; developed an explicit monitoring statistic based on the following idea. For each scalar estimand &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;, label the draws from &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; parallel sequences as &lt;span class=&#34;math inline&#34;&gt;\(\psi^d_{t}\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(t=1,\ldots,T\)&lt;/span&gt; iterations and &lt;span class=&#34;math inline&#34;&gt;\(d=1,\ldots,D\)&lt;/span&gt; sequences, and compute the between &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and within &lt;span class=&#34;math inline&#34;&gt;\(\bar{V}\)&lt;/span&gt; sequence variances as:&lt;/p&gt;
&lt;p&gt;\[
B=\frac{T}{D-1}\sum_{d=1}^D(\bar{\psi}_{d.} - \bar{\psi}_{..})^2, \;\;\; \text{and} \;\;\; \bar{V}=\frac{1}{D}\sum_{d=1}^D s^2_{d},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\bar{\psi}_{d.}=\frac{1}{T}\sum_{t=1}^T \psi_{dt}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bar{\psi}_{..}=\frac{1}{D}\sum_{d=1}^D \bar{\psi}_{d}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(s^2_{d}=\frac{1}{T-1}\sum_{t=1}^T(\psi_{dt} - \bar{\psi}_{d.})^2\)&lt;/span&gt;. We can then estimate the marginal posterior variance of the estimand as&lt;/p&gt;
&lt;p&gt;\[
\widehat{Var}(\psi \mid Y_0) = \frac{T-1}{T}\hat{V} + \frac{1}{T} B,
\]&lt;/p&gt;
&lt;p&gt;which will &lt;em&gt;overestimate&lt;/em&gt; the marginal posterior variance assuming the starting distribution is appropriately over-dispersed but is &lt;em&gt;unbiased&lt;/em&gt; under stationarity (starting distribution equals the target distribution). For any finte &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, the within variance &lt;span class=&#34;math inline&#34;&gt;\(\hat{V}\)&lt;/span&gt; will &lt;em&gt;underestimate&lt;/em&gt; the marginal variance because individual sequences have not had time to range over all the target distribution and should have smaller variance then B. In the limit as &lt;span class=&#34;math inline&#34;&gt;\(T \rightarrow \infty\)&lt;/span&gt; the expecation of &lt;span class=&#34;math inline&#34;&gt;\(\hat{V}\)&lt;/span&gt; approaches the marginal variance. These facts suggest monitoring convergence by estimating the factor by which the scale of the current distribution for &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; might be reduced if the simulations were continued. This is the &lt;em&gt;potential scale reduction factor&lt;/em&gt; and is estimated by&lt;/p&gt;
&lt;p&gt;\[
\sqrt{\hat{R}} = \sqrt{\frac{\widehat{Var}(\psi \mid Y_0)}{\hat{V}}},
\]&lt;/p&gt;
&lt;p&gt;which declines to 1 as &lt;span class=&#34;math inline&#34;&gt;\(T \rightarrow \infty\)&lt;/span&gt;. When this quantity is high, there is evidence to proceed the simulations further to improve our inference about the target distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-simulation-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other Simulation Methods&lt;/h2&gt;
&lt;p&gt;When draws from the sequence of conditional distributions forming the Gibbs’ sampler are not easy to obtain, other simulation approaches are needed. Among these there are the &lt;em&gt;Sequential Imputation&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Kong, Liu, and Wong (1994)&lt;/span&gt;), &lt;em&gt;Sampling Imprtance Resampling&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Gelfand and Smith (1990)&lt;/span&gt;), &lt;em&gt;Rejection Sampling&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Von Neumann and others (1951)&lt;/span&gt;). One of these alternatives are the &lt;em&gt;Metropolis-Hastings&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Metropolis et al. (1953)&lt;/span&gt;) algorithms, of which the Gibbs’ sampler is a particular case, which constitute the so-called &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; (MCMC) algorithms as the sequence of iterates forms a Markov Chain (&lt;span class=&#34;citation&#34;&gt;Gelman et al. (2013)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-gelfand1990sampling&#34;&gt;
&lt;p&gt;Gelfand, Alan E, and Adrian FM Smith. 1990. “Sampling-Based Approaches to Calculating Marginal Densities.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 85 (410): 398–409.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34;&gt;
&lt;p&gt;Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. &lt;em&gt;Bayesian Data Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman1992single&#34;&gt;
&lt;p&gt;Gelman, Andrew, and Donald B Rubin. 1992. “A Single Series from the Gibbs Sampler Provides a False Sense of Security.” &lt;em&gt;Bayesian Statistics&lt;/em&gt; 4: 625–31.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-geyer1992practical&#34;&gt;
&lt;p&gt;Geyer, Charles J. 1992. “Practical Markov Chain Monte Carlo.” &lt;em&gt;Statistical Science&lt;/em&gt;, 473–83.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kong1994sequential&#34;&gt;
&lt;p&gt;Kong, Augustine, Jun S Liu, and Wing Hung Wong. 1994. “Sequential Imputations and Bayesian Missing Data Problems.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 89 (425): 278–88.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-metropolis1953equation&#34;&gt;
&lt;p&gt;Metropolis, Nicholas, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. 1953. “Equation of State Calculations by Fast Computing Machines.” &lt;em&gt;The Journal of Chemical Physics&lt;/em&gt; 21 (6): 1087–92.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tanner1987calculation&#34;&gt;
&lt;p&gt;Tanner, Martin A, and Wing Hung Wong. 1987. “The Calculation of Posterior Distributions by Data Augmentation.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 82 (398): 528–40.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-von1951general&#34;&gt;
&lt;p&gt;Von Neumann, John, and others. 1951. “The General and Logical Theory of Automata.” &lt;em&gt;1951&lt;/em&gt;, 1–41.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Expectation Maximisation Algorithm</title>
      <link>/missmethods/em-algorithm/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/em-algorithm/</guid>
      <description>


&lt;p&gt;Patterns of incomplete data in practice often do not have the forms that allow explicit &lt;em&gt;Maximum Likelihood&lt;/em&gt;(ML) estimates to be calculated. Suppose we have a model for the complete data &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, with density &lt;span class=&#34;math inline&#34;&gt;\(f(Y\mid \theta)\)&lt;/span&gt;, indexed by the set of unknown parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Writing &lt;span class=&#34;math inline&#34;&gt;\(Y=(Y_0,Y_1)\)&lt;/span&gt; in terms of the observed &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt; and missing &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; components, and assuming that the missingness mechanism is &lt;em&gt;Missing At Random&lt;/em&gt;(MAR), we want to maximise the likelihood&lt;/p&gt;
&lt;p&gt;\[
L\left(\theta \mid Y_0 \right) = \int f\left(Y_0, Y_1 \mid \theta \right)dY_1
\]&lt;/p&gt;
&lt;p&gt;with respect to &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. When the likelihood is differentiable and unimodal, ML estimates can be found by solving the likelihood equation&lt;/p&gt;
&lt;p&gt;\[
D_l\left(\theta \mid Y_0 \right) \equiv \frac{\partial ln L\left(\theta \mid Y_0 \right)}{\partial \theta} = 0,
\]&lt;/p&gt;
&lt;p&gt;while, if a closed-form solution cannot be found, iterative methods can be applied. One of these methods is the popular &lt;em&gt;Expectation Maximisation&lt;/em&gt;(EM) algorithm (&lt;span class=&#34;citation&#34;&gt;Dempster, Laird, and Rubin (1977)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The EM algorithm is a general iterative method for ML estimation in incomplete data problems. The basic idea behind it is based on a sequence of steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Replace missing values by estimated values&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Estimate the parameters&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Re-estimate the missing values assuming the new parameter estimates are correct&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Re-estimate parameters&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The procedure is then iterated until apparent convergence. Each iteration of EM consists of an &lt;em&gt;expectation step&lt;/em&gt; (E step) and a &lt;em&gt;maximisation step&lt;/em&gt; (M step) which ensure that, under general conditions, each iteration increases the loglikelihood &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid Y_0)\)&lt;/span&gt;. In addition, if the loglikelihood is bounded, the sequence &lt;span class=&#34;math inline&#34;&gt;\(\{l(\theta_t \mid Y_0), t=(0,1,\ldots)\}\)&lt;/span&gt; converges to a stationary value of &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid Y_0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;the-e-step-and-the-m-step&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The E step and the M step&lt;/h2&gt;
&lt;p&gt;The M step simply consists of performing ML estimation of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; as if there were no missing data, that is, after they had been filled in. The E step finds the conditional expectation of the missing values given the observed data and current estimated parameters. In practice, EM does not necessarily substitute the missing values themselves but its key idea is that they are generally not &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt; but the functions of &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt; appearing in the complete data loglikelihood &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid Y)\)&lt;/span&gt;. Specifically, let &lt;span class=&#34;math inline&#34;&gt;\(\theta_t\)&lt;/span&gt; be the current estimate of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, then the E step finds the expected complete data loglikelihood if &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; were &lt;span class=&#34;math inline&#34;&gt;\(\theta_t\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;\[
Q\left(\theta \mid \theta_t \right) = \int l\left(\theta \mid Y \right)f\left(Y_0 \mid Y_1 , \theta = \theta_t \right)dY_0.
\]&lt;/p&gt;
&lt;p&gt;The M step determines &lt;span class=&#34;math inline&#34;&gt;\(\theta_{t+1}\)&lt;/span&gt; by maximising this expected complete data loglikelihood:&lt;/p&gt;
&lt;p&gt;\[
Q\left(\theta_{t+1} \mid \theta_t \right) \geq Q\left(\theta \mid \theta_t \right),
\]&lt;/p&gt;
&lt;p&gt;for all &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;univariate-normal-data-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Normal Data Example&lt;/h3&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; form a an iid sample from a Normal distribution with population mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n_{cc}\)&lt;/span&gt; observed units and &lt;span class=&#34;math inline&#34;&gt;\(i=n_{cc}+1,\ldots,n\)&lt;/span&gt; missing units. Under the assumption that the missingness mechanism is ignorable, the expectation of each missing &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\sigma^2)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. Since the loglikelihood based on all &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is linear in the sufficient statistics &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^n y_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^n y^2_i\)&lt;/span&gt;, the E step of the algorithm calculates&lt;/p&gt;
&lt;p&gt;\[
E\left(\sum_{i=1}^{n}y_i \mid \theta_t, Y_0 \right) = \sum_{i=1}^{n_{cc}}y_i + (n-n_{cc})\mu_t
\]&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;\[
E\left(\sum_{i=1}^{n}y^2_i \mid \theta_t, Y_0 \right) = \sum_{i=1}^{n_{cc}}y^2_i + (n-n_{cc})\left(\mu^2_t + \sigma^2_t \right)
\]&lt;/p&gt;
&lt;p&gt;for current estimates &lt;span class=&#34;math inline&#34;&gt;\(\theta_t=(\mu_t,\sigma_t)\)&lt;/span&gt; of the parameters. Note that simply substituting &lt;span class=&#34;math inline&#34;&gt;\(\mu_t\)&lt;/span&gt; for the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{n_{cc}+1},\ldots,y_n\)&lt;/span&gt; is not correct since the term &lt;span class=&#34;math inline&#34;&gt;\((n-n_{cc})(\sigma_t^2)\)&lt;/span&gt; is omitted. Without missing data, the ML estimate of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sum_{i=1}^ny_i}{n}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sum_{i=1}^ny^2_i}{n}-\left(\frac{\sum_{i=1}^ny_i}{n}\right)^2\)&lt;/span&gt;, respectively. The M step uses the same expressions based on the current expectations of the sufficient statistics calculated in the E step. Thus, the M step calculates&lt;/p&gt;
&lt;p&gt;\[
\mu_{t+1} = \frac{E\left(\sum_{i=1}^n y_i \mid \theta_t, Y_0 \right)}{n}
\]&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;\[
\sigma^2_{t+1} = \frac{E\left(\sum_{i=1}^n y^2_i \mid \theta_t, Y_0 \right)}{n} - \mu^2_{t+1}.
\]&lt;/p&gt;
&lt;p&gt;Setting &lt;span class=&#34;math inline&#34;&gt;\(\mu_t=\mu_{t+1}=\hat{\mu}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_t=\sigma_{t+1}=\hat{\sigma}\)&lt;/span&gt; in these equations shows that a fixed point of these iterations is &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=\frac{\sum_{i=1}^{n_{cc}}y_i}{n_{cc}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2=\frac{\sum_{i=1}^{n_{cc}}y^2_i}{n_{cc}} - \hat{\mu}^2\)&lt;/span&gt;, which are the ML estimates of the parameters from &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt; assuming MAR and distinctness of the parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;extensions-of-em&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extensions of EM&lt;/h2&gt;
&lt;p&gt;There are a variety of applications where the M step does not have a simple computational form. In such cases, one way to avoid an iterative M step is to increase the Q function, rather than maximising it at each iteration, which corresponds to a &lt;em&gt;Generalised Expectation Maximisation&lt;/em&gt;(GEM) algorithm. GEM inceases the likelihood at each iteration but appropriate convergence is not guaranteed without further specification of the process of increasing the Q function. One specific case of GEM is the &lt;em&gt;Expectation Conditional Maximisation&lt;/em&gt;(ECM) algorithm (&lt;span class=&#34;citation&#34;&gt;Meng and Rubin (1993)&lt;/span&gt;), which replaces the M step with a sequence of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; conditional maximisation (CM) steps, each of which maximises the Q function over &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; but with some vector function of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, say &lt;span class=&#34;math inline&#34;&gt;\(g_s(\theta)\)&lt;/span&gt;, fixed at its previous values for &lt;span class=&#34;math inline&#34;&gt;\(s=1,\ldots,S\)&lt;/span&gt;. Very briefly, assume that we have a parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that can be partitioned into subvectors &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\theta_1,\ldots,\theta_S)\)&lt;/span&gt;, then we can take the &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;-th of the CM steps to be maximisation with respect to &lt;span class=&#34;math inline&#34;&gt;\(\theta_s\)&lt;/span&gt; with all other parameters held fixed. Alternatively, it may be useful to take the &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;-th of the CM steps to be simultaneous maximisation over all of the subvectors expect &lt;span class=&#34;math inline&#34;&gt;\(\theta_s\)&lt;/span&gt;, which is fixed. Because the ECM increases Q, it belongs to the class of GEM algorithms and therefore monotonically increases the likelihood of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. When the set of functions &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is “space filling” in the sense that it allows unconstrained maximisation over &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in its parameter space, ECM converges to a stationary point under the same conditions ensuring convergence of EM.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Expectation Conditional Maximisation Either&lt;/em&gt;(ECME) algorithm (&lt;span class=&#34;citation&#34;&gt;Liu and Rubin (1994)&lt;/span&gt;) is another version of GEM, which replaces some of the CM steps of ECM, maximising the constrained expected complete data loglikelihood function, with steps that maximise the correspondingly constrained actual likelihood function. The algorithm has stable monotone convergence and basic simplicity implementation relative to competing faster converging methods, and can have faster convergence rate than EM or ECM, measured using either the number of iterations or actual computer time. The The &lt;em&gt;Alternative Expectation Conditional Maximisation&lt;/em&gt;(AECM) algorithm (&lt;span class=&#34;citation&#34;&gt;Meng and Van Dyk (1997)&lt;/span&gt;) builds on the ECME idea by maximising functions other than Q or L in particular CM steps, corresponding to varying definitions of what constitutes missing data. An iteration of AECM consists of cycles, each consisting of an E step with a particular definition of complete and missing data, followed by CM steps, which can result in enhanced computational efficiency.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-dempster1977maximum&#34;&gt;
&lt;p&gt;Dempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the Em Algorithm.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Methodological)&lt;/em&gt; 39 (1): 1–22.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-liu1994ecme&#34;&gt;
&lt;p&gt;Liu, Chuanhai, and Donald B Rubin. 1994. “The Ecme Algorithm: A Simple Extension of Em and Ecm with Faster Monotone Convergence.” &lt;em&gt;Biometrika&lt;/em&gt; 81 (4): 633–48.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-meng1993maximum&#34;&gt;
&lt;p&gt;Meng, Xiao-Li, and Donald B Rubin. 1993. “Maximum Likelihood Estimation via the Ecm Algorithm: A General Framework.” &lt;em&gt;Biometrika&lt;/em&gt; 80 (2): 267–78.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-meng1997algorithm&#34;&gt;
&lt;p&gt;Meng, Xiao-Li, and David Van Dyk. 1997. “The Em Algorithm—an Old Folk-Song Sung to a Fast New Tune.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 59 (3): 511–67.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Bayesian Inference</title>
      <link>/missmethods/likelihood-based-methods-ignorable2/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/likelihood-based-methods-ignorable2/</guid>
      <description>


&lt;p&gt;Bayesian inference offers a convenient framework to analyse missing data as it draws no distinction between missing values and parameters, both interprted as unobserved quantities who are associated with a joint posterior distribution conditional on the observed data. In this section, I review basic concepts of Bayesian inference based on fully observed data, with notation and structure mostly taken from &lt;span class=&#34;citation&#34;&gt;Gelman et al. (2013)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;bayesian-inference-for-complete-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian Inference for Complete Data&lt;/h2&gt;
&lt;p&gt;Bayesian inference is the process of fitting a probability model to a set of data &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and summarising the results by a probability distribution on the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; of the model and on unobserved quantities &lt;span class=&#34;math inline&#34;&gt;\(\tilde{Y}\)&lt;/span&gt; (e.g. predictions). Indeed, Bayesian statistical conclusions about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(\tilde{Y}\)&lt;/span&gt;) are made in terms of probability statements, conditional on the observed data &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, typically indicated with the notation &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid y)\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde{y} \mid y)\)&lt;/span&gt;. Conditioning on the observed data is what makes Bayesian inference different from standard statistical approaches which are instead based on the retrospective evaluation of the procedures used to estimate &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y}\)&lt;/span&gt;) over the distribution of possible &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; values conditional on the “true” unknown value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;bayes-rule&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bayes’ Rule&lt;/h3&gt;
&lt;p&gt;In order to make probability statements about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, we start with a model providing a &lt;em&gt;joint probability distribution&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(p(\theta,y)\)&lt;/span&gt;. Thus, the joint probability mass or density function can be written as a product of two densities that are often referred to as the &lt;em&gt;prior distribution&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; and the &lt;em&gt;sampling distribution&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(p(y \mid \theta)\)&lt;/span&gt;, respectively:&lt;/p&gt;
&lt;p&gt;\[
p(\theta,y) = p(\theta)p(y \mid \theta),
\]&lt;/p&gt;
&lt;p&gt;and conditioning on the observed values of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, using the basic property of conditional probability known as &lt;em&gt;Bayes’ rule&lt;/em&gt;, yields the &lt;em&gt;posterior distribution&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;\[
p(\theta \mid y) = \frac{p(\theta,y)}{p(y)} = \frac{p(\theta)p(y \mid \theta)}{p(y)},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(p(y)=\sum_{\theta \in \Theta}p(\theta)p(y\mid \theta)\)&lt;/span&gt; is the sum (or integral in the case of continous &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) over all possible values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in the sample space &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;. We can approximate the above equation by omitting the factor &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; which does not depend on &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and, given &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, can be considered as fixed, yielding the &lt;em&gt;unnormalised posterior density&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;\[
p(\theta \mid y) \propto p(\theta) p(y \mid \theta),
\]&lt;/p&gt;
&lt;p&gt;with the purpose of the analysis being to develop the model &lt;span class=&#34;math inline&#34;&gt;\(p(\theta,y)\)&lt;/span&gt; and adequately summarise &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate-normal-example-known-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Normal Example (known variance)&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,\ldots,y_n)\)&lt;/span&gt; denote an independent and identially distributed sample of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units, which are assumed to come from a Normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, whose sampling density function is&lt;/p&gt;
&lt;p&gt;\[
p(y \mid \mu)=\frac{1}{\sqrt{\left(2\pi\sigma^2\right)^n}}\text{exp}\left(-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2} \right),
\]&lt;/p&gt;
&lt;p&gt;where for the moment we assume the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; to be known (i.e. constant). Consider now a prior probability distribution for the mean parameter &lt;span class=&#34;math inline&#34;&gt;\(p(\mu)\)&lt;/span&gt;, which belongs to the family of &lt;em&gt;conjugate prior densities&lt;/em&gt;, for example a Normal distribution, and parameterised in terms of a prior mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_0\)&lt;/span&gt;. Thus, its prior density function is&lt;/p&gt;
&lt;p&gt;\[
p(\mu) = \frac{1}{\sqrt{2\pi\sigma^2_0}}\text{exp}\left(-\frac{1}{2}\frac{(\mu -\mu_0)^2}{\sigma^2} \right),
\]&lt;/p&gt;
&lt;p&gt;under the assumption tha the hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_0\)&lt;/span&gt; are known. The conjugate prior density implies that the posterior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (with &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; assumed constant) belongs to the same family of distributions of the sampling function, that is Normal, but some algebra is required to reveal its specific form. In particular, the posterior density is&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y) = \frac{p(\mu)p(y\mid \mu)}{p(y)} \propto \frac{1}{\sqrt{2\pi\sigma^2_0}}\frac{1}{\sqrt{\left(2\pi\sigma^2\right)^n}}\text{exp}\left(-\frac{1}{2} \left[\frac{(\mu - \mu_0)^2}{\sigma^2_0} + \sum_{i=1}^n\frac{(y_i-\mu)^2}{\sigma^2} \right] \right).
\]&lt;/p&gt;
&lt;p&gt;Exapanding the components, collecting terms and completing the square in &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; gives&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y) \propto \text{exp}\left(-\frac{(\mu - \mu_1)}{2\tau^2_1} \right),
\]&lt;/p&gt;
&lt;p&gt;that is the posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is Normal with posterior mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_1\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\tau^2_1\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;\[
\mu_1 = \frac{\frac{1}{\tau^2_0}\mu_0 + \frac{n}{\sigma^2}\bar{y}}{\frac{1}{\tau^2_0} + \frac{n}{\sigma^2}} \;\;\; \text{and} \;\;\; \frac{1}{\tau^2_1}=\frac{1}{\tau^2_0} + \frac{n}{\sigma^2}.
\]&lt;/p&gt;
&lt;p&gt;We can see that the posterior distribution depends on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; only through the sample mean &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}=\sum_{i=1}^ny_i\)&lt;/span&gt;, which is a &lt;em&gt;sufficient statistic&lt;/em&gt; in this model. When working with Normal distributions, the inverse of the variance plays a prominent role and is called the &lt;em&gt;precision&lt;/em&gt; and, from the above expressions, it can be seen that for normal data and prior, the posterior precision &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\tau^2_1}\)&lt;/span&gt; equals the sum of the prior precision &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\tau^2_0}\)&lt;/span&gt; and the sampling precision &lt;span class=&#34;math inline&#34;&gt;\(\frac{n}{\sigma^2}\)&lt;/span&gt;. Thus, when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is large, the posterior precision is largely dominated by &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and the sample mean &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt; compared to the corresponding prior parameters. In the specific case where &lt;span class=&#34;math inline&#34;&gt;\(\tau^2_0=\sigma^2\)&lt;/span&gt;, the prior has the same weight as one extra observation with the value of &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; and, as &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt;, we have that &lt;span class=&#34;math inline&#34;&gt;\(p(\mu\mid y)\approx N\left(\mu \mid \bar{y},\frac{\sigma^2}{n}\right)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate-normal-example-unknown-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Normal Example (unknown variance)&lt;/h3&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(p(y \mid \mu,\sigma^2)=N(y \mid \mu, \sigma^2)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; known and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; unknown, the sampling distribution for a vector &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units is&lt;/p&gt;
&lt;p&gt;\[
p(y \mid \sigma^2)=\frac{1}{\sqrt{\left(2\pi\sigma^2\right)^n}}\text{exp}\left(-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2} \right),
\]&lt;/p&gt;
&lt;p&gt;with the corresponding conjugate prior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; being the Inverse-Gamma distribution &lt;span class=&#34;math inline&#34;&gt;\(\Gamma^{-1}(\alpha,\beta)\)&lt;/span&gt; with density function&lt;/p&gt;
&lt;p&gt;\[
p(\sigma^2) \propto (\sigma^2)^{-(\alpha+1)}\text{exp}\left(-\frac{\beta}{\sigma^2}\right),
\]&lt;/p&gt;
&lt;p&gt;indexed by the hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. A convenient parameterisation is as a Scaled Inverse-Chi Squared distribution &lt;span class=&#34;math inline&#34;&gt;\(\text{Inv-}\chi^2(\sigma^2_0,\nu_0)\)&lt;/span&gt; with scale and degrees of freedom parameters &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu_0\)&lt;/span&gt;, respectively. This means that the prior on &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; corresponds to the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma^2_0 \nu_0}{X}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(X\sim \chi^2_{\nu_0}\)&lt;/span&gt; random variable. After some calculations, the resulting posterior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;\[
p(\sigma^2 \mid y) \propto (\sigma^2)^\left(\frac{n+\nu_0}{2}+1\right)\text{exp}\left(-\frac{\nu_0 \sigma^2_0 + n \nu}{2\sigma^2} \right)
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\nu=\frac{1}{n}\sum_{i=1}^n(y_i-\mu)^2\)&lt;/span&gt;. This corresponds to say that&lt;/p&gt;
&lt;p&gt;\[
\sigma^2 \mid y \sim \text{Inv-}\chi^2\left(\nu_0 +n, \frac{\nu_0\sigma^2_0+n\nu}{\nu_0 + n} \right),
\]&lt;/p&gt;
&lt;p&gt;with scale equal to the degrees of freedom-weighted average of the prior and data scales and degrees of freedom equal to the sum of the prior and data degrees of freedom.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate-normal-example-unknown-mean-and-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Normal Example (unknown mean and variance)&lt;/h3&gt;
&lt;p&gt;Suppose now that both the mean and variance parameters are unknown such that&lt;/p&gt;
&lt;p&gt;\[
p(y \mid \mu, \sigma^2) \sim N(\mu, \sigma^2),
\]&lt;/p&gt;
&lt;p&gt;and that the interest is centred on making inference about &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, that is we seek the conditional posterior distribution of the parameters of interest given the observed data &lt;span class=&#34;math inline&#34;&gt;\(p(\mu \mid y)\)&lt;/span&gt;. This can be derived from the joint posterior distribution density &lt;span class=&#34;math inline&#34;&gt;\(p(\mu, \sigma^2 \mid y)\)&lt;/span&gt; by averaging over all possible values of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y)=\int p(\mu, \sigma^2 \mid y)d\sigma^2,
\]&lt;/p&gt;
&lt;p&gt;or, alternatively, the joint posterior can be factored as the product of the marginal distribution of one parameter and the conditional distribution of the other given the former and then taking the average over the values of the “nuisance” parameter&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y)=\int p(\mu \mid \sigma^2, y)p(\sigma^2 \mid y)d\sigma^2.
\]&lt;/p&gt;
&lt;p&gt;The integral forms are rarely computed in practice but this expression helps us to understand that posterior distributions can be expressed in terms of the product of marginal and conditional densities, first drawing &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; from its marginal and then &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; from its conditional given the drawn value of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, so that the integration is indirectly performed. For example, consider the Normal model with both unknown mean and variance and assume a vague prior density &lt;span class=&#34;math inline&#34;&gt;\(p(\mu,\sigma^2)\propto (\sigma^2)^{-1}\)&lt;/span&gt; (corresponding to uniform prior on &lt;span class=&#34;math inline&#34;&gt;\((\mu, \log\sigma)\)&lt;/span&gt;), then the joint posterior distribution is proportional to the sampling distribution multiplied by the factor &lt;span class=&#34;math inline&#34;&gt;\((\sigma^2)^{-1}\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p(\mu,\sigma^2 \mid y)\propto \sigma^{-n-2}\text{exp}\left(-\frac{1}{2\sigma^2}\left[(n-1)s^2+n(\bar{y}-\mu)^2 \right] \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s^2=\frac{1}{n-1}\sum_{i=1}^n(y_i-\bar{y})^2\)&lt;/span&gt; is the sample variance. Next, the conditional posterior density &lt;span class=&#34;math inline&#34;&gt;\(p(\mu \mid \sigma^2)\)&lt;/span&gt; can be shown to be equal to&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid \sigma^2,y) \sim N(\bar{y},\frac{\sigma^2}{n}),
\]&lt;/p&gt;
&lt;p&gt;while the marginal posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2 \mid y)\)&lt;/span&gt; can be obtained by averaging the joint &lt;span class=&#34;math inline&#34;&gt;\(p(\mu,\sigma^2\mid y)\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p(\sigma^2 \mid y)\propto \int \left(\sigma^{-n-2}\text{exp}\left(-\frac{1}{2\sigma^2}\left[(n-1)s^2+n(\bar{y}-\mu)^2 \right] \right)\right)d\mu,
\]&lt;/p&gt;
&lt;p&gt;which leads to&lt;/p&gt;
&lt;p&gt;\[
p(\sigma^2 \mid ,y) \sim \text{Inv-}\chi^2(n-1,s^2).
\]&lt;/p&gt;
&lt;p&gt;Typically, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; represents the estimand of interest and the obejective of the analysis is therefore to make inference about the marginal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\mu \mid y)\)&lt;/span&gt;, which can be obtained by integrating &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; out of the joint posterior&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y)=\int_{0}^{\infty}p(\mu,\sigma^2\mid y)d\sigma^2 \propto \left[1+\frac{n(\mu-\bar{y})}{(n-1)s^2} \right]
\]&lt;/p&gt;
&lt;p&gt;which corresponds to a Student-&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; density with &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt; degrees of freedom&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y)\sim t_{n-1}\left(\bar{y},\frac{s^2}{n}\right)
\]&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-normal-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multivariate Normal Example&lt;/h3&gt;
&lt;p&gt;Similar considerations to those applied to the univariate case can be extended to the multivariate case when &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is formed by &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; components coming from the Multivariate Normal distribution&lt;/p&gt;
&lt;p&gt;\[
p(y\mid \mu, \Sigma) \sim N(\mu, \Sigma),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is a vector of length &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(J\times J\)&lt;/span&gt; covariance matrix, which is symmetric and positive definite. The sampling distribution for a sample of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units is&lt;/p&gt;
&lt;p&gt;\[
p(y\mid \mu, \Sigma) \propto \mid \Sigma \mid^{-n/2}\text{exp}\left(-\frac{1}{2}\sum_{i=1}^n(y_i-\mu)^{T}\Sigma^{-1}(y_i-\mu) \right),
\]&lt;/p&gt;
&lt;p&gt;As with the univariate normal model, we can derive the posterior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; according to the factorisation used of the joint posterior and the prior distributions specified. For example, using the conjugate normal prior for the mean &lt;span class=&#34;math inline&#34;&gt;\(p(\mu)\sim N(\mu_0,\Sigma_0)\)&lt;/span&gt;, given &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; known, the posterior can be shown to be&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y) \sim N(\mu_1,\Sigma_1),
\]&lt;/p&gt;
&lt;p&gt;where the posterior mean is a weighted average of the data and prior mean with weights given by the data and prior precision matrices &lt;span class=&#34;math inline&#34;&gt;\(\mu_1=(\Sigma^{-1}_0+n\Sigma^{-1})^{-1} (\Sigma_0^{-1}\mu_0 + n\Sigma^{-1}\bar{y})\)&lt;/span&gt;, and the posterior precision is the sum of the data and prior precisions &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^{-1}_1=\Sigma^{-1}_0+n\Sigma^{-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the situation in which both &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; are unknown, convenient conjugate prior distributions which generalise those used in the univariate case are the Inverse-Wishart for the covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\sim \text{Inv-Wishart}(\Lambda_0,\nu_0)\)&lt;/span&gt; and the Multivariate Normal for the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\sim N(\mu_0, \Sigma_0)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\nu_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Lambda_0\)&lt;/span&gt; represent the degrees of freedom and the scale matrix for the Inverse-Wishart distribution, while &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_0=\frac{\Sigma}{\kappa_0}\)&lt;/span&gt; are the prior mean and covariance matrix for the Multivariate Normal. Woking out the form of the posterior, it can be shown that the joint posterior distribution has the same form of the sampling distribution with parameters&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid \Sigma, y) \sim N(\mu_1,\Sigma_1) \;\;\; \text{and} \;\;\; p(\Sigma \mid y) \sim \text{Inv-Wishart}(\Lambda_1,\nu_1),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_1=\frac{\Sigma}{\kappa_1}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu_1=\frac{1}{\kappa_0+n}\mu_0+\frac{n}{\kappa_0+n}\bar{y}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\kappa_1=\kappa_0+n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\nu_1=\nu_0+n\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\Lambda_1=\Lambda_0+\sum_{i=1}^n(y_i-\bar{y})(y_i-\bar{y})^T+\frac{\kappa_0 n}{\kappa_0+n}(\bar{y}-\mu_0)(\bar{y}-\mu_0)^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression Models&lt;/h2&gt;
&lt;p&gt;Suppose the data consist in &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units measured on an outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and a set of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; covariates &lt;span class=&#34;math inline&#34;&gt;\(X=(x_{1},\ldots,x_{J})\)&lt;/span&gt; and assume that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is Normal with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_i=\beta_0+\sum_{j=1}^J\beta_jx_{ij}\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;\[
p(y \mid \beta,\sigma^2,X) \sim N(X\beta,\sigma^2I),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta=(\beta_0,\ldots,\beta_J)\)&lt;/span&gt; is the set of regression coefficients and &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(n\times n\)&lt;/span&gt; identity matrix. Within the normal regression model, a convenient vague prior distribution is uniform on &lt;span class=&#34;math inline&#34;&gt;\((\beta,\log\sigma)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;\[
p(\beta,\sigma^2)\propto\sigma^{-2}.
\]&lt;/p&gt;
&lt;p&gt;As with normal distributions with unknown mean and variance we can first determine the marginal posterior of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and factor the joint posterior as &lt;span class=&#34;math inline&#34;&gt;\(p(\beta,\sigma^2)=p(\beta \mid \sigma^2, y)p(\sigma^2 \mid y)\)&lt;/span&gt; (omit X for simplicity). Then, the conditional distribtuion &lt;span class=&#34;math inline&#34;&gt;\(p(\beta \mid \sigma^2,y)\)&lt;/span&gt; is Normal&lt;/p&gt;
&lt;p&gt;\[
p(\beta \mid \sigma^2, y) \sim N(\hat{\beta},V_{\beta}\sigma^2),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}=(X^{T}X)^{-1}(X^{T}y)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_{\beta}=(X^{T}X)^{-1}\)&lt;/span&gt;. The marginal posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2 \mid y)\)&lt;/span&gt; has a scaled Inverse-&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; form&lt;/p&gt;
&lt;p&gt;\[
p(\sigma^2\mid y) \sim \text{Inv-}\chi^2(n-J,s^2),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s^2=\frac{1}{n-J}(y-X\hat{\beta})^{T}(y-X\hat{\beta})\)&lt;/span&gt;. Finally, the marginal posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\beta \mid y)\)&lt;/span&gt;, averaging over &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, is multivariate &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n-J\)&lt;/span&gt; degrees of freedom, even though in practice since we can characterise the joint posterior by drawing from &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2)\)&lt;/span&gt; and then from &lt;span class=&#34;math inline&#34;&gt;\(p(\beta \mid \sigma^2)\)&lt;/span&gt;. When the anaysis is based on improper priors (do not have finite integral), it is important to check tha the posterior is proper. In the case of the regression model, the posterior for &lt;span class=&#34;math inline&#34;&gt;\(\beta \mid \sigma^2\)&lt;/span&gt; is proper only if the number of observations is larger than the number of parameters &lt;span class=&#34;math inline&#34;&gt;\(n&amp;gt;J\)&lt;/span&gt;, and that the rank of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; equals &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; (i.e. the columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; are linearly independent) in order for all &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; coefficients to be uniquely identified from the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generalised-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generalised Linear Models&lt;/h2&gt;
&lt;p&gt;The purpose of &lt;em&gt;Generalised Linear Models&lt;/em&gt;(GLM) is to extend the idea of linear modelling to cases for which the linear relationship between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(E[y\mid X]\)&lt;/span&gt; or the Normal distribution is not appropriate. GLMs are specified in three stages&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Choose the linear predictor &lt;span class=&#34;math inline&#34;&gt;\(\eta=X\beta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose the &lt;em&gt;link fuction&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(g()\)&lt;/span&gt; that relates the linear predictor to the mean of the outcome variable &lt;span class=&#34;math inline&#34;&gt;\(\mu=g^{-1}(\eta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose the random component specifying the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; with mean &lt;span class=&#34;math inline&#34;&gt;\(E[y\mid X]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus, the mean of the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is determined as &lt;span class=&#34;math inline&#34;&gt;\(E[y\mid X]=g^{-1}(X\beta)\)&lt;/span&gt;. The Normal linear model can be thought as a special case of GLMs where the link function is the identity &lt;span class=&#34;math inline&#34;&gt;\(g(\mu)=\mu\)&lt;/span&gt; and the random component is normally distributed. Perhaps, the most commonly used GLMs are those based on Poisson and Binomial distributions to analyse count and binary data, respectively.&lt;/p&gt;
&lt;div id=&#34;poisson&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poisson&lt;/h3&gt;
&lt;p&gt;Counted data are often modelled using Poisson regression models which assume that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is distributed according to a Poisson distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. The link function is typically chosen to be the logarithm so that &lt;span class=&#34;math inline&#34;&gt;\(\log \mu = X\beta\)&lt;/span&gt; and the distribution of the data has density&lt;/p&gt;
&lt;p&gt;\[
p(y\mid \beta)=\prod_{i=1}^n \frac{1}{y_i}\text{exp}\left(-\text{e}^{(\eta_i)}(\text{exp}(\eta_i))^{y_i}\right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\eta_i=(X\beta)_i\)&lt;/span&gt; is the linear predictor for the &lt;span class=&#34;math inline&#34;&gt;\(i-\)&lt;/span&gt;th unit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Binomial&lt;/h3&gt;
&lt;p&gt;Suppose there are some binomial data &lt;span class=&#34;math inline&#34;&gt;\(y_i \sim \text{Bin}(n_i,\mu_i)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(n_i\)&lt;/span&gt; known. It is common to specify the model in terms of the mean of the proportions &lt;span class=&#34;math inline&#34;&gt;\(\frac{y_i}{n_i}\)&lt;/span&gt; rather than the mean of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;. Choosing the logit tranformation of the probability of success &lt;span class=&#34;math inline&#34;&gt;\(g(\mu_i)=\log\left(\frac{\mu_i}{1-\mu_i}\right)\)&lt;/span&gt; as the link function leads to the logistic regression where data have distribution&lt;/p&gt;
&lt;p&gt;\[
p(y \mid \beta)=\prod_{i=1}^n {n_i \choose y_i} {e^{\eta_i} \choose 1+e^{\eta_i}}^{y_i} {1 \choose 1+e^{\eta_i}}^{n_i-y_i}.
\]&lt;/p&gt;
&lt;p&gt;The link functions used in the previous models are known as the &lt;em&gt;canonical link&lt;/em&gt; functions for each family of distributions, which is the function of the mean parameter that appears in the exponent of the exponential family form of the probability density. However, it is also possible to use link functions which are not canonical.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34;&gt;
&lt;p&gt;Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. &lt;em&gt;Bayesian Data Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Maximum Likelihood Estimation</title>
      <link>/missmethods/likelihood-based-methods-ignorable/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/likelihood-based-methods-ignorable/</guid>
      <description>


&lt;p&gt;A possible approach to analyse missing data is to use methods based on the likelihood function under specific modelling assumptions. In this section, I review maximum likelihood methods based on fully observed data alone.&lt;/p&gt;
&lt;div id=&#34;maximum-likelihood-methods-for-complete-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximum Likelihood Methods for Complete Data&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; denote the set of data, which are assumed to be generated according to a certain probability density function &lt;span class=&#34;math inline&#34;&gt;\(f(Y= y,\mid \theta)=f(y \mid \theta)\)&lt;/span&gt; indexed by the set of parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, which lies on the parameter space &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; (i.e. set of values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; for which &lt;span class=&#34;math inline&#34;&gt;\(f(y\mid \theta)\)&lt;/span&gt; is a proper density function). The &lt;em&gt;Likelihood&lt;/em&gt; function, indicated with &lt;span class=&#34;math inline&#34;&gt;\(L(\theta \mid y)\)&lt;/span&gt;, is defined as any function of &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt; proportional that is to &lt;span class=&#34;math inline&#34;&gt;\(f(y \mid \theta)\)&lt;/span&gt;. Note that, in contrast to the density function which is defined as a function of the data &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given the values of the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, instead the likelihood is defined as a function of the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; for fixed data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. In addition, the &lt;em&gt;loglikelihood&lt;/em&gt; function, indicated with &lt;span class=&#34;math inline&#34;&gt;\(l(\theta\mid y)\)&lt;/span&gt; is defined as the natural logarithm of &lt;span class=&#34;math inline&#34;&gt;\(L(\theta \mid y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;univariate-normal-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Normal Example&lt;/h3&gt;
&lt;p&gt;The joint density function of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent and identially distributed units &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,\ldots,y_n)\)&lt;/span&gt; from a Normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, is&lt;/p&gt;
&lt;p&gt;\[
f(y \mid \mu, \sigma^2)=\frac{1}{\sqrt{\left(2\pi\sigma^2\right)^n}}\text{exp}\left(-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2} \right),
\]&lt;/p&gt;
&lt;p&gt;and therefore the loglikelihood is&lt;/p&gt;
&lt;p&gt;\[
l(\mu, \sigma^2 \mid y)= -\frac{n}{2}\text{ln}(2\pi)-\frac{n}{2}\text{ln}(\sigma^2)-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2},
\]&lt;/p&gt;
&lt;p&gt;which is considered as a function of &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\sigma^2)\)&lt;/span&gt; for fixed data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-normal-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multivariate Normal Example&lt;/h3&gt;
&lt;p&gt;If the sample considered has dimension &lt;span class=&#34;math inline&#34;&gt;\(J&amp;gt;1\)&lt;/span&gt;, e.g. we have a set of idependent and identically distributed variables &lt;span class=&#34;math inline&#34;&gt;\(y=(y_{ij})\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; units and &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots,J\)&lt;/span&gt; variables, which comes from a Multivariate Normal distribution with mean vector &lt;span class=&#34;math inline&#34;&gt;\(\mu=(\mu_1,\ldots\mu_J)\)&lt;/span&gt; and covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma=(\sigma_{jk})\)&lt;/span&gt; for $ j=1,,J, k=1,,K$ and &lt;span class=&#34;math inline&#34;&gt;\(J=K\)&lt;/span&gt;, then its density function is&lt;/p&gt;
&lt;p&gt;\[
f(y \mid \mu, \Sigma)=\frac{1}{\sqrt{\left(2\pi \right)^{nK}\left(\mid \Sigma \mid \right)^n}} \text{exp}\left(-\frac{1}{2}\sum_{i=1}^{n}(y_i-\mu)\Sigma^{-1}(y_i-\mu)^{T} \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(|\Sigma|\)&lt;/span&gt; denotes the determinant of the matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; and the superscript &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; denotes the transpose of a matrix or vector, while &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; denotes the row vector of observed values for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. The loglikelihood of &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\Sigma)\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;\[
l(\mu,\Sigma \mid y)= - \frac{n}{2}\text{ln}(2\pi) - \frac{n}{2}\text{ln}(|\Sigma|)-\frac{1}{2}\sum_{i=1}^{n}(y_i-\mu)\Sigma^{-1}(y_i-\mu)^T.
\]&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mle-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MLE estimation&lt;/h2&gt;
&lt;p&gt;Finding the maximum value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that is most likely to have generated the data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, corresponding to maximising the likelihood or &lt;em&gt;Maximum Likelihood Estimation&lt;/em&gt;(MLE), is a standard approach to make inference about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Suppose a specific value for the parameter &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(L(\hat{\theta}\mid y)\geq L(\theta \mid y)\)&lt;/span&gt; for any other value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. This implies that the observed data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is at least as likely under &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; as under any other value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is the value best supported by the data. More specifically, a maximum likelihood estimate of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is a value of &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt; that maximises the likelihood &lt;span class=&#34;math inline&#34;&gt;\(L(\theta \mid y)\)&lt;/span&gt; or, equivalently, that maximises the loglikelihood &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid y)\)&lt;/span&gt;. In general, when the likelihood is differentiable and bounded from above, typically the MLE can be found by differentiating &lt;span class=&#34;math inline&#34;&gt;\(L(\theta \mid y)\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid y)\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, setting the result equal to zero, and solving for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. The resulting equation, &lt;span class=&#34;math inline&#34;&gt;\(D_l(\theta)=\frac{\partial l(\theta \mid y)}{\partial \theta}=0\)&lt;/span&gt;, is known as the &lt;em&gt;likelihood equation&lt;/em&gt; and the derivative of the loglikelihood as the &lt;em&gt;score function&lt;/em&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; consists in a set of &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots,J\)&lt;/span&gt; components, then the likelihood equation corresponds to a set of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; simultaneous equations, obtained by differentiating &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid y)\)&lt;/span&gt; with respect to each component of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;univariate-normal-example-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Normal Example&lt;/h3&gt;
&lt;p&gt;Recall that, for a Normal sample with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units, the loglikelihood is indexed by the set of parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\sigma^2)\)&lt;/span&gt; and has the form&lt;/p&gt;
&lt;p&gt;\[
l(\mu, \sigma^2 \mid y)= -\frac{n}{2}\text{ln}(2\pi)-\frac{n}{2}\text{ln}(\sigma^2)-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2}.
\]&lt;/p&gt;
&lt;p&gt;Next, the MLE can be derived by first differentiating &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid y)\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and set the result equal to zero, that is&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial l(\theta \mid y)}{\partial \mu}= -\frac{2}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)(-1)=\frac{\sum_{i=1}^n y_i - n\mu}{\sigma^2}=0,
\]&lt;/p&gt;
&lt;p&gt;Next, after simplifying a bit, we can retrieve the solution&lt;/p&gt;
&lt;p&gt;\[
\hat{\mu}=\frac{1}{n}\sum_{i=1}^n y_i=\bar{y},
\]&lt;/p&gt;
&lt;p&gt;which corresponds to the sample mean of the observations. Next, we differentiate &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid y)\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, that is we set&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial l(\theta \mid y)}{\partial \sigma^2}= -\frac{n}{2\sigma^2}+\frac{1}{2(\sigma^2)^2}\sum_{i=1}^n (y_i-\mu)^2=0.
\]&lt;/p&gt;
&lt;p&gt;We then simplify and move things around to get&lt;/p&gt;
&lt;p&gt;\[
\frac{1}{\sigma^3}\sum_{i=1}^n(y_i-\mu)^2=\frac{n}{\sigma} \;\;\; \rightarrow \;\;\; \sigma^2=\frac{1}{n}\sum_{i=1}^n(y_i-\mu)^2.
\]&lt;/p&gt;
&lt;p&gt;Finally, we replace &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; in the expression above with the value &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=\bar{y}\)&lt;/span&gt; found before and obtain the solution&lt;/p&gt;
&lt;p&gt;\[
\hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^n(y_i-\bar{y})^2=s^2,
\]&lt;/p&gt;
&lt;p&gt;which, however, is a biased estimator of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and therefore is often replaced with the unbiased estimator &lt;span class=&#34;math inline&#34;&gt;\(\frac{s^2}{(n-1)}\)&lt;/span&gt;. In particular, given a population parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is said to be unbiased when &lt;span class=&#34;math inline&#34;&gt;\(E[\hat{\theta}]=\theta\)&lt;/span&gt;. This is the case, for example, of the sample mean &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=\bar{y}\)&lt;/span&gt; which is an unbiased estimator of the population mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;\[
E\left[\hat{\mu} \right]=E\left[\frac{1}{n}\sum_{i=1}^n y_i \right]=\frac{1}{n}\sum_{i=1}^n E\left[y_i \right]=\frac{1}{n} (n\mu)=\mu.
\]&lt;/p&gt;
&lt;p&gt;However, this is not true for the sample variance &lt;span class=&#34;math inline&#34;&gt;\(s^2\)&lt;/span&gt;. This can be seen by first rewriting the expression of the estimator as&lt;/p&gt;
&lt;p&gt;\[
\hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^n (y_i^2 -2y_i\bar{y}+\bar{y}^2)=\frac{1}{n}\sum_{i=1}^n y_i^2 -2\bar{y}\sum_{i=1}^n y_i + \frac{1}{n}n\bar{y}^2=\frac{1}{n}\sum_{i=1}^n y_i^2 - \bar{y}^2,
\]&lt;/p&gt;
&lt;p&gt;and then by computing the expectation of this quantity:&lt;/p&gt;
&lt;p&gt;\[
E\left[\hat{\sigma}^2 \right]=E\left[\frac{1}{n}\sum_{i=1}^n y_i^2 - \bar{y}^2 \right]=\frac{1}{n}\sum_{i=1}^n E\left[y_i^2 \right] - E\left[\bar{y}^2 \right]=\frac{1}{n}\sum_{i=1}^n (\sigma^2 + \mu^2) - (\frac{\sigma^2}{n}+\mu^2)=\frac{1}{n}\left(n\sigma^2+n\mu^2\right) - \frac{\sigma^2}{n}-\mu^2=\frac{(n-1)\sigma^2}{n}.
\]&lt;/p&gt;
&lt;p&gt;The above result is obtained by pluggin in the expression for the variance of a general variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and retrieving the expression for &lt;span class=&#34;math inline&#34;&gt;\(E[y^2]\)&lt;/span&gt; as a function of the variance and &lt;span class=&#34;math inline&#34;&gt;\(E[y]^2\)&lt;/span&gt;. More specifically, given that&lt;/p&gt;
&lt;p&gt;\[
Var(y)=\sigma^2=E\left[y^2 \right]-E\left[y \right]^2,
\]&lt;/p&gt;
&lt;p&gt;then we know that for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E\left[y^2 \right]=\sigma^2+E[y]^2\)&lt;/span&gt;, and similarly we can derive the same expression for &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt;. However, we can see that &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt; is biased by a factor of &lt;span class=&#34;math inline&#34;&gt;\((n-1)/n\)&lt;/span&gt;. Thus, an unbiased estimator for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is given by multiplying &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\frac{n}{(n-1)}\)&lt;/span&gt;, which gives the unbiased estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^{2\star}=\frac{s^2}{n-1}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(E\left[\hat{\sigma}^{2\star}\right]=\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-normal-example-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multivariate Normal Example&lt;/h3&gt;
&lt;p&gt;The same procedure can be applied to an independent and identically distributed multivariate sample &lt;span class=&#34;math inline&#34;&gt;\(y=(y_{ij})\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; units and &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots,J\)&lt;/span&gt; variables (&lt;span class=&#34;citation&#34;&gt;Anderson (1962)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Rao et al. (1973)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Gelman et al. (2013)&lt;/span&gt;). It can be shown that, maximising the loglikelihood with respect to &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; yields the MLEs&lt;/p&gt;
&lt;p&gt;\[
\hat{\mu}=\bar{y} \;\;\; \text{and} \;\;\; \Sigma=\frac{(n-1)\hat{\sigma}^{2\star}}{n},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}=(\bar{y}_1,\ldots,\bar{y}_{J})\)&lt;/span&gt; is the row vectors of sample means and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^{2\star}=(s^{\star_{jk}})\)&lt;/span&gt; is the sample covariance matrix with &lt;span class=&#34;math inline&#34;&gt;\(jk\)&lt;/span&gt;-th element &lt;span class=&#34;math inline&#34;&gt;\(s^\star_{jk}=\frac{\Sigma_{i=1}^n(y_{ij} - \bar{y}_j)}{(n-1)}\)&lt;/span&gt;. In addition, in general, given a function &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt; of the parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, if &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is a MLE of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(g(\hat{\theta})\)&lt;/span&gt; is a MLE of &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-distribution-of-a-bivariate-normal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional Distribution of a Bivariate Normal&lt;/h2&gt;
&lt;p&gt;Consider an indpendent and identically distributed sample formed by two variables &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,y_2)\)&lt;/span&gt;, each measured on &lt;span class=&#34;math inline&#34;&gt;\(i=1\ldots,n\)&lt;/span&gt; units, which come from a Bivariate Normal distribution with mean vector and covariance matrix&lt;/p&gt;
&lt;p&gt;\[
\mu=(\mu_1,\mu_2) \;\;\; \text{and} \;\;\; \Sigma = \begin{pmatrix} \sigma^2_1 &amp;amp; \rho\sigma_1\sigma_2 \\ \rho\sigma_2\sigma_1 &amp;amp; \sigma_2^2 \ \end{pmatrix},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is a correlation parameter between the two variables. Thus, intuitive MLEs for these parameters are&lt;/p&gt;
&lt;p&gt;\[
\hat{\mu}_j=\bar{y}_j \;\;\; \text{and} \;\;\; \hat{\sigma}_{jk}=\frac{(n-1)s_{jk}}{n},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_j=\sigma_{jj}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\rho\sigma_{j}\sigma_{k}=\sigma_{jk}\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(j,k=1,2\)&lt;/span&gt;. By properites of the Bivariate Normal distribution (&lt;span class=&#34;citation&#34;&gt;Ord and Stuart (1994)&lt;/span&gt;), the marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; and the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_2 \mid y_1\)&lt;/span&gt; are&lt;/p&gt;
&lt;p&gt;\[
y_1 \sim \text{Normal}\left(\mu_1,\sigma^2_1 \right) \;\;\; \text{and} \;\;\; y_2 \mid y_1 \sim \text{Normal}\left(\mu_2 + \beta(y_1-\mu_1 \right), \sigma^2_2 - \sigma^2_1\beta^2),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta=\rho\frac{\sigma_2}{\sigma_1}\)&lt;/span&gt; is the parameter that quantifies the linear dependence between the two variables. The MLEs of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_2\)&lt;/span&gt; can also be derived from the likelihood based on the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_2 \mid y_1\)&lt;/span&gt;, which have strong connections with the least squares estimates derived in a multiple linear regression framework.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-linear-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Linear Regression&lt;/h2&gt;
&lt;p&gt;Suppose the data consist in &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units measured on an outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and a set of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; covariates &lt;span class=&#34;math inline&#34;&gt;\(x=(x_{1},\ldots,x_{J})\)&lt;/span&gt; and assume that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is Normal with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_i=\beta_0+\sum_{j=1}^J\beta_jx_{ij}\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. The loglikelihood of &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\beta,\sigma^2)\)&lt;/span&gt; given the observed data &lt;span class=&#34;math inline&#34;&gt;\((y,x)\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;\[
l(\theta \mid y) = -\frac{n}{2}\text{ln}(2\pi) -\frac{n}{2}\text{ln}(\sigma^2) - \frac{\sum_{i=1}^n \left(y_i - \mu_i \right)^2}{2\sigma^2}.
\]&lt;/p&gt;
&lt;p&gt;Maximising this expression with respect to &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the MLEs are found to be equal to the least squares estimates of the intercept and regression coefficients. Using a matrix notation for the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-th vector of the outcome values &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and the &lt;span class=&#34;math inline&#34;&gt;\(n\times (J+1)\)&lt;/span&gt; matrix of the covariate values (including the constant term), then the MLEs are:&lt;/p&gt;
&lt;p&gt;\[
\hat{\beta}=(X^{T}X)^{-1}X^{T}Y \;\;\; \text{and} \;\;\; \hat{\sigma}^{2}=\frac{(Y-X\hat{\beta})(Y-X\hat{\beta})}{n},
\]&lt;/p&gt;
&lt;p&gt;where the numerator of the fraction is known as the &lt;em&gt;Residual Sum of Squares&lt;/em&gt;(RSS). Because the denominator of is equal to &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, the MLE of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; does not correct for the loss of degrees of freedom when estimating the &lt;span class=&#34;math inline&#34;&gt;\(J+1\)&lt;/span&gt; location parameters. Thus, the MLE should instead divide the RSS by &lt;span class=&#34;math inline&#34;&gt;\(n-(J+1)\)&lt;/span&gt; to obtain an unbiased estimator. An extension of standard multiple linear regression is the so called &lt;em&gt;weighted&lt;/em&gt; multiple linear regression, in which the regression variance is assumed to be equal to&lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma^2}{w_i}\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\((w_i) &amp;gt; 0\)&lt;/span&gt;. Thus, the variable &lt;span class=&#34;math inline&#34;&gt;\((y_i-\mu)\sqrt{w_i}\)&lt;/span&gt; is Normally distributed with mean &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, and the loglikelihood is&lt;/p&gt;
&lt;p&gt;\[
l(\theta \mid y)= - \frac{n}{2}\text{ln}(2\pi) - \frac{n}{2}\text{ln}(\sigma^2) - \frac{\sum_{i=1}^n w_i(y_i - \mu_i)^2}{2\sigma^2}.
\]&lt;/p&gt;
&lt;p&gt;Maximising this function yields MLEs given by the weighted least squares estimates&lt;/p&gt;
&lt;p&gt;\[
\hat{\beta}=\left(X^{T}WX\right)^{-1}\left(X^{T}WY \right) \;\;\; \text{and} \;\;\; \sigma^{2}=\frac{\left(Y-X\hat{\beta}\right)^{T}W\left(Y-X\hat{\beta}\right)}{n},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(W=\text{Diag}(w_1,\ldots,w_n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generalised-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generalised Linear Models&lt;/h2&gt;
&lt;p&gt;Consider the previous example where we had an outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and a set of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; covariates, each measured on &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units. A more general class of models, compare with the Normal model, assumes that, given &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, the values of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are an independent sample from a regular exponential family distribution&lt;/p&gt;
&lt;p&gt;\[
f(y \mid x,\beta,\phi)=\text{exp}\left(\frac{\left(y\delta\left(x,\beta \right) - b\left(\delta\left(x,\beta\right)\right)\right)}{\phi} + c\left(y,\phi\right)\right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\delta()\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b()\)&lt;/span&gt; are known functions that determine the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(c()\)&lt;/span&gt; is a known function indexed by a scale parameter &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. The mean of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is assumed to linearly relate to the covariates via&lt;/p&gt;
&lt;p&gt;\[
E\left[y \mid x,\beta,\phi \right]=g^{-1}\left(\beta_0 + \sum_{j=1}^J\beta_jx_{j} \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(E\left[y \mid x,\beta,\phi \right]=\mu_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g()\)&lt;/span&gt; is a known one to one function which is called &lt;em&gt;link function&lt;/em&gt; because it “links” the expectation of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to a linear combination of the covariates. The canonical link function&lt;/p&gt;
&lt;p&gt;\[
g_c(\mu_i)=\delta(x_{i},\beta)=\beta_0+\sum_{j=1}^J\beta_jx_{ij},
\]&lt;/p&gt;
&lt;p&gt;which is obtained by setting &lt;span class=&#34;math inline&#34;&gt;\(g()\)&lt;/span&gt; equal to the inverse of the derivative of &lt;span class=&#34;math inline&#34;&gt;\(b()\)&lt;/span&gt; with respect to its argument. Examples of canonical links include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Normal linear regression: &lt;span class=&#34;math inline&#34;&gt;\(g_c=\text{identity}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b(\delta)=\frac{\delta^2}{2},\phi=\sigma^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Poisson regression: &lt;span class=&#34;math inline&#34;&gt;\(g_c=\log\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b(\delta)=\text{exp}(\delta),\phi=1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Logistic regression: &lt;span class=&#34;math inline&#34;&gt;\(g_c=\text{logit}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b(\delta)=\log(1+\text{exp}(\delta)),\phi=1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The loglikelihood of &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\beta,\phi)\)&lt;/span&gt; given the observed data &lt;span class=&#34;math inline&#34;&gt;\((y,x)\)&lt;/span&gt;, is&lt;/p&gt;
&lt;p&gt;\[
l(\theta \mid y,x)=\sum_{i=1}^n \left[\frac{\left(y_i\delta\left(x_i,\beta\right)-b\left(\delta\left(x_i,\beta\right)\right) \right)}{\phi}+c\left(y_i,\phi\right)\right],
\]&lt;/p&gt;
&lt;p&gt;which for non-normal cases does not have explicit maxima and numerical maximisation can be achieved using iterative algorithms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-anderson1962introduction&#34;&gt;
&lt;p&gt;Anderson, Theodore Wilbur. 1962. “An Introduction to Multivariate Statistical Analysis.” Wiley New York.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34;&gt;
&lt;p&gt;Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. &lt;em&gt;Bayesian Data Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ord1994kendall&#34;&gt;
&lt;p&gt;Ord, Keith, and Alan Stuart. 1994. “Kendall’s Advanced Theory of Statistics: Distribution Theory.” Edward Arnold.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rao1973linear&#34;&gt;
&lt;p&gt;Rao, Calyampudi Radhakrishna, Calyampudi Radhakrishna Rao, Mathematischer Statistiker, Calyampudi Radhakrishna Rao, and Calyampudi Radhakrishna Rao. 1973. &lt;em&gt;Linear Statistical Inference and Its Applications&lt;/em&gt;. Vol. 2. Wiley New York.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Likelihood Based Inference with Incomplete Data</title>
      <link>/missmethods/likelihood-based-methods-ignorable3/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/likelihood-based-methods-ignorable3/</guid>
      <description>


&lt;p&gt;As for the inference under complete data, inference under incomplete data consists in deriving the likelihood for the parameters based on the available data, either using a &lt;em&gt;Maximum Likelihood&lt;/em&gt; (ML) approach (solving the likelihood equation) or using the &lt;em&gt;Bayes’ rule&lt;/em&gt; incorporating a prior distribution (performing necessary integrations to obtain the posterior distribution). However, asymptotic standard errors obtained from the information matrix, are more questionable when dealing with missing data since the sample will not be typically iid and results that imply the large sample normality of the likelihood function do not immediately apply. More complications arise when dealing with the process that lead to some of the data to be missing. This can be explained with a simple example.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Y=(y_{ij})\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots,J\)&lt;/span&gt;, denote the complete dataset if there were no missing values, with a total of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units and &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; variables. Let &lt;span class=&#34;math inline&#34;&gt;\(M=(m_{ij})\)&lt;/span&gt; denote the fully observed matrix of binary missing data indicators with &lt;span class=&#34;math inline&#34;&gt;\(m_{ij}=1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt; is missing and &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; otherwise. As an example, we can model the density of the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; using the &lt;em&gt;selection model factorisation&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;\[
p(Y=y,M=m \mid \theta, \psi) = f(y \mid \theta)f(m \mid y, \psi),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the parameter vector indexing the response model and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; is the parameter vector indexing the missingness mechanism. The observed values &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; effect a partition &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,y_0)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(y_0=[y_{ij} : m_{ij}=0]\)&lt;/span&gt; is the observed component and &lt;span class=&#34;math inline&#34;&gt;\(y_1=[y_{ij} : m_{ij}=1]\)&lt;/span&gt; is the missing component of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. The full likelihood based on the observed data and the assumed model is&lt;/p&gt;
&lt;p&gt;\[
L_{full}(\theta, \psi \mid y_{0},m) = \int f\left(y_{0},y_{1} \mid \theta \right) f\left(m \mid y_{0},y_{1}, \psi \right)dy_{1}
\]&lt;/p&gt;
&lt;p&gt;and is a function of the parameters &lt;span class=&#34;math inline&#34;&gt;\((\theta,\psi)\)&lt;/span&gt;. Next, we define the likelihood of ignoring the missingness mechanism or &lt;em&gt;ignorable likelihood&lt;/em&gt; as&lt;/p&gt;
&lt;p&gt;\[
L_{ign}\left(\theta \mid y_{0} \right) = \int f(y_{0},y_{1}\mid \theta)dy_{1},
\]&lt;/p&gt;
&lt;p&gt;which does not involve the model for &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;. In practice, modelling the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; is often challenging and, in fact, many approaches to missing data do not model &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and (explicitly or implicitly) base inference about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; on the ignorable likelihood. It is therefore important to assess under which conditions inferences about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; based on &lt;span class=&#34;math inline&#34;&gt;\(L_{ign}\)&lt;/span&gt; can be considered appropriate. More specifically, the missingness mechanism is said to be &lt;em&gt;ignorable&lt;/em&gt; if inferences about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; based on the ignorable likelihood equation evauluated at some realisations of &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; are the same as inferences about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; based on the full likelihood equation, evaluated at the same realisations of &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;. The conditions for ignoring the missingness mechanism depend on whether the inferences are direct likelihood, Bayesian or frequentist.&lt;/p&gt;
&lt;div id=&#34;direct-likelihood-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Direct Likelihood Inference&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Direct Likelihood Inference&lt;/em&gt; refers to inference based solely on likelihood ratios for pair of values of the parameters, with the data fixed at their observed values. The missingness mechanism can be ignored for direct likelihood if the likelihood ratio based on the ignorable likelihood is the same as the ratio based on the full likelihood. More precisely, the missingness mechanism is said to be ignorable for direct likelihood inference at some realisations of &lt;span class=&#34;math inline&#34;&gt;\((y_0,m)\)&lt;/span&gt; if the likelihood ratio for two values &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta^\star\)&lt;/span&gt; is the same whether based on the full or ignorable likelihood. That is&lt;/p&gt;
&lt;p&gt;\[
\frac{L_{full}\left( \theta, \psi \mid y_{0}, m \right)}{L_{full}\left( \theta^{\star}, \psi \mid y_{0}, m \right)}=\frac{L_{ign}\left( \theta \mid y_{0} \right)}{L_{ign}\left( \theta^{\star} \mid y_{0}\right)},
\]&lt;/p&gt;
&lt;p&gt;for all &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta^\star\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;. In general, the missingnes mechanism is ignorable for direct likelihood inference if the following two conditions hold:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Parameter distinctness. The parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; are distinct, in the sense that the joint parameter space &lt;span class=&#34;math inline&#34;&gt;\(\Omega_{\theta,\psi}\)&lt;/span&gt; is the product of the two parameter spaces &lt;span class=&#34;math inline&#34;&gt;\(\Omega_{\theta}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Omega_{\psi}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Factorisation of the full likelihood. The full likelihood factors as&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
L_{full}\left(\theta, \psi \mid y_{0},m \right) = L_{ign}\left(\theta \mid y_{0} \right) L_{rest}\left(\psi \mid y_{0},m \right)
\]&lt;/p&gt;
&lt;p&gt;for all values of &lt;span class=&#34;math inline&#34;&gt;\(\theta,\psi \in \Omega_{\theta,\psi}\)&lt;/span&gt;. The distinctness condition ensures that each value of &lt;span class=&#34;math inline&#34;&gt;\(\psi \in \Omega_{\psi}\)&lt;/span&gt; is compatible with different values of &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Omega_{\theta}\)&lt;/span&gt;. A sufficient condition for the factorisation of the full likelihood is that the missing data are &lt;em&gt;Missing At Random&lt;/em&gt;(MAR) at the specific realisations of &lt;span class=&#34;math inline&#34;&gt;\(y_{0},m\)&lt;/span&gt;. This means that the distribution function of &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;, evaluated at the given realisations &lt;span class=&#34;math inline&#34;&gt;\((y_{0},m)\)&lt;/span&gt;, does not depend on the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
f\left(m \mid y_{0}, y_{1}, \psi \right)=f\left(m \mid y_{0}, y^{\star}_{1} \psi \right),
\]&lt;/p&gt;
&lt;p&gt;for all &lt;span class=&#34;math inline&#34;&gt;\(y_{1},y^\star_{1},\psi\)&lt;/span&gt;. Thus, we have&lt;/p&gt;
&lt;p&gt;\[
f\left(y_{0}, m \mid \theta, \psi \right) = f\left(m \mid y_{0}, \psi \right) \int f\left(y_{0},y_{1} \mid \theta \right)dy_{1} = f\left(m \mid y_{0}, \psi \right) f\left( y_{0} \mid \theta \right).
\]&lt;/p&gt;
&lt;p&gt;From this it follows that, if the missing data are MAR at the given realisations of &lt;span class=&#34;math inline&#34;&gt;\((y_{0},m)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; are distinct, the missingnes mechanism is ignorable for likelihood inference.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian Inference&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Bayesian Inference&lt;/em&gt; under the full model for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; requires that the full likelihood is combined with a prior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\theta,\psi)\)&lt;/span&gt; for the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p\left(\theta, \psi \mid y_{0}, m \right) \propto p(\theta, \psi) L_{full}\left(\theta, \psi \mid y_{0}, m \right).
\]&lt;/p&gt;
&lt;p&gt;Bayesian inference ignoring the missingness mechanism combines the ignorable likelihood with a prior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; alone, that is&lt;/p&gt;
&lt;p&gt;\[
p(\theta \mid y_{0}) \propto p(\theta) L_{ign}\left(\theta \mid y_{0} \right).
\]&lt;/p&gt;
&lt;p&gt;More formally, the missingness mechanism is said to be ignorable for Bayesian inference at the given realisations of &lt;span class=&#34;math inline&#34;&gt;\((y_{0},m)\)&lt;/span&gt; if the posterior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; based on the posterior distribution for the full likelihood and prior distribution for &lt;span class=&#34;math inline&#34;&gt;\((\theta,\psi)\)&lt;/span&gt; is the same as the posterior distribution for the ignorable likelihood and the prior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; alone. This holds when the following conditions are satisfied:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; are &lt;em&gt;a priori&lt;/em&gt; independent, that is the prior distribution has the form&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
p(\theta , \psi) = p(\theta) p(\psi)
\]&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The full likelihood evaluated at the realisations of &lt;span class=&#34;math inline&#34;&gt;\((y_{0},m)\)&lt;/span&gt; factors as for direct likelihood inference&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Under these conditions:&lt;/p&gt;
&lt;p&gt;\[
p(\theta, \psi \mid y_{0}, m) \propto \left(p(\theta)L_{ign}\left( \theta \mid y_{0} \right) \right) \left(p(\psi)L_{rest}\left(\psi \mid y_{0},m \right) \right).
\]&lt;/p&gt;
&lt;p&gt;As for direct likelihood inference, MAR is a sufficient condition for the factorisation of the full likelihood. This means that, if the data are MAR at the given realisations of &lt;span class=&#34;math inline&#34;&gt;\((y_{0},m)\)&lt;/span&gt; and the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; are a prior independent, then the missingness mechanism is ignorable for Bayesian inference. We note that the a priori condition is more stringent than the distinctness condition because paramerers with distinct parameter spaces might have dependent prior distributions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;frequentist-asymptotic-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Frequentist Asymptotic Inference&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Frequentist Asymptotic Inference&lt;/em&gt; requires that, in order to ignore the missingness mechanism, the factorisation of the full likelihood needs to be valid for values of the observed data under repeated sampling. This means that we require&lt;/p&gt;
&lt;p&gt;\[
L_{full}\left(\theta,\psi \mid y_{0}, m \right) = L_{ign}\left(\theta \mid y_{0} \right) L_{rest}\left(\psi \mid y_{0}, m \right)
\]&lt;/p&gt;
&lt;p&gt;for all &lt;span class=&#34;math inline&#34;&gt;\(y_{0},m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta,\psi \in \Omega_{\theta,\psi}\)&lt;/span&gt;. For this form of inference, a sufficient condition for ignoring the missingness mechanism is given by the following conditions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Parameter distinctness as defined for direct likelihood inference.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Missing data are &lt;em&gt;Missing Always At Random&lt;/em&gt; (MAAR), that is&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
f\left(m \mid y_{0},y_{1},\psi \right) = f\left(m \mid y_{0}, y^{\star}_{1},\psi \right)
\]&lt;/p&gt;
&lt;p&gt;for all &lt;span class=&#34;math inline&#34;&gt;\(m,y_{0},y_{1},y^\star_{1},\psi\)&lt;/span&gt;. In the following example we discuss conditions for ignoring the missingness mechanism for direct likelihood and Bayesian inference, which can be extended to the case of frequentist asymptotic inference by requiring that they hold for for values of &lt;span class=&#34;math inline&#34;&gt;\(y_{0},m\)&lt;/span&gt; other than those observed that could arise in repeated sampling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-normal-sample-with-one-variable-subject-to-missingness&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bivariate Normal Sample with One Variable Subject to Missingness&lt;/h2&gt;
&lt;p&gt;Consider a bivariate normal sample &lt;span class=&#34;math inline&#34;&gt;\(y=(y_{i1},y_{i2})\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; units, but with the values of &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt; being missing for &lt;span class=&#34;math inline&#34;&gt;\(i=(n_{cc}+1),\ldots,n\)&lt;/span&gt;. This leads to a monotone missing data pattern with two variables. The loglikelihood of ignoring the missingness mechanism is&lt;/p&gt;
&lt;p&gt;\[
l_{ign}\left(\mu, \Sigma \mid y_{0} \right) = \log\left(L_{ign}\left(\mu,\Sigma \mid y_{0} \right) \right) = - \frac{1}{2}n_{cc}ln \mid \Sigma \mid - \frac{1}{2}\sum_{i=1}^{n_{cc}}(y_i - \mu ) \Sigma^{-1}(y_i - \mu)^{T} - \frac{1}{2}(n-n_{cc})ln\sigma_{1} - \frac{1}{2}\sum_{i=n_{cc}+1}^{n}\frac{(y_{i1}-\mu_1)^2}{\sigma_{1}}.
\]&lt;/p&gt;
&lt;p&gt;This loglikelihood is appropriate for inference provided the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; does not depend on the values of &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\Sigma)\)&lt;/span&gt; is distinct from &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;. Under these conditions, ML estimates of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; can be found by maximising this loglikelihood. For Bayesian inference, if these conditions hold and the prior distribution for &lt;span class=&#34;math inline&#34;&gt;\((\theta,\psi)\)&lt;/span&gt; has the form &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)p(\psi)\)&lt;/span&gt;, then the joint posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is proportional to the product of &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(L_{ign}(\theta \mid y_{0})\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
