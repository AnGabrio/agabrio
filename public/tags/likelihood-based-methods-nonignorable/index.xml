<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Likelihood Based Methods Nonignorable on Andrea Gabrio</title>
    <link>/tags/likelihood-based-methods-nonignorable/</link>
    <description>Recent content in Likelihood Based Methods Nonignorable on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/likelihood-based-methods-nonignorable/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Likelihood Based Inference with Incomplete Data (Nonignorable)</title>
      <link>/missmethods/likelihood-based-methods-nonignorable/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/likelihood-based-methods-nonignorable/</guid>
      <description>


&lt;p&gt;In many cases, analysis methods for missing data are based on the ignorable likelihood&lt;/p&gt;
&lt;p&gt;\[
L_{ign}\left(\theta \mid Y_0, X \right) \propto f\left(Y_0 \mid X, \theta \right),
\]&lt;/p&gt;
&lt;p&gt;regarded as a function of the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; for fixed observed data &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt; and some fully observed covariates &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The density &lt;span class=&#34;math inline&#34;&gt;\(f(Y_0 \mid X, \theta)\)&lt;/span&gt; is obtained by integrating out the missing data &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; from the joint density &lt;span class=&#34;math inline&#34;&gt;\(f(Y \mid X, \theta)=f(Y_0,Y_1\mid X, \theta)\)&lt;/span&gt;. Sufficient conditions for basing inference about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; on the ignorbale likelihood are that the missingness mechanism is &lt;em&gt;Missing At Random&lt;/em&gt;(MAR) and the parameters of the model of analysis &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and those of the missingness mechanism &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; are distinct. Here we focus our attention on the situations where the missingness mechanism is &lt;em&gt;Missing Not At Random&lt;/em&gt;(MNAR) and valid &lt;em&gt;Maximum Likelihood&lt;/em&gt;(ML), &lt;em&gt;Bayesian&lt;/em&gt; and &lt;em&gt;Multiple Imputation&lt;/em&gt;(MI) inferences generally need to be based on the full likelihood&lt;/p&gt;
&lt;p&gt;\[
L_{full}\left(\theta, \psi \mid Y_0, X, M \right) \propto f\left(Y_0, M \mid X, \theta, \psi \right),
\]&lt;/p&gt;
&lt;p&gt;regarded as a function of &lt;span class=&#34;math inline&#34;&gt;\((\theta,\psi)\)&lt;/span&gt; for fixed &lt;span class=&#34;math inline&#34;&gt;\((Y_0,M)\)&lt;/span&gt;. Here, &lt;span class=&#34;math inline&#34;&gt;\(f(Y_0,M\mid \theta, \psi)\)&lt;/span&gt; is obtained by integrating out &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; from the joint density &lt;span class=&#34;math inline&#34;&gt;\(f(Y,M \mid X, \theta, \psi)\)&lt;/span&gt;. Two main approaches for formulating MNAR models can be distinguished, namely &lt;em&gt;selection models&lt;/em&gt;(SM) and &lt;em&gt;pattern mixture models&lt;/em&gt;(PMM).&lt;/p&gt;
&lt;div id=&#34;selection-and-pattern-mixture-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selection and Pattern Mixture Models&lt;/h2&gt;
&lt;p&gt;SMs factor the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;\[
f(m_i,y_i \mid x_i, \theta, \psi) = f(y_i \mid x_i, \theta)f(m_i \mid x_i,y_i,\psi),
\]&lt;/p&gt;
&lt;p&gt;where the first factor is the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in the population while the second factor is the missingness mechanism, with &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; which are assumed to be distinct. Alternatively, PMMs factor the joint distribution as&lt;/p&gt;
&lt;p&gt;\[
f(m_i,y_i \mid x_i, \theta, \psi) = f(y_i \mid x_i, m_i,\xi)f(m_i \mid x_i),
\]&lt;/p&gt;
&lt;p&gt;where the first factor is the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in the strata defined by different patterns of missingness &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; while the second factor models the probabilities of the different patterns, with &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; which are assumed to be distinct (&lt;span class=&#34;citation&#34;&gt;Little (1993)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;). The distinction between the two factorisations becomes clearer when considering a specific example.&lt;/p&gt;
&lt;p&gt;Suppose thta missing values are confined to a single variable and let &lt;span class=&#34;math inline&#34;&gt;\(y_i=(y_{i,1},y_{i2})\)&lt;/span&gt; be a bivariate response outcome where &lt;span class=&#34;math inline&#34;&gt;\(y_{i1}\)&lt;/span&gt; is fully observed and &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt; is observed for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n_{cc}\)&lt;/span&gt; but missing for &lt;span class=&#34;math inline&#34;&gt;\(i=n_{cc}+1,\ldots,n\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(m_{i2}\)&lt;/span&gt; be the missingness indicator for &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt;, then a PMM factors the denisty of &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;\[
f(y_0, M \mid X, \xi)=\prod_{i=1}^{n_{cc}}f(y_{i1},y_{i2}\mid x_i, m_{i2}=0,\xi)Pr(m_{i2}=0 \mid x_i, \omega) \times \prod_{i=n_{cc}+1}^{n}f(y_{i1} \mid x_i, m_{i2}=1,\xi)Pr(m_{i2}=1 \mid x_i, \omega).
\]&lt;/p&gt;
&lt;p&gt;This expression shows that there are no data with which to estimate directly the distribution &lt;span class=&#34;math inline&#34;&gt;\(f(y_{i2} \mid x_i, m_{i2}=1,\xi)\)&lt;/span&gt;, because all units with &lt;span class=&#34;math inline&#34;&gt;\(m_{i2}=1\)&lt;/span&gt; have &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt; missing. Under MAR, this is identified using the distribution of the observed data &lt;span class=&#34;math inline&#34;&gt;\(f(y_{i2} \mid x_i, m_{i2}=1,\xi)=f(y_{i2} \mid x_i, m_{i2}=0,\xi)\)&lt;/span&gt;, while under MNAR it must be identified using other assumptions. The SM formulation is&lt;/p&gt;
&lt;p&gt;\[
f(y_i, m_{i2} \mid \theta, \psi) = f(y_{i1} \mid x_i, \theta)f(y_{i2} \mid x_i, y_{i1},\theta)f(m_{i2}\mid x_i,y_{i1},y_{i2},\psi).
\]&lt;/p&gt;
&lt;p&gt;Typically, the missingness mechanism &lt;span class=&#34;math inline&#34;&gt;\(f(m_{i2} \mid x_i,y_{i1},y_{i2},\psi)\)&lt;/span&gt; is modelled using some additive probit or logit regression of &lt;span class=&#34;math inline&#34;&gt;\(m_{i2}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;,&lt;span class=&#34;math inline&#34;&gt;\(y_{i1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt;. However, the coefficient of &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt; in this regression is not directly estimable from the data and hence the model cannot be fully estimated without extra assumptions.&lt;/p&gt;
&lt;div id=&#34;normal-models-for-mnar-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Normal Models for MNAR data&lt;/h3&gt;
&lt;p&gt;Assume we have a complete sample &lt;span class=&#34;math inline&#34;&gt;\((y_i,x_i)\)&lt;/span&gt; on a continuous variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and a set of fully observed covariates &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt;. Suppose that &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n_{cc}\)&lt;/span&gt; units are observed while the remaining &lt;span class=&#34;math inline&#34;&gt;\(i=n_{cc}+1,\ldots,n\)&lt;/span&gt; units are missing, with &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; being the corresponding missingness indicator. Heckman (&lt;span class=&#34;citation&#34;&gt;Heckman (1976)&lt;/span&gt;) proposed the following selection model to handle missingness:&lt;/p&gt;
&lt;p&gt;\[
y_i \mid x_i, \theta, \psi \sim N(\beta_0 + \beta_1x_i, \sigma^2) \;\;\; \text{and} \;\;\; m_i \mid x_i,y_i,\theta,\psi \sim Bern\left(\Phi(\psi_0 + \psi_1x_i + \psi_2y_i) \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\beta_0,\beta_1,\sigma^2)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; denotes the probit (cumulative normal) distribution function. Note that if &lt;span class=&#34;math inline&#34;&gt;\(\psi_2=0\)&lt;/span&gt;, the missing data are MAR, while if &lt;span class=&#34;math inline&#34;&gt;\(\psi_2 \neq 0\)&lt;/span&gt; the missing data are MNAR since missingness in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; depends on the unobserved value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. This model can be estimated using either a two-step least squares method, ML in combination with an EM algorithm, or a Bayesian approach. The main issue is the lack of information about &lt;span class=&#34;math inline&#34;&gt;\(\psi_2\)&lt;/span&gt;, which can be partly identified through the specific assumptions about the distribution of the observed data of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. This, however, makes the implicit assumption that the assumed distribution can well described the distribution of the complete (observed and missing) data which can never be tested or checked. An alternative approach is to use a PMM factorisation and model:&lt;/p&gt;
&lt;p&gt;\[
y_i \mid m_i=m,x_i,\xi,\omega \sim N(\beta_0^m + \beta_1^mx_i, \sigma^{2m})\;\;\; \text{and} \;\;\; m_i \mid x_i,\xi,\omega \sim Bern\left(\Phi(\omega_0 + \omega_1x_i) \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\xi=(\beta_0^m,\beta_1^m,\sigma^{2m},\;\;\; m=0,1)\)&lt;/span&gt;. This model implies that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in the population is a mixture of two normal distributions with mean&lt;/p&gt;
&lt;p&gt;\[
\left[1 - \Phi(\omega_0 + \omega_1x_i) \right] \left[\beta_0^0 + \beta_1^0 x_i \right] + \left[\Phi(\omega_0 + \omega_1x_i) \right] \left[\beta_0^1 + \beta_1^1 x_i \right].
\]&lt;/p&gt;
&lt;p&gt;The parameters &lt;span class=&#34;math inline&#34;&gt;\((\beta_0^0,\beta_1^0,\sigma^{20},\omega)\)&lt;/span&gt; can be estimated from the data but the parameters &lt;span class=&#34;math inline&#34;&gt;\((\beta_0^1,\beta_1^1,\sigma^{21})\)&lt;/span&gt; are not estimable because &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is missing when &lt;span class=&#34;math inline&#34;&gt;\(m_i=1\)&lt;/span&gt;. Under MAR, the distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the same for units with &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; observed and missing, such that &lt;span class=&#34;math inline&#34;&gt;\(\beta_0^0=\beta_0^1=\beta_0\)&lt;/span&gt; (as well as for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;). Under MNAR, other assumptions are needed to esitmate the parameters indexed by &lt;span class=&#34;math inline&#34;&gt;\(m=1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Some final considerations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Both SM and PMM model the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The SM formulation is more natural when the substantive interest concerns the relationship between &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; in the population. However, these parameters can also be derived in PMM by averaging the patterns specific parameters over the missingness patterns.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The PMM factorisation is more transparent in terms of the underlying assumptions about the unidentified parameters of the model, while SM tends to impose some obscure constraints in order to identify these parameters, which are also difficult to interpret.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Given specific assumptions to identify all the parameters in the model, PMMs are often easier to fit than SMs. In addition, imputations of the missing values are based on the predictive distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M=0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These considerations seem to favour PMM over SM as MNAR approaches, especially when considering &lt;em&gt;sensitivity analysis&lt;/em&gt;. Bayesian approaches can also be used to identify these models, by assigning prior distributions which can be used to identify those parameters which cannot be estimated from the data. Justifications for the choice of these priors are therefore necessary to ensure the plausibility of the assumptions assessed and the impact of these assumptions on the posterior inference.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-heckman1976common&#34;&gt;
&lt;p&gt;Heckman, James J. 1976. “The Common Structure of Statistical Models of Truncation, Sample Selection and Limited Dependent Variables and a Simple Estimator for Such Models.” In &lt;em&gt;Annals of Economic and Social Measurement, Volume 5, Number 4&lt;/em&gt;, 475–92. NBER.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little1993pattern&#34;&gt;
&lt;p&gt;Little, Roderick JA. 1993. “Pattern-Mixture Models for Multivariate Incomplete Data.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 88 (421): 125–34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pattern Mixture Models</title>
      <link>/missmethods/pattern-mixture-models/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/pattern-mixture-models/</guid>
      <description>


&lt;p&gt;It is possible to summarise the steps involved in drawing inference from incomplete data as (&lt;span class=&#34;citation&#34;&gt;Daniels and Hogan (2008)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Specification of a full data model for the response and missingness indicators &lt;span class=&#34;math inline&#34;&gt;\(f(y,r)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Specification of the prior distribution (within a Bayesian approach)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sampling from the posterior distribution of full data parameters, given the observed data &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt; and the missingness indicators &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Identification of a full data model, particularly the part involving the missing data &lt;span class=&#34;math inline&#34;&gt;\(Y_{mis}\)&lt;/span&gt;, requires making unverifiable assumptions about the full data model &lt;span class=&#34;math inline&#34;&gt;\(f(y,r)\)&lt;/span&gt;. Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{mis}\)&lt;/span&gt;, given &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt;. Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Unverifiable parametric assumptions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Informative prior distributions (under a Bayesian approach)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We show some simple examples about how these &lt;em&gt;nonignorable&lt;/em&gt; models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as &lt;em&gt;Pattern Mixture Models&lt;/em&gt;(PMM).&lt;/p&gt;
&lt;div id=&#34;pattern-mixture-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pattern Mixture Models&lt;/h2&gt;
&lt;p&gt;The pattern mixture model approach factors the full data distribution as&lt;/p&gt;
&lt;p&gt;\[
f(y,r \mid \omega) = f(y \mid r, \phi) f(r \mid y,\chi),
\]&lt;/p&gt;
&lt;p&gt;where it is typically assumed that the set of full data parameters &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; can be decomposed as separate parameters for each factor &lt;span class=&#34;math inline&#34;&gt;\((\phi,\chi)\)&lt;/span&gt;. Thus, under the PMM approach, the &lt;em&gt;response model&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(y \mid \theta)\)&lt;/span&gt; can be retrieved as a mixture of the pattern specific distributions&lt;/p&gt;
&lt;p&gt;\[
f(y \mid \theta) = \sum_{r}f(y \mid r, \phi)f(r \mid \chi),
\]&lt;/p&gt;
&lt;p&gt;with weights given by the corresponding probabilities of the different patterns. The &lt;em&gt;missingness mechanism&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(r \mid y, \psi)\)&lt;/span&gt; can also be obtained using Bayes’ rule&lt;/p&gt;
&lt;p&gt;\[
f(y \mid r, \psi) = \frac{f(y \mid r, \phi)f(r\mid \chi)}{f(y \mid \theta)}.
\]&lt;/p&gt;
&lt;p&gt;The construction of PMMs requires the specification of the full data distribution conditional on different missingness patterns, which may be cumbersome when the number of patterns is large, but with the advantage of making explicit the parameters that cannot be identified by the observed data. In particular, PMMs are well suited to show that the distribution of the response within each pattern can be decomposed as&lt;/p&gt;
&lt;p&gt;\[
f(y_{obs},y_{mis} \mid r, \phi)= f(y_{mis} \mid y_{obs},r,\phi_{E})f(y_{obs}\mid r,\phi_{O}),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\phi_E = \lambda_1(\phi)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\phi_O=\lambda_2(\phi)\)&lt;/span&gt; are functions of the mixture component parameter &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. The former subset of parameters indexes the so called &lt;em&gt;extrapolation distribution&lt;/em&gt; and cannot be identified from the data, i.e. the distribution of the missing values given the observed values, while the latter indexes the &lt;em&gt;observed data distribution&lt;/em&gt; and is typically identifiable from the data. Assuming there exists a partition such that &lt;span class=&#34;math inline&#34;&gt;\(\phi_E=(\phi_{EI},\phi_{ENI})\)&lt;/span&gt; and the observed data distribution is a function of &lt;span class=&#34;math inline&#34;&gt;\(\phi_{EI}\)&lt;/span&gt; but not of &lt;span class=&#34;math inline&#34;&gt;\(\phi_{ENI}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\phi_{ENI}\)&lt;/span&gt; is a &lt;em&gt;senstivity parameter&lt;/em&gt; in that it can only be identified using information from sources other than the observed data and thus makes a suitable basis to formulate sensitivity analysis using informative priors.&lt;/p&gt;
&lt;div id=&#34;example-of-pmm-for-bivariate-normal-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example of PMM for bivariate normal data&lt;/h3&gt;
&lt;p&gt;Consider a sample of &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; units from a bivariate normal distribution &lt;span class=&#34;math inline&#34;&gt;\(Y=(Y_1,Y_2)\)&lt;/span&gt;. Assume also that &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; is always observed while &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt; may be missing, and let &lt;span class=&#34;math inline&#34;&gt;\(R=R_2\)&lt;/span&gt; be the missingness indicator for the partially-observed response &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt;. A PMM factors the full data distribution as&lt;/p&gt;
&lt;p&gt;\[
f(y_1,y_2,r \mid \omega) = f(y_1, y_2 \mid r, \phi)f(r \mid ,\chi),
\]&lt;/p&gt;
&lt;p&gt;where, for example, we may have &lt;span class=&#34;math inline&#34;&gt;\(Y \mid R=1 \sim N(\mu^1,\Sigma^1)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y \mid R=0 \sim N(\mu^0,\Sigma^0)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R \sim Bern(\chi)\)&lt;/span&gt;. We define &lt;span class=&#34;math inline&#34;&gt;\(\mu^r=(\mu^r_1)\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^r\)&lt;/span&gt; has elements &lt;span class=&#34;math inline&#34;&gt;\(\sigma^r=(\sigma^r_{11},\sigma^r_{12},\sigma^r_{22})\)&lt;/span&gt;. Similarly, we can define the parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta^r_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta^r_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^r_{2\mid 1}\)&lt;/span&gt; as the intercept, slope and residual variance of the regression of &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; for each pattern &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;. Under this reparameterisation, the full data model parameters are&lt;/p&gt;
&lt;p&gt;\[
\phi=\{\mu^r_1,\sigma^r_{11},\beta^r_0,\beta^1_1,\sigma^r_{2\mid 1}\}.
\]&lt;/p&gt;
&lt;p&gt;The extrapolation and observed data distributions, with associated parameters, are then&lt;/p&gt;
&lt;p&gt;\[
f(y_{mis}\mid y_{obs},\phi_{E}) \rightarrow \phi_{E}=(\beta^0_0, \beta^0_1,\sigma^0_{2\mid1})
\]&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;\[
f(y_{obs}\mid \phi_{O}) \rightarrow \phi_{O}=(\mu^1,\beta^1,\sigma^1_{11},\mu^0_0,\sigma^1_{11}).
\]&lt;/p&gt;
&lt;p&gt;It can be shown that, in this specific example, the observed data distribution does not depend on the parameters indexing the extrapolation distribtuon &lt;span class=&#34;math inline&#34;&gt;\(\phi_{ENI}=(\beta^0_0,\beta^0_1,\sigma^0_{2\mid 1})\)&lt;/span&gt;. It is possible to set &lt;span class=&#34;math inline&#34;&gt;\(\beta^0=\beta=1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^0_{2\mid1}=\sigma^1_{2\mid1}\)&lt;/span&gt; to yield a &lt;em&gt;Missing At Random&lt;/em&gt;(MAR) assumption. Hence, a function that maps identified parameters and sensitivity parameters &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt; to the space of unidentified parameters can be used to quantify departures from MAR. For example, assume we impose&lt;/p&gt;
&lt;p&gt;\[
\beta^0_0=\beta^1_0+\Delta,
\]&lt;/p&gt;
&lt;p&gt;then assigning a point mass prior at &lt;span class=&#34;math inline&#34;&gt;\(\Delta=0\)&lt;/span&gt; implies MAR, while fixing &lt;span class=&#34;math inline&#34;&gt;\(\Delta \neq 0\)&lt;/span&gt; or using any type of inofrmative prior on this parameter implies a &lt;em&gt;Missing Not At Random&lt;/em&gt;(MNAR) assumption.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conlcusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conlcusions&lt;/h2&gt;
&lt;p&gt;To summarise, PMMs have the advantage of being able to find full data parameters indexing the distribution of the missing data that are not identified from the observed data, making inference more transparent. A potential downside is the practical implementation of these models which becomes more difficult as the number of patterns and unidentified parameters grows.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-daniels2008missing&#34;&gt;
&lt;p&gt;Daniels, Michael J, and Joseph W Hogan. 2008. &lt;em&gt;Missing Data in Longitudinal Studies: Strategies for Bayesian Modeling and Sensitivity Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Selection Models</title>
      <link>/missmethods/selection-models/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/selection-models/</guid>
      <description>


&lt;p&gt;It is possible to summarise the steps involved in drawing inference from incomplete data as (&lt;span class=&#34;citation&#34;&gt;Daniels and Hogan (2008)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Specification of a full data model for the response and missingness indicators &lt;span class=&#34;math inline&#34;&gt;\(f(y,r)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Specification of the prior distribution (within a Bayesian approach)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sampling from the posterior distribution of full data parameters, given the observed data &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt; and the missingness indicators &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Identification of a full data model, particularly the part involving the missing data &lt;span class=&#34;math inline&#34;&gt;\(Y_{mis}\)&lt;/span&gt;, requires making unverifiable assumptions about the full data model &lt;span class=&#34;math inline&#34;&gt;\(f(y,r)\)&lt;/span&gt;. Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{mis}\)&lt;/span&gt;, given &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt;. Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Unverifiable parametric assumptions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Informative prior distributions (under a Bayesian approach)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We show some simple examples about how these &lt;em&gt;nonignorable&lt;/em&gt; models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as &lt;em&gt;Selection Models&lt;/em&gt;(SM).&lt;/p&gt;
&lt;div id=&#34;selection-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selection Models&lt;/h2&gt;
&lt;p&gt;The selection model approach factors the full data distribution as&lt;/p&gt;
&lt;p&gt;\[
f(y,r \mid \omega) = f(y \mid \theta) f(r \mid y,\psi),
\]&lt;/p&gt;
&lt;p&gt;where it is typically assumed that the set of full data parameters &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; can be decomposed as separate parameters for each factor &lt;span class=&#34;math inline&#34;&gt;\((\theta,\psi)\)&lt;/span&gt;. Thus, under the SM approach, the &lt;em&gt;response model&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(y \mid \theta)\)&lt;/span&gt; and the &lt;em&gt;missing data mechanism&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(r \mid y, \psi)\)&lt;/span&gt; must be specified by the analyst. SMs can be attractive for several reasons, including&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The possibility to directly specify the model of interest &lt;span class=&#34;math inline&#34;&gt;\(f(y \mid \theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The SM factorisation appeals to Rubin’s missing data taxonomy, enabling easy characterisation of the missing data mechanism&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When the missingness pattern is monotone, the missigness mechanism can be formulated as a hazard function, where the hazard of dropout at some time point &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; can depend on parts of the full data vector &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;example-of-sm-for-bivariate-normal-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example of SM for bivariate normal data&lt;/h3&gt;
&lt;p&gt;Consider a sample of &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; units from a bivariate normal distribution &lt;span class=&#34;math inline&#34;&gt;\(Y=(Y_1,Y_2)\)&lt;/span&gt;. Assume also that &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; is always observed while &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt; may be missing, and let &lt;span class=&#34;math inline&#34;&gt;\(R=R_2\)&lt;/span&gt; be the missingness indicator for the partially-observed response &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt;. A SM factors the full data distribution as&lt;/p&gt;
&lt;p&gt;\[
f(y_1,y_2,r \mid \omega) = f(y_1 \mid \theta)f(r \mid y_1,y_2,\psi),
\]&lt;/p&gt;
&lt;p&gt;where we assume &lt;span class=&#34;math inline&#34;&gt;\(\omega=(\theta,\psi)\)&lt;/span&gt;. Suppose we specify &lt;span class=&#34;math inline&#34;&gt;\(f(y_1,y_2 \mid \theta)\)&lt;/span&gt; as a bivariate normal density with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(2\times2\)&lt;/span&gt; covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;. The distribution of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is assumed to be distributed as a Bernoulli variable with probability &lt;span class=&#34;math inline&#34;&gt;\(\pi_i\)&lt;/span&gt;, such that&lt;/p&gt;
&lt;p&gt;\[
g(\pi_i) = \psi_0 + \psi_1y_{i1} + \psi_2y_{i2},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(g()\)&lt;/span&gt; denotes a given &lt;em&gt;link function&lt;/em&gt; which relates the expected value of the response to the linear predictors in the model. When this is taken as the inverse normal cumulative distribution function &lt;span class=&#34;math inline&#34;&gt;\(\Phi^{-1}()\)&lt;/span&gt; the model corresponds to the Heckman probit selection model (&lt;span class=&#34;citation&#34;&gt;Heckman (1976)&lt;/span&gt;). In general, setting &lt;span class=&#34;math inline&#34;&gt;\(\psi_2=0\)&lt;/span&gt; leads to a &lt;em&gt;Missing At Random&lt;/em&gt;(MAR) assumption; if, in addition, we have distinctness of the parameters &lt;span class=&#34;math inline&#34;&gt;\(f(\mu,\Sigma,\psi)=f(\mu,\Sigma)f(\psi)\)&lt;/span&gt;, we have &lt;em&gt;ignorability&lt;/em&gt;. We note that, even though the parameter &lt;span class=&#34;math inline&#34;&gt;\(\psi_2\)&lt;/span&gt; characterises the association between &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt;, the parametric assumptions made in this example will identify &lt;span class=&#34;math inline&#34;&gt;\(\psi_2\)&lt;/span&gt; even in the absence of informative priors, that is the observed data likelihood is a function of &lt;span class=&#34;math inline&#34;&gt;\(\psi_2\)&lt;/span&gt;. Moreover, the parameter indexes the joint distribution of observables &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and in general can be identified from the observed data. This property of parametric SMs make them ill-suited to assessing sensitivity to assumptions about the missingness mechanism.&lt;/p&gt;
&lt;p&gt;The model can also be generalised to longitudinal data assuming a multivariate normal distribution for &lt;span class=&#34;math inline&#34;&gt;\(Y=(Y_1,\ldots,Y_J)\)&lt;/span&gt; and replacing &lt;span class=&#34;math inline&#34;&gt;\(\pi_i\)&lt;/span&gt; with a discrete time hazard function for dropout&lt;/p&gt;
&lt;p&gt;\[
h\left(t_j \mid \bar{Y}_{j}\right) = \text{Prob}\left(R_j = 0 \mid R_{j-1} = 1, Y_{1},\ldots,Y_{j} \right).
\]&lt;/p&gt;
&lt;p&gt;Using the logit function to model the discrete time hazard in terms of observed response history &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_{j-1}\)&lt;/span&gt; and the current but possibly unobserved &lt;span class=&#34;math inline&#34;&gt;\(Y_j\)&lt;/span&gt; corresponds to the model of &lt;span class=&#34;citation&#34;&gt;Diggle and Kenward (1994)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conlcusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conlcusions&lt;/h2&gt;
&lt;p&gt;To summarise, SMs allows to generalise ignorable models to handle nonignorable missingness by letting &lt;span class=&#34;math inline&#34;&gt;\(f(r \mid y_{obs},y_{mis})\)&lt;/span&gt; to depend on &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}\)&lt;/span&gt; and their structure directly appeals to Rubin’s taxonomy. However, identification of the missing data distribution is accomplished through parametric assumptions about the full data response model &lt;span class=&#34;math inline&#34;&gt;\(f(y \mid \theta)\)&lt;/span&gt; and the explicit form of the missingness mechanism. This makes it difficult to disentagle the type of information that is used to identify the model, i.e. parametric modelling assumptions or information from the observed data, therefore complicating the task of assessing the robustness of the results to a range of transparent and plausible assumptions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-daniels2008missing&#34;&gt;
&lt;p&gt;Daniels, Michael J, and Joseph W Hogan. 2008. &lt;em&gt;Missing Data in Longitudinal Studies: Strategies for Bayesian Modeling and Sensitivity Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-diggle1994informative&#34;&gt;
&lt;p&gt;Diggle, Peter, and Michael G Kenward. 1994. “Informative Drop-Out in Longitudinal Data Analysis.” &lt;em&gt;Journal of the Royal Statistical Society: Series C (Applied Statistics)&lt;/em&gt; 43 (1): 49–73.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-heckman1976common&#34;&gt;
&lt;p&gt;Heckman, James J. 1976. “The Common Structure of Statistical Models of Truncation, Sample Selection and Limited Dependent Variables and a Simple Estimator for Such Models.” In &lt;em&gt;Annals of Economic and Social Measurement, Volume 5, Number 4&lt;/em&gt;, 475–92. NBER.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
