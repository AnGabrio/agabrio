<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Weighting Adjustments on Andrea Gabrio</title>
    <link>/tags/weighting-adjustments/</link>
    <description>Recent content in Weighting Adjustments on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>`{year}`</copyright>
    <lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/weighting-adjustments/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Inverse Probability Weighting</title>
      <link>/missmethods/inverse-probability-weighting/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/inverse-probability-weighting/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In certain cases, it is possible to reduce biases from case deletion by the application of weights. After incomplete cases are removed, the remaining complete cases can be weighted so that their distribution more closely resembles that of the full sample with respect to auxiliary variables. &lt;em&gt;Weighting methods&lt;/em&gt; can eliminate bias due to differential response related to the variables used to model the response probabilities, but it cannot correct for biases related to variables that are unused or unmeasured (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;). &lt;span class=&#34;citation&#34;&gt;Robins, Rotnitzky, and Zhao (1994)&lt;/span&gt; introduced &lt;em&gt;Inverse Probability Weighting&lt;/em&gt; (IPW) as a weighted regression approach that require an explicit model for the missingness but relaxes some of the parametric assumptions in the data model. Their method is an extension of &lt;em&gt;Generalized Estimating Equations&lt;/em&gt; (GEE), a popular technique for modeling marginal or populationaveraged relationships between a response variable and predictors (&lt;span class=&#34;citation&#34;&gt;Zeger, Liang, and Albert (1988)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(y_i=(y_{i1},\ldots,y_{iK})\)&lt;/span&gt; denote a vector of variables for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; subject to missing values with &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; being fully observed for &lt;span class=&#34;math inline&#34;&gt;\(i=1\ldots,n_r\)&lt;/span&gt; units and partially-observed for &lt;span class=&#34;math inline&#34;&gt;\(i=n_r+1,\ldots,n\)&lt;/span&gt; units. Define &lt;span class=&#34;math inline&#34;&gt;\(m_i=1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is incomplete and &lt;span class=&#34;math inline&#34;&gt;\(m_i=0\)&lt;/span&gt; if complete. Let &lt;span class=&#34;math inline&#34;&gt;\(x_i=(x_{i1},\ldots,x_{ip})\)&lt;/span&gt; denote a vector of fully observed covariates and suppose the interest is in estimating the mean of the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;, having the form &lt;span class=&#34;math inline&#34;&gt;\(g(x_i,\beta)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(g()\)&lt;/span&gt; is a possibly non-linear regression function indexed by a parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; of dimension &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Let also &lt;span class=&#34;math inline&#34;&gt;\(z_i=(z_{i1},\ldots,z_{iq})\)&lt;/span&gt; be a vector of fully observed auxiliary variables that potentially predictive of missingness but are not included in the model for &lt;span class=&#34;math inline&#34;&gt;\(y_i \mid x_i\)&lt;/span&gt;. When there are no missing data, a consistent estimate of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is given by the solution to the following GEE, under mild regularity conditions (&lt;span class=&#34;citation&#34;&gt;Liang and Zeger (1986)&lt;/span&gt;),&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n = D_i(x_i,\beta)(y_i-g(x_i,\beta))=0,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(D_i(x_i,\beta)\)&lt;/span&gt; is a suitably chosen &lt;span class=&#34;math inline&#34;&gt;\((d\times k)\)&lt;/span&gt; matrix of known functions of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. With missing data, the equation is applied only to the complete cases (&lt;span class=&#34;math inline&#34;&gt;\(n_{r}\)&lt;/span&gt;), which yields consistent estimates provided that&lt;/p&gt;
&lt;p&gt;\[
p(m_i=1 \mid x_i,y_i,z_i,\phi)=p(m_i=1\mid x_i,\phi),
\]&lt;/p&gt;
&lt;p&gt;that is, missingness does not depend on &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; after conditioning on &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. IPW GEE methods (&lt;span class=&#34;citation&#34;&gt;Robins and Rotnitzky (1995)&lt;/span&gt;) replace the equation with&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^{n_r} = w_i(\hat{\alpha})D_i(x_i,\beta)(y_i-g(x_i,\beta))=0,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(w_i(\hat{\alpha})=\frac{1}{p(x_i,z_i \mid \hat{\alpha})}\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(p(x_i,z_i \mid \hat{\alpha})\)&lt;/span&gt; being an estimate of the probability of being a complete unit, obtained for example via logistic regressions on &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;. If the logistic regression is correctly specified, IPW GEE yields a consistent estimator of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; provided that&lt;/p&gt;
&lt;p&gt;\[
p(m_i=1 \mid x_i,y_i,z_i,\phi)=p(m_i=1\mid x_i,z_i\phi).
\]&lt;/p&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose the full data consists of a single outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and an additional variable &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; and that the objective is to estimate the population outcome mean &lt;span class=&#34;math inline&#34;&gt;\(\mu=\text{E}[y]\)&lt;/span&gt;. If data were fully observed for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; individuals, an obvious estimator of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; would be the sample outcome mean&lt;/p&gt;
&lt;p&gt;\[
\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i,
\]&lt;/p&gt;
&lt;p&gt;which is equivalent to the solution to the estimating equation &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^n(y_i-\mu)=0\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is partially observed (while &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is always fully observed), individuals may fall into one of two missingness patterns &lt;span class=&#34;math inline&#34;&gt;\(r=(r_{y},r_{z})\)&lt;/span&gt;, namely &lt;span class=&#34;math inline&#34;&gt;\(r=(1,1)\)&lt;/span&gt; if both variables are observed or &lt;span class=&#34;math inline&#34;&gt;\(r=(1,0)\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is missing. Let &lt;span class=&#34;math inline&#34;&gt;\(c=1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(r=(1,1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c=0\)&lt;/span&gt; otherwise, so that the observed data can be summarised as &lt;span class=&#34;math inline&#34;&gt;\((c,cy,z)\)&lt;/span&gt;. Assuming that missingness only depends on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p(c=1 \mid y,z)=p(c=1 \mid z)=\pi(z),
\]&lt;/p&gt;
&lt;p&gt;then the missing data mechanism is &lt;em&gt;Missing At Random&lt;/em&gt; (MAR). Under these conditions, the sample mean of the complete cases &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{cc}=\frac{\sum_{i=1}^nc_iy_i}{c_i}\)&lt;/span&gt;, i.e.Â the solution to the equation &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^nc_i(y_i-\mu)=0\)&lt;/span&gt;, is not a consistent estimator of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. To correct for this, the IPW complete case estimating equation&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n\frac{c_i}{\pi(z_i)}(y_i-\mu)=0,
\]&lt;/p&gt;
&lt;p&gt;can be used to weight the contribution of each complete case by the inverse of &lt;span class=&#34;math inline&#34;&gt;\(\pi(z_i)\)&lt;/span&gt;. The solution of the equation corresponds to the IPW estimator&lt;/p&gt;
&lt;p&gt;\[
\mu_{ipw}=\left(\sum_{i=1}^n \frac{c_i}{\pi(z_i)} \right)^{-1} \sum_{i=1}^n \frac{c_iy_i}{\pi(z_i)},
\]&lt;/p&gt;
&lt;p&gt;which is unbiased under MAR and for &lt;span class=&#34;math inline&#34;&gt;\(\pi(z)&amp;gt;0\)&lt;/span&gt;. In case you want to have a look at the &lt;a href=&#34;https://www4.stat.ncsu.edu/~davidian/st790/notes/chap5.pdf&#34;&gt;proof&lt;/a&gt; of this I put here the link. In most situations &lt;span class=&#34;math inline&#34;&gt;\(\pi(z_i)\)&lt;/span&gt; is not known and must be estimated from the data, typically posing some model for &lt;span class=&#34;math inline&#34;&gt;\(p(c=1 \mid z, \hat{\alpha})\)&lt;/span&gt;, indexed by some parameter &lt;span class=&#34;math inline&#34;&gt;\(\hat{\alpha}\)&lt;/span&gt;, for example a logistic regression&lt;/p&gt;
&lt;p&gt;\[
\text{logit}(\pi)=\alpha_0 + \alpha_1z.
\]&lt;/p&gt;
&lt;p&gt;Of course, if the model for &lt;span class=&#34;math inline&#34;&gt;\(\pi(z)\)&lt;/span&gt; is misspecified, &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ipw}\)&lt;/span&gt; can be an inconsistent estimator. In addition, IPW methods typically used data only from the completers discarding all the partially observed values, which is clearly inefficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Thus, IPW estimators can correct for the bias of unweighted estimators due to the dependence of the missingness mechanism on &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;). The basic intuition of IPW methods is that each subjectâs contribution to the weighted &lt;em&gt;Complete Case Analysis&lt;/em&gt; (CCA) is replicated &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; times in order to account once for herself and &lt;span class=&#34;math inline&#34;&gt;\((1-w_i)\)&lt;/span&gt; times for those subjects with the same responses and covariates who are missing. These models are called &lt;em&gt;semiparametric&lt;/em&gt; because they apart from requiring the regression equation to have a specific form, they do not specify any probability distribution for the response variable (&lt;span class=&#34;citation&#34;&gt;Molenberghs et al. (2014)&lt;/span&gt;). Older GEE methods can accommodate missing values only if they are &lt;em&gt;Missing Completely At Random&lt;/em&gt; (MCAR), while more recent methods allow them to be MAR or even &lt;em&gt;Missing Not At Random&lt;/em&gt; (MNAR), provided that a model for the missingness is correctly specified (&lt;span class=&#34;citation&#34;&gt;Robins, Rotnitzky, and Zhao (1995)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Rotnitzky, Robins, and Scharfstein (1998)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references hanging-indent&#34;&gt;
&lt;div id=&#34;ref-liang1986longitudinal&#34;&gt;
&lt;p&gt;Liang, Kung-Yee, and Scott L Zeger. 1986. âLongitudinal Data Analysis Using Generalized Linear Models.â &lt;em&gt;Biometrika&lt;/em&gt; 73 (1): 13â22.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-molenberghs2014handbook&#34;&gt;
&lt;p&gt;Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. &lt;em&gt;Handbook of Missing Data Methodology&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins1995semiparametric&#34;&gt;
&lt;p&gt;Robins, James M, and Andrea Rotnitzky. 1995. âSemiparametric Efficiency in Multivariate Regression Models with Missing Data.â &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 90 (429): 122â29.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins1994estimation&#34;&gt;
&lt;p&gt;Robins, James M, Andrea Rotnitzky, and Lue Ping Zhao. 1994. âEstimation of Regression Coefficients When Some Regressors Are Not Always Observed.â &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 89 (427): 846â66.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins1995analysis&#34;&gt;
&lt;p&gt;âââ. 1995. âAnalysis of Semiparametric Regression Models for Repeated Outcomes in the Presence of Missing Data.â &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 90 (429): 106â21.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rotnitzky1998semiparametric&#34;&gt;
&lt;p&gt;Rotnitzky, Andrea, James M Robins, and Daniel O Scharfstein. 1998. âSemiparametric Regression for Repeated Outcomes with Nonignorable Nonresponse.â &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 93 (444): 1321â39.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. âMissing Data: Our View of the State of the Art.â &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-zeger1988models&#34;&gt;
&lt;p&gt;Zeger, Scott L, Kung-Yee Liang, and Paul S Albert. 1988. âModels for Longitudinal Data: A Generalized Estimating Equation Approach.â &lt;em&gt;Biometrics&lt;/em&gt;, 1049â60.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Augmented Inverse Probability Weighting</title>
      <link>/missmethods/augmented-inverse-probability-weighting/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/augmented-inverse-probability-weighting/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A general problem associated with the implementatio of &lt;em&gt;Inverse Probability Weighting&lt;/em&gt; (IPW) methods is that information in some available data is ignored by focussing only on the complete cases (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;). This has provided room to extend these methods to make a more efficient use of the available information through the incorporation of an âaugmentationâ term, which lead to the development of the so called &lt;em&gt;Augmented Inverse Probability Weighting&lt;/em&gt; (AIPW) methods. These approaches extend IPW methods by creating predictions from a model to recover the information in the incomplete units and applying IPW to the residuals from the model (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Considering the IPW &lt;em&gt;Generalised Estimating Equation&lt;/em&gt; (GEE)&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^{n_r} = w_i(\hat{\alpha})D_i(x_i,\beta)(y_i-g(x_i,\beta))=0,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(w_i(\hat{\alpha})=\frac{1}{p(x_i,z_i \mid \hat{\alpha})}\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(p(x_i,z_i \mid \hat{\alpha})\)&lt;/span&gt; an estimate of the probability of being a complete unit estimated for example using logistic regressions of the missingness indicator &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; on the vectors of the covariate and auxiliary variables &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;, respectively. A problem of this IPW estimator is that it has poor small sample properties when the propensity score gets close to zero or one for some observations, which will lead to high variance in the estimator. AIPW methods can provide estimators of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; which are more efficient than their nonaugmented IPW versions. In general, AIPW estimating functions provide a method for constructing estimators of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; based on two terms:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The usual IPW term &lt;span class=&#34;math inline&#34;&gt;\(p(x_i,z_i \mid \hat{\alpha})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An augmentation term &lt;span class=&#34;math inline&#34;&gt;\(g^\star(x_i,\beta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The basis for the first term is a complete data unbiased estimating function for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, whereas the basis for the second term is some function of the observed data chosen so it has conditional mean of zero given the complete data (&lt;span class=&#34;citation&#34;&gt;Molenberghs et al. (2014)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;doubly-robust-estimators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Doubly Robust Estimators&lt;/h2&gt;
&lt;p&gt;An important class of AIPW methods is known as &lt;em&gt;doubly robust&lt;/em&gt; estimators, which have desirable robustness properties (&lt;span class=&#34;citation&#34;&gt;Robins, Rotnitzky, and Laan (2000)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Robins and Rotnitzky (2001)&lt;/span&gt;). The key feature of these estimators is that they relax the assumption that the model of the missingness probabilities is correctly specified, although requiring additional assumptions on the model for &lt;span class=&#34;math inline&#34;&gt;\(y_i \mid x_i\)&lt;/span&gt;. For example, doubly robust estimators for a population mean parameter &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; could be obtained as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Fit a logistic regression model for the probability of observing &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; to derive the individual weights &lt;span class=&#34;math inline&#34;&gt;\(w_i(\hat{\alpha})\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fit a generalized linear model for the outcome of responders in function of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; using weights &lt;span class=&#34;math inline&#34;&gt;\(w_i(\hat{\alpha})\)&lt;/span&gt; and let &lt;span class=&#34;math inline&#34;&gt;\(g^\star(x_i,\beta)\)&lt;/span&gt; denote the fitted values for subject &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take the sample average of the fitted values &lt;span class=&#34;math inline&#34;&gt;\(g^\star(x_i,\beta)\)&lt;/span&gt; of both respondents and nonrespondents as an estimate of the population mean &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Doubly robust estimators require the specification of two models: one for the missingness probability and another for the distribution of the incomplete data. When the augmentation term &lt;span class=&#34;math inline&#34;&gt;\(g^\star(x_i,\beta)\)&lt;/span&gt; is selected and modelled correctly according to the distribution of the complete data, the resulting estimator of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is consistent even if the model of missingness is misspecified. On the other hand, if the model of missingness is correctly specified, the augmentation term no longer needs to be correctly specified to yield consistent estimators of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Scharfstein, Daniels, and Robins (2003)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Bang and Robins (2005)&lt;/span&gt;). Doubly robust estimators therefore allow to obtain an unbiased estimating function for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; if either the model for the incomplete data or the model for the missingness mechanism has been correctly specified.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose the full data consists of a single outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and an additional variable &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; and that the objective is to estimate the population outcome mean &lt;span class=&#34;math inline&#34;&gt;\(\mu=\text{E}[y]\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is partially observed (while &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is always fully observed), individuals may fall into one of two missingness patterns &lt;span class=&#34;math inline&#34;&gt;\(r=(r_{y},r_{z})\)&lt;/span&gt;, namely &lt;span class=&#34;math inline&#34;&gt;\(r=(1,1)\)&lt;/span&gt; if both variables are observed or &lt;span class=&#34;math inline&#34;&gt;\(r=(1,0)\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is missing. Let &lt;span class=&#34;math inline&#34;&gt;\(c=1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(r=(1,1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c=0\)&lt;/span&gt; otherwise, so that the observed data can be summarised as &lt;span class=&#34;math inline&#34;&gt;\((c,cy,z)\)&lt;/span&gt;. Assuming that missingness only depends on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p(c=1 \mid y,z)=p(c=1 \mid z)=\pi(z),
\]&lt;/p&gt;
&lt;p&gt;then the missing data mechanism is &lt;em&gt;Missing At Random&lt;/em&gt; (MAR). Under these conditions, consider the consistent IPW complete case estimating equation&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n\frac{c_i}{\pi(z_i \mid \hat{\alpha})}(y_i-\mu)=0,
\]&lt;/p&gt;
&lt;p&gt;which can be used to weight the contribution of each complete case by the inverse of &lt;span class=&#34;math inline&#34;&gt;\(\pi(z_i \mid \hat{\alpha})\)&lt;/span&gt;, typically estimated via logistic regressions. A general problem of this type of estimators is that they discard all the available data among the non-completers and are therefore inefficient. However, it is possible to augment the simple IPW complete case estimating equation to improve efficiency. The optimal estimator for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; within this class is the solution to the estimating equation&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n \left(\frac{c_i}{\pi(z_i \mid \hat{\alpha})}(y_i-\mu) - \frac{c_i-\pi(z_i \mid \hat{\alpha})}{\pi(z_i \mid \hat{\alpha})}\text{E}[(y_i-\mu)\mid z_i] \right),
\]&lt;/p&gt;
&lt;p&gt;which leads to the estimator&lt;/p&gt;
&lt;p&gt;\[
\mu_{aipw}=\frac{1}{n}\sum_{i=1}^n \left(\frac{c_iy_i}{\pi(z_i\mid \hat{\alpha})} - \frac{c_i - \pi(z_i\mid \hat{\alpha})}{\pi(z_i\mid \hat{\alpha})} \text{E}[y_i \mid z_i] \right).
\]&lt;/p&gt;
&lt;p&gt;The conditional expectation &lt;span class=&#34;math inline&#34;&gt;\(\text{E}[y_i \mid z_i]\)&lt;/span&gt; is not known and must be estimated from the data. Under a &lt;em&gt;Missing At Random&lt;/em&gt; (MAR) assumption we have that &lt;span class=&#34;math inline&#34;&gt;\(\text{E}[y \mid z]=\text{E}[y \mid z, c=1]\)&lt;/span&gt;, that is the conditional expecation of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is the same as that among the completers. Thus, we can specify a model &lt;span class=&#34;math inline&#34;&gt;\(m(z,\xi)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\text{E}[y \mid z]\)&lt;/span&gt;, indexed by the parameter &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;, that can be estimated from the completers. If &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is continuous, a simple choice is to estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{\xi}\)&lt;/span&gt; by OLS from the completers. The AIPW estimator for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; then becomes&lt;/p&gt;
&lt;p&gt;\[
\mu_{aipw}=\frac{1}{n}\sum_{i=1}^n \left(\frac{c_iy_i}{\pi(z_i\mid \hat{\alpha})} - \frac{c_i - \pi(z_i\mid \hat{\alpha})}{\pi(z_i\mid \hat{\alpha})} m(z_i\mid \hat{\xi}) \right).
\]&lt;/p&gt;
&lt;p&gt;It can be shown that this estimator is more efficient that the simple IPW complete case estimator for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and that it has a double robustness property. This ensures that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{aipw}\)&lt;/span&gt; is a consitent estimator of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; if &lt;strong&gt;either&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the model &lt;span class=&#34;math inline&#34;&gt;\(\pi(z\mid\alpha)\)&lt;/span&gt; is correctly specified, &lt;strong&gt;or&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the model &lt;span class=&#34;math inline&#34;&gt;\(m(z\mid \xi)\)&lt;/span&gt; is correctly specified.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To see a derivation of the double robustness property I put here a link to some nice &lt;a href=&#34;https://www4.stat.ncsu.edu/~davidian/st790/notes/chap5.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conlcusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conlcusions&lt;/h2&gt;
&lt;p&gt;As all weighting methods, such as IPW, AIPW methods are &lt;em&gt;semiparametric&lt;/em&gt; methods that aim to achieve robustness and good performance over more general classes of population distributions. However, semiparametric estimators can be less efficient and less powerful than &lt;em&gt;Maximum Likelihood&lt;/em&gt; or &lt;em&gt;Bayesian&lt;/em&gt; estimators under a well specified parametric model. With missing data, &lt;span class=&#34;citation&#34;&gt;Rubin (1976)&lt;/span&gt; results show that likelihood-based methods perform uniformly well over any &lt;em&gt;Missing At Random&lt;/em&gt; (MAR) missingness distribution, and the user does not need to specify that distribution. However, semiparametric methods that relax assumptions about the data must in turn assume a specific form for the distribution of missingness. It has been argued that, for these semiparametric methods to gain a substantial advantage over well-specified likelihood methods, the parametric model has to be grossly misspecified (&lt;span class=&#34;citation&#34;&gt;Meng (2000)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references hanging-indent&#34;&gt;
&lt;div id=&#34;ref-bang2005doubly&#34;&gt;
&lt;p&gt;Bang, Heejung, and James M Robins. 2005. âDoubly Robust Estimation in Missing Data and Causal Inference Models.â &lt;em&gt;Biometrics&lt;/em&gt; 61 (4): 962â73.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-meng2000missing&#34;&gt;
&lt;p&gt;Meng, Xiao-Li. 2000. âMissing Data: Dial M for???â &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 95 (452): 1325â30.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-molenberghs2014handbook&#34;&gt;
&lt;p&gt;Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. &lt;em&gt;Handbook of Missing Data Methodology&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins2001comment&#34;&gt;
&lt;p&gt;Robins, James M, and Andrea Rotnitzky. 2001. âComment on the Bickel and Kwon Article,âInference for Semiparametric Models: Some Questions and an Answerâ.â &lt;em&gt;Statistica Sinica&lt;/em&gt; 11 (4): 920â36.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins2000profile&#34;&gt;
&lt;p&gt;Robins, James M, Andrea Rotnitzky, and Mark van der Laan. 2000. âOn Profile Likelihood: Comment.â &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 95 (450): 477â82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1976inference&#34;&gt;
&lt;p&gt;Rubin, Donald B. 1976. âInference and Missing Data.â &lt;em&gt;Biometrika&lt;/em&gt; 63 (3): 581â92.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. âMissing Data: Our View of the State of the Art.â &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-scharfstein2003incorporating&#34;&gt;
&lt;p&gt;Scharfstein, Daniel O, Michael J Daniels, and James M Robins. 2003. âIncorporating Prior Beliefs About Selection Bias into the Analysis of Randomized Trials with Missing Outcomes.â &lt;em&gt;Biostatistics&lt;/em&gt; 4 (4): 495â512.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Weighting Adjustments</title>
      <link>/missmethods/weighting-adjustments/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/weighting-adjustments/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The notion of reducing bias due to missingness through &lt;em&gt;reweighting methods&lt;/em&gt; has its root in the survey literature and the basic idea is closely related to weighting in randomisation inference for finite population surveys (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;). In particular, in probability sampling, a unit selected from a target population with probability &lt;span class=&#34;math inline&#34;&gt;\(\pi_i\)&lt;/span&gt; can be thought as ârepresentingâ &lt;span class=&#34;math inline&#34;&gt;\(\pi^{-1}_i\)&lt;/span&gt; units in the population and hence should be given weight &lt;span class=&#34;math inline&#34;&gt;\(\pi^{-1}_i\)&lt;/span&gt; when estimating population quantities. For example, in a stratified random sample, a selected unit in stratum &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; represents &lt;span class=&#34;math inline&#34;&gt;\(\frac{N_j}{n_j}\)&lt;/span&gt; population units, where &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; indicates the units sampled from the &lt;span class=&#34;math inline&#34;&gt;\(N_j\)&lt;/span&gt; population units in stratum &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots,J\)&lt;/span&gt;. The population total &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; can then be estimated by the weighted sum&lt;/p&gt;
&lt;p&gt;\[
T = \sum_{i=1}^{n}y_i\pi^{-1}_i,
\]&lt;/p&gt;
&lt;p&gt;known as the Horvitz-Thompson estimate (&lt;span class=&#34;citation&#34;&gt;Horvitz and Thompson (1952)&lt;/span&gt;), while the stratified mean can be written as&lt;/p&gt;
&lt;p&gt;\[
\bar{y}_{w} = \frac{1}{n}\sum_{i=1}^{n}w_iy_i,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(w_i=\frac{n\pi^{-1}_i}{\sum_{k=1}^n\pi^{-1}_k}\)&lt;/span&gt; is the sampling weight attached to the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th unit scaled tosum up to the sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Weighting class estimators extend this approach to handle missing data such that, if the probabilities of response for unit &lt;span class=&#34;math inline&#34;&gt;\(\phi_i\)&lt;/span&gt; were known, then the probability of selection and response is &lt;span class=&#34;math inline&#34;&gt;\(\pi_i\phi_i\)&lt;/span&gt; and we have&lt;/p&gt;
&lt;p&gt;\[
\bar{y}_{w} = \frac{1}{n_r}\sum_{i=1}^{n_r}w_iy_i,
\]&lt;/p&gt;
&lt;p&gt;where the sum is now over responding units and &lt;span class=&#34;math inline&#34;&gt;\(w_i=\frac{n_r(\pi_i\phi_i)^{-1}}{\sum_{k=1}^{n_r}(\pi_k\phi_k)^{-1}}\)&lt;/span&gt;. In practice, the response probability &lt;span class=&#34;math inline&#34;&gt;\(\phi_i\)&lt;/span&gt; is not known and is typically estimated based on the information available for respondents and nonrespondents (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;weighting-class-estimator-of-the-mean&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weighting Class Estimator of the Mean&lt;/h2&gt;
&lt;p&gt;A simple reweighting approach is to partition the sample into &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; âweighting classesâ according to the variables observed for respondents and nonrespondents. If &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; is the sample size, &lt;span class=&#34;math inline&#34;&gt;\(n_{rj}\)&lt;/span&gt; the number of respondents in class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(n_r=\sum_{j=1}^Jr_j\)&lt;/span&gt;, then a simple estimator of the response probability for units in class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is given by &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_{rj}}{n_j}\)&lt;/span&gt;. Thus, responding units in class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; receive weight &lt;span class=&#34;math inline&#34;&gt;\(w_i=\frac{n_r(\pi_i\hat{\phi}_i)^{-1}}{\sum_{k=1}^{n_r}(\pi_k\hat{\phi}_k)^{-1}}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\phi}_i=\frac{n_{rj}}{n_j}\)&lt;/span&gt; for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. The weighting class estimate of the mean is then&lt;/p&gt;
&lt;p&gt;\[
\bar{y}_{w} = \frac{1}{n_r}\sum_{i=1}^{n_r}w_iy_i,
\]&lt;/p&gt;
&lt;p&gt;which is unbiased under the &lt;em&gt;quasirandomisation&lt;/em&gt; assumption (&lt;span class=&#34;citation&#34;&gt;Oh and Scheuren (1983)&lt;/span&gt;), which requires respondents in weighting class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; to be a random sample of the sampled units, i.e.Â data are &lt;em&gt;Missing Completely At Random&lt;/em&gt; (MCAR) within adjustment class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. Weighting class adjustments are simple because the same weights are obtained regardless of the outcome tp which they are applied, but these are inefficient and generally involves an increase in sampling variance for outcomes that are weakly related to the weighting class variable. Assuming random sampling within weighting classes, a constant variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; for an outcome &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and ignoring sampling variation in the weights, the increase in sampling variance of a sample mean is&lt;/p&gt;
&lt;p&gt;\[
\text{Var}\left(\frac{1}{n_{r}}\sum_{i=1}^{n_{r}}w_iy_i \right) = \frac{\sigma^2}{n_{r}^2}\left(\sum_{i=1}^{n_{r}}w_{i}^{2} \right) = \frac{\sigma^2}{n_{r}}(1+\text{cv}^2(w_i)),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{cv}(w_i)\)&lt;/span&gt; is the coefficient of variation of the weights (scaled to average one), which is a rough measure of the proportional increase in sampling variance due to weighting (&lt;span class=&#34;citation&#34;&gt;Kish (1992)&lt;/span&gt;). When the weighting class variable is predictive of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, weighting methods can lead to a reduction in sampling variance. &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt; summarise the effect of weighting on the bias and sampling variance of an estimated mean, according to whether the associations between the adjustment cells and the outcome &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and missing indicator &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; are high or low.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Effect of weighting adjustments on bias and sampling variance of a mean.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Low (y)
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
High (y)
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Low (m)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bias: /, var: /
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bias: /, var: -
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
High (m)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bias: /, var: +
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bias: -, var: -
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Thus, weighting is only effective when the outcome is associated with the adjustment cell variable because otherwise the sampling variance is increased with no bias reduction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;propensity-weighting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Propensity Weighting&lt;/h2&gt;
&lt;p&gt;In some settings, weighting class estimates cannot be feasibly derived by all recorded variables X because the number of classes become too large and some may include cells with nonrespondents but no respondents for which the nonresponse weight is infinite. The theory of propensity scores (&lt;span class=&#34;citation&#34;&gt;Rosenbaum and Rubin (1983)&lt;/span&gt;) provides a prescription for choosing the coarsest reduction of the variables to a weighting class variable &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. Suppose the data are &lt;em&gt;Missing At Random&lt;/em&gt; (MAR) such that&lt;/p&gt;
&lt;p&gt;\[
p(m\mid X,y,\phi)=p(m\mid X,\phi),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; are unknown parameters and define the nonresponse propensity for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;\[
\rho(x_i,\phi)=p(m_i=1 \mid \phi),
\]&lt;/p&gt;
&lt;p&gt;assuming that this is strictly positive for all values of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. Then, it can be shown that&lt;/p&gt;
&lt;p&gt;\[
p(m\mid \rho(X,\phi),y,\phi)=p(m\mid \rho(X,\phi),\phi),
\]&lt;/p&gt;
&lt;p&gt;so that respondents are a random subsample within strata defined by the propensity score &lt;span class=&#34;math inline&#34;&gt;\(\rho(X,\phi)\)&lt;/span&gt;. In practice the parameter &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is unknown and must be estimated from sample data, for example via logistic, probit or robit regressions of the missingness indicator &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; based on respondent and nonrespondent data (&lt;span class=&#34;citation&#34;&gt;Liu (2004)&lt;/span&gt;). A variant of this procedure is to weight respondents &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; directly by the inverse of the estimated propensity score &lt;span class=&#34;math inline&#34;&gt;\(\rho(X,\hat{\phi})^{-1}\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Cassel, Sarndal, and Wretman (1983)&lt;/span&gt;), which allows to remove bias but may cause two problems: 1) estimates may be associated with very high sampling variances due to nonrespondents with low response propensity estimates receiving large nonresponse weights; 2) more reliance on correct model specification of the propensity score regression than response propensity stratification.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references hanging-indent&#34;&gt;
&lt;div id=&#34;ref-cassel1983some&#34;&gt;
&lt;p&gt;Cassel, Claes M, Carl-Erik Sarndal, and Jan H Wretman. 1983. âSome Uses of Statistical Models in Connection with the Nonresponse Problem.â &lt;em&gt;Incomplete Data in Sample Surveys&lt;/em&gt; 3: 143â60.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-horvitz1952generalization&#34;&gt;
&lt;p&gt;Horvitz, Daniel G, and Donovan J Thompson. 1952. âA Generalization of Sampling Without Replacement from a Finite Universe.â &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 47 (260): 663â85.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kish1992weighting&#34;&gt;
&lt;p&gt;Kish, Leslie. 1992. âWeighting for Unequal Pi.â &lt;em&gt;Journal of Official Statistics&lt;/em&gt; 8 (2): 183.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-liu2004robit&#34;&gt;
&lt;p&gt;Liu, Chuanhai. 2004. âRobit Regression: A Simple Robust Alternative to Logistic and Probit Regression.â &lt;em&gt;Applied Bayesian Modeling and Casual Inference from Incomplete-Data Perspectives&lt;/em&gt;, 227â38.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-oh1983weighting&#34;&gt;
&lt;p&gt;Oh, H, and F Scheuren. 1983. âWeighting Adjustment for Unit Nonresponse. Chap. 13 in Vol. 2, Part 4 of Incomplete Data in Sample Surveys, Edited by William G. Madow, Harold Nisselson, and Ingram Olkin.â New York: Academic Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rosenbaum1983central&#34;&gt;
&lt;p&gt;Rosenbaum, Paul R, and Donald B Rubin. 1983. âThe Central Role of the Propensity Score in Observational Studies for Causal Effects.â &lt;em&gt;Biometrika&lt;/em&gt; 70 (1): 41â55.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. âMissing Data: Our View of the State of the Art.â &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
