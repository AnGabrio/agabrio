[{"authors":["agabrio"],"categories":null,"content":"I am a Research Fellow in Statistics and Health Economics in the Department of Statistical Science \u0026amp; in the Department of Primary Care and Population Health at University College London (UK). I graduated in Applied Economics from the University of Pavia (Italy) and in Statistics and Econometrics from the University of Essex (UK). I then completed a PhD programme in Statistics at University College London, after a short visiting period in the Department of Statistics at University of Florida (USA).\nMy main interests are in Bayesian statistical modelling for cost-effectiveness analysis and decision-making problems in the health systems. During my PhD I have specifically focused on the study and adoption of Bayesian methods to handle missing data in health economic evaluations and to assess the impact of their uncertainty on the output of the decision-making process. My research area involves different topics: from systematic literature reviews, case study applications, survival analysis, meta-analytic methods, multilevel models and trial-based clinical and economic analyses. Here you can find my CV with more details on what I have done so far and my interests.\nI am very interested in the analysis of longitudinal data, with a focus on different types of statistical methods to deal with missingness. My preferred statistical programming software and the one I am most familiar with is R/RStudio by far, but I do also possess a good knowledge of other software such as STATA and MATLAB. I am quite expert in the use of free open-source Bayesian software programs, such as OpenBUGS, JAGS and STAN.\nI am a member of the Statistics for Health Economic Evaluation research group in the Department of Statistical Science at UCL, which is mainly focused on the development and application of Bayesian methods for health economic evaluations. The group works in collaboration with academics from different institutions and its activities are aimed at providing advice to statisticians, health economists and clinicians working in economic evaluations.\nI am also a member of the Health Economics Analysis and Research methodology Team in the Institute for Clinical Trials and Methodology at UCL, working primarily with the members of the Priment Clinical Trials Unit. The group focuses on the development of methodological tools for the analysis of the economic components in randomised control trials across a wide range of clinical areas and is formed by a group of interdisciplinary and varied experience.\n\u0026#9818; King's note! \u0026#9818; \r\r\rI am a huge fan of RStudio and its tools, such as Rmarkdown and blogdown packages, which are aimed at the construction of documents that combine text, R code and the output from the execution of that code: from html and pdf files to multi-page web sites and e-books (yes this website is written in Markdown!). Oh, and I ❤ $\\rm \\LaTeX$.\r\r\r\r","date":1517788800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1517788800,"objectID":"a4a58741d21f71aab0e211bcb1160621","permalink":"/authors/agabrio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/agabrio/","section":"authors","summary":"I am a Research Fellow in Statistics and Health Economics in the Department of Statistical Science \u0026amp; in the Department of Primary Care and Population Health at University College London (UK). I graduated in Applied Economics from the University of Pavia (Italy) and in Statistics and Econometrics from the University of Essex (UK). I then completed a PhD programme in Statistics at University College London, after a short visiting period in the Department of Statistics at University of Florida (USA).","tags":null,"title":"Andrea Gabrio","type":"authors"},{"authors":null,"categories":["rubric"],"content":"\r\rComplete case analysis (CCA), also known as case or listwise deletion (LD), is one of the oldest methods to handle missing data and consists in discarding any unit or case whose information is incomplete. Only the cases with observed values for all the variables under consideration are used in the analysis. For example, suppose we have a data set formed by \\(i=1,\\ldots,n\\) individuals and that we want to fit a linear regression on some outcome variable \\(y_i\\) using some other variables \\(x_{i1},\\ldots,x_{ik}\\) as covariates. CCA uses only the subset of cases with observed values on all the variables included in the analysis (completers).\nCCA has been a quite popular approach to deal with missingness, mainly because it is very easy to implement (used by default in many statistical programs) and it allows the comparison of different univariate statistics in a straightforward way (calculated on a common set of cases). However, there are a number of potential disadvantages which threatens the validity of this method:\nBias, when the missing data mechanism is not missing completely at random (MCAR) and the completers are not a random samples of all the cases\n\rLoss of efficiency, due to the potential loss of information in discarding the incomplete cases.\n\r\rCCA may be justified when the loss of precision and bias are minimal, which is more likley to occur when the proportion of completers is high, although it is difficult to formulate rules that apply in general circumstances. Indeed, both the degree of loss of precision and bias depend not only on the fraction of completers and missingness patterns, but also on the extent to which complete and incomplete cases differ and the parameters of interest.\nLet \\(\\hat{\\theta}_{cc}\\) be an estimate of a parameter of interest from the completers. One might measure the increase in variance of \\(\\hat{\\theta}_{cc}\\) with respect to the estimate \\(\\hat{\\theta}\\) that would be obtained in the absence of missing values. Using the notation of Little and Rubin (2019):\n\\[\r\\text{Var}(\\hat{\\theta}_{cc}) = \\text{Var}(\\hat{\\theta})(1 + \\Delta^{\\star}_{cc}),\r\\]\nwhere \\(\\Delta^{\\star}_{cc}\\) is the proportional increase in variance from the loss of information. A more practical measure of the loss of inofrmation is \\(\\Delta_{cc}\\), where\n\\[\r\\text{Var}(\\hat{\\theta}_{cc}) = \\text{Var}(\\hat{\\theta}_{eff})(1 + \\Delta_{cc}),\r\\]\nand \\(\\hat{\\theta}_{eff}\\) is an efficient estimate based on all the available data.\nExample 1\rConsider bivariate normal monotone data \\(\\bf y = (y_1,y_2)\\), where \\(n_{cc}\\) out of \\(n\\) cases are complete and \\(n - n_{cc}\\) cases have observed values only on \\(y_1\\). Assume for simplicity that the missingness mechanism is MCAR and that the mean of \\(y_j\\) is estimated by the empirical mean from the complete cases \\(\\bar{y}^{cc}_j\\). Then, the loss in sample size for estimating the mean of \\(y_1\\) is:\n\\[\r\\Delta_{cc}(\\bar{y}_1) = \\frac{n - n_{cc}}{n_{cc}},\r\\]\nso that if half the cases are missing, the variance is doubled. For the mean of \\(y_2\\), the loss of information alos depends on the squared correlation \\(\\rho^{2}\\) between the variables: (Little and Rubin (2019))\n\\[\r\\Delta_{cc}(\\bar{y}_2) \\approx \\frac{(n - n_{cc})\\rho^{2}}{n_{cc}(1 - \\rho^{2}) + n_{cc}\\rho^{2}}.\r\\]\n\\(\\Delta_{cc}(\\bar{y}_2)\\) varies from zero (when \\(\\rho=0\\)) to \\(\\Delta_{cc}(\\bar{y}_1)\\) as \\(\\rho^{2} \\rightarrow 1\\). However, for the regression coefficients of \\(y_2\\) on \\(y_1\\) we have that \\(\\Delta_{cc}=0\\) since the incomplete observations of \\(y_1\\) provide no information for estimating the parameters of the regression of \\(y_2\\) on \\(y_1\\).\n\rExample 2\rFor inference about the population mean \\(\\mu\\), the bias of CCA depends on the proportion of the completers \\(\\pi_{cc}\\) and the extent to which complete and incomplete cases differ on the variables of interest. Suppose a variable \\(y\\) is partially-observed and that we partition the data into the subset of the completers \\(y_{cc}\\) and incompleters \\(y_{ic}\\), with associated population means \\(\\mu_{cc}\\) and \\(\\mu_{ic}\\), respectively. The overall mean can be written as a weighted average of the means of the two subsets\n\\[\r\\mu = \\pi_{cc}\\mu_{cc} + (1 - \\pi_{cc})\\mu_{ic}.\r\\]\nThe bias of CCA is then equal to the expected fraction of incomplete cases multiplied by the differences in the means for complete and incomplete cases\n\\[\r\\mu_{cc} - \\mu = (1 - \\pi_{cc})(\\mu_{cc} - \\mu_{ic}).\n\\]\nUnder MCAR, we have that \\(\\mu_{cc} = \\mu_{ic}\\) and therefore the bias is zero.\n\rExample 3\rConsider the estimation of the regression of \\(y\\) on \\(x_1,\\ldots,x_K\\) from data with potential missing values on all variables and with the regression function correctly specified. The bias of CCA for estimating the regression coefficients \\(\\beta_1,\\ldots,\\beta_K\\) associated with the covariates is null if the probbaility of being a completer depends on the \\(x\\)s but not \\(y\\), since the analysis conditions on the values of the covariates (Glynn and Laird (1986), White and Carlin (2010)). This class of missing data mechanisms includes missing not at random (MNAR), where the probability that a covariate is missing depends on the value of that covariate. However, CCA is biased if the probability of being a completer depends on \\(y\\) after conditioning on the covariates. A nice example of this particular topic and its implications for the analysis has been provided by professor Bartlett using some nice slides\n\rConclusions\rThe main virtue of case deletion is simplicity. If a missing data problem can be resolved by discarding only a small part of the sample, then the method can be quite effective. However, even in that situation, one should explore the data (Schafer and Graham (2002)). The discarded information from incomplete cases can be used to study whether the complete cases are plausibly a random subsample of the original sample, that is, whether MCAR is a reasonable assumption. A simple procedure is to compare the distribution of a particular variable \\(y\\) based on complete cases with the distribution of \\(y\\) based on incomplete cases for which \\(y\\) is recorded. Significant differences indicate that the MCAR assumption is invalid, and the complete-case analysis yields potentially biased estimates. Such tests are useful but have limited power when the sample of incomplete cases is small. Also the tests can offer no direct evidence on the validity of the missing at random (MAR) assumption.\n\rReferences\rGlynn, RJ, and NM Laird. 1986. “Regression Estimates and Missing Data: Complete Case Analysis.” Cambridge MA: Harvard School of Public Health, Department of Biostatistics.\n\rLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley \u0026amp; Sons.\n\rSchafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” Psychological Methods 7 (2): 147.\n\rWhite, Ian R, and John B Carlin. 2010. “Bias and Efficiency of Multiple Imputation Compared with Complete-Case Analysis for Missing Covariate Values.” Statistics in Medicine 29 (28): 2920–31.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"b43fa5b40709a7b7bdfe09bc43ee465e","permalink":"/missmethods/complete-case-analysis/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/complete-case-analysis/","section":"missmethods","summary":"Complete case analysis is the term used to describe a statistical analysis that only includes participants for which we do not have missing data on the variables of interest","tags":["Delete Case Methods","Complete Case Analysis","Listwise Deletion"],"title":"Complete Case Analysis","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\r\rAll case deletion methods, such as Complete Case Analysis(CCA) or Available Case Analysis(ACA) make no use of units with partially observed data, when estimating the marginal distribution of the variables under study or the covariation between variables. Clearly, this is inefficient and a tempting alternative would be to impute or “fill in” the unobserved data with some plausible values. When a single value is used to replace each missing data, we talk about Single Imputation(SI) methods and, according to the precedure used to generate these imputations, different SI methods can be used. In general, the idea of imputing the missing values is really appealing as it allows to recover the full sample on which standard complete data methods can be applied to derive the estimates of interest.\nHowever, it is important to be aware of the potential problems of imputing missing data without a clear understanding about the process underlying the values we want to impute, which is the key factor to determine whether the selected approach would be plausible in the context considered. Indeed, imputation should be conceptualised as draws from a predictive distribution of the missing values and require methods for creating a predictive distribution for the imputation based on the observed data. According to Little and Rubin (2019), these predictive distributions can be created using\nExplicit modelling, when the distribution is based on formal statistical models which make the underlying assumptions explicit.\n\rImplicit modelling, when the distribution is based on an algorithm which implicitly relies on some underlying model assumptions.\n\r\rIn this part, we focus on some of the most popular Explicit Single Imputation methods. These include: Mean Imputation(SI-M), where means from the observed data are used as imputed values; Regression Imputation(SI-R), where missing values are replaced with values predicited from a regression of the missing variable on some other observed variables; and Stochastic Regression Imputation(SI-SR), where unobserved data are substituted with the predicted values from a regression imputation plus a randomly selected residual drawn to reflect uncertainty in the predicted values.\nMean Imputation\rThe simplest type of SI-M consists in replacing the missing values in a variable with the mean of the observed units from the same variable, a method known as Unconditional Mean Imputation (Little and Rubin (2019),Schafer and Graham (2002)). Let \\(y_{ij}\\) be the value of variable \\(j\\) for unit \\(i\\), such that the unconditional mean based on the observed values of \\(y_j\\) is given by \\(\\bar{y}_j\\). The sample mean of the observed and imputed values is then \\(\\bar{y}^{m}_j=\\bar{y}^{ac}_j\\), i.e. the estimate from ACA, while the sample variance is given by\n\\[\rs^{m}_{j}=s^{ac}_{j}\\frac{(n^{ac}-1)}{(n-1)},\r\\]\nwhere \\(s^{ac}_j\\) is the sample variance estimated from the \\(n^{ac}\\) available units. Under a Missing Completely At Random(MCAR) assumption, \\(s^{ac}_j\\) is a consistent estimator of the tru variance so that the sample variance from the imputed data \\(s^m_j\\) systematically underestimates the true variance by a factor of \\(\\frac{(n^{ac}-1)}{(n-1)}\\), which clearly comes from the fact that missing data are imputed using values at the centre of the distribution. The imputation distorts theempirical distribution of the observed values as well as any quantities that are not linear in the data (e.g. variances, percentiles, measures of shape). The sampel covariance of \\(y_j\\) and \\(y_k\\) from the imputed data is\n\\[\rs^{m}_{jk}=s^{ac}_{jk}\\frac{(n^{as}_{jk}-1)}{(n-1)},\r\\]\nwhere \\(n^{ac}_{jk}\\) is the number of units with both variables observed and \\(s^{ac}_{jk}\\) is the corresponding covariance estimate from ACA. Under MCAR \\(s^{ac}_{jk}\\) is a consistent estimator of the true covariance, so that \\(s^{m}_{jk}\\) underestimates the magnitude of the covariance by a factor of \\(\\frac{(n^{ac}_{jk}-1)}{(n-1)}\\). Obvious adjustments for the variance (\\(\\frac{(n-1)}{(n^{ac}_j-1)}\\)) and the covariance (\\(\\frac{(n-1)}{(n^{ac}_{jk}-1)}\\)) yield ACA estimates, which could lead to covariance matrices that are not positive definite.\n\rRegression Imputation\rAn improvement over SI-M is to impute each missing data using the conditional means given the observed values, a method known SI-R or Conditional Mean Imputation. To be precise, it would also be possible to impute conditional means without using a regression approach, for example by grouping individuals into adjustment classes (analogous to weighting methods) based on the observed data and then impute the missing values using the observed means in each adjustment class (Little and Rubin (2019)). However, for the sake of simplicity, here we will assume that SI-R and conditional mean imputation are the same.\nTo generate imputations under SI-R, consider a set of \\(J-1\\) fully observed response variables \\(y_1,\\ldots,y_{J-1}\\) and a partially observed response variable \\(y_J\\) which has the first \\(n_{cc}\\) units observed and the remaiing \\(n-n_{cc}\\) units missing. SI-R computes the regression of \\(y_J\\) on \\(y_1,\\ldots,y_{J-1}\\) based on the \\(n_{cc}\\) complete units and then fills in the missing values as predictions from the regression. For example, for unit \\(i\\), the missing value \\(y_{iJ}\\) is imputed using\n\\[\r\\hat{y}_{iJ}=\\hat{\\beta}_{J0}+\\sum_{j=1}^{J-1}\\hat{\\beta}_{Jj}y_{ij},\r\\]\nwhere \\(\\hat{\\beta}_{J0}\\) is the intercept and \\(\\hat{\\beta}_{Jj}\\) is the \\(j\\) coefficient of of the regression of \\(y_J\\) on \\(y_1,\\ldots,y_{J-1}\\) based on the \\(n_{cc}\\) units.\nAn extension of regression imputation to a general pattern of missing data is known as Buck’s method (Buck (1960)). This approach first estimates the population mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\) from the sample mean and covariance matrix of the complete units and then uses these estimates to calculate the OLS regressions of the missing variables on the observed variables for each missing data pattern. Predictions of the missing data for each observation are obtained by replacing the values of the present variables in the regressions. The average of the observed and imputed values from this method are consistent estimates of the means and MCAR and mild assumptions about the moments of the distribution (Buck (1960)). They are also consistent when the missingness mechanism depends on observed variables, i.e. under a Missing At Random(MAR) assumption, although addtional assumptions are required in this case (e.g. using linear regressions it assumes that the “true” regression of the missing varables on the observed variables is linear).\nThe filled in data from Buck’s method typically yield reasonable estimates of means, while the sample variances and covariances are biased, although the bias is less than the one associated with unconditional mean imputation. Specifically, the sample variance \\(\\sigma^{2,SI-R}_j\\) from the imputed data underestimates the true variance \\(\\sigma^2_j\\) by a factor of \\(\\frac{1}{n-1}\\sum_{i=1}^n\\sigma^{2}_{ji}\\), where \\(\\sigma^{2}_{ji}\\) is the residual variance from regressing \\(y_j\\) on the variables observed in unit \\(i\\) if \\(y_{ij}\\) is missing and zero if \\(y_{ij}\\) is observed. The sample covariance of \\(y_j\\) and \\(y_k\\) has a bias of \\(\\frac{1}{n-1}\\sum_{i=1}^n\\sigma_{jki}\\), where \\(\\sigma_{jki}\\) is the residual covariance from the multivariate regression of \\((y_{ij},y_{ik})\\) on the variables observed in unit \\(i\\) if both variables are missing and zero otherwise. A consistent estimator of \\(\\Sigma\\) can be constructed under MCAR by replacing consistent estimates of \\(\\sigma^{2}_{ji}\\) and \\(\\sigma_{jki}\\) in the expressions for bias and then adding the resulting quantities to the sample covariance matrix of the filled-in data.\n\rStochastic Regression Imputation\rAny type of mean or regression imputation will lead to bias when the interest is in the tails of the distributions because “best prediction” imputation systematically underestimates variability and standard errors calculated from the imputed data are typically too small. These considerations suggest an alternative imputation strategy, where imputed values are drawn from a predictive distribution of a plausible set of values rather than from the centre of the distribution. This is the idea behind SI-SR, which imputes a conditional draw\n\\[\r\\hat{y}_{iJ}=\\hat{\\beta}_{J0}+\\sum_{j=1}^{J-1}\\hat{\\beta}_{Jj}y_{ij}+z_{iJ},\r\\]\nwhere \\(z_{iJ}\\) is a random normal deviate with mean 0 and variance \\(\\hat{\\sigma}^2_J\\), the residual variance from the regression of \\(y_J\\) on \\(y_1,\\ldots,y_{J-1}\\) based on the complete units. The addition of the random deviate makes the imputation a random draw from the predictive distribution of the missing values, rather than the mean, which is likely to ameliorate the distortion of the predictive distributions (Little and Rubin (2019)).\nExample\rConsider a bivariate normal monotone missing data with \\(y_1\\) fully observed and \\(y_2\\) missing for a fraction \\(\\lambda=\\frac{(n-n_{cc})}{n}\\) and a MCAR mechanism. The following table shows the large sample bias of standard OLS estimates obtained from the filled-in data about the mean, the variance of \\(y_2\\), the regression coefficient of \\(y_2\\) on \\(y_1\\), and the regression coefficient of \\(y_1\\) on \\(y_2\\), using four different single imputation methods: uncondtional mean (UM), unconditional draw (UD), conditional mean (CM), and conditional draw (CD).\n\rTable 1: Bivariate normal monotone MCAR data; large sample bias of four imputation methods.\r\r\r\r\rmu_2\r\rsigma_2\r\rbeta_21\r\rbeta_12\r\r\r\r\r\rUM\r\r0\r\r-lambda * sigma_2\r\r-lambda * beta_21\r\r0\r\r\r\rUD\r\r0\r\r0\r\r-lambda * beta_21\r\r-lambda * beta_21\r\r\r\rCM\r\r0\r\r-lambda * (1-rho^2) * sigma_2\r\r0\r\r((lambda * (1-rho^2)) / (1-lambda * (1-rho^2)) ) * beta_12\r\r\r\rCD\r\r0\r\r0\r\r0\r\r0\r\r\r\r\rUnder MCAR, all four methods yield consistent estimates of \\(\\mu_2\\) but both UM and CM underestimate the variance \\(\\sigma_2\\), UD leads to attenuation of the regression coefficients, while CD yields consistent estimates of all four parameters. However, CD has some important drawbacks. First, adding random draws to the conditional mean imputations is inefficient as the large sample variance of the CD estimates of \\(\\mu_2\\) can be shown (Little and Rubin (2019)) to be\n\\[\r\\frac{[1-\\lambda\\rho^2+(1-\\rho^2)\\lambda(1-\\lambda)]\\sigma_2}{n_{cc}},\r\\]\nwhich is larger than the large sample sampling variance of the CM estimate of \\(\\mu_2\\), namely \\(\\frac{[1-\\lambda\\rho^2]\\sigma_2}{n_{cc}}\\). Second, the standard errors of the CD estimates from the imputed data are too small because they do not incorporate imputation uncertainty.\nWhen the analysis involves units with some covariates missing and other observed, it is common practice to condition on the observed covariates when generating the imputations for the missing covariates. It is also possible to condition on the outcome \\(y\\) to impute missing covariates, even if the final objective is to regress \\(y\\) on the full set of covariates and conditioning on \\(y\\) will lead to bias when conditional means are imputed. However, if predictive draws are imputed, this approach will yield consistent estimates of the regression coefficients. Imputing missing covariates using the means by conditioning only the observed covariates (and not also on \\(y\\)) also yields consistent estimates of the regression coefficients under certain conditions, although these are typically less efficient then those from CCA, but yields inconsistent estimates of other parameters such as variances and correlations (Little (1992)).\n\r\rConclusions\rAccording to Little and Rubin (2019), imputation should generally be\nConditional on observed variables, to reduce bias, improve precision and preserve association between variables.\n\rMultivariate, to preserve association between missing variables.\n\rDraws from the predictive distributions rather than means, to provide valid estimates of a wide range of estimands.\n\r\rNevertheless, a main problem of SI methods is that inferences based on the imputed data do not account for imputation uncertainty and standard errors are therefore systematically underestimated, p-values of tests are too significant and confidence intervals are too narrow.\n\rReferences\rBuck, Samuel F. 1960. “A Method of Estimation of Missing Values in Multivariate Data Suitable for Use with an Electronic Computer.” Journal of the Royal Statistical Society: Series B (Methodological) 22 (2): 302–6.\n\rLittle, Roderick JA. 1992. “Regression with Missing X’s: A Review.” Journal of the American Statistical Association 87 (420): 1227–37.\n\rLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley \u0026amp; Sons.\n\rSchafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” Psychological Methods 7 (2): 147.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"dd5ff6f647ec08049ef781494fb9c395","permalink":"/missmethods/mean-imputation/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/mean-imputation/","section":"missmethods","summary":"Explicit Single imputation denotes a method based on an explicit model which replaces a missing datum with a single value. In this method the sample size is retrieved. However, the imputed values are assumed to be the real values that would have been observed when the data would have been complete","tags":["Explicit Single Imputation","Implicit Single Imputation","Single Imputation","Mean Imputation","Regression Imputation","Stochastic Regression Imputation"],"title":"Explicit Single Imputation","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rA possible approach to analyse missing data is to use methods based on the likelihood function under specific modelling assumptions. In this section, I review maximum likelihood methods based on fully observed data alone.\nMaximum Likelihood Methods for Complete Data\rLet \\(Y\\) denote the set of data, which are assumed to be generated according to a certain probability density function \\(f(Y= y,\\mid \\theta)=f(y \\mid \\theta)\\) indexed by the set of parameters \\(\\theta\\), which lies on the parameter space \\(\\Theta\\) (i.e. set of values of \\(\\theta\\) for which \\(f(y\\mid \\theta)\\) is a proper density function). The Likelihood function, indicated with \\(L(\\theta \\mid y)\\), is defined as any function of \\(\\theta \\in \\Theta\\) proportional that is to \\(f(y \\mid \\theta)\\). Note that, in contrast to the density function which is defined as a function of the data \\(Y\\) given the values of the parameters \\(\\theta\\), instead the likelihood is defined as a function of the parameters \\(\\theta\\) for fixed data \\(y\\). In addition, the loglikelihood function, indicated with \\(l(\\theta\\mid y)\\) is defined as the natural logarithm of \\(L(\\theta \\mid y)\\).\nUnivariate Normal Example\rThe joint density function of \\(n\\) independent and identially distributed units \\(y=(y_1,\\ldots,y_n)\\) from a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), is\n\\[\rf(y \\mid \\mu, \\sigma^2)=\\frac{1}{\\sqrt{\\left(2\\pi\\sigma^2\\right)^n}}\\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right),\r\\]\nand therefore the loglikelihood is\n\\[\rl(\\mu, \\sigma^2 \\mid y)= -\\frac{n}{2}\\text{ln}(2\\pi)-\\frac{n}{2}\\text{ln}(\\sigma^2)-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2},\r\\]\nwhich is considered as a function of \\(\\theta=(\\mu,\\sigma^2)\\) for fixed data \\(y\\).\n\rMultivariate Normal Example\rIf the sample considered has dimension \\(J\u0026gt;1\\), e.g. we have a set of idependent and identically distributed variables \\(y=(y_{ij})\\), for \\(i=1,\\ldots,n\\) units and \\(j=1,\\ldots,J\\) variables, which comes from a Multivariate Normal distribution with mean vector \\(\\mu=(\\mu_1,\\ldots\\mu_J)\\) and covariance matrix \\(\\Sigma=(\\sigma_{jk})\\) for $ j=1,,J, k=1,,K$ and \\(J=K\\), then its density function is\n\\[\rf(y \\mid \\mu, \\Sigma)=\\frac{1}{\\sqrt{\\left(2\\pi \\right)^{nK}\\left(\\mid \\Sigma \\mid \\right)^n}} \\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^{n}(y_i-\\mu)\\Sigma^{-1}(y_i-\\mu)^{T} \\right),\r\\]\nwhere \\(|\\Sigma|\\) denotes the determinant of the matrix \\(\\Sigma\\) and the superscript \\(T\\) denotes the transpose of a matrix or vector, while \\(y_i\\) denotes the row vector of observed values for unit \\(i\\). The loglikelihood of \\(\\theta=(\\mu,\\Sigma)\\) is\n\\[\rl(\\mu,\\Sigma \\mid y)= - \\frac{n}{2}\\text{ln}(2\\pi) - \\frac{n}{2}\\text{ln}(|\\Sigma|)-\\frac{1}{2}\\sum_{i=1}^{n}(y_i-\\mu)\\Sigma^{-1}(y_i-\\mu)^T.\r\\]\n\r\rMLE estimation\rFinding the maximum value of \\(\\theta\\) that is most likely to have generated the data \\(y\\), corresponding to maximising the likelihood or Maximum Likelihood Estimation(MLE), is a standard approach to make inference about \\(\\theta\\). Suppose a specific value for the parameter \\(\\hat{\\theta}\\) such that \\(L(\\hat{\\theta}\\mid y)\\geq L(\\theta \\mid y)\\) for any other value of \\(\\theta\\). This implies that the observed data \\(y\\) is at least as likely under \\(\\hat{\\theta}\\) as under any other value of \\(\\theta\\), i.e. \\(\\hat{\\theta}\\) is the value best supported by the data. More specifically, a maximum likelihood estimate of \\(\\theta\\) is a value of \\(\\theta \\in \\Theta\\) that maximises the likelihood \\(L(\\theta \\mid y)\\) or, equivalently, that maximises the loglikelihood \\(l(\\theta \\mid y)\\). In general, when the likelihood is differentiable and bounded from above, typically the MLE can be found by differentiating \\(L(\\theta \\mid y)\\) or \\(l(\\theta \\mid y)\\) with respect to \\(\\theta\\), setting the result equal to zero, and solving for \\(\\theta\\). The resulting equation, \\(D_l(\\theta)=\\frac{\\partial l(\\theta \\mid y)}{\\partial \\theta}=0\\), is known as the likelihood equation and the derivative of the loglikelihood as the score function. When \\(\\theta\\) consists in a set of \\(j=1,\\ldots,J\\) components, then the likelihood equation corresponds to a set of \\(J\\) simultaneous equations, obtained by differentiating \\(l(\\theta \\mid y)\\) with respect to each component of \\(\\theta\\).\nUnivariate Normal Example\rRecall that, for a Normal sample with \\(n\\) units, the loglikelihood is indexed by the set of parameters \\(\\theta=(\\mu,\\sigma^2)\\) and has the form\n\\[\rl(\\mu, \\sigma^2 \\mid y)= -\\frac{n}{2}\\text{ln}(2\\pi)-\\frac{n}{2}\\text{ln}(\\sigma^2)-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2}.\r\\]\nNext, the MLE can be derived by first differentiating \\(l(\\theta \\mid y)\\) with respect to \\(\\mu\\) and set the result equal to zero, that is\n\\[\r\\frac{\\partial l(\\theta \\mid y)}{\\partial \\mu}= -\\frac{2}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)(-1)=\\frac{\\sum_{i=1}^n y_i - n\\mu}{\\sigma^2}=0,\r\\]\nNext, after simplifying a bit, we can retrieve the solution\n\\[\r\\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^n y_i=\\bar{y},\r\\]\nwhich corresponds to the sample mean of the observations. Next, we differentiate \\(l(\\theta \\mid y)\\) with respect to \\(\\sigma^2\\), that is we set\n\\[\r\\frac{\\partial l(\\theta \\mid y)}{\\partial \\sigma^2}= -\\frac{n}{2\\sigma^2}+\\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (y_i-\\mu)^2=0.\r\\]\nWe then simplify and move things around to get\n\\[\r\\frac{1}{\\sigma^3}\\sum_{i=1}^n(y_i-\\mu)^2=\\frac{n}{\\sigma} \\;\\;\\; \\rightarrow \\;\\;\\; \\sigma^2=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\mu)^2.\r\\]\nFinally, we replace \\(\\mu\\) in the expression above with the value \\(\\hat{\\mu}=\\bar{y}\\) found before and obtain the solution\n\\[\r\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\bar{y})^2=s^2,\r\\]\nwhich, however, is a biased estimator of \\(\\sigma^2\\) and therefore is often replaced with the unbiased estimator \\(\\frac{s^2}{(n-1)}\\). In particular, given a population parameter \\(\\theta\\), the estimator \\(\\hat{\\theta}\\) for \\(\\theta\\) is said to be unbiased when \\(E[\\hat{\\theta}]=\\theta\\). This is the case, for example, of the sample mean \\(\\hat{\\mu}=\\bar{y}\\) which is an unbiased estimator of the population mean \\(\\mu\\):\n\\[\rE\\left[\\hat{\\mu} \\right]=E\\left[\\frac{1}{n}\\sum_{i=1}^n y_i \\right]=\\frac{1}{n}\\sum_{i=1}^n E\\left[y_i \\right]=\\frac{1}{n} (n\\mu)=\\mu.\r\\]\nHowever, this is not true for the sample variance \\(s^2\\). This can be seen by first rewriting the expression of the estimator as\n\\[\r\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n (y_i^2 -2y_i\\bar{y}+\\bar{y}^2)=\\frac{1}{n}\\sum_{i=1}^n y_i^2 -2\\bar{y}\\sum_{i=1}^n y_i + \\frac{1}{n}n\\bar{y}^2=\\frac{1}{n}\\sum_{i=1}^n y_i^2 - \\bar{y}^2,\r\\]\nand then by computing the expectation of this quantity:\n\\[\rE\\left[\\hat{\\sigma}^2 \\right]=E\\left[\\frac{1}{n}\\sum_{i=1}^n y_i^2 - \\bar{y}^2 \\right]=\\frac{1}{n}\\sum_{i=1}^n E\\left[y_i^2 \\right] - E\\left[\\bar{y}^2 \\right]=\\frac{1}{n}\\sum_{i=1}^n (\\sigma^2 + \\mu^2) - (\\frac{\\sigma^2}{n}+\\mu^2)=\\frac{1}{n}\\left(n\\sigma^2+n\\mu^2\\right) - \\frac{\\sigma^2}{n}-\\mu^2=\\frac{(n-1)\\sigma^2}{n}.\r\\]\nThe above result is obtained by pluggin in the expression for the variance of a general variable \\(y\\) and retrieving the expression for \\(E[y^2]\\) as a function of the variance and \\(E[y]^2\\). More specifically, given that\n\\[\rVar(y)=\\sigma^2=E\\left[y^2 \\right]-E\\left[y \\right]^2,\r\\]\nthen we know that for \\(y\\), \\(E\\left[y^2 \\right]=\\sigma^2+E[y]^2\\), and similarly we can derive the same expression for \\(\\bar{y}\\). However, we can see that \\(\\hat{\\sigma}^2\\) is biased by a factor of \\((n-1)/n\\). Thus, an unbiased estimator for \\(\\sigma^2\\) is given by multiplying \\(\\hat{\\sigma}^2\\) by \\(\\frac{n}{(n-1)}\\), which gives the unbiased estimator \\(\\hat{\\sigma}^{2\\star}=\\frac{s^2}{n-1}\\), where \\(E\\left[\\hat{\\sigma}^{2\\star}\\right]=\\sigma^2\\).\n\rMultivariate Normal Example\rThe same procedure can be applied to an independent and identically distributed multivariate sample \\(y=(y_{ij})\\), for \\(i=1,\\ldots,n\\) units and \\(j=1,\\ldots,J\\) variables (Anderson (1962),Rao et al. (1973),Gelman et al. (2013)). It can be shown that, maximising the loglikelihood with respect to \\(\\mu\\) and \\(\\Sigma\\) yields the MLEs\n\\[\r\\hat{\\mu}=\\bar{y} \\;\\;\\; \\text{and} \\;\\;\\; \\Sigma=\\frac{(n-1)\\hat{\\sigma}^{2\\star}}{n},\r\\]\nwhere \\(\\bar{y}=(\\bar{y}_1,\\ldots,\\bar{y}_{J})\\) is the row vectors of sample means and \\(\\hat{\\sigma}^{2\\star}=(s^{\\star_{jk}})\\) is the sample covariance matrix with \\(jk\\)-th element \\(s^\\star_{jk}=\\frac{\\Sigma_{i=1}^n(y_{ij} - \\bar{y}_j)}{(n-1)}\\). In addition, in general, given a function \\(g(\\theta)\\) of the parameter \\(\\theta\\), if \\(\\hat{\\theta}\\) is a MLE of \\(\\theta\\), then \\(g(\\hat{\\theta})\\) is a MLE of \\(g(\\theta)\\).\n\r\rConditional Distribution of a Bivariate Normal\rConsider an indpendent and identically distributed sample formed by two variables \\(y=(y_1,y_2)\\), each measured on \\(i=1\\ldots,n\\) units, which come from a Bivariate Normal distribution with mean vector and covariance matrix\n\\[\r\\mu=(\\mu_1,\\mu_2) \\;\\;\\; \\text{and} \\;\\;\\; \\Sigma = \\begin{pmatrix} \\sigma^2_1 \u0026amp; \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_2\\sigma_1 \u0026amp; \\sigma_2^2 \\ \\end{pmatrix},\r\\]\nwhere \\(\\rho\\) is a correlation parameter between the two variables. Thus, intuitive MLEs for these parameters are\n\\[\r\\hat{\\mu}_j=\\bar{y}_j \\;\\;\\; \\text{and} \\;\\;\\; \\hat{\\sigma}_{jk}=\\frac{(n-1)s_{jk}}{n},\r\\]\nwhere \\(\\sigma^2_j=\\sigma_{jj}\\), \\(\\rho\\sigma_{j}\\sigma_{k}=\\sigma_{jk}\\), for \\(j,k=1,2\\). By properites of the Bivariate Normal distribution (Ord and Stuart (1994)), the marginal distribution of \\(y_1\\) and the conditional distribution of \\(y_2 \\mid y_1\\) are\n\\[\ry_1 \\sim \\text{Normal}\\left(\\mu_1,\\sigma^2_1 \\right) \\;\\;\\; \\text{and} \\;\\;\\; y_2 \\mid y_1 \\sim \\text{Normal}\\left(\\mu_2 + \\beta(y_1-\\mu_1 \\right), \\sigma^2_2 - \\sigma^2_1\\beta^2),\r\\]\nwhere \\(\\beta=\\rho\\frac{\\sigma_2}{\\sigma_1}\\) is the parameter that quantifies the linear dependence between the two variables. The MLEs of \\(\\beta\\) and \\(\\sigma^2_2\\) can also be derived from the likelihood based on the conditional distribution of \\(y_2 \\mid y_1\\), which have strong connections with the least squares estimates derived in a multiple linear regression framework.\n\rMultiple Linear Regression\rSuppose the data consist in \\(n\\) units measured on an outcome variable \\(y\\) and a set of \\(J\\) covariates \\(x=(x_{1},\\ldots,x_{J})\\) and assume that the distribution of \\(y\\) given \\(x\\) is Normal with mean \\(\\mu_i=\\beta_0+\\sum_{j=1}^J\\beta_jx_{ij}\\) and variance \\(\\sigma^2\\). The loglikelihood of \\(\\theta=(\\beta,\\sigma^2)\\) given the observed data \\((y,x)\\) is given by\n\\[\rl(\\theta \\mid y) = -\\frac{n}{2}\\text{ln}(2\\pi) -\\frac{n}{2}\\text{ln}(\\sigma^2) - \\frac{\\sum_{i=1}^n \\left(y_i - \\mu_i \\right)^2}{2\\sigma^2}.\r\\]\nMaximising this expression with respect to \\(\\theta\\), the MLEs are found to be equal to the least squares estimates of the intercept and regression coefficients. Using a matrix notation for the \\(n\\)-th vector of the outcome values \\(Y\\) and the \\(n\\times (J+1)\\) matrix of the covariate values (including the constant term), then the MLEs are:\n\\[\r\\hat{\\beta}=(X^{T}X)^{-1}X^{T}Y \\;\\;\\; \\text{and} \\;\\;\\; \\hat{\\sigma}^{2}=\\frac{(Y-X\\hat{\\beta})(Y-X\\hat{\\beta})}{n},\r\\]\nwhere the numerator of the fraction is known as the Residual Sum of Squares(RSS). Because the denominator of is equal to \\(n\\), the MLE of \\(\\sigma^2\\) does not correct for the loss of degrees of freedom when estimating the \\(J+1\\) location parameters. Thus, the MLE should instead divide the RSS by \\(n-(J+1)\\) to obtain an unbiased estimator. An extension of standard multiple linear regression is the so called weighted multiple linear regression, in which the regression variance is assumed to be equal to\\(\\frac{\\sigma^2}{w_i}\\), for \\((w_i) \u0026gt; 0\\). Thus, the variable \\((y_i-\\mu)\\sqrt{w_i}\\) is Normally distributed with mean \\(0\\) and variance \\(\\sigma^2\\), and the loglikelihood is\n\\[\rl(\\theta \\mid y)= - \\frac{n}{2}\\text{ln}(2\\pi) - \\frac{n}{2}\\text{ln}(\\sigma^2) - \\frac{\\sum_{i=1}^n w_i(y_i - \\mu_i)^2}{2\\sigma^2}.\r\\]\nMaximising this function yields MLEs given by the weighted least squares estimates\n\\[\r\\hat{\\beta}=\\left(X^{T}WX\\right)^{-1}\\left(X^{T}WY \\right) \\;\\;\\; \\text{and} \\;\\;\\; \\sigma^{2}=\\frac{\\left(Y-X\\hat{\\beta}\\right)^{T}W\\left(Y-X\\hat{\\beta}\\right)}{n},\r\\]\nwhere \\(W=\\text{Diag}(w_1,\\ldots,w_n)\\).\n\rGeneralised Linear Models\rConsider the previous example where we had an outcome variable \\(y\\) and a set of \\(J\\) covariates, each measured on \\(n\\) units. A more general class of models, compare with the Normal model, assumes that, given \\(x\\), the values of \\(y\\) are an independent sample from a regular exponential family distribution\n\\[\rf(y \\mid x,\\beta,\\phi)=\\text{exp}\\left(\\frac{\\left(y\\delta\\left(x,\\beta \\right) - b\\left(\\delta\\left(x,\\beta\\right)\\right)\\right)}{\\phi} + c\\left(y,\\phi\\right)\\right),\r\\]\nwhere \\(\\delta()\\) and \\(b()\\) are known functions that determine the distribution of \\(y\\), and \\(c()\\) is a known function indexed by a scale parameter \\(\\phi\\). The mean of \\(y\\) is assumed to linearly relate to the covariates via\n\\[\rE\\left[y \\mid x,\\beta,\\phi \\right]=g^{-1}\\left(\\beta_0 + \\sum_{j=1}^J\\beta_jx_{j} \\right),\r\\]\nwhere \\(E\\left[y \\mid x,\\beta,\\phi \\right]=\\mu_i\\) and \\(g()\\) is a known one to one function which is called link function because it “links” the expectation of \\(y\\) to a linear combination of the covariates. The canonical link function\n\\[\rg_c(\\mu_i)=\\delta(x_{i},\\beta)=\\beta_0+\\sum_{j=1}^J\\beta_jx_{ij},\r\\]\nwhich is obtained by setting \\(g()\\) equal to the inverse of the derivative of \\(b()\\) with respect to its argument. Examples of canonical links include\n\rNormal linear regression: \\(g_c=\\text{identity}\\), \\(b(\\delta)=\\frac{\\delta^2}{2},\\phi=\\sigma^2\\)\n\rPoisson regression: \\(g_c=\\log\\), \\(b(\\delta)=\\text{exp}(\\delta),\\phi=1\\)\n\rLogistic regression: \\(g_c=\\text{logit}\\), \\(b(\\delta)=\\log(1+\\text{exp}(\\delta)),\\phi=1\\)\n\r\rThe loglikelihood of \\(\\theta=(\\beta,\\phi)\\) given the observed data \\((y,x)\\), is\n\\[\rl(\\theta \\mid y,x)=\\sum_{i=1}^n \\left[\\frac{\\left(y_i\\delta\\left(x_i,\\beta\\right)-b\\left(\\delta\\left(x_i,\\beta\\right)\\right) \\right)}{\\phi}+c\\left(y_i,\\phi\\right)\\right],\r\\]\nwhich for non-normal cases does not have explicit maxima and numerical maximisation can be achieved using iterative algorithms.\n\rReferences\rAnderson, Theodore Wilbur. 1962. “An Introduction to Multivariate Statistical Analysis.” Wiley New York.\n\rGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis. Chapman; Hall/CRC.\n\rOrd, Keith, and Alan Stuart. 1994. “Kendall’s Advanced Theory of Statistics: Distribution Theory.” Edward Arnold.\n\rRao, Calyampudi Radhakrishna, Calyampudi Radhakrishna Rao, Mathematischer Statistiker, Calyampudi Radhakrishna Rao, and Calyampudi Radhakrishna Rao. 1973. Linear Statistical Inference and Its Applications. Vol. 2. Wiley New York.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"27002a3bca8067a44c90f52f701d24fb","permalink":"/missmethods/likelihood-based-methods-ignorable/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/likelihood-based-methods-ignorable/","section":"missmethods","summary":"Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a probability distribution by maximising a likelihood function, so that under the assumed statistical model the observed data is most probable","tags":["Maximum Likelihood Estimation","Likelihood Based Methods Ignorable"],"title":"Introduction to Maximum Likelihood Estimation","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rIn certain cases, it is possible to reduce biases from case deletion by the application of weights. After incomplete cases are removed, the remaining complete cases can be weighted so that their distribution more closely resembles that of the full sample with respect to auxiliary variables. Weighting methods can eliminate bias due to differential response related to the variables used to model the response probabilities, but it cannot correct for biases related to variables that are unused or unmeasured (Little and Rubin (2019)). Robins, Rotnitzky, and Zhao (1994) introduced Inverse Probability Weighting (IPW) as a weighted regression approach that require an explicit model for the missingness but relaxes some of the parametric assumptions in the data model. Their method is an extension of Generalized Estimating Equations (GEE), a popular technique for modeling marginal or populationaveraged relationships between a response variable and predictors (Zeger, Liang, and Albert (1988)).\nLet \\(y_i=(y_{i1},\\ldots,y_{iK})\\) denote a vector of variables for unit \\(i\\) subject to missing values with \\(y_i\\) being fully observed for \\(i=1\\ldots,n_r\\) units and partially-observed for \\(i=n_r+1,\\ldots,n\\) units. Define \\(m_i=1\\) if \\(y_i\\) is incomplete and \\(m_i=0\\) if complete. Let \\(x_i=(x_{i1},\\ldots,x_{ip})\\) denote a vector of fully observed covariates and suppose the interest is in estimating the mean of the distribution of \\(y_i\\) given \\(x_i\\), having the form \\(g(x_i,\\beta)\\), where \\(g()\\) is a possibly non-linear regression function indexed by a parameter \\(\\beta\\) of dimension \\(d\\). Let also \\(z_i=(z_{i1},\\ldots,z_{iq})\\) be a vector of fully observed auxiliary variables that potentially predictive of missingness but are not included in the model for \\(y_i \\mid x_i\\). When there are no missing data, a consistent estimate of \\(\\beta\\) is given by the solution to the following GEE, under mild regularity conditions (Liang and Zeger (1986)),\n\\[\r\\sum_{i=1}^n = D_i(x_i,\\beta)(y_i-g(x_i,\\beta))=0,\r\\]\nwhere \\(D_i(x_i,\\beta)\\) is a suitably chosen \\((d\\times k)\\) matrix of known functions of \\(x_i\\). With missing data, the equation is applied only to the complete cases (\\(n_{r}\\)), which yields consistent estimates provided that\n\\[\rp(m_i=1 \\mid x_i,y_i,z_i,\\phi)=p(m_i=1\\mid x_i,\\phi),\r\\]\nthat is, missingness does not depend on \\(y_i\\) or \\(z_i\\) after conditioning on \\(x_i\\). IPW GEE methods (Robins and Rotnitzky (1995)) replace the equation with\n\\[\r\\sum_{i=1}^{n_r} = w_i(\\hat{\\alpha})D_i(x_i,\\beta)(y_i-g(x_i,\\beta))=0,\r\\]\nwhere \\(w_i(\\hat{\\alpha})=\\frac{1}{p(x_i,z_i \\mid \\hat{\\alpha})}\\), with \\(p(x_i,z_i \\mid \\hat{\\alpha})\\) being an estimate of the probability of being a complete unit, obtained for example via logistic regressions on \\(m_i\\) on \\(x_i\\) and \\(z_i\\). If the logistic regression is correctly specified, IPW GEE yields a consistent estimator of \\(\\beta\\) provided that\n\\[\rp(m_i=1 \\mid x_i,y_i,z_i,\\phi)=p(m_i=1\\mid x_i,z_i\\phi).\r\\]\nExample\rSuppose the full data consists of a single outcome variable \\(y\\) and an additional variable \\(z\\) and that the objective is to estimate the population outcome mean \\(\\mu=\\text{E}[y]\\). If data were fully observed for \\(i=1,\\ldots,n\\) individuals, an obvious estimator of \\(\\mu\\) would be the sample outcome mean\n\\[\r\\bar{y}=\\frac{1}{n}\\sum_{i=1}^ny_i,\r\\]\nwhich is equivalent to the solution to the estimating equation \\(\\sum_{i=1}^n(y_i-\\mu)=0\\). When \\(y\\) is partially observed (while \\(Z\\) is always fully observed), individuals may fall into one of two missingness patterns \\(r=(r_{y},r_{z})\\), namely \\(r=(1,1)\\) if both variables are observed or \\(r=(1,0)\\) if \\(y\\) is missing. Let \\(c=1\\) if \\(r=(1,1)\\) and \\(c=0\\) otherwise, so that the observed data can be summarised as \\((c,cy,z)\\). Assuming that missingness only depends on \\(z\\), that is\n\\[\rp(c=1 \\mid y,z)=p(c=1 \\mid z)=\\pi(z),\r\\]\nthen the missing data mechanism is Missing At Random (MAR). Under these conditions, the sample mean of the complete cases \\(\\bar{y}_{cc}=\\frac{\\sum_{i=1}^nc_iy_i}{c_i}\\), i.e. the solution to the equation \\(\\sum_{i=1}^nc_i(y_i-\\mu)=0\\), is not a consistent estimator of \\(\\mu\\). To correct for this, the IPW complete case estimating equation\n\\[\r\\sum_{i=1}^n\\frac{c_i}{\\pi(z_i)}(y_i-\\mu)=0,\r\\]\ncan be used to weight the contribution of each complete case by the inverse of \\(\\pi(z_i)\\). The solution of the equation corresponds to the IPW estimator\n\\[\r\\mu_{ipw}=\\left(\\sum_{i=1}^n \\frac{c_i}{\\pi(z_i)} \\right)^{-1} \\sum_{i=1}^n \\frac{c_iy_i}{\\pi(z_i)},\r\\]\nwhich is unbiased under MAR and for \\(\\pi(z)\u0026gt;0\\). In case you want to have a look at the proof of this I put here the link. In most situations \\(\\pi(z_i)\\) is not known and must be estimated from the data, typically posing some model for \\(p(c=1 \\mid z, \\hat{\\alpha})\\), indexed by some parameter \\(\\hat{\\alpha}\\), for example a logistic regression\n\\[\r\\text{logit}(\\pi)=\\alpha_0 + \\alpha_1z.\r\\]\nOf course, if the model for \\(\\pi(z)\\) is misspecified, \\(\\mu_{ipw}\\) can be an inconsistent estimator. In addition, IPW methods typically used data only from the completers discarding all the partially observed values, which is clearly inefficient.\n\rConclusions\rThus, IPW estimators can correct for the bias of unweighted estimators due to the dependence of the missingness mechanism on \\(z_i\\) (Schafer and Graham (2002)). The basic intuition of IPW methods is that each subject’s contribution to the weighted Complete Case Analysis (CCA) is replicated \\(w_i\\) times in order to account once for herself and \\((1-w_i)\\) times for those subjects with the same responses and covariates who are missing. These models are called semiparametric because they apart from requiring the regression equation to have a specific form, they do not specify any probability distribution for the response variable (Molenberghs et al. (2014)). Older GEE methods can accommodate missing values only if they are Missing Completely At Random (MCAR), while more recent methods allow them to be MAR or even Missing Not At Random (MNAR), provided that a model for the missingness is correctly specified (Robins, Rotnitzky, and Zhao (1995),Rotnitzky, Robins, and Scharfstein (1998)).\n\rReferences\rLiang, Kung-Yee, and Scott L Zeger. 1986. “Longitudinal Data Analysis Using Generalized Linear Models.” Biometrika 73 (1): 13–22.\n\rLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley \u0026amp; Sons.\n\rMolenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. Handbook of Missing Data Methodology. Chapman; Hall/CRC.\n\rRobins, James M, and Andrea Rotnitzky. 1995. “Semiparametric Efficiency in Multivariate Regression Models with Missing Data.” Journal of the American Statistical Association 90 (429): 122–29.\n\rRobins, James M, Andrea Rotnitzky, and Lue Ping Zhao. 1994. “Estimation of Regression Coefficients When Some Regressors Are Not Always Observed.” Journal of the American Statistical Association 89 (427): 846–66.\n\r———. 1995. “Analysis of Semiparametric Regression Models for Repeated Outcomes in the Presence of Missing Data.” Journal of the American Statistical Association 90 (429): 106–21.\n\rRotnitzky, Andrea, James M Robins, and Daniel O Scharfstein. 1998. “Semiparametric Regression for Repeated Outcomes with Nonignorable Nonresponse.” Journal of the American Statistical Association 93 (444): 1321–39.\n\rSchafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” Psychological Methods 7 (2): 147.\n\rZeger, Scott L, Kung-Yee Liang, and Paul S Albert. 1988. “Models for Longitudinal Data: A Generalized Estimating Equation Approach.” Biometrics, 1049–60.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"b7412e7bb27e826cf74f8b74c863ac86","permalink":"/missmethods/inverse-probability-weighting/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/inverse-probability-weighting/","section":"missmethods","summary":"The inverse probability weighting (IPW) approach preserves the semiparametric structure of the underlying model of substantive interest and clearly separates the model of substantive interest from the model used to account for the missing data","tags":["Weighting Methods","Semiparametric Methods","Weighting Adjustments","Inverse Probability Weighting","Augmented Inverse Probability Weighting"],"title":"Inverse Probability Weighting","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rIn many cases, analysis methods for missing data are based on the ignorable likelihood\n\\[\rL_{ign}\\left(\\theta \\mid Y_0, X \\right) \\propto f\\left(Y_0 \\mid X, \\theta \\right),\r\\]\nregarded as a function of the parameters \\(\\theta\\) for fixed observed data \\(Y_0\\) and some fully observed covariates \\(X\\). The density \\(f(Y_0 \\mid X, \\theta)\\) is obtained by integrating out the missing data \\(Y_1\\) from the joint density \\(f(Y \\mid X, \\theta)=f(Y_0,Y_1\\mid X, \\theta)\\). Sufficient conditions for basing inference about \\(\\theta\\) on the ignorbale likelihood are that the missingness mechanism is Missing At Random(MAR) and the parameters of the model of analysis \\(\\theta\\) and those of the missingness mechanism \\(\\psi\\) are distinct. Here we focus our attention on the situations where the missingness mechanism is Missing Not At Random(MNAR) and valid Maximum Likelihood(ML), Bayesian and Multiple Imputation(MI) inferences generally need to be based on the full likelihood\n\\[\rL_{full}\\left(\\theta, \\psi \\mid Y_0, X, M \\right) \\propto f\\left(Y_0, M \\mid X, \\theta, \\psi \\right),\r\\]\nregarded as a function of \\((\\theta,\\psi)\\) for fixed \\((Y_0,M)\\). Here, \\(f(Y_0,M\\mid \\theta, \\psi)\\) is obtained by integrating out \\(Y_1\\) from the joint density \\(f(Y,M \\mid X, \\theta, \\psi)\\). Two main approaches for formulating MNAR models can be distinguished, namely selection models(SM) and pattern mixture models(PMM).\nSelection and Pattern Mixture Models\rSMs factor the joint distribution of \\(m_i\\) and \\(y_i\\) as\n\\[\rf(m_i,y_i \\mid x_i, \\theta, \\psi) = f(y_i \\mid x_i, \\theta)f(m_i \\mid x_i,y_i,\\psi),\r\\]\nwhere the first factor is the distribution of \\(y_i\\) in the population while the second factor is the missingness mechanism, with \\(\\theta\\) and \\(\\psi\\) which are assumed to be distinct. Alternatively, PMMs factor the joint distribution as\n\\[\rf(m_i,y_i \\mid x_i, \\theta, \\psi) = f(y_i \\mid x_i, m_i,\\xi)f(m_i \\mid x_i),\r\\]\nwhere the first factor is the distribution of \\(y_i\\) in the strata defined by different patterns of missingness \\(m_i\\) while the second factor models the probabilities of the different patterns, with \\(\\xi\\) which are assumed to be distinct (Little (1993),Little and Rubin (2019)). The distinction between the two factorisations becomes clearer when considering a specific example.\nSuppose thta missing values are confined to a single variable and let \\(y_i=(y_{i,1},y_{i2})\\) be a bivariate response outcome where \\(y_{i1}\\) is fully observed and \\(y_{i2}\\) is observed for \\(i=1,\\ldots,n_{cc}\\) but missing for \\(i=n_{cc}+1,\\ldots,n\\). Let \\(m_{i2}\\) be the missingness indicator for \\(y_{i2}\\), then a PMM factors the denisty of \\(Y_0\\) and \\(M\\) given \\(X\\) as\n\\[\rf(y_0, M \\mid X, \\xi)=\\prod_{i=1}^{n_{cc}}f(y_{i1},y_{i2}\\mid x_i, m_{i2}=0,\\xi)Pr(m_{i2}=0 \\mid x_i, \\omega) \\times \\prod_{i=n_{cc}+1}^{n}f(y_{i1} \\mid x_i, m_{i2}=1,\\xi)Pr(m_{i2}=1 \\mid x_i, \\omega).\r\\]\nThis expression shows that there are no data with which to estimate directly the distribution \\(f(y_{i2} \\mid x_i, m_{i2}=1,\\xi)\\), because all units with \\(m_{i2}=1\\) have \\(y_{i2}\\) missing. Under MAR, this is identified using the distribution of the observed data \\(f(y_{i2} \\mid x_i, m_{i2}=1,\\xi)=f(y_{i2} \\mid x_i, m_{i2}=0,\\xi)\\), while under MNAR it must be identified using other assumptions. The SM formulation is\n\\[\rf(y_i, m_{i2} \\mid \\theta, \\psi) = f(y_{i1} \\mid x_i, \\theta)f(y_{i2} \\mid x_i, y_{i1},\\theta)f(m_{i2}\\mid x_i,y_{i1},y_{i2},\\psi).\r\\]\nTypically, the missingness mechanism \\(f(m_{i2} \\mid x_i,y_{i1},y_{i2},\\psi)\\) is modelled using some additive probit or logit regression of \\(m_{i2}\\) on \\(x_i\\),\\(y_{i1}\\) and \\(y_{i2}\\). However, the coefficient of \\(y_{i2}\\) in this regression is not directly estimable from the data and hence the model cannot be fully estimated without extra assumptions.\nNormal Models for MNAR data\rAssume we have a complete sample \\((y_i,x_i)\\) on a continuous variable \\(Y\\) and a set of fully observed covariates \\(X\\), for \\(i=1,\\ldots,n\\). Suppose that \\(i=1,\\ldots,n_{cc}\\) units are observed while the remaining \\(i=n_{cc}+1,\\ldots,n\\) units are missing, with \\(m_i\\) being the corresponding missingness indicator. Heckman (Heckman (1976)) proposed the following selection model to handle missingness:\n\\[\ry_i \\mid x_i, \\theta, \\psi \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2) \\;\\;\\; \\text{and} \\;\\;\\; m_i \\mid x_i,y_i,\\theta,\\psi \\sim Bern\\left(\\Phi(\\psi_0 + \\psi_1x_i + \\psi_2y_i) \\right),\r\\]\nwhere \\(\\theta=(\\beta_0,\\beta_1,\\sigma^2)\\) and \\(\\Phi\\) denotes the probit (cumulative normal) distribution function. Note that if \\(\\psi_2=0\\), the missing data are MAR, while if \\(\\psi_2 \\neq 0\\) the missing data are MNAR since missingness in \\(Y\\) depends on the unobserved value of \\(Y\\). This model can be estimated using either a two-step least squares method, ML in combination with an EM algorithm, or a Bayesian approach. The main issue is the lack of information about \\(\\psi_2\\), which can be partly identified through the specific assumptions about the distribution of the observed data of \\(Y\\). This, however, makes the implicit assumption that the assumed distribution can well described the distribution of the complete (observed and missing) data which can never be tested or checked. An alternative approach is to use a PMM factorisation and model:\n\\[\ry_i \\mid m_i=m,x_i,\\xi,\\omega \\sim N(\\beta_0^m + \\beta_1^mx_i, \\sigma^{2m})\\;\\;\\; \\text{and} \\;\\;\\; m_i \\mid x_i,\\xi,\\omega \\sim Bern\\left(\\Phi(\\omega_0 + \\omega_1x_i) \\right),\r\\]\nwhere \\(\\xi=(\\beta_0^m,\\beta_1^m,\\sigma^{2m},\\;\\;\\; m=0,1)\\). This model implies that the distribution of \\(y_i\\) given \\(x_i\\) in the population is a mixture of two normal distributions with mean\n\\[\r\\left[1 - \\Phi(\\omega_0 + \\omega_1x_i) \\right] \\left[\\beta_0^0 + \\beta_1^0 x_i \\right] + \\left[\\Phi(\\omega_0 + \\omega_1x_i) \\right] \\left[\\beta_0^1 + \\beta_1^1 x_i \\right].\r\\]\nThe parameters \\((\\beta_0^0,\\beta_1^0,\\sigma^{20},\\omega)\\) can be estimated from the data but the parameters \\((\\beta_0^1,\\beta_1^1,\\sigma^{21})\\) are not estimable because \\(y_i\\) is missing when \\(m_i=1\\). Under MAR, the distribution of \\(Y\\) given \\(X\\) is the same for units with \\(Y\\) observed and missing, such that \\(\\beta_0^0=\\beta_0^1=\\beta_0\\) (as well as for \\(\\beta_1\\) and \\(\\sigma^2\\)). Under MNAR, other assumptions are needed to esitmate the parameters indexed by \\(m=1\\).\nSome final considerations:\n\rBoth SM and PMM model the joint distribution of \\(Y\\) and \\(M\\).\n\rThe SM formulation is more natural when the substantive interest concerns the relationship between \\(Y\\) and \\(X\\) in the population. However, these parameters can also be derived in PMM by averaging the patterns specific parameters over the missingness patterns.\n\rThe PMM factorisation is more transparent in terms of the underlying assumptions about the unidentified parameters of the model, while SM tends to impose some obscure constraints in order to identify these parameters, which are also difficult to interpret.\n\rGiven specific assumptions to identify all the parameters in the model, PMMs are often easier to fit than SMs. In addition, imputations of the missing values are based on the predictive distribution of \\(Y\\) given \\(X\\) and \\(M=0\\).\n\r\rThese considerations seem to favour PMM over SM as MNAR approaches, especially when considering sensitivity analysis. Bayesian approaches can also be used to identify these models, by assigning prior distributions which can be used to identify those parameters which cannot be estimated from the data. Justifications for the choice of these priors are therefore necessary to ensure the plausibility of the assumptions assessed and the impact of these assumptions on the posterior inference.\n\r\rReferences\rHeckman, James J. 1976. “The Common Structure of Statistical Models of Truncation, Sample Selection and Limited Dependent Variables and a Simple Estimator for Such Models.” In Annals of Economic and Social Measurement, Volume 5, Number 4, 475–92. NBER.\n\rLittle, Roderick JA. 1993. “Pattern-Mixture Models for Multivariate Incomplete Data.” Journal of the American Statistical Association 88 (421): 125–34.\n\rLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley \u0026amp; Sons.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"aa1d3630c280c833da24f17594133eb3","permalink":"/missmethods/likelihood-based-methods-nonignorable/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/likelihood-based-methods-nonignorable/","section":"missmethods","summary":"Specific methods are required to make inference under nonignorable nonresponse assumptions, that is when the value of the variable that is missing is related to some values which are not observed by the analyst (e.g. the missing values themselves)","tags":["Maximum Likelihood Estimation","Bayesian Inference","Likelihood Based Methods Nonignorable"],"title":"Likelihood Based Inference with Incomplete Data (Nonignorable)","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rMultiple Imputation(MI) refers to the procedure of replacing each missing value by a set of \\(H\\geq 2\\) imputed values. These are ordered in the sense that \\(H\\) completed data sets can be created from the sets of imputations, where the first imputed value replaces the missing value in the first completed data set, the second imputed value in the second completed data set, and so on. Next, standard complete data methods are used to analyse each completed data set. When the \\(H\\) sets of imputations are repeated random draws from the predictive distribution of the missing data under a particular model of missingness, the \\(H\\) completed data inferences can be combined to form one inference that properly reflects uncertainty due to missing values under that model. In general, MI procedures can be summarised in three main steps:\nSpecify an imputation model to generate \\(H\\) imputed values, typically taken as random draws from the predictive distribution of the missing values given the observed values, and create \\(H\\) completed data sets using these imputations and the observed data.\n\rAnalyse each completed data sets using standard complete data methods based on an analysis model, and derive \\(H\\) completed data inferences\n\rPool together the \\(H\\) completed data inferences into a single inference using standard MI formulas, which ensure that missing data uncertainty is taken into account\n\r\rMi was first proposed by Rubin (Rubin (1978)) and has become more popular over time (Rubin (1996), Schafer and Graham (2002), Little and Rubin (2019)), as well as the focus of research for methodological and practical applications in a variety of fields (Herzog and Rubin (1983), Rubin and Schenker (1987), Schafer (1999), Carpenter and Kenward (2012), Molenberghs et al. (2014), Van Buuren (2018)). MI shares both advantages of Single Imputaiton (SI) methods and solves both disadvantages. Indeed, like SI, MI methods allow the analyst to use familiar complete data methods when analysing the completed data sets. The only disadvantage of MI compared with SI methods is that it takes more time to generate the imputations and analyse the completed data sets. However, Rubin (2004) showed that in order to obtain sufficiently precise estimates, a relatively small number of imputations (typically \\(10\\)) is required. For example, considering a situation with \\(\\lambda=50\\%\\) missing information and \\(H=10\\) imputations, the efficiency of MI can be shown to be equal to \\((1+\\frac{\\lambda}{H})^{-1}=95\\%\\). In addition, in today’s computing environments, the work of analysing the completed data sets is quite modest since it involves performing the same task \\(H\\) times. Thus, once a precedure to combine multiple completed data sets is established, the additonal time and effort to handle \\(50\\), \\(20\\), or \\(10\\) imputations if often of little consequence.\nIn the first step of MI, imputations should ideally be created as repeated draws from the posterior predictive distribution of the missing values \\(y_{mis}\\) given the observed values \\(y_{obs}\\), each repetition being an independent drawing of the parameters and missing values. In practice, implicit imputation models can also be used in place of explicit imputation models (Herzog and Rubin (1983)). In the second step, each completed data set is analysed using the same complete data method that would be used in the absence of missingness. Finally, in the last step, standard procedures should be used to combine the compelted data inferences into a single one. The simplest and most popular method for combining the reuslts of \\(H\\) completed data sets is known as Rubin’s rules (Rubin (2004)), which can be explained with a simple example.\nRubin’s rules\rLet \\(\\hat{\\theta}_h\\) and \\(V_h\\), for \\(h=1,\\ldots,H\\), be the completed data estimates and sampling variances for a scalar estimand \\(\\theta\\), calculated from \\(H\\) repeated imputations under a given imputation model. Then, according to Rubin’s rules, the combined estimate is simply the average of the \\(H\\) completed data estimates, that is\n\\[\r\\bar{\\theta}_{H}=\\frac{1}{H}\\sum_{h=1}^{H}\\hat{\\theta}_{h}.\r\\]\nBecause the imputations under MI are conditional draws, under a good imputaton model, they provide valid estimates for a wide range of estimands. In addition, the averaging over \\(H\\) imputed data sets increases the efficiency of estimation over that obtained from a single completed data set. The variability associated with the pooled estimate has two components: the average within-imputation variance \\(\\bar{V}_H\\) and the between-imputation variance \\(B_H\\), defined as\n\\[\r\\bar{V}_{H}=\\frac{1}{H}\\sum_{h=1}^{H}V_{h} \\;\\;\\; \\text{and} \\;\\;\\; B_{H}=\\frac{1}{H-1}\\sum_{h=1}^{H}(\\hat{\\theta}_{h}-\\bar{\\theta}_{H})^2.\r\\]\nThe total variability associated with \\(\\bar{\\theta}_H\\) is the computed as\n\\[\rT_{H}=\\bar{V}_H + \\frac{H+1}{H}B_{H},\r\\]\nwhere \\((1+\\frac{1}{H})\\) is an adjustment factor for finite due to estimating \\(\\theta\\) by \\(\\bar{\\theta}_H\\). Thus, \\(\\hat{\\lambda}_H=(1+\\frac{1}{H})\\frac{B_H}{T_H}\\) is known as the fraction of missing information and is an estimate of the fraction of information about \\(\\theta\\) that is missing due to nonresponse. For large sample sizes and scalar quantities like \\(\\theta\\), the reference distribution for interval estimates and significance tests is a \\(t\\) distribution\n\\[\r(\\theta - \\bar{\\theta}_H)\\frac{1}{\\sqrt{T^2_H}} \\sim t_v,\r\\]\nwhere the degrees of freedom \\(v\\) can be approximated with the quantity \\(v=(H-1)\\left(1+\\frac{1}{H+1}\\frac{\\bar{V}_H}{B_H} \\right)^2\\) (Rubin and Schenker (1987)). In small data sets, an improved version of \\(v\\) can be obtained as \\(v^\\star=(\\frac{1}{v}+\\frac{1}{\\hat{v}_{obs}})^{-1}\\), where\n\\[\r\\hat{v}_{obs}=(1-\\hat{\\lambda}_{H})\\left(\\frac{v_{com}+1}{v_{com}+3}\\right)v_{com},\r\\]\nwith \\(v_{com}\\) being the degrees of freedom for appropriate or exact \\(t\\) inferences about \\(\\theta\\) when there are no missing values (Barnard and Rubin (1999)).\nThe validity of MI rests on how the imputations are created and how that procedure relates to the model used to subsequently analyze the data. Creating MIs often requires special algorithms (Schafer (1997)). In general, they should be drawn from a distribution for the missing data that reflects uncertainty about the parameters of the data model. Recall that with SI methods, it is desirable to impute from the conditional distribution \\(p(y_{mis}\\mid y_{obs},\\hat{\\theta})\\), where \\(\\hat{\\theta}\\) is an estimate derived from the observed data. MI extends this approach by first simulating \\(H\\) independent plausible values for the parameters \\(\\theta_1,\\ldots,\\theta_H\\) and then drawing the missing values \\(y_{mis}^h\\) from \\(p(y_{mis}\\mid y_{obs}, \\theta_h)\\). Treating parameters as random rather than fixed is\ran essential part of MI. For this reason, it is natural (but not essential) to motivate MI from the Bayesian perspective, in which the state of knowledge about parameters is represented through a posterior distribution.\n\rMultiple Imputation by Chained Equations\rMI by Chained Equations, also known as Fully Conditional Specification(FCS), imputes multivariate missing data on a variable-by-variable basis, and therefore requires the specification of an imputation model for each incomplete variable to create imputations per variable in an iterative fashion (Van Buuren (2007)). In contrast to Joint MI, MICE specifies the multivariate distribution for the outcome and missingness pattern \\(p(y,r\\mid \\theta, \\phi)\\), indexed by the parameter vectors of the outcome (\\(\\theta\\)) and missingness models (\\(\\phi\\)), through a set of conditional densities \\(p(y_j \\mid y_{-j},r,\\theta_j, \\phi_j)\\), which is used to impute \\(y_j\\) given the other variables. Starting from a random draw from the marginal distribution of \\(y_1\\), imputation is then carried out by iterating over the conditionally specified imputation models for each \\(y_j=(y_2,\\ldots,y_J)\\) separately given the set of all the other variables \\(y_{-j}\\).\nTha main idea of MICE is to directly draw the missing data from the predictive distribution of conditional densities, therefore avoiding the need to specify a joint multivariate model for all the data. Different approaches can be used to implement MICE. For example, a possible strategy is the following:\nStart at iteration \\(t=0\\) by drawing randomly from the the distribution of the missing data given the observed data and all other variables, according to some probability model for each variable \\(y_j\\), that is\r\r\\[\r\\hat{y}^{mis}_{j,0} \\sim p(y^{mis}_{j} \\mid y^{obs}_{j}, y_{-j}, r)\r\\]\nAt each iteration \\(t=1,\\ldots,T\\) and for each variable \\(j=\\ldots,J\\), set\r\r\\[\r\\hat{y}^{mis}_{-j,t}=\\left(\\hat{y}_{1,t},\\ldots, \\hat{y}_{j-1,t}, \\hat{y}_{j+1,t}, \\ldots, \\hat{y}_{J,t} \\right)\r\\]\nas the currently completed data except \\(y_j\\)\nDraw \\(h=1,\\ldots,H\\) imputations for each variable \\(y_j\\) from the predictive distribution of the missing data given the observed data and the currently imputed data at \\(t\\), that is\r\r\\[\r\\hat{y}^{mis}_{j,t} \\sim p(y^{mis}_{j} \\mid y^{obs}_{j}, \\hat{y}_{-j,t}, r)\r\\]\nand repeat the steps 2 and 3 until convergence. It is important to stress out that MICE is essentially a Markov Chain Monte Carlo(MCMC) algorithm (Brooks et al. (2011)), where the state space is the collection of all imputed values. More specifically, when the conditional distributions of all variables are compatible with a joint multivariate distribution, the algorithm corresponds to a Gibbs sampler, a Bayesian simulation method that samples from the conditional distributions in order to obtain samples from the joint multivariate distribution of all variables via some conditional factorisation of the latter (Casella and George (1992), Gilks, Richardson, and Spiegelhalter (1996)). A potential issue of MICE is that, since the conditional distributions are specified freely by the user, these may not be compatible with a joint distribution and therefore it is not clear from which distribution the algorithm is sampling from. However, a general advatage of MICE is that it gives freedom to the user for the specification of the univariate models for the variables, which can be tailored to handle different types of variabes (e.g. continuous and categorical) and different statistical issues for each variable (e.g. skewness and non-liner associations).\nRegardless of the theoretical implications of MICE, as a MCMC method, the algorithm converges to a stationary distribution when three conditions are satisfied (Roberts (1996),Brooks et al. (2011)):\n\rThe chain is irreducible, i.e. must be able to reach any state from any state in the state space\n\rThe chain is aperiodic, i.e. must be able to return to each state after some unknown number of steps or transitions\n\rThe chain is recurrent, i.e. there is probability of one of eventually returning to each state after some number of steps\n\r\rTypically periodicity and non-recurrence can be a problem in MICE when the imputation models are not compatible, possibly leading to different inferences based on the stopping point of the chain or to non-stationary behaviours of the chain.\n\rReferences\rBarnard, John, and Donald B Rubin. 1999. “Miscellanea. Small-Sample Degrees of Freedom with Multiple Imputation.” Biometrika 86 (4): 948–55.\n\rBrooks, Steve, Andrew Gelman, Galin Jones, and Xiao-Li Meng. 2011. Handbook of Markov Chain Monte Carlo. CRC press.\n\rCarpenter, James, and Michael Kenward. 2012. Multiple Imputation and Its Application. John Wiley \u0026amp; Sons.\n\rCasella, George, and Edward I George. 1992. “Explaining the Gibbs Sampler.” The American Statistician 46 (3): 167–74.\n\rGilks, Walter R, Sylvia Richardson, and David J Spiegelhalter. 1996. “Introducing Markov Chain Monte Carlo.” Markov Chain Monte Carlo in Practice 1: 19.\n\rHerzog, Thomas N, and Donald B Rubin. 1983. “Using Multiple Imputations to Handle Nonresponse in Sample Surveys.” Incomplete Data in Sample Surveys 2: 209–45.\n\rLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley \u0026amp; Sons.\n\rMolenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. Handbook of Missing Data Methodology. Chapman; Hall/CRC.\n\rRoberts, Gareth O. 1996. “Markov Chain Concepts Related to Sampling Algorithms.” Markov Chain Monte Carlo in Practice 57.\n\rRubin, Donald B. 1978. “Multiple Imputations in Sample Surveys a Phenomenological Bayesian Approach to Nonresponse.” Proceedings of the Survey Research Methods Section of the American Statistical Association 1: 20–34.\n\r———. 1996. “Multiple Imputation After 18 Years.” Journal of the American Statistical Association 91 (434): 473–89.\n\r———. 2004. Multiple Imputation for Nonresponse in Surveys. John Wiley \u0026amp; Sons.\n\rRubin, Donald B, and Nathaniel Schenker. 1987. “Interval Estimation from Multiply Imputed Data: A Case Study Using Census Agriculture Industry Codes.” Journal of Official Statistics 3 (4): 375.\n\rSchafer, Joseph L. 1997. Analysis of Incomplete Multivariate Data. Chapman; Hall/CRC.\n\r———. 1999. “Multiple Imputation: A Primer.” Statistical Methods in Medical Research 8 (1): 3–15.\n\rSchafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” Psychological Methods 7 (2): 147.\n\rVan Buuren, Stef. 2007. “Multiple Imputation of Discrete and Continuous Data by Fully Conditional Specification.” Statistical Methods in Medical Research 16 (3): 219–42.\n\r———. 2018. Flexible Imputation of Missing Data. Chapman; Hall/CRC.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d05d59bacb71b6211d6709c985f21444","permalink":"/missmethods/multiple-imputation-by-chained-equations/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/multiple-imputation-by-chained-equations/","section":"missmethods","summary":"Multiple Imputation by Chained Equations (MICE) allows most models to be fit to a dataset with missing values on the independent and/or dependent variables, and provides rigorous standard errors for the fitted parameters. The basic idea is to treat each variable with missing values as the dependent variable in a regression, with some or all of the remaining variables as its predictors","tags":["Joint Multiple Imputation","Multiple Imputation by Chained Equations","Multiple Imputation"],"title":"Multiple Imputation by Chained Equations","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rA general problem associated with the implementatio of Inverse Probability Weighting (IPW) methods is that information in some available data is ignored by focussing only on the complete cases (Schafer and Graham (2002)). This has provided room to extend these methods to make a more efficient use of the available information through the incorporation of an “augmentation” term, which lead to the development of the so called Augmented Inverse Probability Weighting (AIPW) methods. These approaches extend IPW methods by creating predictions from a model to recover the information in the incomplete units and applying IPW to the residuals from the model (Little and Rubin (2019)).\nConsidering the IPW Generalised Estimating Equation (GEE)\n\\[\r\\sum_{i=1}^{n_r} = w_i(\\hat{\\alpha})D_i(x_i,\\beta)(y_i-g(x_i,\\beta))=0,\r\\]\nwhere \\(w_i(\\hat{\\alpha})=\\frac{1}{p(x_i,z_i \\mid \\hat{\\alpha})}\\), with \\(p(x_i,z_i \\mid \\hat{\\alpha})\\) an estimate of the probability of being a complete unit estimated for example using logistic regressions of the missingness indicator \\(m_i\\) on the vectors of the covariate and auxiliary variables \\(x_i\\) and \\(z_i\\), respectively. A problem of this IPW estimator is that it has poor small sample properties when the propensity score gets close to zero or one for some observations, which will lead to high variance in the estimator. AIPW methods can provide estimators of \\(\\beta\\) which are more efficient than their nonaugmented IPW versions. In general, AIPW estimating functions provide a method for constructing estimators of \\(\\beta\\) based on two terms:\nThe usual IPW term \\(p(x_i,z_i \\mid \\hat{\\alpha})\\)\n\rAn augmentation term \\(g^\\star(x_i,\\beta)\\)\n\r\rThe basis for the first term is a complete data unbiased estimating function for \\(\\beta\\), whereas the basis for the second term is some function of the observed data chosen so it has conditional mean of zero given the complete data (Molenberghs et al. (2014)).\nDoubly Robust Estimators\rAn important class of AIPW methods is known as doubly robust estimators, which have desirable robustness properties (Robins, Rotnitzky, and Laan (2000),Robins and Rotnitzky (2001)). The key feature of these estimators is that they relax the assumption that the model of the missingness probabilities is correctly specified, although requiring additional assumptions on the model for \\(y_i \\mid x_i\\). For example, doubly robust estimators for a population mean parameter \\(\\mu\\) could be obtained as follows:\nFit a logistic regression model for the probability of observing \\(y_i\\) as a function of \\(x_i\\) and \\(z_i\\) to derive the individual weights \\(w_i(\\hat{\\alpha})\\).\n\rFit a generalized linear model for the outcome of responders in function of \\(x_i\\) using weights \\(w_i(\\hat{\\alpha})\\) and let \\(g^\\star(x_i,\\beta)\\) denote the fitted values for subject \\(i\\).\n\rTake the sample average of the fitted values \\(g^\\star(x_i,\\beta)\\) of both respondents and nonrespondents as an estimate of the population mean \\(\\hat{\\mu}\\)\n\r\rDoubly robust estimators require the specification of two models: one for the missingness probability and another for the distribution of the incomplete data. When the augmentation term \\(g^\\star(x_i,\\beta)\\) is selected and modelled correctly according to the distribution of the complete data, the resulting estimator of \\(\\beta\\) is consistent even if the model of missingness is misspecified. On the other hand, if the model of missingness is correctly specified, the augmentation term no longer needs to be correctly specified to yield consistent estimators of \\(\\beta\\) (Scharfstein, Daniels, and Robins (2003),Bang and Robins (2005)). Doubly robust estimators therefore allow to obtain an unbiased estimating function for \\(\\beta\\) if either the model for the incomplete data or the model for the missingness mechanism has been correctly specified.\n\rExample\rSuppose the full data consists of a single outcome variable \\(y\\) and an additional variable \\(z\\) and that the objective is to estimate the population outcome mean \\(\\mu=\\text{E}[y]\\). When \\(y\\) is partially observed (while \\(Z\\) is always fully observed), individuals may fall into one of two missingness patterns \\(r=(r_{y},r_{z})\\), namely \\(r=(1,1)\\) if both variables are observed or \\(r=(1,0)\\) if \\(y\\) is missing. Let \\(c=1\\) if \\(r=(1,1)\\) and \\(c=0\\) otherwise, so that the observed data can be summarised as \\((c,cy,z)\\). Assuming that missingness only depends on \\(z\\), that is\n\\[\rp(c=1 \\mid y,z)=p(c=1 \\mid z)=\\pi(z),\r\\]\nthen the missing data mechanism is Missing At Random (MAR). Under these conditions, consider the consistent IPW complete case estimating equation\n\\[\r\\sum_{i=1}^n\\frac{c_i}{\\pi(z_i \\mid \\hat{\\alpha})}(y_i-\\mu)=0,\r\\]\nwhich can be used to weight the contribution of each complete case by the inverse of \\(\\pi(z_i \\mid \\hat{\\alpha})\\), typically estimated via logistic regressions. A general problem of this type of estimators is that they discard all the available data among the non-completers and are therefore inefficient. However, it is possible to augment the simple IPW complete case estimating equation to improve efficiency. The optimal estimator for \\(\\mu\\) within this class is the solution to the estimating equation\n\\[\r\\sum_{i=1}^n \\left(\\frac{c_i}{\\pi(z_i \\mid \\hat{\\alpha})}(y_i-\\mu) - \\frac{c_i-\\pi(z_i \\mid \\hat{\\alpha})}{\\pi(z_i \\mid \\hat{\\alpha})}\\text{E}[(y_i-\\mu)\\mid z_i] \\right),\r\\]\nwhich leads to the estimator\n\\[\r\\mu_{aipw}=\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{c_iy_i}{\\pi(z_i\\mid \\hat{\\alpha})} - \\frac{c_i - \\pi(z_i\\mid \\hat{\\alpha})}{\\pi(z_i\\mid \\hat{\\alpha})} \\text{E}[y_i \\mid z_i] \\right).\r\\]\nThe conditional expectation \\(\\text{E}[y_i \\mid z_i]\\) is not known and must be estimated from the data. Under a Missing At Random (MAR) assumption we have that \\(\\text{E}[y \\mid z]=\\text{E}[y \\mid z, c=1]\\), that is the conditional expecation of \\(y\\) given \\(z\\) is the same as that among the completers. Thus, we can specify a model \\(m(z,\\xi)\\) for \\(\\text{E}[y \\mid z]\\), indexed by the parameter \\(\\xi\\), that can be estimated from the completers. If \\(y\\) is continuous, a simple choice is to estimate \\(\\hat{\\xi}\\) by OLS from the completers. The AIPW estimator for \\(\\mu\\) then becomes\n\\[\r\\mu_{aipw}=\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{c_iy_i}{\\pi(z_i\\mid \\hat{\\alpha})} - \\frac{c_i - \\pi(z_i\\mid \\hat{\\alpha})}{\\pi(z_i\\mid \\hat{\\alpha})} m(z_i\\mid \\hat{\\xi}) \\right).\r\\]\nIt can be shown that this estimator is more efficient that the simple IPW complete case estimator for \\(\\mu\\) and that it has a double robustness property. This ensures that \\(\\mu_{aipw}\\) is a consitent estimator of \\(\\mu\\) if either\n\rthe model \\(\\pi(z\\mid\\alpha)\\) is correctly specified, or\n\rthe model \\(m(z\\mid \\xi)\\) is correctly specified.\n\r\rTo see a derivation of the double robustness property I put here a link to some nice paper.\n\rConlcusions\rAs all weighting methods, such as IPW, AIPW methods are semiparametric methods that aim to achieve robustness and good performance over more general classes of population distributions. However, semiparametric estimators can be less efficient and less powerful than Maximum Likelihood or Bayesian estimators under a well specified parametric model. With missing data, Rubin (1976) results show that likelihood-based methods perform uniformly well over any Missing At Random (MAR) missingness distribution, and the user does not need to specify that distribution. However, semiparametric methods that relax assumptions about the data must in turn assume a specific form for the distribution of missingness. It has been argued that, for these semiparametric methods to gain a substantial advantage over well-specified likelihood methods, the parametric model has to be grossly misspecified (Meng (2000)).\n\rReferences\rBang, Heejung, and James M Robins. 2005. “Doubly Robust Estimation in Missing Data and Causal Inference Models.” Biometrics 61 (4): 962–73.\n\rLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley \u0026amp; Sons.\n\rMeng, Xiao-Li. 2000. “Missing Data: Dial M for???” Journal of the American Statistical Association 95 (452): 1325–30.\n\rMolenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. Handbook of Missing Data Methodology. Chapman; Hall/CRC.\n\rRobins, James M, and Andrea Rotnitzky. 2001. “Comment on the Bickel and Kwon Article,‘Inference for Semiparametric Models: Some Questions and an Answer’.” Statistica Sinica 11 (4): 920–36.\n\rRobins, James M, Andrea Rotnitzky, and Mark van der Laan. 2000. “On Profile Likelihood: Comment.” Journal of the American Statistical Association 95 (450): 477–82.\n\rRubin, Donald B. 1976. “Inference and Missing Data.” Biometrika 63 (3): 581–92.\n\rSchafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” Psychological Methods 7 (2): 147.\n\rScharfstein, Daniel O, Michael J Daniels, and James M Robins. 2003. “Incorporating Prior Beliefs About Selection Bias into the Analysis of Randomized Trials with Missing Outcomes.” Biostatistics 4 (4): 495–512.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d2220861f56419a1996bd91e28893150","permalink":"/missmethods/augmented-inverse-probability-weighting/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/augmented-inverse-probability-weighting/","section":"missmethods","summary":"Augmented Inverse Probability Weighting (AIPW) is a IPW technique that derives estimators using a combination of the propensity score and the regression model. This approach has the attractive doubly robust property that estimators are consistent as long as either the propensity score or the outcome regression model is correctly specified","tags":["Weighting Methods","Semiparametric Methods","Weighting Adjustments","Inverse Probability Weighting","Augmented Inverse Probability Weighting"],"title":"Augmented Inverse Probability Weighting","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rComplete case analysis (CCA) can be particularly inefficient for data sets with a large number of variables which are partially observed. An alternative approach that can be used to conduct univariate analyses in known as Available Case Analysis (ACA), which uses all the available cases, separately for each variable under examination, to estimate the quantities of interest.\nThe main drawback of ACA is that the sample used to perform the analysis varies from variable to variable according to the patterns of missing data, which generates problems of comparability across variables if the missingness mechanism is not missing completely at random (MCAR), i.e. the missing data probabilities depend on the variables under study. While estimates of means and variances can be easily computed, measures of covariation need to be adjusted. In particular, for estimating sample covariances, this approach is known as pairwise deletion or pairwise inclusion\nPairwise measures of covariation\rOne possible approach to estimate pairwise measures of covariation for \\(y_j\\) and \\(y_k\\) is to use only those units \\(i=1,\\ldots,n_{ac}\\) for which both variables are observed (Little and Rubin (2019)). For example, one can compute pairwise sample covariances as:\n\\[\rs^{ac}_{jk} = \\frac{\\sum_{i \\in I_{ac}}(y_{ij}-\\bar{y}_{j}^{ac})(y_{ik}-\\bar{y}_{k}^{ac})}{(n_{ac}-1)},\r\\]\nwhere \\(I_{ac}\\) is the set of \\(n_{ac}\\) with both \\(y_j\\) and \\(y_k\\) observed, while the sample means \\(\\bar{y}^{ac}_{j}\\) and \\(\\bar{y}^{ac}_{k}\\) are calculated over this set of units. We can also estimate the sample correlation\n\\[\rr^{\\star}_{jk} = \\frac{s^{ac}_{jk}}{\\sqrt{s^2_{j}s^{2}_{k}}},\r\\]\nwhere \\(s^2_{j}\\) and \\(s^2_{k}\\) are the sample variances computed over the sets of observed units \\(I_{j}\\) and \\(I_{k}\\), respectively. A problem of this type of correlation estimate is that it can lie outside the range \\((-1,1)\\), which is typically addressed by computing pairwise correlations (Wilks (1932)), where variances are estimated from the set of units with both variables observed \\(I_{jk}\\), i.e. \\[\rr^{ac}_{jk} = \\frac{s^{ac}_{jk}}{\\sqrt{s^{2,ac}_{j}s^{2,ac}_{k}}}.\r\\]\nIn addition, we could also replace the sample means \\(\\bar{y}^{ac}_{j}\\) and \\(\\bar{y}^{ac}_{k}\\), evaluated on the common set of units \\(I_{jk}\\), with \\(\\bar{y}_{j}\\) and \\(\\bar{y}_{k}\\), which are evaluated on the sets of units \\(I_{j}\\) and \\(I_{k}\\), respectively. This leads to the following estimates for the sample covariances (Matthai (1951)):\n\\[\rs^{\\star}_{jk} = \\frac{\\sum_{i \\in I_{ac}}(y_{ij}-\\bar{y}_{j})(y_{ik}-\\bar{y}_{k})}{(n_{ac}-1)},\r\\]\nPairwise AC estimates aim at recovering information from partially-observed units that are lost by CCA. However, when considered together, the estimates suffer from inconsistencies that undermine the validity of these methods. For example, pairwise correlation matrices may be not positive definite. Because parameters are estimated from different sets of units, different approaches can be used to obtain estimate of the measures of uncertainty (Schafer and Graham (2002)).\n\rConclusions\rAC estimates allow to make use of all the available evidence in the data and may be more efficient that CCA when the missingness mechanism is MCAR and correlations are modest (Kim and Curry (1977)). However, when correlations are more substantial, ACA may become even less efficient than CCA (Haitovsky (1968), Azen and Van Guilder (1981)).\n\rReferences\rAzen, S, and M Van Guilder. 1981. “Conclusions Regarding Algorithms for Handling Incomplete Data.” 1981 Proceedings of the Statistical Computing Section, 53–56.\n\rHaitovsky, Yoel. 1968. “Missing Data in Regression Analysis.” Journal of the Royal Statistical Society: Series B (Methodological) 30 (1): 67–82.\n\rKim, Jae-On, and James Curry. 1977. “The Treatment of Missing Data in Multivariate Analysis.” Sociological Methods \u0026amp; Research 6 (2): 215–40.\n\rLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley \u0026amp; Sons.\n\rMatthai, Abraham. 1951. “Estimation of Parameters from Incomplete Data with Application to Design of Sample Surveys.” Sankhyā: The Indian Journal of Statistics, 145–52.\n\rSchafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” Psychological Methods 7 (2): 147.\n\rWilks, Samuel S. 1932. “Moments and Distributions of Estimates of Population Parameters from Fragmentary Samples.” The Annals of Mathematical Statistics 3 (3): 163–95.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"0095ad23651f3b9fc6ef8bdbe5528a6b","permalink":"/missmethods/available-case-analysis/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/available-case-analysis/","section":"missmethods","summary":"Available-case analysis also arises when a researcher simply excludes a variable or set of variables from the analysis because of their missing-data rates","tags":["Delete Case Methods","Available Case Analysis","Listwise Deletion","Complete Case Analysis"],"title":"Available Case Analysis","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rAll case deletion methods, such as Complete Case Analysis(CCA) or Available Case Analysis(ACA) make no use of units with partially observed data, when estimating the marginal distribution of the variables under study or the covariation between variables. Clearly, this is inefficient and a tempting alternative would be to impute or “fill in” the unobserved data with some plausible values. When a single value is used to replace each missing data, we talk about Single Imputation(SI) methods and, according to the precedure used to generate these imputations, different SI methods can be used. In general, the idea of imputing the missing values is really appealing as it allows to recover the full sample on which standard complete data methods can be applied to derive the estimates of interest.\nHowever, it is important to be aware of the potential problems of imputing missing data without a clear understanding about the process underlying the values we want to impute, which is the key factor to determine whether the selected approach would be plausible in the context considered. Indeed, imputation should be conceptualised as draws from a predictive distribution of the missing values and require methods for creating a predictive distribution for the imputation based on the observed data. According to Little and Rubin (2019), these predictive distributions can be created using\nExplicit modelling, when the distribution is based on formal statistical models which make the underlying assumptions explicit.\n\rImplicit modelling, when the distribution is based on an algorithm which implicitly relies on some underlying model assumptions.\n\r\rIn this part, we focus on some of the most popular Implicit Single Imputation methods. These include: Hot Deck Imputation(SI-HD), where missing values are imputed using observed values from similar responding units in the sample; Substitution(SI-S), where nonresponding units are replaced with alternative units not yet selected into the sample; Cold Deck Imputation(SI-CD), where missing values are replaced with a constant value from an external source; Composite Methods, which combine procedures from the previous approaches. We will specifically focus on SI-HD methods, which are the most popular among these.\nHot Deck Imputation\rSI-HD procedures refer to the deck of match Hollerith cards for the donors available for a nonrespondent. Suppose that a sample of \\(n\\) out of \\(N\\) units is selected and that \\(n_{cc}\\) out of \\(n\\) are recorded. Given an equal probability sampling scheme, the mean of \\(y\\) can be estimated from the filled-in data as the mean of the responding and the imputed units\n\\[\r\\bar{y}_{HD}=\\frac{(n_{cc}\\bar{y}_{cc}+(n-n_{cc})\\bar{y}^{\\star})}{n},\r\\]\nwhere \\(\\bar{y}_{cc}\\) is the mean of the responding units, and \\(\\bar{y}^\\star=\\sum_{i=1}^{n_{cc}}\\frac{H_iy_i}{n-n_{cc}}\\). \\(H_i\\) is the number of times \\(y_i\\) is used as substitute for a missing value of \\(y\\), with \\(\\sum_{i=1}^{n_{cc}}H_i=n-n_{cc}\\) being the number of missing units. The proprties of \\(bar{y}_{HD}\\) depend on the procedure used to generate the numbers \\(H_i\\) and in general the mean and sampling variance of this estimator can be written as\n\\[\rE[\\bar{y}_{HD}]=E[E[\\bar{y}_{HD}\\mid y_{obs}]] ;;; \\text{and} ;;; Var(\\bar{y}_{HD})=Var(E[\\bar{y}_{HD} \\mid y_{obs}]) + E[Var(\\bar{y}_{HD} \\mid y_{obs})],\r\\]\nwhere the inner expectations and variances are taken over the distribution of \\(H_i\\) given the observed data \\(y_{obs}\\), and the outer expectations and variances are taken over the model distribution of \\(y\\). The term \\(E[Var(\\bar{y}_{HD} \\mid y_{obs})]\\) represents the additional sampling variance from the stochastic imputation procedure. Examples of these procedures include predictive mean matching or PMM(Little and Rubin (2019)) and last value carried forward or LVCF(Little and Rubin (2019)).\nPredictive Mean Matching\rA general approach to hot-deck imputation is to define a metric \\(d(i,j)\\) measuring the distance between units based on observed variables \\(x_{i1},\\ldots,x_{iJ}\\) and then choose the imputed values that come from responding units close to the unit with the missing value, i.e. we choose the imputed value for \\(y_i\\) from a donor pool of units \\(j\\) that are such that \\(y_j,x_1,\\ldots,x_J\\) are observed and \\(d(i,j)\\) is less than some value \\(d_0\\). Varying the value for \\(d_0\\) can control the number of available donors \\(j\\). When the choice of the metric has the form\n\\[\rd(i,j)=(\\hat{y}(x_i)-\\hat{y}(x_j))^2,\r\\]\nwhere \\(\\hat{y}(x_i)\\) is the predicted value of \\(y\\) from the regression of \\(y\\) on \\(x\\) from the complete units, then the procedure is known as PMM. A powerful aspect of this metric is that it weights predictors according to their ability to predict the missing variable, which allows to have some protection against misspecification of the regression of \\(y\\) on \\(x\\), even though better approaches are available when good matches to donor units cannot be found or the sample size is small.\n\rLast Value Carried Forward\rLongitudinal data are often subject to attrition when units leave the study prematurely. Let \\(y_i=(y_{i1},\\ldots,y_{iJ})\\) be a \\((J\\times1)\\) vector of partially-observed outcomes for subject \\(i\\), and denote with \\(y_{i,obs}\\) and \\(y_{i,mis}\\) the observed and missing components of \\(y_i\\), i.e. \\(y=(y_{i,obs},y_{i,mis})\\). Define the indicator variable \\(m_i\\) taking value 0 for complete units and \\(j\\) if subject \\(i\\) drops out between \\(j-1\\) and \\(j\\) time points. LVCF, also called last observation carried forward(Pocock (2013)), imputes all missing values for individual \\(i\\) (for whom \\(m_i=j\\)) using the last recorded value for that unit, that is\n\\[\r\\hat{y}_{it}=y_{i,j-1},\r\\]\nwhere \\(t=j,\\ldots,J\\). Although simple, this approach makes the often unrealistic assumption that the value of the outcome remains unchanged after dropout.\n\r\rConclusions\rAccording to Little and Rubin (2019), imputation should generally be\nConditional on observed variables, to reduce bias, improve precision and preserve association between variables.\n\rMultivariate, to preserve association between missing variables.\n\rDraws from the predictive distributions rather than means, to provide valid estimates of a wide range of estimands.\n\r\rNevertheless, a main problem of SI methods is that inferences based on the imputed data do not account for imputation uncertainty and standard errors are therefore systematically underestimated, p-values of tests are too significant and confidence intervals are too narrow.\n\rReferences\rLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley \u0026amp; Sons.\n\rPocock, Stuart J. 2013. Clinical Trials: A Practical Approach. John Wiley \u0026amp; Sons.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"c767296d98fa02fb04c7ada48d9ef33d","permalink":"/missmethods/last-value-carried-forward/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/last-value-carried-forward/","section":"missmethods","summary":"Implicit Single imputation denotes a method not based on an explicit model which replaces a missing datum with a single value. In this method the sample size is retrieved. However, the imputed values are assumed to be the real values that would have been observed when the data would have been complete","tags":["Explicit Single Imputation","Implicit Single Imputation","Single Imputation","Hot Deck Methods","Predictive Mean Matching","Last Value Carried Forward"],"title":"Implicit Single Imputation","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rBayesian inference offers a convenient framework to analyse missing data as it draws no distinction between missing values and parameters, both interprted as unobserved quantities who are associated with a joint posterior distribution conditional on the observed data. In this section, I review basic concepts of Bayesian inference based on fully observed data, with notation and structure mostly taken from Gelman et al. (2013).\nBayesian Inference for Complete Data\rBayesian inference is the process of fitting a probability model to a set of data \\(Y\\) and summarising the results by a probability distribution on the parameters \\(\\theta\\) of the model and on unobserved quantities \\(\\tilde{Y}\\) (e.g. predictions). Indeed, Bayesian statistical conclusions about \\(\\theta\\) (or \\(\\tilde{Y}\\)) are made in terms of probability statements, conditional on the observed data \\(Y\\), typically indicated with the notation \\(p(\\theta \\mid y)\\) or \\(p(\\tilde{y} \\mid y)\\). Conditioning on the observed data is what makes Bayesian inference different from standard statistical approaches which are instead based on the retrospective evaluation of the procedures used to estimate \\(\\theta\\) (or \\(\\tilde{y}\\)) over the distribution of possible \\(y\\) values conditional on the “true” unknown value of \\(\\theta\\).\nBayes’ Rule\rIn order to make probability statements about \\(\\theta\\) given \\(y\\), we start with a model providing a joint probability distribution \\(p(\\theta,y)\\). Thus, the joint probability mass or density function can be written as a product of two densities that are often referred to as the prior distribution \\(p(\\theta)\\) and the sampling distribution \\(p(y \\mid \\theta)\\), respectively:\n\\[\rp(\\theta,y) = p(\\theta)p(y \\mid \\theta),\r\\]\nand conditioning on the observed values of \\(y\\), using the basic property of conditional probability known as Bayes’ rule, yields the posterior distribution\n\\[\rp(\\theta \\mid y) = \\frac{p(\\theta,y)}{p(y)} = \\frac{p(\\theta)p(y \\mid \\theta)}{p(y)},\r\\]\nwhere \\(p(y)=\\sum_{\\theta \\in \\Theta}p(\\theta)p(y\\mid \\theta)\\) is the sum (or integral in the case of continous \\(\\theta\\)) over all possible values of \\(\\theta\\) in the sample space \\(\\Theta\\). We can approximate the above equation by omitting the factor \\(p(y)\\) which does not depend on \\(\\theta\\) and, given \\(y\\), can be considered as fixed, yielding the unnormalised posterior density\n\\[\rp(\\theta \\mid y) \\propto p(\\theta) p(y \\mid \\theta),\r\\]\nwith the purpose of the analysis being to develop the model \\(p(\\theta,y)\\) and adequately summarise \\(p(\\theta \\mid y)\\).\n\rUnivariate Normal Example (known variance)\rLet \\(y=(y_1,\\ldots,y_n)\\) denote an independent and identially distributed sample of \\(n\\) units, which are assumed to come from a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), whose sampling density function is\n\\[\rp(y \\mid \\mu)=\\frac{1}{\\sqrt{\\left(2\\pi\\sigma^2\\right)^n}}\\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right),\r\\]\nwhere for the moment we assume the variance \\(\\sigma^2\\) to be known (i.e. constant). Consider now a prior probability distribution for the mean parameter \\(p(\\mu)\\), which belongs to the family of conjugate prior densities, for example a Normal distribution, and parameterised in terms of a prior mean \\(\\mu_0\\) and variance \\(\\sigma^2_0\\). Thus, its prior density function is\n\\[\rp(\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}}\\text{exp}\\left(-\\frac{1}{2}\\frac{(\\mu -\\mu_0)^2}{\\sigma^2} \\right),\r\\]\nunder the assumption tha the hyperparameters \\(\\mu_0\\) and \\(\\sigma^2_0\\) are known. The conjugate prior density implies that the posterior distribution for \\(\\mu\\) (with \\(\\sigma^2\\) assumed constant) belongs to the same family of distributions of the sampling function, that is Normal, but some algebra is required to reveal its specific form. In particular, the posterior density is\n\\[\rp(\\mu \\mid y) = \\frac{p(\\mu)p(y\\mid \\mu)}{p(y)} \\propto \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}}\\frac{1}{\\sqrt{\\left(2\\pi\\sigma^2\\right)^n}}\\text{exp}\\left(-\\frac{1}{2} \\left[\\frac{(\\mu - \\mu_0)^2}{\\sigma^2_0} + \\sum_{i=1}^n\\frac{(y_i-\\mu)^2}{\\sigma^2} \\right] \\right).\r\\]\nExapanding the components, collecting terms and completing the square in \\(\\mu\\) gives\n\\[\rp(\\mu \\mid y) \\propto \\text{exp}\\left(-\\frac{(\\mu - \\mu_1)}{2\\tau^2_1} \\right),\r\\]\nthat is the posterior distribution of \\(\\mu\\) given \\(y\\) is Normal with posterior mean \\(\\mu_1\\) and variance \\(\\tau^2_1\\), where\n\\[\r\\mu_1 = \\frac{\\frac{1}{\\tau^2_0}\\mu_0 + \\frac{n}{\\sigma^2}\\bar{y}}{\\frac{1}{\\tau^2_0} + \\frac{n}{\\sigma^2}} \\;\\;\\; \\text{and} \\;\\;\\; \\frac{1}{\\tau^2_1}=\\frac{1}{\\tau^2_0} + \\frac{n}{\\sigma^2}.\r\\]\nWe can see that the posterior distribution depends on \\(y\\) only through the sample mean \\(\\bar{y}=\\sum_{i=1}^ny_i\\), which is a sufficient statistic in this model. When working with Normal distributions, the inverse of the variance plays a prominent role and is called the precision and, from the above expressions, it can be seen that for normal data and prior, the posterior precision \\(\\frac{1}{\\tau^2_1}\\) equals the sum of the prior precision \\(\\frac{1}{\\tau^2_0}\\) and the sampling precision \\(\\frac{n}{\\sigma^2}\\). Thus, when \\(n\\) is large, the posterior precision is largely dominated by \\(\\sigma^2\\) and the sample mean \\(\\bar{y}\\) compared to the corresponding prior parameters. In the specific case where \\(\\tau^2_0=\\sigma^2\\), the prior has the same weight as one extra observation with the value of \\(\\mu_0\\) and, as \\(n\\rightarrow\\infty\\), we have that \\(p(\\mu\\mid y)\\approx N\\left(\\mu \\mid \\bar{y},\\frac{\\sigma^2}{n}\\right)\\).\n\rUnivariate Normal Example (unknown variance)\rFor \\(p(y \\mid \\mu,\\sigma^2)=N(y \\mid \\mu, \\sigma^2)\\) with \\(\\mu\\) known and \\(\\sigma^2\\) unknown, the sampling distribution for a vector \\(y\\) of \\(n\\) units is\n\\[\rp(y \\mid \\sigma^2)=\\frac{1}{\\sqrt{\\left(2\\pi\\sigma^2\\right)^n}}\\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right),\r\\]\nwith the corresponding conjugate prior for \\(\\sigma^2\\) being the Inverse-Gamma distribution \\(\\Gamma^{-1}(\\alpha,\\beta)\\) with density function\n\\[\rp(\\sigma^2) \\propto (\\sigma^2)^{-(\\alpha+1)}\\text{exp}\\left(-\\frac{\\beta}{\\sigma^2}\\right),\r\\]\nindexed by the hyperparameters \\(\\alpha\\) and \\(\\beta\\). A convenient parameterisation is as a Scaled Inverse-Chi Squared distribution \\(\\text{Inv-}\\chi^2(\\sigma^2_0,\\nu_0)\\) with scale and degrees of freedom parameters \\(\\sigma^2_0\\) and \\(\\nu_0\\), respectively. This means that the prior on \\(\\sigma^2\\) corresponds to the distribution of \\(\\frac{\\sigma^2_0 \\nu_0}{X}\\), where \\(X\\sim \\chi^2_{\\nu_0}\\) random variable. After some calculations, the resulting posterior for \\(\\sigma^2\\) is\n\\[\rp(\\sigma^2 \\mid y) \\propto (\\sigma^2)^\\left(\\frac{n+\\nu_0}{2}+1\\right)\\text{exp}\\left(-\\frac{\\nu_0 \\sigma^2_0 + n \\nu}{2\\sigma^2} \\right)\r\\]\nwhere \\(\\nu=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\mu)^2\\). This corresponds to say that\n\\[\r\\sigma^2 \\mid y \\sim \\text{Inv-}\\chi^2\\left(\\nu_0 +n, \\frac{\\nu_0\\sigma^2_0+n\\nu}{\\nu_0 + n} \\right),\r\\]\nwith scale equal to the degrees of freedom-weighted average of the prior and data scales and degrees of freedom equal to the sum of the prior and data degrees of freedom.\n\rUnivariate Normal Example (unknown mean and variance)\rSuppose now that both the mean and variance parameters are unknown such that\n\\[\rp(y \\mid \\mu, \\sigma^2) \\sim N(\\mu, \\sigma^2),\r\\]\nand that the interest is centred on making inference about \\(\\mu\\), that is we seek the conditional posterior distribution of the parameters of interest given the observed data \\(p(\\mu \\mid y)\\). This can be derived from the joint posterior distribution density \\(p(\\mu, \\sigma^2 \\mid y)\\) by averaging over all possible values of \\(\\sigma^2\\), that is\n\\[\rp(\\mu \\mid y)=\\int p(\\mu, \\sigma^2 \\mid y)d\\sigma^2,\r\\]\nor, alternatively, the joint posterior can be factored as the product of the marginal distribution of one parameter and the conditional distribution of the other given the former and then taking the average over the values of the “nuisance” parameter\n\\[\rp(\\mu \\mid y)=\\int p(\\mu \\mid \\sigma^2, y)p(\\sigma^2 \\mid y)d\\sigma^2.\r\\]\nThe integral forms are rarely computed in practice but this expression helps us to understand that posterior distributions can be expressed in terms of the product of marginal and conditional densities, first drawing \\(\\sigma^2\\) from its marginal and then \\(\\mu\\) from its conditional given the drawn value of \\(\\sigma^2\\), so that the integration is indirectly performed. For example, consider the Normal model with both unknown mean and variance and assume a vague prior density \\(p(\\mu,\\sigma^2)\\propto (\\sigma^2)^{-1}\\) (corresponding to uniform prior on \\((\\mu, \\log\\sigma)\\)), then the joint posterior distribution is proportional to the sampling distribution multiplied by the factor \\((\\sigma^2)^{-1}\\), that is\n\\[\rp(\\mu,\\sigma^2 \\mid y)\\propto \\sigma^{-n-2}\\text{exp}\\left(-\\frac{1}{2\\sigma^2}\\left[(n-1)s^2+n(\\bar{y}-\\mu)^2 \\right] \\right),\r\\]\nwhere \\(s^2=\\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\bar{y})^2\\) is the sample variance. Next, the conditional posterior density \\(p(\\mu \\mid \\sigma^2)\\) can be shown to be equal to\n\\[\rp(\\mu \\mid \\sigma^2,y) \\sim N(\\bar{y},\\frac{\\sigma^2}{n}),\r\\]\nwhile the marginal posterior \\(p(\\sigma^2 \\mid y)\\) can be obtained by averaging the joint \\(p(\\mu,\\sigma^2\\mid y)\\) over \\(\\mu\\), that is\n\\[\rp(\\sigma^2 \\mid y)\\propto \\int \\left(\\sigma^{-n-2}\\text{exp}\\left(-\\frac{1}{2\\sigma^2}\\left[(n-1)s^2+n(\\bar{y}-\\mu)^2 \\right] \\right)\\right)d\\mu,\r\\]\nwhich leads to\n\\[\rp(\\sigma^2 \\mid ,y) \\sim \\text{Inv-}\\chi^2(n-1,s^2).\r\\]\nTypically, \\(\\mu\\) represents the estimand of interest and the obejective of the analysis is therefore to make inference about the marginal distribution \\(p(\\mu \\mid y)\\), which can be obtained by integrating \\(\\sigma^2\\) out of the joint posterior\n\\[\rp(\\mu \\mid y)=\\int_{0}^{\\infty}p(\\mu,\\sigma^2\\mid y)d\\sigma^2 \\propto \\left[1+\\frac{n(\\mu-\\bar{y})}{(n-1)s^2} \\right]\r\\]\nwhich corresponds to a Student-\\(t\\) density with \\(n-1\\) degrees of freedom\n\\[\rp(\\mu \\mid y)\\sim t_{n-1}\\left(\\bar{y},\\frac{s^2}{n}\\right)\r\\]\n\rMultivariate Normal Example\rSimilar considerations to those applied to the univariate case can be extended to the multivariate case when \\(y\\) is formed by \\(J\\) components coming from the Multivariate Normal distribution\n\\[\rp(y\\mid \\mu, \\Sigma) \\sim N(\\mu, \\Sigma),\r\\]\nwhere \\(\\mu\\) is a vector of length \\(J\\) and \\(\\Sigma\\) is a \\(J\\times J\\) covariance matrix, which is symmetric and positive definite. The sampling distribution for a sample of \\(n\\) units is\n\\[\rp(y\\mid \\mu, \\Sigma) \\propto \\mid \\Sigma \\mid^{-n/2}\\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^n(y_i-\\mu)^{T}\\Sigma^{-1}(y_i-\\mu) \\right),\r\\]\nAs with the univariate normal model, we can derive the posterior distribution for \\(\\mu\\) and \\(\\Sigma\\) according to the factorisation used of the joint posterior and the prior distributions specified. For example, using the conjugate normal prior for the mean \\(p(\\mu)\\sim N(\\mu_0,\\Sigma_0)\\), given \\(\\Sigma\\) known, the posterior can be shown to be\n\\[\rp(\\mu \\mid y) \\sim N(\\mu_1,\\Sigma_1),\r\\]\nwhere the posterior mean is a weighted average of the data and prior mean with weights given by the data and prior precision matrices \\(\\mu_1=(\\Sigma^{-1}_0+n\\Sigma^{-1})^{-1} (\\Sigma_0^{-1}\\mu_0 + n\\Sigma^{-1}\\bar{y})\\), and the posterior precision is the sum of the data and prior precisions \\(\\Sigma^{-1}_1=\\Sigma^{-1}_0+n\\Sigma^{-1}\\).\nIn the situation in which both \\(\\mu\\) and \\(\\Sigma\\) are unknown, convenient conjugate prior distributions which generalise those used in the univariate case are the Inverse-Wishart for the covariance matrix \\(\\Sigma\\sim \\text{Inv-Wishart}(\\Lambda_0,\\nu_0)\\) and the Multivariate Normal for the mean \\(\\mu\\sim N(\\mu_0, \\Sigma_0)\\), where \\(\\nu_0\\) and \\(\\Lambda_0\\) represent the degrees of freedom and the scale matrix for the Inverse-Wishart distribution, while \\(\\mu_0\\) and \\(\\Sigma_0=\\frac{\\Sigma}{\\kappa_0}\\) are the prior mean and covariance matrix for the Multivariate Normal. Woking out the form of the posterior, it can be shown that the joint posterior distribution has the same form of the sampling distribution with parameters\n\\[\rp(\\mu \\mid \\Sigma, y) \\sim N(\\mu_1,\\Sigma_1) \\;\\;\\; \\text{and} \\;\\;\\; p(\\Sigma \\mid y) \\sim \\text{Inv-Wishart}(\\Lambda_1,\\nu_1),\r\\]\nwhere \\(\\Sigma_1=\\frac{\\Sigma}{\\kappa_1}\\), \\(\\mu_1=\\frac{1}{\\kappa_0+n}\\mu_0+\\frac{n}{\\kappa_0+n}\\bar{y}\\), \\(\\kappa_1=\\kappa_0+n\\), \\(\\nu_1=\\nu_0+n\\), and \\(\\Lambda_1=\\Lambda_0+\\sum_{i=1}^n(y_i-\\bar{y})(y_i-\\bar{y})^T+\\frac{\\kappa_0 n}{\\kappa_0+n}(\\bar{y}-\\mu_0)(\\bar{y}-\\mu_0)^2\\).\n\r\rRegression Models\rSuppose the data consist in \\(n\\) units measured on an outcome variable \\(y\\) and a set of \\(J\\) covariates \\(X=(x_{1},\\ldots,x_{J})\\) and assume that the distribution of \\(y\\) given \\(x\\) is Normal with mean \\(\\mu_i=\\beta_0+\\sum_{j=1}^J\\beta_jx_{ij}\\) and variance \\(\\sigma^2\\)\n\\[\rp(y \\mid \\beta,\\sigma^2,X) \\sim N(X\\beta,\\sigma^2I),\r\\]\nwhere \\(\\beta=(\\beta_0,\\ldots,\\beta_J)\\) is the set of regression coefficients and \\(I\\) is the \\(n\\times n\\) identity matrix. Within the normal regression model, a convenient vague prior distribution is uniform on \\((\\beta,\\log\\sigma)\\)\n\\[\rp(\\beta,\\sigma^2)\\propto\\sigma^{-2}.\r\\]\nAs with normal distributions with unknown mean and variance we can first determine the marginal posterior of \\(\\sigma^2\\) and factor the joint posterior as \\(p(\\beta,\\sigma^2)=p(\\beta \\mid \\sigma^2, y)p(\\sigma^2 \\mid y)\\) (omit X for simplicity). Then, the conditional distribtuion \\(p(\\beta \\mid \\sigma^2,y)\\) is Normal\n\\[\rp(\\beta \\mid \\sigma^2, y) \\sim N(\\hat{\\beta},V_{\\beta}\\sigma^2),\r\\]\nwhere \\(\\hat{\\beta}=(X^{T}X)^{-1}(X^{T}y)\\) and \\(V_{\\beta}=(X^{T}X)^{-1}\\). The marginal posterior \\(p(\\sigma^2 \\mid y)\\) has a scaled Inverse-\\(\\chi^2\\) form\n\\[\rp(\\sigma^2\\mid y) \\sim \\text{Inv-}\\chi^2(n-J,s^2),\r\\]\nwhere \\(s^2=\\frac{1}{n-J}(y-X\\hat{\\beta})^{T}(y-X\\hat{\\beta})\\). Finally, the marginal posterior \\(p(\\beta \\mid y)\\), averaging over \\(\\sigma^2\\), is multivariate \\(t\\) with \\(n-J\\) degrees of freedom, even though in practice since we can characterise the joint posterior by drawing from \\(p(\\sigma^2)\\) and then from \\(p(\\beta \\mid \\sigma^2)\\). When the anaysis is based on improper priors (do not have finite integral), it is important to check tha the posterior is proper. In the case of the regression model, the posterior for \\(\\beta \\mid \\sigma^2\\) is proper only if the number of observations is larger than the number of parameters \\(n\u0026gt;J\\), and that the rank of \\(X\\) equals \\(J\\) (i.e. the columns of \\(X\\) are linearly independent) in order for all \\(J\\) coefficients to be uniquely identified from the data.\n\rGeneralised Linear Models\rThe purpose of Generalised Linear Models(GLM) is to extend the idea of linear modelling to cases for which the linear relationship between \\(X\\) and \\(E[y\\mid X]\\) or the Normal distribution is not appropriate. GLMs are specified in three stages\nChoose the linear predictor \\(\\eta=X\\beta\\)\n\rChoose the link fuction \\(g()\\) that relates the linear predictor to the mean of the outcome variable \\(\\mu=g^{-1}(\\eta)\\)\n\rChoose the random component specifying the distribution of \\(y\\) with mean \\(E[y\\mid X]\\)\n\r\rThus, the mean of the distribution of \\(y\\) given \\(X\\) is determined as \\(E[y\\mid X]=g^{-1}(X\\beta)\\). The Normal linear model can be thought as a special case of GLMs where the link function is the identity \\(g(\\mu)=\\mu\\) and the random component is normally distributed. Perhaps, the most commonly used GLMs are those based on Poisson and Binomial distributions to analyse count and binary data, respectively.\nPoisson\rCounted data are often modelled using Poisson regression models which assume that \\(y\\) is distributed according to a Poisson distribution with mean \\(\\mu\\). The link function is typically chosen to be the logarithm so that \\(\\log \\mu = X\\beta\\) and the distribution of the data has density\n\\[\rp(y\\mid \\beta)=\\prod_{i=1}^n \\frac{1}{y_i}\\text{exp}\\left(-\\text{e}^{(\\eta_i)}(\\text{exp}(\\eta_i))^{y_i}\\right),\r\\]\nwhere \\(\\eta_i=(X\\beta)_i\\) is the linear predictor for the \\(i-\\)th unit.\n\rBinomial\rSuppose there are some binomial data \\(y_i \\sim \\text{Bin}(n_i,\\mu_i)\\), with \\(n_i\\) known. It is common to specify the model in terms of the mean of the proportions \\(\\frac{y_i}{n_i}\\) rather than the mean of \\(y_i\\). Choosing the logit tranformation of the probability of success \\(g(\\mu_i)=\\log\\left(\\frac{\\mu_i}{1-\\mu_i}\\right)\\) as the link function leads to the logistic regression where data have distribution\n\\[\rp(y \\mid \\beta)=\\prod_{i=1}^n {n_i \\choose y_i} {e^{\\eta_i} \\choose 1+e^{\\eta_i}}^{y_i} {1 \\choose 1+e^{\\eta_i}}^{n_i-y_i}.\r\\]\nThe link functions used in the previous models are known as the canonical link functions for each family of distributions, which is the function of the mean parameter that appears in the exponent of the exponential family form of the probability density. However, it is also possible to use link functions which are not canonical.\n\r\rReferences\rGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis. Chapman; Hall/CRC.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"7b5281e9615c17197896f28a67a25603","permalink":"/missmethods/likelihood-based-methods-ignorable2/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/likelihood-based-methods-ignorable2/","section":"missmethods","summary":"Bayesian inference is a method of statistical inference in which Bayes theorem is used to update the probability for a hypothesis as more evidence or information becomes available","tags":["Bayesian Inference","Likelihood Based Methods Ignorable"],"title":"Introduction to Bayesian Inference","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rMultiple Imputation(MI) refers to the procedure of replacing each missing value by a set of \\(H\\geq 2\\) imputed values. These are ordered in the sense that \\(H\\) completed data sets can be created from the sets of imputations, where the first imputed value replaces the missing value in the first completed data set, the second imputed value in the second completed data set, and so on. Next, standard complete data methods are used to analyse each completed data set. When the \\(H\\) sets of imputations are repeated random draws from the predictive distribution of the missing data under a particular model of missingness, the \\(H\\) completed data inferences can be combined to form one inference that properly reflects uncertainty due to missing values under that model. In general, MI procedures can be summarised in three main steps:\nSpecify an imputation model to generate \\(H\\) imputed values, typically taken as random draws from the predictive distribution of the missing values given the observed values, and create \\(H\\) completed data sets using these imputations and the observed data.\n\rAnalyse each completed data sets using standard complete data methods based on an analysis model, and derive \\(H\\) completed data inferences\n\rPool together the \\(H\\) completed data inferences into a single inference using standard MI formulas, which ensure that missing data uncertainty is taken into account\n\r\rMi was first proposed by Rubin (Rubin (1978)) and has become more popular over time (Rubin (1996), Schafer and Graham (2002), Little and Rubin (2019)), as well as the focus of research for methodological and practical applications in a variety of fields (Herzog and Rubin (1983), Rubin and Schenker (1987), Schafer (1999), Carpenter and Kenward (2012), Molenberghs et al. (2014), Van Buuren (2018)). MI shares both advantages of Single Imputaiton (SI) methods and solves both disadvantages. Indeed, like SI, MI methods allow the analyst to use familiar complete data methods when analysing the completed data sets. The only disadvantage of MI compared with SI methods is that it takes more time to generate the imputations and analyse the completed data sets. However, Rubin (2004) showed that in order to obtain sufficiently precise estimates, a relatively small number of imputations (typically \\(10\\)) is required. For example, considering a situation with \\(\\lambda=50\\%\\) missing information and \\(H=10\\) imputations, the efficiency of MI can be shown to be equal to \\((1+\\frac{\\lambda}{H})^{-1}=95\\%\\). In addition, in today’s computing environments, the work of analysing the completed data sets is quite modest since it involves performing the same task \\(H\\) times. Thus, once a precedure to combine multiple completed data sets is established, the additonal time and effort to handle \\(50\\), \\(20\\), or \\(10\\) imputations if often of little consequence.\nIn the first step of MI, imputations should ideally be created as repeated draws from the posterior predictive distribution of the missing values \\(y_{mis}\\) given the observed values \\(y_{obs}\\), each repetition being an independent drawing of the parameters and missing values. In practice, implicit imputation models can also be used in place of explicit imputation models (Herzog and Rubin (1983)). In the second step, each completed data set is analysed using the same complete data method that would be used in the absence of missingness. Finally, in the last step, standard procedures should be used to combine the compelted data inferences into a single one. The simplest and most popular method for combining the reuslts of \\(H\\) completed data sets is known as Rubin’s rules (Rubin (2004)), which can be explained with a simple example.\nRubin’s rules\rLet \\(\\hat{\\theta}_h\\) and \\(V_h\\), for \\(h=1,\\ldots,H\\), be the completed data estimates and sampling variances for a scalar estimand \\(\\theta\\), calculated from \\(H\\) repeated imputations under a given imputation model. Then, according to Rubin’s rules, the combined estimate is simply the average of the \\(H\\) completed data estimates, that is\n\\[\r\\bar{\\theta}_{H}=\\frac{1}{H}\\sum_{h=1}^{H}\\hat{\\theta}_{h}.\r\\]\nBecause the imputations under MI are conditional draws, under a good imputaton model, they provide valid estimates for a wide range of estimands. In addition, the averaging over \\(H\\) imputed data sets increases the efficiency of estimation over that obtained from a single completed data set. The variability associated with the pooled estimate has two components: the average within-imputation variance \\(\\bar{V}_H\\) and the between-imputation variance \\(B_H\\), defined as\n\\[\r\\bar{V}_{H}=\\frac{1}{H}\\sum_{h=1}^{H}V_{h} \\;\\;\\; \\text{and} \\;\\;\\; B_{H}=\\frac{1}{H-1}\\sum_{h=1}^{H}(\\hat{\\theta}_{h}-\\bar{\\theta}_{H})^2.\r\\]\nThe total variability associated with \\(\\bar{\\theta}_H\\) is the computed as\n\\[\rT_{H}=\\bar{V}_H + \\frac{H+1}{H}B_{H},\r\\]\nwhere \\((1+\\frac{1}{H})\\) is an adjustment factor for finite due to estimating \\(\\theta\\) by \\(\\bar{\\theta}_H\\). Thus, \\(\\hat{\\lambda}_H=(1+\\frac{1}{H})\\frac{B_H}{T_H}\\) is known as the fraction of missing information and is an estimate of the fraction of information about \\(\\theta\\) that is missing due to nonresponse. For large sample sizes and scalar quantities like \\(\\theta\\), the reference distribution for interval estimates and significance tests is a \\(t\\) distribution\n\\[\r(\\theta - \\bar{\\theta}_H)\\frac{1}{\\sqrt{T^2_H}} \\sim t_v,\r\\]\nwhere the degrees of freedom \\(v\\) can be approximated with the quantity \\(v=(H-1)\\left(1+\\frac{1}{H+1}\\frac{\\bar{V}_H}{B_H} \\right)^2\\) (Rubin and Schenker (1987)). In small data sets, an improved version of \\(v\\) can be obtained as \\(v^\\star=(\\frac{1}{v}+\\frac{1}{\\hat{v}_{obs}})^{-1}\\), where\n\\[\r\\hat{v}_{obs}=(1-\\hat{\\lambda}_{H})\\left(\\frac{v_{com}+1}{v_{com}+3}\\right)v_{com},\r\\]\nwith \\(v_{com}\\) being the degrees of freedom for appropriate or exact \\(t\\) inferences about \\(\\theta\\) when there are no missing values (Barnard and Rubin (1999)).\nThe validity of MI rests on how the imputations are created and how that procedure relates to the model used to subsequently analyze the data. Creating MIs often requires special algorithms (Schafer (1997)). In general, they should be drawn from a distribution for the missing data that reflects uncertainty about the parameters of the data model. Recall that with SI methods, it is desirable to impute from the conditional distribution \\(p(y_{mis}\\mid y_{obs},\\hat{\\theta})\\), where \\(\\hat{\\theta}\\) is an estimate derived from the observed data. MI extends this approach by first simulating \\(H\\) independent plausible values for the parameters \\(\\theta_1,\\ldots,\\theta_H\\) and then drawing the missing values \\(y_{mis}^h\\) from \\(p(y_{mis}\\mid y_{obs}, \\theta_h)\\). Treating parameters as random rather than fixed is\ran essential part of MI. For this reason, it is natural (but not essential) to motivate MI from the Bayesian perspective, in which the state of knowledge about parameters is represented through a posterior distribution.\n\rJoint Multiple Imputation\rJoint MI starts from the assumption that the data can be described by a multivariate distribution which in many cases, mostly for practical reasons, corresponds to assuming a multivariate Normal distribution. The general idea is that, for a general missing data pattern $ r$, missingness may occur anywhere in the multivariate outcome vector $ y=(y_1,,y_J)$, so that the distribution from which imputations should be drawn varies based on the observed variables in each pattern. For example, given $ r=(0,0,1,1)$, then imputations should be drawn from the bivariate distribution of the missing variables given the observed variables in that pattern, that is from \\(f(y^{mis}_1,y^{mis}_2 \\mid y^{obs}_3, y^{obs}_4, \\phi_{12})\\), where \\(\\phi_{12}\\) is the probability of being in pattern $ r$ where the first two variables are missing.\nConsider the multivariate Normal distribution \\(y \\sim N(\\mu,\\Sigma)\\), where \\(\\theta=(\\mu,\\Sigma)\\) represent the vector of the parameters of interest which need to be identified. Indeed, for non-monotone missing data, $ $ cannot be generally identified based on the observed data directly $ y^{obs}$, and the typical solution is to iterate imputation and parameter estimation using a general algorithm known as data augmentation(Tanner and Wong (1987)). Following Van Buuren (2018), the general procedure of the algorithm can be summarised as follows:\nDefine some plausible starting values for all parameters \\(\\theta_0=(\\mu_0,\\Sigma_0)\\)\n\rAt each iteration \\(t=1,\\ldots,T\\), draw \\(h=1,\\ldots,H\\) imputations for each missing value from the predictive distribution of the missing data given the observed data and the current value of the parameters at \\(t-1\\), that is\n\r\r\\[\r\\hat{y}^{mis}_{t} \\sim p(y^{mis} \\mid y^{obs},\\theta_{t-1})\r\\]\nRe-estimate the parameters \\(\\theta\\) using the observed and imputed data at \\(t\\) based on the multivariate Normal model, that is\r\r\\[\r\\hat{\\theta}_{t} \\sim p(\\theta \\mid y^{obs}, \\hat{y}^{mis}_{t})\r\\]\nAnd reiterate the steps 2 and 3 until convergence, where the stopping rule typically consists in imposing that the change in the parameters between iterations \\(t-1\\) and \\(t\\) should be smaller than a predefined “small” threshold \\(\\epsilon\\). Schafer (1997) showed that imputations generated under the multivariate Normal model can be robust to non-normal data, even though it is generally more efficient to transform the data towards normality, especially when the parameters of interest are difficult to estimate, such as quantiles and variances.\nThe multivariate Normal model is also often applied to categorical data, with different types of specifications that have been proposed in the literature (Schafer (1997),Horton, Lipsitz, and Parzen (2003),Allison (2005),Bernaards, Belin, and Schafer (2007),Yucel, He, and Zaslavsky (2008),Demirtas (2009)). For examples, missing data in contingency tables can be imputed using log-linear models (Schafer (1997)); mixed continuous-categorical data can be imputed under the general location model which combines a log-linear and multivariate Normal model (Olkin, Tate, and others (1961)); two-way imputation can be applied to missing test item responses by imputing missing categorical data by conditioning on the row and column sum scores of the multivariate data (Van Ginkel et al. (2007)).\n\rReferences\rAllison, Paul D. 2005. “Imputation of Categorical Variables with Proc Mi.” SUGI 30 Proceedings 113 (30): 1–14.\n\rBarnard, John, and Donald B Rubin. 1999. “Miscellanea. Small-Sample Degrees of Freedom with Multiple Imputation.” Biometrika 86 (4): 948–55.\n\rBernaards, Coen A, Thomas R Belin, and Joseph L Schafer. 2007. “Robustness of a Multivariate Normal Approximation for Imputation of Incomplete Binary Data.” Statistics in Medicine 26 (6): 1368–82.\n\rCarpenter, James, and Michael Kenward. 2012. Multiple Imputation and Its Application. John Wiley \u0026amp; Sons.\n\rDemirtas, Hakan. 2009. “Rounding Strategies for Multiply Imputed Binary Data.” Biometrical Journal: Journal of Mathematical Methods in Biosciences 51 (4): 677–88.\n\rHerzog, Thomas N, and Donald B Rubin. 1983. “Using Multiple Imputations to Handle Nonresponse in Sample Surveys.” Incomplete Data in Sample Surveys 2: 209–45.\n\rHorton, Nicholas J, Stuart R Lipsitz, and Michael Parzen. 2003. “A Potential for Bias When Rounding in Multiple Imputation.” The American Statistician 57 (4): 229–32.\n\rLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley \u0026amp; Sons.\n\rMolenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. Handbook of Missing Data Methodology. Chapman; Hall/CRC.\n\rOlkin, Ingram, Robert Fleming Tate, and others. 1961. “Multivariate Correlation Models with Mixed Discrete and Continuous Variables.” The Annals of Mathematical Statistics 32 (2): 448–65.\n\rRubin, Donald B. 1978. “Multiple Imputations in Sample Surveys a Phenomenological Bayesian Approach to Nonresponse.” Proceedings of the Survey Research Methods Section of the American Statistical Association 1: 20–34.\n\r———. 1996. “Multiple Imputation After 18 Years.” Journal of the American Statistical Association 91 (434): 473–89.\n\r———. 2004. Multiple Imputation for Nonresponse in Surveys. John Wiley \u0026amp; Sons.\n\rRubin, Donald B, and Nathaniel Schenker. 1987. “Interval Estimation from Multiply Imputed Data: A Case Study Using Census Agriculture Industry Codes.” Journal of Official Statistics 3 (4): 375.\n\rSchafer, Joseph L. 1997. Analysis of Incomplete Multivariate Data. Chapman; Hall/CRC.\n\r———. 1999. “Multiple Imputation: A Primer.” Statistical Methods in Medical Research 8 (1): 3–15.\n\rSchafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” Psychological Methods 7 (2): 147.\n\rTanner, Martin A, and Wing Hung Wong. 1987. “The Calculation of Posterior Distributions by Data Augmentation.” Journal of the American Statistical Association 82 (398): 528–40.\n\rVan Buuren, Stef. 2018. Flexible Imputation of Missing Data. Chapman; Hall/CRC.\n\rVan Ginkel, Joost R, L Andries Van der Ark, Klaas Sijtsma, and Jeroen K Vermunt. 2007. “Two-Way Imputation: A Bayesian Method for Estimating Missing Scores in Tests and Questionnaires, and an Accurate Approximation.” Computational Statistics \u0026amp; Data Analysis 51 (8): 4013–27.\n\rYucel, Recai M, Yulei He, and Alan M Zaslavsky. 2008. “Using Calibration to Improve Rounding in Imputation.” The American Statistician 62 (2): 125–29.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2fe39d9953178e8abeabce9506d9389f","permalink":"/missmethods/joint-multiple-imputation/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/joint-multiple-imputation/","section":"missmethods","summary":"Joint Multiple Imputation (JOMO) commonly assumes that the incomplete variables follow a multivariate normal distribution, often referred to as multivariate normal imputation and, under this assumption, provides rigorous standard errors for the fitted parameters","tags":["Joint Multiple Imputation","Multiple Imputation by Chained Equations","Multiple Imputation"],"title":"Joint Multiple Imputation","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rIt is possible to summarise the steps involved in drawing inference from incomplete data as (Daniels and Hogan (2008)):\n\rSpecification of a full data model for the response and missingness indicators \\(f(y,r)\\)\n\rSpecification of the prior distribution (within a Bayesian approach)\n\rSampling from the posterior distribution of full data parameters, given the observed data \\(Y_{obs}\\) and the missingness indicators \\(R\\)\n\r\rIdentification of a full data model, particularly the part involving the missing data \\(Y_{mis}\\), requires making unverifiable assumptions about the full data model \\(f(y,r)\\). Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between \\(R\\) and \\(Y_{mis}\\), given \\(Y_{obs}\\). Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:\nUnverifiable parametric assumptions\n\rInformative prior distributions (under a Bayesian approach)\n\r\rWe show some simple examples about how these nonignorable models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as Selection Models(SM).\nSelection Models\rThe selection model approach factors the full data distribution as\n\\[\rf(y,r \\mid \\omega) = f(y \\mid \\theta) f(r \\mid y,\\psi),\r\\]\nwhere it is typically assumed that the set of full data parameters \\(\\omega\\) can be decomposed as separate parameters for each factor \\((\\theta,\\psi)\\). Thus, under the SM approach, the response model \\(f(y \\mid \\theta)\\) and the missing data mechanism \\(f(r \\mid y, \\psi)\\) must be specified by the analyst. SMs can be attractive for several reasons, including\n\rThe possibility to directly specify the model of interest \\(f(y \\mid \\theta)\\)\n\rThe SM factorisation appeals to Rubin’s missing data taxonomy, enabling easy characterisation of the missing data mechanism\n\rWhen the missingness pattern is monotone, the missigness mechanism can be formulated as a hazard function, where the hazard of dropout at some time point \\(j\\) can depend on parts of the full data vector \\(Y\\)\n\r\rExample of SM for bivariate normal data\rConsider a sample of \\(i=1,\\ldots,n\\) units from a bivariate normal distribution \\(Y=(Y_1,Y_2)\\). Assume also that \\(Y_1\\) is always observed while \\(Y_2\\) may be missing, and let \\(R=R_2\\) be the missingness indicator for the partially-observed response \\(Y_2\\). A SM factors the full data distribution as\n\\[\rf(y_1,y_2,r \\mid \\omega) = f(y_1 \\mid \\theta)f(r \\mid y_1,y_2,\\psi),\r\\]\nwhere we assume \\(\\omega=(\\theta,\\psi)\\). Suppose we specify \\(f(y_1,y_2 \\mid \\theta)\\) as a bivariate normal density with mean \\(\\mu\\) and \\(2\\times2\\) covariance matrix \\(\\Sigma\\). The distribution of \\(r\\) is assumed to be distributed as a Bernoulli variable with probability \\(\\pi_i\\), such that\n\\[\rg(\\pi_i) = \\psi_0 + \\psi_1y_{i1} + \\psi_2y_{i2},\r\\]\nwhere \\(g()\\) denotes a given link function which relates the expected value of the response to the linear predictors in the model. When this is taken as the inverse normal cumulative distribution function \\(\\Phi^{-1}()\\) the model corresponds to the Heckman probit selection model (Heckman (1976)). In general, setting \\(\\psi_2=0\\) leads to a Missing At Random(MAR) assumption; if, in addition, we have distinctness of the parameters \\(f(\\mu,\\Sigma,\\psi)=f(\\mu,\\Sigma)f(\\psi)\\), we have ignorability. We note that, even though the parameter \\(\\psi_2\\) characterises the association between \\(R\\) and \\(Y_2\\), the parametric assumptions made in this example will identify \\(\\psi_2\\) even in the absence of informative priors, that is the observed data likelihood is a function of \\(\\psi_2\\). Moreover, the parameter indexes the joint distribution of observables \\(Y_{obs}\\) and \\(R\\) and in general can be identified from the observed data. This property of parametric SMs make them ill-suited to assessing sensitivity to assumptions about the missingness mechanism.\nThe model can also be generalised to longitudinal data assuming a multivariate normal distribution for \\(Y=(Y_1,\\ldots,Y_J)\\) and replacing \\(\\pi_i\\) with a discrete time hazard function for dropout\n\\[\rh\\left(t_j \\mid \\bar{Y}_{j}\\right) = \\text{Prob}\\left(R_j = 0 \\mid R_{j-1} = 1, Y_{1},\\ldots,Y_{j} \\right).\r\\]\nUsing the logit function to model the discrete time hazard in terms of observed response history \\(\\bar{Y}_{j-1}\\) and the current but possibly unobserved \\(Y_j\\) corresponds to the model of Diggle and Kenward (1994).\n\r\rConlcusions\rTo summarise, SMs allows to generalise ignorable models to handle nonignorable missingness by letting \\(f(r \\mid y_{obs},y_{mis})\\) to depend on \\(y_{mis}\\) and their structure directly appeals to Rubin’s taxonomy. However, identification of the missing data distribution is accomplished through parametric assumptions about the full data response model \\(f(y \\mid \\theta)\\) and the explicit form of the missingness mechanism. This makes it difficult to disentagle the type of information that is used to identify the model, i.e. parametric modelling assumptions or information from the observed data, therefore complicating the task of assessing the robustness of the results to a range of transparent and plausible assumptions.\n\rReferences\rDaniels, Michael J, and Joseph W Hogan. 2008. Missing Data in Longitudinal Studies: Strategies for Bayesian Modeling and Sensitivity Analysis. Chapman; Hall/CRC.\n\rDiggle, Peter, and Michael G Kenward. 1994. “Informative Drop-Out in Longitudinal Data Analysis.” Journal of the Royal Statistical Society: Series C (Applied Statistics) 43 (1): 49–73.\n\rHeckman, James J. 1976. “The Common Structure of Statistical Models of Truncation, Sample Selection and Limited Dependent Variables and a Simple Estimator for Such Models.” In Annals of Economic and Social Measurement, Volume 5, Number 4, 475–92. NBER.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"5e5577911e5b60bb5b8eee1c351979c9","permalink":"/missmethods/selection-models/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/selection-models/","section":"missmethods","summary":"Selection Models (SM) are typically used to handle nonignorable missingness. They factorise the joint likelihood of measurement process and missingness process into a marginal density of the measurement process and the density of the missingness process conditional on the outcomes, which describes the missing data selection based on the complete data.","tags":["Selection Models","Full Data Models under MNAR","Likelihood Based Methods Nonignorable"],"title":"Selection Models","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rAs for the inference under complete data, inference under incomplete data consists in deriving the likelihood for the parameters based on the available data, either using a Maximum Likelihood (ML) approach (solving the likelihood equation) or using the Bayes’ rule incorporating a prior distribution (performing necessary integrations to obtain the posterior distribution). However, asymptotic standard errors obtained from the information matrix, are more questionable when dealing with missing data since the sample will not be typically iid and results that imply the large sample normality of the likelihood function do not immediately apply. More complications arise when dealing with the process that lead to some of the data to be missing. This can be explained with a simple example.\nLet \\(Y=(y_{ij})\\), for \\(i=1,\\ldots,n\\) and \\(j=1,\\ldots,J\\), denote the complete dataset if there were no missing values, with a total of \\(n\\) units and \\(J\\) variables. Let \\(M=(m_{ij})\\) denote the fully observed matrix of binary missing data indicators with \\(m_{ij}=1\\) if \\(y_{ij}\\) is missing and \\(0\\) otherwise. As an example, we can model the density of the joint distribution of \\(Y\\) and \\(M\\) using the selection model factorisation (Little and Rubin (2019))\n\\[\rp(Y=y,M=m \\mid \\theta, \\psi) = f(y \\mid \\theta)f(m \\mid y, \\psi),\r\\]\nwhere \\(\\theta\\) is the parameter vector indexing the response model and \\(\\psi\\) is the parameter vector indexing the missingness mechanism. The observed values \\(m\\) effect a partition \\(y=(y_1,y_0)\\), where \\(y_0=[y_{ij} : m_{ij}=0]\\) is the observed component and \\(y_1=[y_{ij} : m_{ij}=1]\\) is the missing component of \\(y\\). The full likelihood based on the observed data and the assumed model is\n\\[\rL_{full}(\\theta, \\psi \\mid y_{0},m) = \\int f\\left(y_{0},y_{1} \\mid \\theta \\right) f\\left(m \\mid y_{0},y_{1}, \\psi \\right)dy_{1}\r\\]\nand is a function of the parameters \\((\\theta,\\psi)\\). Next, we define the likelihood of ignoring the missingness mechanism or ignorable likelihood as\n\\[\rL_{ign}\\left(\\theta \\mid y_{0} \\right) = \\int f(y_{0},y_{1}\\mid \\theta)dy_{1},\r\\]\nwhich does not involve the model for \\(M\\). In practice, modelling the joint distribution of \\(Y\\) and \\(M\\) is often challenging and, in fact, many approaches to missing data do not model \\(M\\) and (explicitly or implicitly) base inference about \\(\\theta\\) on the ignorable likelihood. It is therefore important to assess under which conditions inferences about \\(\\theta\\) based on \\(L_{ign}\\) can be considered appropriate. More specifically, the missingness mechanism is said to be ignorable if inferences about \\(\\theta\\) based on the ignorable likelihood equation evauluated at some realisations of \\(y_0\\) and \\(m\\) are the same as inferences about \\(\\theta\\) based on the full likelihood equation, evaluated at the same realisations of \\(y_0\\) and \\(m\\). The conditions for ignoring the missingness mechanism depend on whether the inferences are direct likelihood, Bayesian or frequentist.\nDirect Likelihood Inference\rDirect Likelihood Inference refers to inference based solely on likelihood ratios for pair of values of the parameters, with the data fixed at their observed values. The missingness mechanism can be ignored for direct likelihood if the likelihood ratio based on the ignorable likelihood is the same as the ratio based on the full likelihood. More precisely, the missingness mechanism is said to be ignorable for direct likelihood inference at some realisations of \\((y_0,m)\\) if the likelihood ratio for two values \\(\\theta\\) and \\(\\theta^\\star\\) is the same whether based on the full or ignorable likelihood. That is\n\\[\r\\frac{L_{full}\\left( \\theta, \\psi \\mid y_{0}, m \\right)}{L_{full}\\left( \\theta^{\\star}, \\psi \\mid y_{0}, m \\right)}=\\frac{L_{ign}\\left( \\theta \\mid y_{0} \\right)}{L_{ign}\\left( \\theta^{\\star} \\mid y_{0}\\right)},\r\\]\nfor all \\(\\theta\\), \\(\\theta^\\star\\) and \\(\\psi\\). In general, the missingnes mechanism is ignorable for direct likelihood inference if the following two conditions hold:\nParameter distinctness. The parameters \\(\\theta\\) and \\(\\psi\\) are distinct, in the sense that the joint parameter space \\(\\Omega_{\\theta,\\psi}\\) is the product of the two parameter spaces \\(\\Omega_{\\theta}\\) and \\(\\Omega_{\\psi}\\).\rFactorisation of the full likelihood. The full likelihood factors as\r\r\\[\rL_{full}\\left(\\theta, \\psi \\mid y_{0},m \\right) = L_{ign}\\left(\\theta \\mid y_{0} \\right) L_{rest}\\left(\\psi \\mid y_{0},m \\right)\r\\]\nfor all values of \\(\\theta,\\psi \\in \\Omega_{\\theta,\\psi}\\). The distinctness condition ensures that each value of \\(\\psi \\in \\Omega_{\\psi}\\) is compatible with different values of \\(\\theta \\in \\Omega_{\\theta}\\). A sufficient condition for the factorisation of the full likelihood is that the missing data are Missing At Random(MAR) at the specific realisations of \\(y_{0},m\\). This means that the distribution function of \\(M\\), evaluated at the given realisations \\((y_{0},m)\\), does not depend on the missing values \\(y_1\\), that is\n\\[\rf\\left(m \\mid y_{0}, y_{1}, \\psi \\right)=f\\left(m \\mid y_{0}, y^{\\star}_{1} \\psi \\right),\r\\]\nfor all \\(y_{1},y^\\star_{1},\\psi\\). Thus, we have\n\\[\rf\\left(y_{0}, m \\mid \\theta, \\psi \\right) = f\\left(m \\mid y_{0}, \\psi \\right) \\int f\\left(y_{0},y_{1} \\mid \\theta \\right)dy_{1} = f\\left(m \\mid y_{0}, \\psi \\right) f\\left( y_{0} \\mid \\theta \\right).\r\\]\nFrom this it follows that, if the missing data are MAR at the given realisations of \\((y_{0},m)\\) and \\(\\theta\\) and \\(\\psi\\) are distinct, the missingnes mechanism is ignorable for likelihood inference.\n\rBayesian Inference\rBayesian Inference under the full model for \\(Y\\) and \\(M\\) requires that the full likelihood is combined with a prior distribution \\(p(\\theta,\\psi)\\) for the parameters \\(\\theta\\) and \\(\\psi\\), that is\n\\[\rp\\left(\\theta, \\psi \\mid y_{0}, m \\right) \\propto p(\\theta, \\psi) L_{full}\\left(\\theta, \\psi \\mid y_{0}, m \\right).\r\\]\nBayesian inference ignoring the missingness mechanism combines the ignorable likelihood with a prior distribution for \\(\\theta\\) alone, that is\n\\[\rp(\\theta \\mid y_{0}) \\propto p(\\theta) L_{ign}\\left(\\theta \\mid y_{0} \\right).\r\\]\nMore formally, the missingness mechanism is said to be ignorable for Bayesian inference at the given realisations of \\((y_{0},m)\\) if the posterior distribution for \\(\\theta\\) based on the posterior distribution for the full likelihood and prior distribution for \\((\\theta,\\psi)\\) is the same as the posterior distribution for the ignorable likelihood and the prior distribution for \\(\\theta\\) alone. This holds when the following conditions are satisfied:\nThe parameters \\(\\theta\\) and \\(\\psi\\) are a priori independent, that is the prior distribution has the form\r\r\\[\rp(\\theta , \\psi) = p(\\theta) p(\\psi)\r\\]\nThe full likelihood evaluated at the realisations of \\((y_{0},m)\\) factors as for direct likelihood inference\r\rUnder these conditions:\n\\[\rp(\\theta, \\psi \\mid y_{0}, m) \\propto \\left(p(\\theta)L_{ign}\\left( \\theta \\mid y_{0} \\right) \\right) \\left(p(\\psi)L_{rest}\\left(\\psi \\mid y_{0},m \\right) \\right).\r\\]\nAs for direct likelihood inference, MAR is a sufficient condition for the factorisation of the full likelihood. This means that, if the data are MAR at the given realisations of \\((y_{0},m)\\) and the parameters \\(\\theta\\) and \\(\\psi\\) are a prior independent, then the missingness mechanism is ignorable for Bayesian inference. We note that the a priori condition is more stringent than the distinctness condition because paramerers with distinct parameter spaces might have dependent prior distributions.\n\rFrequentist Asymptotic Inference\rFrequentist Asymptotic Inference requires that, in order to ignore the missingness mechanism, the factorisation of the full likelihood needs to be valid for values of the observed data under repeated sampling. This means that we require\n\\[\rL_{full}\\left(\\theta,\\psi \\mid y_{0}, m \\right) = L_{ign}\\left(\\theta \\mid y_{0} \\right) L_{rest}\\left(\\psi \\mid y_{0}, m \\right)\r\\]\nfor all \\(y_{0},m\\) and \\(\\theta,\\psi \\in \\Omega_{\\theta,\\psi}\\). For this form of inference, a sufficient condition for ignoring the missingness mechanism is given by the following conditions:\nParameter distinctness as defined for direct likelihood inference.\n\rMissing data are Missing Always At Random (MAAR), that is\n\r\r\\[\rf\\left(m \\mid y_{0},y_{1},\\psi \\right) = f\\left(m \\mid y_{0}, y^{\\star}_{1},\\psi \\right)\r\\]\nfor all \\(m,y_{0},y_{1},y^\\star_{1},\\psi\\). In the following example we discuss conditions for ignoring the missingness mechanism for direct likelihood and Bayesian inference, which can be extended to the case of frequentist asymptotic inference by requiring that they hold for for values of \\(y_{0},m\\) other than those observed that could arise in repeated sampling.\n\rBivariate Normal Sample with One Variable Subject to Missingness\rConsider a bivariate normal sample \\(y=(y_{i1},y_{i2})\\), for \\(i=1,\\ldots,n\\) units, but with the values of \\(y_{i2}\\) being missing for \\(i=(n_{cc}+1),\\ldots,n\\). This leads to a monotone missing data pattern with two variables. The loglikelihood of ignoring the missingness mechanism is\n\\[\rl_{ign}\\left(\\mu, \\Sigma \\mid y_{0} \\right) = \\log\\left(L_{ign}\\left(\\mu,\\Sigma \\mid y_{0} \\right) \\right) = - \\frac{1}{2}n_{cc}ln \\mid \\Sigma \\mid - \\frac{1}{2}\\sum_{i=1}^{n_{cc}}(y_i - \\mu ) \\Sigma^{-1}(y_i - \\mu)^{T} - \\frac{1}{2}(n-n_{cc})ln\\sigma_{1} - \\frac{1}{2}\\sum_{i=n_{cc}+1}^{n}\\frac{(y_{i1}-\\mu_1)^2}{\\sigma_{1}}.\r\\]\nThis loglikelihood is appropriate for inference provided the conditional distribution of \\(M\\) does not depend on the values of \\(y_{i2}\\), and \\(\\theta=(\\mu,\\Sigma)\\) is distinct from \\(\\psi\\). Under these conditions, ML estimates of \\(\\theta\\) can be found by maximising this loglikelihood. For Bayesian inference, if these conditions hold and the prior distribution for \\((\\theta,\\psi)\\) has the form \\(p(\\theta)p(\\psi)\\), then the joint posterior distribution of \\(\\theta\\) is proportional to the product of \\(p(\\theta)\\) and \\(L_{ign}(\\theta \\mid y_{0})\\).\n\rReferences\rLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley \u0026amp; Sons.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d3592131ea628b2e84c6e27479d5ba54","permalink":"/missmethods/likelihood-based-methods-ignorable3/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/likelihood-based-methods-ignorable3/","section":"missmethods","summary":"When making inference with missing data, any statistical method must rely on either explicit or implicit assumptions about the mechanism which lead some of the values to be missing","tags":["Maximum Likelihood Estimation","Bayesian Inference","Likelihood Based Methods Ignorable"],"title":"Likelihood Based Inference with Incomplete Data","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rIt is possible to summarise the steps involved in drawing inference from incomplete data as (Daniels and Hogan (2008)):\n\rSpecification of a full data model for the response and missingness indicators \\(f(y,r)\\)\n\rSpecification of the prior distribution (within a Bayesian approach)\n\rSampling from the posterior distribution of full data parameters, given the observed data \\(Y_{obs}\\) and the missingness indicators \\(R\\)\n\r\rIdentification of a full data model, particularly the part involving the missing data \\(Y_{mis}\\), requires making unverifiable assumptions about the full data model \\(f(y,r)\\). Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between \\(R\\) and \\(Y_{mis}\\), given \\(Y_{obs}\\). Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:\nUnverifiable parametric assumptions\n\rInformative prior distributions (under a Bayesian approach)\n\r\rWe show some simple examples about how these nonignorable models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as Pattern Mixture Models(PMM).\nPattern Mixture Models\rThe pattern mixture model approach factors the full data distribution as\n\\[\rf(y,r \\mid \\omega) = f(y \\mid r, \\phi) f(r \\mid y,\\chi),\r\\]\nwhere it is typically assumed that the set of full data parameters \\(\\omega\\) can be decomposed as separate parameters for each factor \\((\\phi,\\chi)\\). Thus, under the PMM approach, the response model \\(f(y \\mid \\theta)\\) can be retrieved as a mixture of the pattern specific distributions\n\\[\rf(y \\mid \\theta) = \\sum_{r}f(y \\mid r, \\phi)f(r \\mid \\chi),\r\\]\nwith weights given by the corresponding probabilities of the different patterns. The missingness mechanism \\(f(r \\mid y, \\psi)\\) can also be obtained using Bayes’ rule\n\\[\rf(y \\mid r, \\psi) = \\frac{f(y \\mid r, \\phi)f(r\\mid \\chi)}{f(y \\mid \\theta)}.\r\\]\nThe construction of PMMs requires the specification of the full data distribution conditional on different missingness patterns, which may be cumbersome when the number of patterns is large, but with the advantage of making explicit the parameters that cannot be identified by the observed data. In particular, PMMs are well suited to show that the distribution of the response within each pattern can be decomposed as\n\\[\rf(y_{obs},y_{mis} \\mid r, \\phi)= f(y_{mis} \\mid y_{obs},r,\\phi_{E})f(y_{obs}\\mid r,\\phi_{O}),\r\\]\nwhere \\(\\phi_E = \\lambda_1(\\phi)\\) and \\(\\phi_O=\\lambda_2(\\phi)\\) are functions of the mixture component parameter \\(\\phi\\). The former subset of parameters indexes the so called extrapolation distribution and cannot be identified from the data, i.e. the distribution of the missing values given the observed values, while the latter indexes the observed data distribution and is typically identifiable from the data. Assuming there exists a partition such that \\(\\phi_E=(\\phi_{EI},\\phi_{ENI})\\) and the observed data distribution is a function of \\(\\phi_{EI}\\) but not of \\(\\phi_{ENI}\\), then \\(\\phi_{ENI}\\) is a senstivity parameter in that it can only be identified using information from sources other than the observed data and thus makes a suitable basis to formulate sensitivity analysis using informative priors.\nExample of PMM for bivariate normal data\rConsider a sample of \\(i=1,\\ldots,n\\) units from a bivariate normal distribution \\(Y=(Y_1,Y_2)\\). Assume also that \\(Y_1\\) is always observed while \\(Y_2\\) may be missing, and let \\(R=R_2\\) be the missingness indicator for the partially-observed response \\(Y_2\\). A PMM factors the full data distribution as\n\\[\rf(y_1,y_2,r \\mid \\omega) = f(y_1, y_2 \\mid r, \\phi)f(r \\mid ,\\chi),\r\\]\nwhere, for example, we may have \\(Y \\mid R=1 \\sim N(\\mu^1,\\Sigma^1)\\), \\(Y \\mid R=0 \\sim N(\\mu^0,\\Sigma^0)\\) and \\(R \\sim Bern(\\chi)\\). We define \\(\\mu^r=(\\mu^r_1)\\), while \\(\\Sigma^r\\) has elements \\(\\sigma^r=(\\sigma^r_{11},\\sigma^r_{12},\\sigma^r_{22})\\). Similarly, we can define the parameters \\(\\beta^r_0\\), \\(\\beta^r_1\\) and \\(\\sigma^r_{2\\mid 1}\\) as the intercept, slope and residual variance of the regression of \\(Y_2\\) on \\(Y_1\\) for each pattern \\(r\\). Under this reparameterisation, the full data model parameters are\n\\[\r\\phi=\\{\\mu^r_1,\\sigma^r_{11},\\beta^r_0,\\beta^1_1,\\sigma^r_{2\\mid 1}\\}.\r\\]\nThe extrapolation and observed data distributions, with associated parameters, are then\n\\[\rf(y_{mis}\\mid y_{obs},\\phi_{E}) \\rightarrow \\phi_{E}=(\\beta^0_0, \\beta^0_1,\\sigma^0_{2\\mid1})\r\\]\nand\n\\[\rf(y_{obs}\\mid \\phi_{O}) \\rightarrow \\phi_{O}=(\\mu^1,\\beta^1,\\sigma^1_{11},\\mu^0_0,\\sigma^1_{11}).\r\\]\nIt can be shown that, in this specific example, the observed data distribution does not depend on the parameters indexing the extrapolation distribtuon \\(\\phi_{ENI}=(\\beta^0_0,\\beta^0_1,\\sigma^0_{2\\mid 1})\\). It is possible to set \\(\\beta^0=\\beta=1\\) and \\(\\sigma^0_{2\\mid1}=\\sigma^1_{2\\mid1}\\) to yield a Missing At Random(MAR) assumption. Hence, a function that maps identified parameters and sensitivity parameters \\(\\Delta\\) to the space of unidentified parameters can be used to quantify departures from MAR. For example, assume we impose\n\\[\r\\beta^0_0=\\beta^1_0+\\Delta,\r\\]\nthen assigning a point mass prior at \\(\\Delta=0\\) implies MAR, while fixing \\(\\Delta \\neq 0\\) or using any type of inofrmative prior on this parameter implies a Missing Not At Random(MNAR) assumption.\n\r\rConlcusions\rTo summarise, PMMs have the advantage of being able to find full data parameters indexing the distribution of the missing data that are not identified from the observed data, making inference more transparent. A potential downside is the practical implementation of these models which becomes more difficult as the number of patterns and unidentified parameters grows.\n\rReferences\rDaniels, Michael J, and Joseph W Hogan. 2008. Missing Data in Longitudinal Studies: Strategies for Bayesian Modeling and Sensitivity Analysis. Chapman; Hall/CRC.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"ade887abdca5fce869b560f1807f7591","permalink":"/missmethods/pattern-mixture-models/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/pattern-mixture-models/","section":"missmethods","summary":"Pattern Mixture Models (PMM) are typically used to handle nonignorable missingness. They factorise the joint likelihood of measurement process and missingness process into a marginal density of the missingness process and the density of the measurement process conditional on the missing data patterns, where the model of interest is fitted for each pattern.","tags":["Pattern Mixture Models","Full Data Models under MNAR","Likelihood Based Methods Nonignorable"],"title":"Pattern Mixture Models","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\r\rThe notion of reducing bias due to missingness through reweighting methods has its root in the survey literature and the basic idea is closely related to weighting in randomisation inference for finite population surveys (Little and Rubin (2019)). In particular, in probability sampling, a unit selected from a target population with probability \\(\\pi_i\\) can be thought as “representing” \\(\\pi^{-1}_i\\) units in the population and hence should be given weight \\(\\pi^{-1}_i\\) when estimating population quantities. For example, in a stratified random sample, a selected unit in stratum \\(j\\) represents \\(\\frac{N_j}{n_j}\\) population units, where \\(n_j\\) indicates the units sampled from the \\(N_j\\) population units in stratum \\(j=1,\\ldots,J\\). The population total \\(T\\) can then be estimated by the weighted sum\n\\[\rT = \\sum_{i=1}^{n}y_i\\pi^{-1}_i,\r\\]\nknown as the Horvitz-Thompson estimate (Horvitz and Thompson (1952)), while the stratified mean can be written as\n\\[\r\\bar{y}_{w} = \\frac{1}{n}\\sum_{i=1}^{n}w_iy_i,\r\\]\nwhere \\(w_i=\\frac{n\\pi^{-1}_i}{\\sum_{k=1}^n\\pi^{-1}_k}\\) is the sampling weight attached to the \\(i\\)-th unit scaled tosum up to the sample size \\(n\\). Weighting class estimators extend this approach to handle missing data such that, if the probabilities of response for unit \\(\\phi_i\\) were known, then the probability of selection and response is \\(\\pi_i\\phi_i\\) and we have\n\\[\r\\bar{y}_{w} = \\frac{1}{n_r}\\sum_{i=1}^{n_r}w_iy_i,\r\\]\nwhere the sum is now over responding units and \\(w_i=\\frac{n_r(\\pi_i\\phi_i)^{-1}}{\\sum_{k=1}^{n_r}(\\pi_k\\phi_k)^{-1}}\\). In practice, the response probability \\(\\phi_i\\) is not known and is typically estimated based on the information available for respondents and nonrespondents (Schafer and Graham (2002)).\nWeighting Class Estimator of the Mean\rA simple reweighting approach is to partition the sample into \\(J\\) “weighting classes” according to the variables observed for respondents and nonrespondents. If \\(n_j\\) is the sample size, \\(n_{rj}\\) the number of respondents in class \\(j\\), with \\(n_r=\\sum_{j=1}^Jr_j\\), then a simple estimator of the response probability for units in class \\(j\\) is given by \\(\\frac{n_{rj}}{n_j}\\). Thus, responding units in class \\(j\\) receive weight \\(w_i=\\frac{n_r(\\pi_i\\hat{\\phi}_i)^{-1}}{\\sum_{k=1}^{n_r}(\\pi_k\\hat{\\phi}_k)^{-1}}\\), where \\(\\hat{\\phi}_i=\\frac{n_{rj}}{n_j}\\) for unit \\(i\\) in class \\(j\\). The weighting class estimate of the mean is then\n\\[\r\\bar{y}_{w} = \\frac{1}{n_r}\\sum_{i=1}^{n_r}w_iy_i,\r\\]\nwhich is unbiased under the quasirandomisation assumption (Oh and Scheuren (1983)), which requires respondents in weighting class \\(j\\) to be a random sample of the sampled units, i.e. data are Missing Completely At Random (MCAR) within adjustment class \\(j\\). Weighting class adjustments are simple because the same weights are obtained regardless of the outcome tp which they are applied, but these are inefficient and generally involves an increase in sampling variance for outcomes that are weakly related to the weighting class variable. Assuming random sampling within weighting classes, a constant variance \\(\\sigma^2\\) for an outcome \\(y\\), and ignoring sampling variation in the weights, the increase in sampling variance of a sample mean is\n\\[\r\\text{Var}\\left(\\frac{1}{n_{r}}\\sum_{i=1}^{n_{r}}w_iy_i \\right) = \\frac{\\sigma^2}{n_{r}^2}\\left(\\sum_{i=1}^{n_{r}}w_{i}^{2} \\right) = \\frac{\\sigma^2}{n_{r}}(1+\\text{cv}^2(w_i)),\r\\]\nwhere \\(\\text{cv}(w_i)\\) is the coefficient of variation of the weights (scaled to average one), which is a rough measure of the proportional increase in sampling variance due to weighting (Kish (1992)). When the weighting class variable is predictive of \\(y\\), weighting methods can lead to a reduction in sampling variance. Little and Rubin (2019) summarise the effect of weighting on the bias and sampling variance of an estimated mean, according to whether the associations between the adjustment cells and the outcome \\(y\\) and missing indicator \\(m\\) are high or low.\n\rTable 1: Effect of weighting adjustments on bias and sampling variance of a mean.\r\r\r\r\rLow (y)\r\rHigh (y)\r\r\r\r\r\rLow (m)\r\rbias: /, var: /\r\rbias: /, var: -\r\r\r\rHigh (m)\r\rbias: /, var: +\r\rbias: -, var: -\r\r\r\r\rThus, weighting is only effective when the outcome is associated with the adjustment cell variable because otherwise the sampling variance is increased with no bias reduction.\n\rPropensity Weighting\rIn some settings, weighting class estimates cannot be feasibly derived by all recorded variables X because the number of classes become too large and some may include cells with nonrespondents but no respondents for which the nonresponse weight is infinite. The theory of propensity scores (Rosenbaum and Rubin (1983)) provides a prescription for choosing the coarsest reduction of the variables to a weighting class variable \\(c\\). Suppose the data are Missing At Random (MAR) such that\n\\[\rp(m\\mid X,y,\\phi)=p(m\\mid X,\\phi),\r\\]\nwhere \\(\\phi\\) are unknown parameters and define the nonresponse propensity for unit \\(i\\) as\n\\[\r\\rho(x_i,\\phi)=p(m_i=1 \\mid \\phi),\r\\]\nassuming that this is strictly positive for all values of \\(x_i\\). Then, it can be shown that\n\\[\rp(m\\mid \\rho(X,\\phi),y,\\phi)=p(m\\mid \\rho(X,\\phi),\\phi),\r\\]\nso that respondents are a random subsample within strata defined by the propensity score \\(\\rho(X,\\phi)\\). In practice the parameter \\(\\phi\\) is unknown and must be estimated from sample data, for example via logistic, probit or robit regressions of the missingness indicator \\(m\\) on \\(X\\) based on respondent and nonrespondent data (Liu (2004)). A variant of this procedure is to weight respondents \\(i\\) directly by the inverse of the estimated propensity score \\(\\rho(X,\\hat{\\phi})^{-1}\\) (Cassel, Sarndal, and Wretman (1983)), which allows to remove bias but may cause two problems: 1) estimates may be associated with very high sampling variances due to nonrespondents with low response propensity estimates receiving large nonresponse weights; 2) more reliance on correct model specification of the propensity score regression than response propensity stratification.\n\rReferences\rCassel, Claes M, Carl-Erik Sarndal, and Jan H Wretman. 1983. “Some Uses of Statistical Models in Connection with the Nonresponse Problem.” Incomplete Data in Sample Surveys 3: 143–60.\n\rHorvitz, Daniel G, and Donovan J Thompson. 1952. “A Generalization of Sampling Without Replacement from a Finite Universe.” Journal of the American Statistical Association 47 (260): 663–85.\n\rKish, Leslie. 1992. “Weighting for Unequal Pi.” Journal of Official Statistics 8 (2): 183.\n\rLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. Vol. 793. John Wiley \u0026amp; Sons.\n\rLiu, Chuanhai. 2004. “Robit Regression: A Simple Robust Alternative to Logistic and Probit Regression.” Applied Bayesian Modeling and Casual Inference from Incomplete-Data Perspectives, 227–38.\n\rOh, H, and F Scheuren. 1983. “Weighting Adjustment for Unit Nonresponse. Chap. 13 in Vol. 2, Part 4 of Incomplete Data in Sample Surveys, Edited by William G. Madow, Harold Nisselson, and Ingram Olkin.” New York: Academic Press.\n\rRosenbaum, Paul R, and Donald B Rubin. 1983. “The Central Role of the Propensity Score in Observational Studies for Causal Effects.” Biometrika 70 (1): 41–55.\n\rSchafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” Psychological Methods 7 (2): 147.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"cd76644e8e97252ab237218522353d37","permalink":"/missmethods/weighting-adjustments/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/weighting-adjustments/","section":"missmethods","summary":"Weighting to compensate for nonresponse attaches weights to subjects included in the analysis to restore the representation in the original sample which is distorted because of missing values","tags":["Weighting Methods","Semiparametric Methods","Weighting Adjustments","Inverse Probability Weighting","Augmented Inverse Probability Weighting"],"title":"Weighting Adjustments","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rPatterns of incomplete data in practice often do not have the forms that allow explicit Maximum Likelihood(ML) estimates to be calculated. Suppose we have a model for the complete data \\(Y\\), with density \\(f(Y\\mid \\theta)\\), indexed by the set of unknown parameters \\(\\theta\\). Writing \\(Y=(Y_0,Y_1)\\) in terms of the observed \\(Y_0\\) and missing \\(Y_1\\) components, and assuming that the missingness mechanism is Missing At Random(MAR), we want to maximise the likelihood\n\\[\rL\\left(\\theta \\mid Y_0 \\right) = \\int f\\left(Y_0, Y_1 \\mid \\theta \\right)dY_1\r\\]\nwith respect to \\(\\theta\\). When the likelihood is differentiable and unimodal, ML estimates can be found by solving the likelihood equation\n\\[\rD_l\\left(\\theta \\mid Y_0 \\right) \\equiv \\frac{\\partial ln L\\left(\\theta \\mid Y_0 \\right)}{\\partial \\theta} = 0,\r\\]\nwhile, if a closed-form solution cannot be found, iterative methods can be applied. One of these methods is the popular Expectation Maximisation(EM) algorithm (Dempster, Laird, and Rubin (1977)).\nThe EM algorithm is a general iterative method for ML estimation in incomplete data problems. The basic idea behind it is based on a sequence of steps:\n\rReplace missing values by estimated values\n\rEstimate the parameters\n\rRe-estimate the missing values assuming the new parameter estimates are correct\n\rRe-estimate parameters\n\r\rThe procedure is then iterated until apparent convergence. Each iteration of EM consists of an expectation step (E step) and a maximisation step (M step) which ensure that, under general conditions, each iteration increases the loglikelihood \\(l(\\theta \\mid Y_0)\\). In addition, if the loglikelihood is bounded, the sequence \\(\\{l(\\theta_t \\mid Y_0), t=(0,1,\\ldots)\\}\\) converges to a stationary value of \\(l(\\theta \\mid Y_0)\\).\nThe E step and the M step\rThe M step simply consists of performing ML estimation of \\(\\theta\\) as if there were no missing data, that is, after they had been filled in. The E step finds the conditional expectation of the missing values given the observed data and current estimated parameters. In practice, EM does not necessarily substitute the missing values themselves but its key idea is that they are generally not \\(Y_0\\) but the functions of \\(Y_0\\) appearing in the complete data loglikelihood \\(l(\\theta \\mid Y)\\). Specifically, let \\(\\theta_t\\) be the current estimate of \\(\\theta\\), then the E step finds the expected complete data loglikelihood if \\(\\theta\\) were \\(\\theta_t\\):\n\\[\rQ\\left(\\theta \\mid \\theta_t \\right) = \\int l\\left(\\theta \\mid Y \\right)f\\left(Y_0 \\mid Y_1 , \\theta = \\theta_t \\right)dY_0.\r\\]\nThe M step determines \\(\\theta_{t+1}\\) by maximising this expected complete data loglikelihood:\n\\[\rQ\\left(\\theta_{t+1} \\mid \\theta_t \\right) \\geq Q\\left(\\theta \\mid \\theta_t \\right),\r\\]\nfor all \\(\\theta\\).\nUnivariate Normal Data Example\rSuppose \\(y_i\\) form a an iid sample from a Normal distribution with population mean \\(\\mu\\) and variance \\(\\sigma^2\\), for \\(i=1,\\ldots,n_{cc}\\) observed units and \\(i=n_{cc}+1,\\ldots,n\\) missing units. Under the assumption that the missingness mechanism is ignorable, the expectation of each missing \\(y_i\\) given \\(Y_{obs}\\) and \\(\\theta=(\\mu,\\sigma^2)\\) is \\(\\mu\\). Since the loglikelihood based on all \\(y_i\\) is linear in the sufficient statistics \\(\\sum_{i=1}^n y_i\\) and \\(\\sum_{i=1}^n y^2_i\\), the E step of the algorithm calculates\n\\[\rE\\left(\\sum_{i=1}^{n}y_i \\mid \\theta_t, Y_0 \\right) = \\sum_{i=1}^{n_{cc}}y_i + (n-n_{cc})\\mu_t\r\\]\nand\n\\[\rE\\left(\\sum_{i=1}^{n}y^2_i \\mid \\theta_t, Y_0 \\right) = \\sum_{i=1}^{n_{cc}}y^2_i + (n-n_{cc})\\left(\\mu^2_t + \\sigma^2_t \\right)\r\\]\nfor current estimates \\(\\theta_t=(\\mu_t,\\sigma_t)\\) of the parameters. Note that simply substituting \\(\\mu_t\\) for the missing values \\(y_{n_{cc}+1},\\ldots,y_n\\) is not correct since the term \\((n-n_{cc})(\\sigma_t^2)\\) is omitted. Without missing data, the ML estimate of \\(\\mu\\) and \\(\\sigma^2\\) are \\(\\frac{\\sum_{i=1}^ny_i}{n}\\) and \\(\\frac{\\sum_{i=1}^ny^2_i}{n}-\\left(\\frac{\\sum_{i=1}^ny_i}{n}\\right)^2\\), respectively. The M step uses the same expressions based on the current expectations of the sufficient statistics calculated in the E step. Thus, the M step calculates\n\\[\r\\mu_{t+1} = \\frac{E\\left(\\sum_{i=1}^n y_i \\mid \\theta_t, Y_0 \\right)}{n}\r\\]\nand\n\\[\r\\sigma^2_{t+1} = \\frac{E\\left(\\sum_{i=1}^n y^2_i \\mid \\theta_t, Y_0 \\right)}{n} - \\mu^2_{t+1}.\r\\]\nSetting \\(\\mu_t=\\mu_{t+1}=\\hat{\\mu}\\) and \\(\\sigma_t=\\sigma_{t+1}=\\hat{\\sigma}\\) in these equations shows that a fixed point of these iterations is \\(\\hat{\\mu}=\\frac{\\sum_{i=1}^{n_{cc}}y_i}{n_{cc}}\\) and \\(\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^{n_{cc}}y^2_i}{n_{cc}} - \\hat{\\mu}^2\\), which are the ML estimates of the parameters from \\(Y_0\\) assuming MAR and distinctness of the parameters.\n\r\rExtensions of EM\rThere are a variety of applications where the M step does not have a simple computational form. In such cases, one way to avoid an iterative M step is to increase the Q function, rather than maximising it at each iteration, which corresponds to a Generalised Expectation Maximisation(GEM) algorithm. GEM inceases the likelihood at each iteration but appropriate convergence is not guaranteed without further specification of the process of increasing the Q function. One specific case of GEM is the Expectation Conditional Maximisation(ECM) algorithm (Meng and Rubin (1993)), which replaces the M step with a sequence of \\(S\\) conditional maximisation (CM) steps, each of which maximises the Q function over \\(\\theta\\) but with some vector function of \\(\\theta\\), say \\(g_s(\\theta)\\), fixed at its previous values for \\(s=1,\\ldots,S\\). Very briefly, assume that we have a parameter \\(\\theta\\) that can be partitioned into subvectors \\(\\theta=(\\theta_1,\\ldots,\\theta_S)\\), then we can take the \\(s\\)-th of the CM steps to be maximisation with respect to \\(\\theta_s\\) with all other parameters held fixed. Alternatively, it may be useful to take the \\(s\\)-th of the CM steps to be simultaneous maximisation over all of the subvectors expect \\(\\theta_s\\), which is fixed. Because the ECM increases Q, it belongs to the class of GEM algorithms and therefore monotonically increases the likelihood of \\(\\theta\\). When the set of functions \\(g\\) is “space filling” in the sense that it allows unconstrained maximisation over \\(\\theta\\) in its parameter space, ECM converges to a stationary point under the same conditions ensuring convergence of EM.\nThe Expectation Conditional Maximisation Either(ECME) algorithm (Liu and Rubin (1994)) is another version of GEM, which replaces some of the CM steps of ECM, maximising the constrained expected complete data loglikelihood function, with steps that maximise the correspondingly constrained actual likelihood function. The algorithm has stable monotone convergence and basic simplicity implementation relative to competing faster converging methods, and can have faster convergence rate than EM or ECM, measured using either the number of iterations or actual computer time. The The Alternative Expectation Conditional Maximisation(AECM) algorithm (Meng and Van Dyk (1997)) builds on the ECME idea by maximising functions other than Q or L in particular CM steps, corresponding to varying definitions of what constitutes missing data. An iteration of AECM consists of cycles, each consisting of an E step with a particular definition of complete and missing data, followed by CM steps, which can result in enhanced computational efficiency.\n\rReferences\rDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the Em Algorithm.” Journal of the Royal Statistical Society: Series B (Methodological) 39 (1): 1–22.\n\rLiu, Chuanhai, and Donald B Rubin. 1994. “The Ecme Algorithm: A Simple Extension of Em and Ecm with Faster Monotone Convergence.” Biometrika 81 (4): 633–48.\n\rMeng, Xiao-Li, and Donald B Rubin. 1993. “Maximum Likelihood Estimation via the Ecm Algorithm: A General Framework.” Biometrika 80 (2): 267–78.\n\rMeng, Xiao-Li, and David Van Dyk. 1997. “The Em Algorithm—an Old Folk-Song Sung to a Fast New Tune.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 59 (3): 511–67.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"5d1ab0b24b19df7c82eb56dcf0551da2","permalink":"/missmethods/em-algorithm/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/em-algorithm/","section":"missmethods","summary":"An Expectation–Maximization (EM) algorithm is an iterative method to find maximum likelihood or maximum a posteriori estimates of parameters in statistical models, where the model depends on unobserved latent variables","tags":["Expectation Maximisation Algorithm","Likelihood Based Methods Ignorable"],"title":"Expectation Maximisation Algorithm","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rIt is possible to summarise the steps involved in drawing inference from incomplete data as (Daniels and Hogan (2008)):\n\rSpecification of a full data model for the response and missingness indicators \\(f(y,r)\\)\n\rSpecification of the prior distribution (within a Bayesian approach)\n\rSampling from the posterior distribution of full data parameters, given the observed data \\(Y_{obs}\\) and the missingness indicators \\(R\\)\n\r\rIdentification of a full data model, particularly the part involving the missing data \\(Y_{mis}\\), requires making unverifiable assumptions about the full data model \\(f(y,r)\\). Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between \\(R\\) and \\(Y_{mis}\\), given \\(Y_{obs}\\). Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:\nUnverifiable parametric assumptions\n\rInformative prior distributions (under a Bayesian approach)\n\r\rWe show some simple examples about how these nonignorable models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as Shared Parameter Models(SPM).\nShared Parameter Models\rThe shared parameter model approach consists in an explicit multilevel specification, where random effects \\(b\\) are modelled jointly with \\(Y\\) and \\(R\\) (Wu and Carroll (1988)). The general form of the full data modelling using a SPM approach is\n\\[\rf(y,r \\mid \\omega) = \\int f(y, r, b \\mid \\omega)db.\r\\]\nNext, specific SPMs are formulated by making assumptions about the joint distribution under the integral sign. Main advantages of this models is that they are quite easy to specify and that, through the use of random effects, high-dimensional or multilevel data modelling is relatively easy to accomplish. The main drawback is that the underlying missingness mechanism is often difficult to understand and may not have even a closed form.\nExample random coefficients selection model\rWu and Carroll (1988) specified a SPM assuming the response follow a linear random effects model\n\\[\rY_i \\mid x_i,b_i \\sim N(x_i\\beta + w_ib_i, \\Sigma_i(\\phi)),\r\\]\nwhere \\(w_i\\) are the random effects covariates with rows \\(w_i=(1,t_{ij})\\), therefore implying that each individual has a random slope and intercept. The random effects \\(b_i=(b_{i1},b_{i2})\\) are assumed to follow a bivariate normal distribution\n\\[\rb_i \\sim N(0,\\Omega),\r\\]\nwhile the hazard of dropout is Bernoulli with\n\\[\rR_{ij} \\mid R_{ij-1}=1,b_i \\sim Bern(\\pi_{ij}),\r\\]\nwhich depends on the random effects via\n\\[\rg(\\pi_{ij}) = \\psi_0 + \\psi_1b_{i1} + \\psi_2b_{i2}.\r\\]\nThe model can be seen as a special case of the general SPM formulation by noticing that the joint distribution under the integral sign can be factored as\n\\[\rf(y,r,b \\mid x, \\omega) = f(r \\mid b,x,\\psi)f(y \\mid b,x,\\beta,\\phi)f(b \\mid \\Omega)\r\\]\nunder the assumption that \\(R\\) is independent of both \\(Y_{obs}\\) and \\(Y_{mis}\\), conditionally on \\(b\\). However, integrating over the random effects, dependence between \\(R\\) and \\(Y_{mis}\\), given \\(Y_{obs}\\), is induced and therefore the model characterises a Missing Not At Random(MNAR) mechanism.\nThe conditional linear model (Wu and Bailey (1989)) can also be seen as a version of the SPM, which is formulated as\n\\[\rf(y,r,b \\mid x) = f(y \\mid r,b,x)f(b \\mid r,x)f(r \\mid x).\r\\]\n\r\rConlcusions\rTo summarise, shared parameter models are very useful for characterizing joint distributions of repeated measures and event times, and can be particularly useful as a method of data reduction when the dimension of \\(Y\\) is high. Nonetheless, their application to the problem of making full data inference from incomplete longitudinal data should be made with caution and with an eye toward justifying the required assumptions. Sensitivity analysis is an open area of research for these models.\n\rReferences\rDaniels, Michael J, and Joseph W Hogan. 2008. Missing Data in Longitudinal Studies: Strategies for Bayesian Modeling and Sensitivity Analysis. Chapman; Hall/CRC.\n\rWu, Margaret C, and Kent R Bailey. 1989. “Estimation and Comparison of Changes in the Presence of Informative Right Censoring: Conditional Linear Model.” Biometrics, 939–55.\n\rWu, Margaret C, and Raymond J Carroll. 1988. “Estimation and Comparison of Changes in the Presence of Informative Right Censoring by Modeling the Censoring Process.” Biometrics, 175–88.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"6c20666e1ae46902829ffa7f1fd8aa86","permalink":"/missmethods/shared-parameter-models/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/shared-parameter-models/","section":"missmethods","summary":"Shared Parameter Models (SPM) are typically used to handle nonignorable missingness. In these models a random effect is shared between the repeated measures model and the missing data mechanism model","tags":["Shared Parameter Models","Full Data Models under MNAR","Likelihood Based Methods Nonignorable"],"title":"Shared Parameter Models","type":"missmethods"},{"authors":null,"categories":["rubric"],"content":"\r\rA useful alternative approach to Maximum Likelihood(ML) methods, particularly when the sample size is small, is to include a reasonable prior distribution for the parameters and compute the posterior distribution of the parameters of interest. The posterior distribution for a model with ignorable missingness is\n\\[\rp(\\theta \\mid Y_0, M) \\equiv p(\\theta \\mid Y_0) \\propto p(\\theta)f(Y_0 \\mid \\theta),\r\\]\nwhere \\(p(\\theta)\\) is the prior and \\(f(Y_0 \\mid \\theta)\\) is the density of the observed data \\(Y_0\\). Simulation from the posterior without iteration can be accomplished if the likelihood can be factored into complete data components, while for general patterns of missing data, Bayesian simulation requires iteration.\nData Augmentation\rData Augmentation(Tanner and Wong (1987)), or DA, is an iterative method of simulating the posteiror distribution of \\(\\theta\\) that combines features of the Expecation Maximisation(EM) algorithm and Multiple Imputation(MI). Starting with an initial draw \\(\\theta_0\\) from an approximation to the posterior, then given the value \\(\\theta_t\\) at iteration \\(t\\):\nDraw \\(Y_{1,t+1}\\) with density \\(p(Y_1 \\mid Y_0, \\theta_t)\\) (I step).\n\rDraw \\(\\theta_{t+1}\\) with density \\(p(\\theta \\mid Y_0, Y_{1,t+1})\\) (P step).\n\r\rThe procedure is motivated by the fact that the distributions in these two steps are often much easier to draw from than either of the posteriors \\(p(Y_1 \\mid Y_0)\\) and \\(p(\\theta \\mid Y_0)\\), or the joint posterior \\(p(\\theta, Y_1 \\mid Y_0)\\). The procedure can be shown to eventually yield a draw from the joint posterior of \\(Y_1\\) and \\(\\theta\\) given \\(Y_0\\), in the sense that as \\(t\\) tends to infinity this sequence converges to a draw from the joint distribution.\nBivariate Normal Data Example\rSuppose having a sample \\(y_i=(y_{1i},y_{2i})\\) from a Bivariate Normal distribution for \\(i=1,\\ldots,n\\) units, with mean vector \\(\\mu=(\\mu_1,\\mu_2)\\) and \\(2\\times2\\) covariance matrix \\(\\Sigma\\). Assume that one group of units has \\(Y_1\\) observed and \\(Y_2\\) missing, while a second group of units has both variables observed and a third group of units has \\(Y_1\\) missing and \\(Y_2\\) observed. Under DA methods, each iteration \\(t\\) consists of an I step and a P step. In the first, missing data are replaced with draws from its conditional distribution given the observed data and current values of the parameters (rather then its conditional mean as in the EM algorithm). Because units are conditionally independent given the parameters, each missing \\(y_{2i}\\) is drawn independently as\n\\[\ry_{2i,t+1} \\sim N\\left(\\beta_{20t} + \\beta_{21t}y_{1i}, \\sigma^2_{2t} \\right),\r\\]\nwhere \\(\\beta_{20t},\\beta_{21t}\\) and \\(\\sigma^2_{2t}\\) are the \\(t\\)-th iterates of the regression parameters of \\(Y_2\\) on \\(Y_1\\). Analogously, each missing \\(y_{1i}\\) is drawn independently as\n\\[\ry_{1i,t+1} \\sim N\\left(\\beta_{10t} + \\beta_{11t}y_{2i}, \\sigma^2_{1t} \\right),\r\\]\nwhere \\(\\beta_{10t},\\beta_{11t}\\) and \\(\\sigma^2_{1t}\\) are the \\(t\\)-th iterates of the regression parameters of \\(Y_1\\) on \\(Y_2\\). In the second step, these drawn values are treated as if they were the observed values and one draw of the bivariate Normal parameters is made from the complete data posterior. In the limit, the draws are from the joint posterior of the missing values and the parameters. Thus, a run of DA generates both a draw from the posterior predictive distribution of \\(Y_1\\) and a draw from the posterior of \\(\\theta\\), and the procedure can be run \\(D\\) times to obtain \\(D\\) iid draws from the joint posterior of \\(\\theta\\) and \\(Y_1\\). Unlike the EM, estimates of the sampling covariance matrix from the filled-in data can be computed without any corrections to the estimated variances because draws from the posterior predictive distribution of the missing values are imputed in the I step of DA, rather than the conditional means as in the E step of EM. The loss of efficiency from imputing draws is limited when the posterior mean from DA is computed over many draws from the posterior.\n\r\rThe Gibbs’ Sampler\rThe Gibbs’s sampler is an iterative simulation method that is designed to yield draws from the joint posterior distribution in the case of a general pattern of missingness and provides a Bayesian analogous to the Expectation Conditonal Maximisation (ECM) algorithm for ML estimation. The Gibbs’ sampler eventually generates a draw from the distribution \\(p(x_1,\\ldots,x_J)\\) of a set of \\(J\\) random variables \\(X_1,\\ldots,X_J\\) in settings where draws from the joint distribution are hard to compute but draws from the conditional distributions \\(p(x_j \\mid x_1,\\ldots,x_{j-1},x_{j+1},\\ldots, x_J)\\) are relatively easy to compute. Initial values \\(x_{10},\\ldots,x_{J0}\\) are chosen in some way and then, given current values of \\(x_{1t},\\ldots,x_{Jt}\\) at iteration \\(t\\), new values are found by drawing from the following sequence of conditional distributions:\n\\[\rx_{1t+1} \\sim p\\left(x_1 \\mid x_{2t},\\ldots,x_{Jt} \\right),\r\\]\n\\[\rx_{2t+1} \\sim p\\left(x_2 \\mid x_{1t+1},\\ldots,x_{Jt} \\right),\r\\]\nup to\n\\[\rx_{Jt+1} \\sim p\\left(x_J \\mid x_{2t+1},\\ldots,x_{J-1t+1} \\right).\r\\]\nIt can be shown that, under general conditions, the sequence of \\(J\\) iterates converges to a draw from the joint posterior of the variables. When \\(J=2\\), the Gibbs’ sampler is the same as DA if \\(x_1=Y_1\\) and \\(x_2=\\theta\\) and the distributions condition on \\(Y_0\\). We can then obtain a draw from the joint posterior of \\(Y_1,\\theta \\mid Y_0\\) by applying the Gibbs’ sampler, where at iteration \\(t\\) for the \\(d\\)-th imputed data set:\n\\[\rY^d_{1t+1} \\sim p\\left(Y_1 \\mid Y_0, \\theta^d_{t}\\right) \\;\\;\\; \\text{and} \\;\\;\\; \\theta^d_{t+1} \\sim p\\left(\\theta \\mid Y^d_{1t+1}, Y_0\\right),\r\\]\nsuch that one run of the sampler converges to a draw from the posterior predictive distribution of \\(Y_1\\) and a draw from the posterior of \\(\\theta\\). The sampler can be run independently \\(D\\) times to generate \\(D\\) iid draws from the approximate joint posterior of \\(\\theta\\) and \\(Y_1\\). The values of \\(Y_1\\) are multiple imputations of the missing values, drawn from their posterior predictive distribution.\n\rAssessing Convergence\rAssessing convergence of the sequence of draws to the target distribution is more difficult than assessing convergence of an EM-type algorithm because there is no single target quantity to monitor like the maximum value of the likelihood. Methods have been proposed to assess convergence of a single sequence (Geyer (1992)), but a more reliable approach is to simulate \\(D\u0026gt;1\\) sequences with starting values dispersed throughout the parameter space, and the convergence of all quantities of interest can then be monitored by comparing variation between and within simulated sequences, until the “within” variation roughly equals the “between” variation. The idea is that when the distribution of each simulated sequence is close enough to the distribution of all the sequences mixed together, they can all be approximating the target distribution. Gelman and Rubin (1992) developed an explicit monitoring statistic based on the following idea. For each scalar estimand \\(\\psi\\), label the draws from \\(D\\) parallel sequences as \\(\\psi^d_{t}\\), for \\(t=1,\\ldots,T\\) iterations and \\(d=1,\\ldots,D\\) sequences, and compute the between \\(B\\) and within \\(\\bar{V}\\) sequence variances as:\n\\[\rB=\\frac{T}{D-1}\\sum_{d=1}^D(\\bar{\\psi}_{d.} - \\bar{\\psi}_{..})^2, \\;\\;\\; \\text{and} \\;\\;\\; \\bar{V}=\\frac{1}{D}\\sum_{d=1}^D s^2_{d},\r\\]\nwhere \\(\\bar{\\psi}_{d.}=\\frac{1}{T}\\sum_{t=1}^T \\psi_{dt}\\), \\(\\bar{\\psi}_{..}=\\frac{1}{D}\\sum_{d=1}^D \\bar{\\psi}_{d}\\), and \\(s^2_{d}=\\frac{1}{T-1}\\sum_{t=1}^T(\\psi_{dt} - \\bar{\\psi}_{d.})^2\\). We can then estimate the marginal posterior variance of the estimand as\n\\[\r\\widehat{Var}(\\psi \\mid Y_0) = \\frac{T-1}{T}\\hat{V} + \\frac{1}{T} B,\r\\]\nwhich will overestimate the marginal posterior variance assuming the starting distribution is appropriately over-dispersed but is unbiased under stationarity (starting distribution equals the target distribution). For any finte \\(T\\), the within variance \\(\\hat{V}\\) will underestimate the marginal variance because individual sequences have not had time to range over all the target distribution and should have smaller variance then B. In the limit as \\(T \\rightarrow \\infty\\) the expecation of \\(\\hat{V}\\) approaches the marginal variance. These facts suggest monitoring convergence by estimating the factor by which the scale of the current distribution for \\(\\psi\\) might be reduced if the simulations were continued. This is the potential scale reduction factor and is estimated by\n\\[\r\\sqrt{\\hat{R}} = \\sqrt{\\frac{\\widehat{Var}(\\psi \\mid Y_0)}{\\hat{V}}},\r\\]\nwhich declines to 1 as \\(T \\rightarrow \\infty\\). When this quantity is high, there is evidence to proceed the simulations further to improve our inference about the target distribution.\n\rOther Simulation Methods\rWhen draws from the sequence of conditional distributions forming the Gibbs’ sampler are not easy to obtain, other simulation approaches are needed. Among these there are the Sequential Imputation (Kong, Liu, and Wong (1994)), Sampling Imprtance Resampling (Gelfand and Smith (1990)), Rejection Sampling (Von Neumann and others (1951)). One of these alternatives are the Metropolis-Hastings (Metropolis et al. (1953)) algorithms, of which the Gibbs’ sampler is a particular case, which constitute the so-called Markov Chain Monte Carlo (MCMC) algorithms as the sequence of iterates forms a Markov Chain (Gelman et al. (2013)).\n\rReferences\rGelfand, Alan E, and Adrian FM Smith. 1990. “Sampling-Based Approaches to Calculating Marginal Densities.” Journal of the American Statistical Association 85 (410): 398–409.\n\rGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis. Chapman; Hall/CRC.\n\rGelman, Andrew, and Donald B Rubin. 1992. “A Single Series from the Gibbs Sampler Provides a False Sense of Security.” Bayesian Statistics 4: 625–31.\n\rGeyer, Charles J. 1992. “Practical Markov Chain Monte Carlo.” Statistical Science, 473–83.\n\rKong, Augustine, Jun S Liu, and Wing Hung Wong. 1994. “Sequential Imputations and Bayesian Missing Data Problems.” Journal of the American Statistical Association 89 (425): 278–88.\n\rMetropolis, Nicholas, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. 1953. “Equation of State Calculations by Fast Computing Machines.” The Journal of Chemical Physics 21 (6): 1087–92.\n\rTanner, Martin A, and Wing Hung Wong. 1987. “The Calculation of Posterior Distributions by Data Augmentation.” Journal of the American Statistical Association 82 (398): 528–40.\n\rVon Neumann, John, and others. 1951. “The General and Logical Theory of Automata.” 1951, 1–41.\n\r\r\r","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d3aeeaf1ddf6209f7edfb9afb1cf907c","permalink":"/missmethods/bayesian-methods/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/missmethods/bayesian-methods/","section":"missmethods","summary":"The most popular class of Bayesian iterative methods is called Markov chain Monte Carlo (MCMC), which comprises different algorithms for sampling from a probability distribution. The more steps that are included, the more closely the distribution of the sample matches the actual desired distribution","tags":["Bayesian Iterative Simulation Methods","Bayesian Inference","Likelihood Based Methods Ignorable"],"title":"Bayesian Iterative Simulation Methods","type":"missmethods"},{"authors":["Andrea Gabrio"],"categories":["news","website","missingHE"],"content":"It has been a while from my last update on this website, but this has been an incredibly busy period with lots of routine work that I had to do. Now the situation has clamed down a bit, and I have also some news to report. So, here I am finally.\nIn the past few days, I had an extremely interesting email correspondence with one guy (don\u0026rsquo;t want to say the name for privacy) interested in using my R package missingHE to do some trial-based CEA. Awesome, I thought. He was looking for some advice for how to customise some models in the package and how to get the results he wanted. Unfortunately for my pride, but fortunately for my package, we discovered a small bug in the code when trying to specify a hurdle model (two-part model) to handle the zero costs when not including any covariate inside the logit model to estimate the probability of having a zero cost. Essentially, under these specific circumstances, the function did not correctly backtransformed the estimate of the mean costs on the appropriate scale and the results provided were incorrect. Sorry about that!\nTo be honest, you can get away by simply including some baseline covariate into the logit model for the structural zero costs, in which case the estimates produced by the function are correct. I have immediately updated the package version to correct this bug on my GitHub page, where you can find the most up to date version (1.4.1). The version on CRAN will be updated at the next iteration as I have recently uploaded the 1.4.0 version in May. In the meantime, if you want to avoid having that issue above, you can download and install the updated version from my GitHub page.\nI think this is also a good chance to tell that I have updated an old paper on my on my Arxiv account. Both the content and title of the paper have changed considerably, but overall I feel that the overall message and quality of the article has improved. It is still an on-going version, but I am quite satisfied with its current status given all the effort I put into it. Have a lool in case you are interested. New title : \u0026ldquo;Joint longitudinal models for dealing with missing at random data in trial-based economic evaluations\u0026rdquo;.\nMore information on the paper can be found at this dedicated page\nI believe that is all for this update. Not much going on due to the whole lockdown situation here in the UK but hopefully things are improving a little in a two-months time we will be able to at least go to the office. Let\u0026rsquo;s see.\n","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589801643,"objectID":"1b376581825f6571a77a6bdf40de63f3","permalink":"/post/update-may/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/post/update-may/","section":"post","summary":"It has been a while from my last update on this website, but this has been an incredibly busy period with lots of routine work that I had to do. Now the situation has clamed down a bit, and I have also some news to report. So, here I am finally.\nIn the past few days, I had an extremely interesting email correspondence with one guy (don\u0026rsquo;t want to say the name for privacy) interested in using my R package missingHE to do some trial-based CEA.","tags":["News","Academic","missingHE"],"title":"Sorry, an error occurred","type":"post"},{"authors":["A Gabrio","R Hunter","AJ Mason","G Baio"],"categories":null,"content":"","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"59de577875bb81ae76f7296d27d711a0","permalink":"/publication/gabrio2019d/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/publication/gabrio2019d/","section":"publication","summary":"Health economic evaluations based on patient-level data collected alongside clinical trials (e.g. health related quality of life and resource use measures) are an important component of the process which informs resource allocation decisions. Almost inevitably, the analysis is complicated by the fact that some individuals drop out from the study, which causes their data to be unobserved at some time point. Current practice performs the evaluation by handling the missing data at the level of aggregated variables (e.g. QALYs), which are obtained by combining the economic data over the duration of the study, and are often conducted under a missing at random (MAR) assumption. However, this approach may lead to incorrect inferences since it ignores the longitudinal nature of the data and may end up discarding a considerable amount of observations from the analysis. We propose the use of joint longitudinal models to extend standard cost-effectiveness analysis methods by taking into account the longitudinal structure and incorporate all available data to improve the estimation of the targeted quantities under MAR. Our approach is compared to popular missingness approaches in trial-based analyses, motivated by an exploratory simulation study, and applied to data from two real case studies.","tags":["Missing Data","Economic Evaluations"],"title":"Joint longitudinal models for dealing with missing at random data in trial-based economic evaluations","type":"publication"},{"authors":["Andrea Gabrio"],"categories":["news","website","missingHE"],"content":"In spite of how incredibly busy I am at the moment, which is also weird considering the whole lockdown situation still going on, I managed to upload a new version (1.4.0) of my R package missingHE with exciting updates!\nFor those who do not know, missingHE is specifically designed to implement Bayesian models for the analysis of trial-based economic evaluations and provides different methods to handle missingness in either or both the effectiveness and cost outcomes. The cool new things in this version are the following:\n  First, random effects can now be specified for each model implemented in missingHE (I know, Bayesians should not talk about \u0026ldquo;random\u0026rdquo; or \u0026ldquo;fixed\u0026rdquo; effects as we know that there are no real \u0026ldquo;fixed\u0026rdquo; effects but the terms have become quite popular and many people would prefer this way). These include selection, hurdle and pattern mixture models. The package allows a flexible implementation of either random intercept only, random slope only and both random intecept and slope models based on the input given by the user. The random effects term is specified via the formula y ~ x + (x | z) where x is a covariate included also as a fixed effects in the model and z is the clustering variable over which the random effects for x are specified. It is possible to remove the random intercept if desired by adding 0 + inside the brackets (by default this is included).\n  Second, new types of posterior predictive checks can now be chosen using the function ppc for each type of model fitted using the function of the package. These include plotting the Bayesian posterior p-values (which should not be confused with the usual p-values as they are completely different) based on the posterior replications of the models and a given statistics computed from the observed data. The statistic can be provided by the user under the form of a univariate function (e.g. mean or sd) or a specific type of bivariate function (e.g. cor).\n  Third, a new generic function called coef has been added which allows to extract the regression coefficients from each type of model, either in terms of fixed effects or random effects (if specified).\n  I am quite proud of this new update as it is something I considered for a long time which is now available. If even one person find this useful, I think it will be worth all my effort. Very nice.\nOh, and yes you can also find the new version of missingHE on my GitHub page. I plan to upload a more serious tutorial on how to use all the functions of the package at some point (hopefully not too far from now).\nSo, now that all the fun part is done, I need to go back to doing meetings, reviews, writing papers, etc\u0026hellip;. It will be a quite busy period again but now I feel motivated. Let\u0026rsquo;s see for how long this will last.\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587382443,"objectID":"f8a05bdbcc1d113ee82a533eaa5469df","permalink":"/post/update2-april/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/post/update2-april/","section":"post","summary":"In spite of how incredibly busy I am at the moment, which is also weird considering the whole lockdown situation still going on, I managed to upload a new version (1.4.0) of my R package missingHE with exciting updates!\nFor those who do not know, missingHE is specifically designed to implement Bayesian models for the analysis of trial-based economic evaluations and provides different methods to handle missingness in either or both the effectiveness and cost outcomes.","tags":["News","Academic","R"],"title":"New updates for missingHE","type":"post"},{"authors":["Andrea Gabrio"],"categories":["news","website"],"content":"The lockdown proceeds also here in the UK, as in the rest of the world, and at the moment we have no clear idea how long it will last. Not much we can do apart from staying at home all the time and practicing social distancing. I am still ok with living at home 24/7 but this has affected my productivity, especially in terms of collaborating with other people.\nAlthough I have a lot of time to dedicate to some works, I am making a slow progress and the time of the day seems to fly in an instant with so many things to do. This months is particularly busy as I am trying to submit a revision for a paper which hanted me for quite a lot of time now and which I must finish by the first days of May. I am also working on side projects but these have been slowed down due to the current situation. I hope I can find the time (and the will) to do some more updates to my website by adding more tutorials and similar stuff. I do have some nice ideas about possible projects and collaboration but I need to wait until after this weird period.\nI am also planning to prepare a new version for my R package missingHE to add some nice additional features for post-processing the results of Bayesian models and to implement new types of models. These, however, will take time, which at the moment is one thing that I do have but that I also do not have.\nAnyway, not much of an update this one, but I hope things will move quicker in the next couple of months or so.\n","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585740843,"objectID":"8d52979bd90eec7c1a479a211c35f6b3","permalink":"/post/update-april/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/post/update-april/","section":"post","summary":"The lockdown proceeds also here in the UK, as in the rest of the world, and at the moment we have no clear idea how long it will last. Not much we can do apart from staying at home all the time and practicing social distancing. I am still ok with living at home 24/7 but this has affected my productivity, especially in terms of collaborating with other people.\nAlthough I have a lot of time to dedicate to some works, I am making a slow progress and the time of the day seems to fly in an instant with so many things to do.","tags":["News","Academic"],"title":"So much time but also not really","type":"post"},{"authors":["Andrea Gabrio"],"categories":["R","JAGS","item response theory"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rItem response theory (IRT) is a paradigm for investigating the relationship between an individual’s response to a single test item and their performance on an overall measure of the ability or trait that item was intended to measure. Many models exist in the IRT field for evaulating how well an item captures an underlying latent trait, but some of the most popular IRT models are logistic IRT models for dichotmous responses. In particular, the main types of models are:\n1 1 parameter logistic model\n2 2 parameter logistic model\n3 3 parameter logistic model\nThroughout this tutorial, I assume that the reader has some basic understanding of IRT model and working knowledge of a software implementation of the JAGS language. However, if this is not the case, excellent sources for learning IRT are Baker and Kim (2004), who provide a mathematically detailed introduction to IRT, and Hambleton, Swaminathan, and Rogers (1991), who give an intuitive introduction to the topic. For an in-depth description of how to implement different types of IRT models in OpenBUGS and JAGS, I also refer to this very nice review of Curtis and others (2010) and this other online tutorial.\nAt the core of all the IRT models presented in this tutorial is the Item Response Function (IRF). The IRF estimates the probability of getting item \\(j\\) “correct” as a function of item characteristics and the \\(i\\)-th individual’s latent trait/ability level (\\(\\theta_i\\)). These item response functions are defined by a logistic curve (i.e. an \\(S\\)-shape from \\(0-1\\)).\n\r1 parameter logistic model (1PLM)\rThe 1PLM is used for data collected on \\(n\\) individuals who have each given responses on \\(p\\)\rdifferent items. The items have binary outcomes, i.e. the items are scored as \\(1\\) if correct\rand \\(0\\) if not. The \\(i\\)-th individual in the sample is assumed to have a latent ability \\(\\theta_i\\), and the \\(i\\)-th individual’s response on the \\(j\\)-th item is a random variable \\(Y_{ij}\\) with a Bernoulli distribution. The probability that the \\(i\\)-th individual correctly answers the \\(j\\)-th item (i.e. the probability that \\(Y_{ij} = 1\\)) is assumed to have the following IRF form:\n\\[p_{ij} = P(Y_{ij}=1 \\mid \\theta_i,\\delta_j)=\\frac{1}{1+\\text{exp}(\\theta_i-\\delta_j)},\\]\nwhere \\(\\delta_j\\) is the difficulty parameter for item \\(j\\) of the test, and is assumed to be normally distributed according to some mean \\(\\mu_{\\delta}\\) and standard deviation \\(\\sigma_{\\delta}\\) which must be specified by the analyst. Each latent ability parameter \\(\\theta_i\\) is also assumed to be distributed according to a standard normal distribution.\nLoad the data\rI read in the data from the file wideformat.csv, which contains (simulated) data from \\(n=1000\\) individuals taking a \\(5\\)-item test. Items are coded \\(1\\) for correct and \\(0\\) for incorrect responses. When we get descriptives of the data, we see that the items differ in terms of the proportion of people who answered correctly, so we expect that we have some differences in item difficulty here.\n\u0026gt; data_dicho\u0026lt;-read.csv(\u0026quot;wideformat.csv\u0026quot;, sep = \u0026quot;,\u0026quot;)\r\u0026gt; head(data_dicho)\rID gender age Item.1 Item.2 Item.3 Item.4 Item.5\r1 person1 Male 40 0 0 0 0 0\r2 person2 Female 27 0 0 0 0 0\r3 person3 Male 13 0 0 0 0 0\r4 person4 Female 17 0 0 0 0 1\r5 person5 Female 30 0 0 0 0 1\r6 person6 Female 46 0 0 0 0 1\r\u0026gt; \u0026gt; #check proportion of correct responses by item\r\u0026gt; apply(data_dicho[,4:8], 2, sum)/nrow(data_dicho)\rItem.1 Item.2 Item.3 Item.4 Item.5 0.924 0.709 0.553 0.763 0.870 \u0026gt; \u0026gt; #summarise the data\r\u0026gt; library(psych)\r\u0026gt; describe(data_dicho)\rvars n mean sd median trimmed mad min max range skew\rID* 1 1000 500.50 288.82 500.5 500.50 370.65 1 1000 999 0.00\rgender* 2 1000 1.50 0.50 2.0 1.51 0.00 1 2 1 -0.02\rage 3 1000 25.37 14.43 25.0 25.36 17.79 1 50 49 0.01\rItem.1 4 1000 0.92 0.27 1.0 1.00 0.00 0 1 1 -3.20\rItem.2 5 1000 0.71 0.45 1.0 0.76 0.00 0 1 1 -0.92\rItem.3 6 1000 0.55 0.50 1.0 0.57 0.00 0 1 1 -0.21\rItem.4 7 1000 0.76 0.43 1.0 0.83 0.00 0 1 1 -1.24\rItem.5 8 1000 0.87 0.34 1.0 0.96 0.00 0 1 1 -2.20\rkurtosis se\rID* -1.20 9.13\rgender* -2.00 0.02\rage -1.21 0.46\rItem.1 8.22 0.01\rItem.2 -1.16 0.01\rItem.3 -1.96 0.02\rItem.4 -0.48 0.01\rItem.5 2.83 0.01\r\rFit the model\rWe fit the 1PLM to the data. First, I rename and preprocess the data to be passed to JAGS.\n\u0026gt; Y\u0026lt;-data_dicho[,4:8]\r\u0026gt; n\u0026lt;-nrow(Y)\r\u0026gt; p\u0026lt;-ncol(Y)\r\u0026gt; data_list\u0026lt;-list(\u0026quot;Y\u0026quot;,\u0026quot;n\u0026quot;,\u0026quot;p\u0026quot;)\rThen I specify the model using the following JAGS code.\n\u0026gt; model1\u0026lt;-\u0026quot;\r+ model {\r+ for (i in 1:n){\r+ for (j in 1:p){\r+ Y[i , j] ~ dbern ( prob [i , j])\r+ logit ( prob [i , j]) \u0026lt;- theta [i] - delta [j]\r+ }\r+ theta [i] ~ dnorm (0.0 , 1.0)\r+ }\r+ + for (j in 1:p){\r+ delta [j] ~ dnorm (mu.delta , pr.delta )\r+ }\r+ pr.delta \u0026lt;- pow(s.delta , -2)\r+ mu.delta ~ dnorm(0, 5)\r+ s.delta ~ dunif(0, 10)\r+ + for(i in 1:n){\r+ for(j in 1:p){\r+ Y.rep[i, j] ~ dbern(prob[i, j])\r+ loglik_y[i, j]\u0026lt;-logdensity.bern(Y[i,j], prob[i, j])\r+ }\r+ }\r+ }\r+ \u0026quot;\r\u0026gt; ## write the model to a text file\r\u0026gt; writeLines(model1, con = \u0026quot;model1PLM.txt\u0026quot;)\rNext, I define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;delta\u0026quot;, \u0026quot;theta\u0026quot;, \u0026quot;prob\u0026quot;,\u0026quot;loglik_y\u0026quot;,\u0026quot;Y.rep\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2500 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1750\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Run the JAGS code via the R2jags interface and the jags function.\n\u0026gt; library(R2jags)\r\u0026gt; library(coda)\r\u0026gt; set.seed(3456)\r\u0026gt; m1.r2jags \u0026lt;- jags(data = data_list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;model1PLM.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\r\rPlot the item characteristic curves\rItem characteristic curves (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the \\(x\\)-axis (higher values represent hight ability). Probability of a “correct” answer (\\(Y_{ij}=1\\)) to an item is plotted on the \\(y\\)-axis.\n\u0026gt; #see average value of item difficulty\r\u0026gt; diff\u0026lt;-m1.r2jags$BUGSoutput$sims.list$delta\r\u0026gt; apply(diff,2,mean)\r[1] -2.8583972 -1.0620835 -0.2608953 -1.3853516 -2.2159276\r\u0026gt; \u0026gt; #plot icc for each individual with respect to each of the 5 items\r\u0026gt; theta\u0026lt;-apply(m1.r2jags$BUGSoutput$sims.list$theta, 2, mean)\r\u0026gt; prob\u0026lt;-apply(m1.r2jags$BUGSoutput$sims.list$prob,c(2,3),mean)\r\u0026gt; plot(theta,prob[,1], type = \u0026quot;n\u0026quot;, ylab = \u0026quot;probability of correct response\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,1))\r\u0026gt; lines(theta,prob[,1],col=\u0026quot;red\u0026quot;)\r\u0026gt; lines(theta,prob[,2],col=\u0026quot;blue\u0026quot;)\r\u0026gt; lines(theta,prob[,3],col=\u0026quot;orange\u0026quot;)\r\u0026gt; lines(theta,prob[,4],col=\u0026quot;green\u0026quot;)\r\u0026gt; lines(theta,prob[,5],col=\u0026quot;black\u0026quot;)\r\u0026gt; legend(\u0026quot;bottomright\u0026quot;,legend = c(\u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;5\u0026quot;), lty = c(1), col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;black\u0026quot;), bty = \u0026quot;n\u0026quot;, cex = 0.5)\rWe see that item \\(3\\) is the most difficult item (it’s curve is farthest to the right), and item \\(1\\) is the easiest (it’s curve is farthest to the left). The same conclusions can be drawn by checking the difficulty estimates above.\n\rPlot the item information curves\rItem information curves (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the \\(1\\)st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\n\u0026gt; #plot iic for each individual with respect to each of the 5 items\r\u0026gt; neg_prob\u0026lt;-1-prob\r\u0026gt; information\u0026lt;-prob*neg_prob\r\u0026gt; plot(theta,information[,1], type = \u0026quot;n\u0026quot;, ylab = \u0026quot;information\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,0.3))\r\u0026gt; lines(theta,information[,1],col=\u0026quot;red\u0026quot;)\r\u0026gt; lines(theta,information[,2],col=\u0026quot;blue\u0026quot;)\r\u0026gt; lines(theta,information[,3],col=\u0026quot;orange\u0026quot;)\r\u0026gt; lines(theta,information[,4],col=\u0026quot;green\u0026quot;)\r\u0026gt; lines(theta,information[,5],col=\u0026quot;black\u0026quot;)\r\u0026gt; legend(\u0026quot;bottomleft\u0026quot;,legend = c(\u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;5\u0026quot;), lty = c(1), col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;black\u0026quot;), bty = \u0026quot;n\u0026quot;, cex = 0.5)\rSimilar to the ICCs, we see that item \\(3\\) provides the most information about high ability levels (the peak of its IIC is farthest to the right) and item \\(1\\) and \\(5\\) provides the most information about lower ability levels (the peak of its IIC is farthest to the left). We have seen that all ICCs and IICs for the items have the same shape in the 1PL model (i.e. all items are equally good at providing information about the latent trait). In the 2PL and 3PL models, we will see that this does not have to be the case.\nNext, we plot the information curve for the whole test. This is simply the sum of the individual IICs above. Ideally, we want a test which provides fairly good covereage of a wide range of latent ability levels. Otherwise, the test is only good at identifying a limited range of ability levels.\n\u0026gt; #plot iic for each individual with respect to whole test\r\u0026gt; information_test\u0026lt;-apply(information,1,sum)\r\u0026gt; plot(theta,information_test, type = \u0026quot;n\u0026quot;, ylab = \u0026quot;information (test)\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,1.5))\r\u0026gt; lines(theta,information_test,col=\u0026quot;black\u0026quot;,lty=2)\r\u0026gt; \u0026gt; summary(information_test)\rMin. 1st Qu. Median Mean 3rd Qu. Max. 0.5472 0.5710 0.7669 0.7756 0.9305 1.0786 \rWe see that this test provides the most information about low ability levels (the peak is around ability level \\(-1.5\\)), and less information about very high ability levels.\n\rAssess fit\rWe perform posterior predictive checks to test whether individual items fit the 1PLM by comparing quantities computed from the predictions of the model with those from the observed data. If these match reasonably well, then there is indication that the model has a good fit.\n\u0026gt; library(bayesplot)\r\u0026gt; library(ggplot2)\r\u0026gt; Y.rep\u0026lt;-m1.r2jags$BUGSoutput$sims.list$Y.rep\r\u0026gt; \u0026gt; #Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars\r\u0026gt; ppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(\u0026quot;Item 1\u0026quot;)\r\u0026gt; ppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(\u0026quot;Item 2\u0026quot;)\r\u0026gt; ppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(\u0026quot;Item 3\u0026quot;)\r\u0026gt; ppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(\u0026quot;Item 4\u0026quot;)\r\u0026gt; ppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(\u0026quot;Item 5\u0026quot;)\r\rPlot ability scores\rWe can conclude by summarising and plotting the latent ability scores of the participants\n\u0026gt; #summary stats for theta across both iterations and individuals\r\u0026gt; summary(theta)\rMin. 1st Qu. Median Mean 3rd Qu. Max. -2.074587 -0.483828 0.078138 0.001189 0.692185 0.763989 \u0026gt; \u0026gt; #histogram and kernel density plot of theta averaged across iterations\r\u0026gt; dens.theta\u0026lt;-density(theta, bw=0.3)\r\u0026gt; hist(theta, breaks = 5, prob = T)\r\u0026gt; lines(dens.theta, lwd=2, col=\u0026quot;red\u0026quot;)\rWe see that the mean of ability scores is around \\(0\\), and the standard deviation about \\(1\\) (these are estimated ability scores are standardised).\n\r\r2 parameter logistic model (2PLM)\rIn the 2PLM, the probability that the \\(i\\)-th individual correctly answers the \\(j\\)-th item (i.e. the probability that \\(Y_{ij} = 1\\)) is assumed to have the following IRF form:\n\\[p_{ij} = P(Y_{ij}=1 \\mid \\theta_i,\\delta_j,\\alpha_j)=\\frac{1}{1+\\text{exp}(\\alpha_j(\\theta_i-\\delta_j))},\\]\nwhere \\(\\alpha_j\\) is the discrimination parameter for item \\(j\\) of the test, and is assumed to be positive and normally distributed (truncated above zero) according to some mean \\(\\mu_{\\alpha}\\) and standard deviation \\(\\sigma_{\\alpha}\\) which must be specified by the analyst. The item discriminability \\(\\alpha_j\\) indicates how well an item is able to discriminate between persons with different ability levels. Item discriminability is reflected in the steepness of the slope of the ICC.\nFit the model\rWe fit the 2PLM to the data using the following JAGS code.\n\u0026gt; model2\u0026lt;-\u0026quot;\r+ model {\r+ for (i in 1:n){\r+ for (j in 1:p){\r+ Y[i , j] ~ dbern ( prob [i , j])\r+ logit ( prob [i , j]) \u0026lt;- alpha [j] * ( theta [i] - delta [j])\r+ }\r+ theta [i] ~ dnorm (0.0 , 1.0)\r+ }\r+ + for (j in 1:p){\r+ delta [j] ~ dnorm (m.delta , pr.delta )\r+ alpha [j] ~ dnorm (m.alpha , pr.alpha ) T(0 , )\r+ }\r+ pr.delta \u0026lt;- pow(s.delta , -2)\r+ pr.alpha \u0026lt;- pow(s.alpha , -2)\r+ + m.delta ~ dnorm(0,5)\r+ m.alpha ~ dnorm(0,5)\r+ s.delta ~ dunif(0,10)\r+ s.alpha ~ dunif(0,10)\r+ + for(i in 1:n){\r+ for(j in 1:p){\r+ Y.rep[i, j] ~ dbern(prob[i, j])\r+ loglik_y[i, j]\u0026lt;-logdensity.bern(Y[i,j], prob[i, j])\r+ }\r+ }\r+ }\r+ \u0026quot;\r\u0026gt; ## write the model to a text file\r\u0026gt; writeLines(model2, con = \u0026quot;model2PLM.txt\u0026quot;)\rNext, I define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;delta\u0026quot;, \u0026quot;alpha\u0026quot;,\u0026quot;theta\u0026quot;, \u0026quot;prob\u0026quot;,\u0026quot;loglik_y\u0026quot;,\u0026quot;Y.rep\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2500 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1750\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Run the JAGS code via the R2jags interface and the jags function.\n\u0026gt; m2.r2jags \u0026lt;- jags(data = data_list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;model2PLM.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\r\rPlot the item characteristic curves\rItem characteristic curves (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the \\(x\\)-axis (higher values represent hight ability). Probability of a “correct” answer (\\(Y_{ij}=1\\)) to an item is plotted on the \\(y\\)-axis.\n\u0026gt; discr\u0026lt;-m2.r2jags$BUGSoutput$sims.list$alpha\r\u0026gt; diff\u0026lt;-m2.r2jags$BUGSoutput$sims.list$delta\r\u0026gt; #see average value of item difficulty\r\u0026gt; apply(diff,2,mean)\r[1] -3.4898494 -1.4275914 -0.3210807 -1.8517091 -3.0020341\r\u0026gt; #see average value of item discriminability\r\u0026gt; apply(discr,2,mean)\r[1] 0.8300819 0.7204114 0.7766516 0.7243196 0.7142145\r\u0026gt; \u0026gt; #plot icc for each individual with respect to each of the 5 items\r\u0026gt; theta\u0026lt;-apply(m2.r2jags$BUGSoutput$sims.list$theta, 2, mean)\r\u0026gt; prob\u0026lt;-apply(m2.r2jags$BUGSoutput$sims.list$prob,c(2,3),mean)\r\u0026gt; plot(theta,prob[,1], type = \u0026quot;n\u0026quot;, ylab = \u0026quot;probability of correct response\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,1))\r\u0026gt; lines(theta,prob[,1],col=\u0026quot;red\u0026quot;)\r\u0026gt; lines(theta,prob[,2],col=\u0026quot;blue\u0026quot;)\r\u0026gt; lines(theta,prob[,3],col=\u0026quot;orange\u0026quot;)\r\u0026gt; lines(theta,prob[,4],col=\u0026quot;green\u0026quot;)\r\u0026gt; lines(theta,prob[,5],col=\u0026quot;black\u0026quot;)\r\u0026gt; legend(\u0026quot;bottomright\u0026quot;,legend = c(\u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;5\u0026quot;), lty = c(1), col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;black\u0026quot;), bty = \u0026quot;n\u0026quot;, cex = 0.5)\rUnlike the ICCs for the 1PLM, the ICCs for the 2PLM do not all have the same shape. Item curves which are more “spread out” indicate lower discriminability (i.e. that individuals of a range of ability levels have some probability of getting the item correct). Compare this to an item with high discriminability (steep slope): for this item, we have a better estimate of the individual’s latent ability based on whether they got the question right or wrong. Because of the differing slopes, the rank-order of item difficulty changes across different latent ability levels. We can see that item \\(3\\) is still the most difficult item (i.e. lowest probability of getting correct for most latent trait values, up until about \\(\\theta=0.2\\)). Items \\(1\\) and \\(5\\) are the easiest.\n\rPlot the item information curves\rItem information curves (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the \\(1\\)st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\n\u0026gt; #plot iic for each individual with respect to each of the 5 items\r\u0026gt; neg_prob\u0026lt;-1-prob\r\u0026gt; information\u0026lt;-prob*neg_prob\r\u0026gt; information2\u0026lt;-information*(apply(discr,2,mean))^2\r\u0026gt; plot(theta,information2[,1], type = \u0026quot;n\u0026quot;, ylab = \u0026quot;information\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,0.3))\r\u0026gt; lines(theta,information2[,1],col=\u0026quot;red\u0026quot;)\r\u0026gt; lines(theta,information2[,2],col=\u0026quot;blue\u0026quot;)\r\u0026gt; lines(theta,information2[,3],col=\u0026quot;orange\u0026quot;)\r\u0026gt; lines(theta,information2[,4],col=\u0026quot;green\u0026quot;)\r\u0026gt; lines(theta,information2[,5],col=\u0026quot;black\u0026quot;)\r\u0026gt; legend(\u0026quot;bottomleft\u0026quot;,legend = c(\u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;5\u0026quot;), lty = c(1), col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;black\u0026quot;), bty = \u0026quot;n\u0026quot;, cex = 0.5)\rThe item IICs demonstrate that some items provide more information about latent ability for different ability levels. The higher the item discriminability estimate, the more information an item provides about ability levels around the point where there is a \\(50\\%\\) chance of getting the item right (i.e. the steepest point in the ICC slope). For example, item \\(3\\) (orange) clearly provides the most information at high ability levels, around \\(\\theta=-0.5\\), but almost no information about low ability levels (\\(\u0026lt; -1\\)) because the item is already too hard for those participants. In contrast, item \\(1\\) (red), which has low discriminability, doesn’t give very much information overall, but covers a wide range of ability levels.\nNext, we plot the item information curve for the whole test. This is the sum of all the item IICs above.\n\u0026gt; #plot iic for each individual with respect to whole test\r\u0026gt; information_test\u0026lt;-apply(information2,1,sum)\r\u0026gt; plot(theta,information_test, type = \u0026quot;n\u0026quot;, ylab = \u0026quot;information (test)\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,1.5))\r\u0026gt; lines(theta,information_test,col=\u0026quot;black\u0026quot;,lty=2)\r\u0026gt; \u0026gt; summary(information_test)\rMin. 1st Qu. Median Mean 3rd Qu. Max. 0.3242 0.3973 0.4500 0.4541 0.5182 0.7528 \rThe IIC for the whole test shows that the test provides the most information for slightly-lower-than average ability levels (about \\(\\theta=-1\\)), but does not provide much information about extremely high ability levels.\n\rAssess fit\rNext, we check how well the 2PLM fits the data.\n\u0026gt; Y.rep\u0026lt;-m2.r2jags$BUGSoutput$sims.list$Y.rep\r\u0026gt; \u0026gt; #Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars\r\u0026gt; ppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(\u0026quot;Item 1\u0026quot;)\r\u0026gt; ppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(\u0026quot;Item 2\u0026quot;)\r\u0026gt; ppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(\u0026quot;Item 3\u0026quot;)\r\u0026gt; ppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(\u0026quot;Item 4\u0026quot;)\r\u0026gt; ppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(\u0026quot;Item 5\u0026quot;)\rWe can also compare the fit of the 1PLM and 2PLM using relative measures of fit or information criteria. These are computed based on the deviance and a penalty for model complexity called the effective number of parameters \\(p\\). Here we consider two Bayesian measures known as the Widely Applicable (WAIC) and Leave One Out (LOOIC) Information Criterion, which can be easily obtained through the functions waic and loo in the package loo.\n\u0026gt; library(loo)\r\u0026gt; #extract log-likelihood\r\u0026gt; loglik_m1\u0026lt;-m1.r2jags$BUGSoutput$sims.list$loglik_y\r\u0026gt; loglik_m2\u0026lt;-m2.r2jags$BUGSoutput$sims.list$loglik_y\r\u0026gt; \u0026gt; #waic\r\u0026gt; waic_m1\u0026lt;-waic(loglik_m1)\r\u0026gt; waic_m2\u0026lt;-waic(loglik_m2)\r\u0026gt; \u0026gt; #looic\r\u0026gt; looic_m1\u0026lt;-loo(loglik_m1)\r\u0026gt; looic_m2\u0026lt;-loo(loglik_m2)\r\u0026gt; \u0026gt; #compare\r\u0026gt; table_waic\u0026lt;-rbind(waic_m1$estimates[2:3,1],waic_m2$estimates[2:3,1])\r\u0026gt; table_looic\u0026lt;-rbind(looic_m1$estimates[2:3,1],looic_m2$estimates[2:3,1])\r\u0026gt; rownames(table_waic)\u0026lt;-rownames(table_looic)\u0026lt;-c(\u0026quot;1PLM\u0026quot;,\u0026quot;2PLM\u0026quot;)\r\u0026gt; knitr::kable(cbind(table_waic,table_looic), \u0026quot;pandoc\u0026quot;, align = \u0026quot;c\u0026quot;)\r\r\r\rp_waic\rwaic\rp_loo\rlooic\r\r\r\r1PLM\r1.525000\r6.517631\r1.867805\r7.203242\r\r2PLM\r1.395955\r6.446221\r1.649366\r6.953044\r\r\r\rBoth criteria suggest that the 2PLM has a slightly better fit to the data.\n\rPlot ability scores\rPlot the density curve of the estimated ability scores\n\u0026gt; #summary stats for theta across both iterations and individuals\r\u0026gt; summary(theta)\rMin. 1st Qu. Median Mean 3rd Qu. Max. -1.917888 -0.419394 0.078240 0.001822 0.608518 0.677354 \u0026gt; \u0026gt; #histogram and kernel density plot of theta averaged across iterations\r\u0026gt; dens.theta\u0026lt;-density(theta, bw=0.3)\r\u0026gt; hist(theta, breaks = 5, prob = T)\r\u0026gt; lines(dens.theta, lwd=2, col=\u0026quot;red\u0026quot;)\rWe see that the mean of ability scores is around \\(0\\), and the standard deviation about \\(1\\) (these are estimated ability scores are standardised).\n\r\r3 parameter logistic model (3PLM)\rIn the 3PLM, the probability that the \\(i\\)-th individual correctly answers the \\(j\\)-th item (i.e. the probability that \\(Y_{ij} = 1\\)) is assumed to have the following IRF form:\n\\[p_{ij} = P(Y_{ij}=1 \\mid \\theta_i,\\delta_j,\\alpha_j,\\eta_j)=\\eta_j + (1-\\eta_j) \\frac{1}{1+\\text{exp}(\\alpha_j(\\theta_i-\\delta_j))},\\]\nwhere \\(\\eta_j\\) is the guessing parameter. Under this model, individuals with zero ability have a nonzero chance of endorsing any item, just by guessing randomly. The guessing parameter is reflected in the \\(y\\)-intercept (i.e. probability) of the ICC. The parameter is normally distributed according to some mean \\(\\mu_{\\eta}\\) and standard deviation \\(\\sigma_{\\eta}\\) which must be specified by the analyst.\nFit the model\rWe fit the 3PLM to the data using the following JAGS code.\n\u0026gt; model3\u0026lt;-\u0026quot;\r+ model {\r+ for (i in 1:n){\r+ for (j in 1:p){\r+ Y[i , j] ~ dbern ( prob [i , j])\r+ logit ( prob.star [i , j]) \u0026lt;- alpha [j]*( theta [i] - delta [j])\r+ prob [i , j] \u0026lt;- eta[j ] + (1 - eta[j]) * prob.star [i , j]\r+ }\r+ theta [i] ~ dnorm (0.0 , 1.0)\r+ }\r+ + for (j in 1:p){\r+ delta [j] ~ dnorm (m.delta , pr.delta )\r+ alpha [j] ~ dnorm (m.alpha , pr.alpha ) T(0 , )\r+ eta[j] ~ dbeta (a.eta , b.eta)\r+ }\r+ pr.delta \u0026lt;- pow(s.delta , -2)\r+ pr.alpha \u0026lt;- pow(s.alpha , -2)\r+ + m.delta ~ dnorm(0,5)\r+ m.alpha ~ dnorm(0,5)\r+ s.delta ~ dunif(0,10)\r+ s.alpha ~ dunif(0,10)\r+ a.eta ~ dunif(0,100)\r+ b.eta ~ dunif(0,100)\r+ + for(i in 1:n){\r+ for(j in 1:p){\r+ Y.rep[i, j] ~ dbern(prob[i, j])\r+ loglik_y[i, j]\u0026lt;-logdensity.bern(Y[i,j], prob[i, j])\r+ }\r+ }\r+ }\r+ \u0026quot;\r\u0026gt; ## write the model to a text file\r\u0026gt; writeLines(model3, con = \u0026quot;model3PLM.txt\u0026quot;)\rNext, I define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;delta\u0026quot;, \u0026quot;alpha\u0026quot;, \u0026quot;eta\u0026quot;,\u0026quot;theta\u0026quot;, \u0026quot;prob\u0026quot;,\u0026quot;loglik_y\u0026quot;,\u0026quot;Y.rep\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2500 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1750\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Run the JAGS code via the R2jags interface and the jags function.\n\u0026gt; m3.r2jags \u0026lt;- jags(data = data_list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;model3PLM.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\r\rPlot the item characteristic curves\rItem characteristic curves (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the \\(x\\)-axis (higher values represent hight ability). Probability of a “correct” answer (\\(Y_{ij}=1\\)) to an item is plotted on the \\(y\\)-axis.\n\u0026gt; discr\u0026lt;-m3.r2jags$BUGSoutput$sims.list$alpha\r\u0026gt; diff\u0026lt;-m3.r2jags$BUGSoutput$sims.list$delta\r\u0026gt; gues\u0026lt;-m3.r2jags$BUGSoutput$sims.list$eta\r\u0026gt; #see average value of item difficulty\r\u0026gt; apply(diff,2,mean)\r[1] -2.8361776 -0.6672983 0.3323608 -1.0648926 -2.1758250\r\u0026gt; #see average value of item discriminability\r\u0026gt; apply(discr,2,mean)\r[1] 0.9249227 1.1278880 1.4997368 0.8733855 0.8561385\r\u0026gt; #see average value of item guessing\r\u0026gt; apply(gues,2,mean)\r[1] 0.2446947 0.2360422 0.2150625 0.2438348 0.2395731\r\u0026gt; \u0026gt; #plot icc for each individual with respect to each of the 5 items\r\u0026gt; theta\u0026lt;-apply(m3.r2jags$BUGSoutput$sims.list$theta, 2, mean)\r\u0026gt; prob\u0026lt;-apply(m3.r2jags$BUGSoutput$sims.list$prob,c(2,3),mean)\r\u0026gt; plot(theta,prob[,1], type = \u0026quot;n\u0026quot;, ylab = \u0026quot;probability of correct response\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,1))\r\u0026gt; lines(theta,prob[,1],col=\u0026quot;red\u0026quot;)\r\u0026gt; lines(theta,prob[,2],col=\u0026quot;blue\u0026quot;)\r\u0026gt; lines(theta,prob[,3],col=\u0026quot;orange\u0026quot;)\r\u0026gt; lines(theta,prob[,4],col=\u0026quot;green\u0026quot;)\r\u0026gt; lines(theta,prob[,5],col=\u0026quot;black\u0026quot;)\r\u0026gt; legend(\u0026quot;bottomright\u0026quot;,legend = c(\u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;5\u0026quot;), lty = c(1), col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;black\u0026quot;), bty = \u0026quot;n\u0026quot;, cex = 0.5)\rThe slopes of the ICCs look very similar to those of the 2PLM. We can see that all items have \\(y\\)-intercepts greater than zero, so that even at very low ability levels, there is some chance of getting these items correct (via guessing).\n\rPlot the item information curves\rItem information curves (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the \\(1\\)st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\nHere I plot the IICs using points, rather than lines, to better display the patterns of the individuals, which vary substantially according to whether the item was correctly chosen due to chance or not.\n\u0026gt; #plot iic for each individual with respect to each of the 5 items\r\u0026gt; neg_prob\u0026lt;-1-prob\r\u0026gt; information.p1\u0026lt;-neg_prob/prob\r\u0026gt; information.p2\u0026lt;-(prob-apply(gues,2,mean))^2/(1-apply(gues,2,mean))^2\r\u0026gt; information3\u0026lt;-(apply(discr,2,mean))^2*(information.p2)*(information.p1)\r\u0026gt; plot(theta,information3[,1], type = \u0026quot;n\u0026quot;, ylab = \u0026quot;information\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,0.7))\r\u0026gt; points(theta,information3[,1],col=\u0026quot;red\u0026quot;)\r\u0026gt; points(theta,information3[,2],col=\u0026quot;blue\u0026quot;)\r\u0026gt; points(theta,information3[,3],col=\u0026quot;orange\u0026quot;)\r\u0026gt; points(theta,information3[,4],col=\u0026quot;green\u0026quot;)\r\u0026gt; points(theta,information3[,5],col=\u0026quot;black\u0026quot;)\r\u0026gt; legend(\u0026quot;bottomleft\u0026quot;,legend = c(\u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;5\u0026quot;), lty = c(1), col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;black\u0026quot;), bty = \u0026quot;n\u0026quot;, cex = 0.5)\rNext, we plot the item information curve for the whole test. This is the sum of all the item IICs above.\n\u0026gt; #plot iic for each individual with respect to whole test\r\u0026gt; information_test\u0026lt;-apply(information3,1,sum)\r\u0026gt; plot(theta,information_test, type = \u0026quot;n\u0026quot;, ylab = \u0026quot;information (test)\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,1.5))\r\u0026gt; points(theta,information_test,col=\u0026quot;black\u0026quot;,lty=2)\r\u0026gt; \u0026gt; summary(information_test)\rMin. 1st Qu. Median Mean 3rd Qu. Max. 0.3793 0.4489 0.5062 0.6978 0.7956 1.5096 \r\rAssess fit\rNext, we check how well the 3PLM fits the data.\n\u0026gt; Y.rep\u0026lt;-m3.r2jags$BUGSoutput$sims.list$Y.rep\r\u0026gt; \u0026gt; #Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars\r\u0026gt; ppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(\u0026quot;Item 1\u0026quot;)\r\u0026gt; ppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(\u0026quot;Item 2\u0026quot;)\r\u0026gt; ppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(\u0026quot;Item 3\u0026quot;)\r\u0026gt; ppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(\u0026quot;Item 4\u0026quot;)\r\u0026gt; ppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(\u0026quot;Item 5\u0026quot;)\rWe can also compare the fit of the 1PLM, 2PLM and 3PLM using relative measures of fit or information criteria. These are computed based on the deviance and a penalty for model complexity called the effective number of parameters \\(p\\). Here we consider two Bayesian measures known as the Widely Applicable (WAIC) and Leave One Out (LOOIC) Information Criterion, which can be easily obtained through the functions waic and loo in the package loo.\n\u0026gt; #extract log-likelihood\r\u0026gt; loglik_m3\u0026lt;-m3.r2jags$BUGSoutput$sims.list$loglik_y\r\u0026gt; \u0026gt; #waic\r\u0026gt; waic_m3\u0026lt;-waic(loglik_m3)\r\u0026gt; \u0026gt; #looic\r\u0026gt; looic_m3\u0026lt;-loo(loglik_m3)\r\u0026gt; \u0026gt; #compare\r\u0026gt; table_waic\u0026lt;-rbind(waic_m1$estimates[2:3,1],waic_m2$estimates[2:3,1],waic_m3$estimates[2:3,1])\r\u0026gt; table_looic\u0026lt;-rbind(looic_m1$estimates[2:3,1],looic_m2$estimates[2:3,1],looic_m3$estimates[2:3,1])\r\u0026gt; rownames(table_waic)\u0026lt;-rownames(table_looic)\u0026lt;-c(\u0026quot;1PLM\u0026quot;,\u0026quot;2PLM\u0026quot;,\u0026quot;3PLM\u0026quot;)\r\u0026gt; knitr::kable(cbind(table_waic,table_looic), \u0026quot;pandoc\u0026quot;, align = \u0026quot;c\u0026quot;)\r\r\r\rp_waic\rwaic\rp_loo\rlooic\r\r\r\r1PLM\r1.525000\r6.517631\r1.867805\r7.203242\r\r2PLM\r1.395955\r6.446221\r1.649366\r6.953044\r\r3PLM\r1.368535\r6.395247\r1.654432\r6.967042\r\r\r\rBoth criteria suggest that both 1PLM and 2PLM have a better fit to the data than 3PLM.\n\rPlot ability scores\rPlot the density curve of the estimated ability scores\n\u0026gt; #summary stats for theta across both iterations and individuals\r\u0026gt; summary(theta)\rMin. 1st Qu. Median Mean 3rd Qu. Max. -1.7435119 -0.4170588 0.0471025 0.0006053 0.6298597 0.7189831 \u0026gt; \u0026gt; #histogram and kernel density plot of theta averaged across iterations\r\u0026gt; dens.theta\u0026lt;-density(theta, bw=0.3)\r\u0026gt; hist(theta, breaks = 5, prob = T)\r\u0026gt; lines(dens.theta, lwd=2, col=\u0026quot;red\u0026quot;)\rWe see that the mean of ability scores is around \\(0\\), and the standard deviation about \\(1\\) (these are estimated ability scores are standardised).\n\r\rConclusions\rThe use of JAGS software to estimate IRT models allows the user to alter existing\rcode to fit new variations of current models that cannot be fit in existing software packages. For example, longitudinal or multilevel data can easily be accommodated by small changes to existing JAGS code. The JAGS software takes care of the “grunt work” involved in estimating model parameters by constructing an MCMC algorithm to sample from the posterior distribution. Using JAGS frees the user to experiment with different models that may be more appropriate for specialised data than the models that can currently be fit in other software packages. Of course, more complicated models involve more parameters than simpler models, and the analyst must specify prior distributions for these new parameters. This is a small price to pay, however, for the flexibility that the Bayesian framework and JAGS software provide.\n\rReferences\rBaker, Frank B, and Seock-Ho Kim. 2004. Item Response Theory: Parameter Estimation Techniques. CRC Press.\n\rCurtis, S McKay, and others. 2010. “BUGS Code for Item Response Theory.” Journal of Statistical Software 36 (1): 1–34.\n\rHambleton, Ronald K, Hariharan Swaminathan, and H Jane Rogers. 1991. Fundamentals of Item Response Theory. Sage.\n\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1584843194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584843194,"objectID":"dbe2474697249e84df01e41ee5502cec","permalink":"/jags/irt-jags/irt-jags/","publishdate":"2020-03-21T21:13:14-05:00","relpermalink":"/jags/irt-jags/irt-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","item response theory"],"title":"Item Response Theory Models - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","STAN","item response theory"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nOverview\rItem response theory (IRT) is a paradigm for investigating the relationship between an individual’s response to a single test item and their performance on an overall measure of the ability or trait that item was intended to measure. Many models exist in the IRT field for evaulating how well an item captures an underlying latent trait, but some of the most popular IRT models are logistic IRT models for dichotmous responses. In particular, the main types of models are:\n1 1 parameter logistic model\n2 2 parameter logistic model\n3 3 parameter logistic model\nThroughout this tutorial, I assume that the reader has some basic understanding of IRT model and working knowledge of a software implementation of the STAN language. However, if this is not the case, excellent sources for learning IRT are Baker and Kim (2004), who provide a mathematically detailed introduction to IRT, and Hambleton, Swaminathan, and Rogers (1991), who give an intuitive introduction to the topic. For an in-depth description of how to implement different types of IRT models in STAN, I also refer to this very nice review of Luo and Jiao (2018) and this other online tutorial.\nAt the core of all the IRT models presented in this tutorial is the Item Response Function (IRF). The IRF estimates the probability of getting item \\(j\\) “correct” as a function of item characteristics and the \\(i\\)-th individual’s latent trait/ability level (\\(\\theta_i\\)). These item response functions are defined by a logistic curve (i.e. an \\(S\\)-shape from \\(0-1\\)).\n\r1 parameter logistic model (1PLM)\rThe 1PLM is used for data collected on \\(n\\) individuals who have each given responses on \\(p\\)\rdifferent items. The items have binary outcomes, i.e. the items are scored as \\(1\\) if correct and \\(0\\) if not. The \\(i\\)-th individual in the sample is assumed to have a latent ability \\(\\theta_i\\), and the \\(i\\)-th individual’s response on the \\(j\\)-th item is a random variable \\(Y_{ij}\\) with a Bernoulli distribution. The probability that the \\(i\\)-th individual correctly answers the \\(j\\)-th item (i.e. the probability that \\(Y_{ij} = 1\\)) is assumed to have the following IRF form:\n\\[p_{ij} = P(Y_{ij}=1 \\mid \\theta_i,\\delta_j)=\\frac{1}{1+\\text{exp}(\\theta_i-\\delta_j)},\\]\nwhere \\(\\delta_j\\) is the difficulty parameter for item \\(j\\) of the test, and is assumed to be normally distributed according to some mean \\(\\mu_{\\delta}\\) and standard deviation \\(\\sigma_{\\delta}\\) which must be specified by the analyst. Each latent ability parameter \\(\\theta_i\\) is also assumed to be distributed according to a standard normal distribution.\nLoad the data\rI read in the data from the file wideformat.csv, which contains (simulated) data from \\(n=1000\\) individuals taking a \\(5\\)-item test. Items are coded \\(1\\) for correct and \\(0\\) for incorrect responses. When we get descriptives of the data, we see that the items differ in terms of the proportion of people who answered correctly, so we expect that we have some differences in item difficulty here.\n\u0026gt; data_dicho\u0026lt;-read.csv(\u0026quot;wideformat.csv\u0026quot;, sep = \u0026quot;,\u0026quot;)\r\u0026gt; head(data_dicho)\rID gender age Item.1 Item.2 Item.3 Item.4 Item.5\r1 person1 Male 40 0 0 0 0 0\r2 person2 Female 27 0 0 0 0 0\r3 person3 Male 13 0 0 0 0 0\r4 person4 Female 17 0 0 0 0 1\r5 person5 Female 30 0 0 0 0 1\r6 person6 Female 46 0 0 0 0 1\r\u0026gt; \u0026gt; #check proportion of correct responses by item\r\u0026gt; apply(data_dicho[,4:8], 2, sum)/nrow(data_dicho)\rItem.1 Item.2 Item.3 Item.4 Item.5 0.924 0.709 0.553 0.763 0.870 \u0026gt; \u0026gt; #summarise the data\r\u0026gt; library(psych)\r\u0026gt; describe(data_dicho)\rvars n mean sd median trimmed mad min max range skew\rID* 1 1000 500.50 288.82 500.5 500.50 370.65 1 1000 999 0.00\rgender* 2 1000 1.50 0.50 2.0 1.51 0.00 1 2 1 -0.02\rage 3 1000 25.37 14.43 25.0 25.36 17.79 1 50 49 0.01\rItem.1 4 1000 0.92 0.27 1.0 1.00 0.00 0 1 1 -3.20\rItem.2 5 1000 0.71 0.45 1.0 0.76 0.00 0 1 1 -0.92\rItem.3 6 1000 0.55 0.50 1.0 0.57 0.00 0 1 1 -0.21\rItem.4 7 1000 0.76 0.43 1.0 0.83 0.00 0 1 1 -1.24\rItem.5 8 1000 0.87 0.34 1.0 0.96 0.00 0 1 1 -2.20\rkurtosis se\rID* -1.20 9.13\rgender* -2.00 0.02\rage -1.21 0.46\rItem.1 8.22 0.01\rItem.2 -1.16 0.01\rItem.3 -1.96 0.02\rItem.4 -0.48 0.01\rItem.5 2.83 0.01\r\rFit the model\rWe fit the 1PLM to the data. First, I rename and preprocess the data to be passed to STAN.\n\u0026gt; Y\u0026lt;-data_dicho[,4:8]\r\u0026gt; n\u0026lt;-nrow(Y)\r\u0026gt; p\u0026lt;-ncol(Y)\r\u0026gt; data_list\u0026lt;-list(Y=Y,n=n,p=p)\rThen I specify the model using the following STAN code.\n\u0026gt; model1\u0026lt;-\u0026quot;\r+ data {\r+ int\u0026lt;lower=0\u0026gt; n;\r+ int\u0026lt;lower=0\u0026gt; p;\r+ int\u0026lt;lower=0,upper=1\u0026gt; Y[n,p];\r+ }\r+ parameters {\r+ vector[n] theta;\r+ vector[p] delta;\r+ real mu_delta;\r+ real\u0026lt;lower=0\u0026gt; sigma_delta;\r+ }\r+ transformed parameters{\r+ vector\u0026lt;lower=0,upper=1\u0026gt;[p] prob[n];\r+ for(i in 1:n){\r+ for (j in 1:p){\r+ prob[i,j] = inv_logit(theta[i] - delta[j]);\r+ }\r+ }\r+ }\r+ model {\r+ theta ~ normal(0,1);\r+ delta ~ normal(mu_delta,sigma_delta);\r+ mu_delta ~ normal(0,5);\r+ sigma_delta ~ cauchy(0,5);\r+ for(i in 1:n){\r+ for (j in 1:p){\r+ Y[i,j] ~ bernoulli(prob[i,j]);\r+ }\r+ }\r+ }\r+ generated quantities {\r+ vector[p] loglik_y[n];\r+ vector[p] Y_rep[n];\r+ for (i in 1: n){\r+ for (j in 1: p){\r+ loglik_y[i,j] = bernoulli_lpmf(Y[i,j] | prob[i,j]);\r+ Y_rep[i,j] = bernoulli_rng(prob[i,j]); + }\r+ }\r+ }\r+ \u0026quot;\r\u0026gt; ## write the model to a text file\r\u0026gt; writeLines(model1, con = \u0026quot;model1PLM.stan\u0026quot;)\rNext, I define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;delta\u0026quot;, \u0026quot;theta\u0026quot;, \u0026quot;prob\u0026quot;,\u0026quot;loglik_y\u0026quot;,\u0026quot;Y_rep\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2500 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1750\rStart the STAN model (check the model, load data into the model, specify the number of chains and compile the model). Run the STAN code via the rstan interface and the stan function.\n\u0026gt; library(rstan)\r\u0026gt; set.seed(3456)\r\u0026gt; model1_stan\u0026lt;- stan(data = data_list, file = \u0026quot;model1PLM.stan\u0026quot;, + chains = nChains, pars = params, iter = nIter, + warmup = burnInSteps, thin = thinSteps)\r\rPlot the item characteristic curves\rItem characteristic curves (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the \\(x\\)-axis (higher values represent hight ability). Probability of a “correct” answer (\\(Y_{ij}=1\\)) to an item is plotted on the \\(y\\)-axis.\n\u0026gt; #extract parameters\r\u0026gt; model1_stan_par\u0026lt;-extract(model1_stan)\r\u0026gt; \u0026gt; #see average value of item difficulty\r\u0026gt; diff\u0026lt;-model1_stan_par$delta\r\u0026gt; apply(diff,2,mean)\r[1] -2.8564001 -1.0625142 -0.2600015 -1.3879639 -2.2119688\r\u0026gt; \u0026gt; #plot icc for each individual with respect to each of the 5 items\r\u0026gt; theta\u0026lt;-apply(model1_stan_par$theta, 2, mean)\r\u0026gt; prob\u0026lt;-apply(model1_stan_par$prob,c(2,3),mean)\r\u0026gt; plot(theta,prob[,1], type = \u0026quot;n\u0026quot;, ylab = \u0026quot;probability of correct response\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,1))\r\u0026gt; lines(theta,prob[,1],col=\u0026quot;red\u0026quot;)\r\u0026gt; lines(theta,prob[,2],col=\u0026quot;blue\u0026quot;)\r\u0026gt; lines(theta,prob[,3],col=\u0026quot;orange\u0026quot;)\r\u0026gt; lines(theta,prob[,4],col=\u0026quot;green\u0026quot;)\r\u0026gt; lines(theta,prob[,5],col=\u0026quot;black\u0026quot;)\r\u0026gt; legend(\u0026quot;bottomright\u0026quot;,legend = c(\u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;5\u0026quot;), lty = c(1), col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;black\u0026quot;), bty = \u0026quot;n\u0026quot;, cex = 0.5)\rWe see that item \\(3\\) is the most difficult item (it’s curve is farthest to the right), and item \\(1\\) is the easiest (it’s curve is farthest to the left). The same conclusions can be drawn by checking the difficulty estimates above.\n\rPlot the item information curves\rItem information curves (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the \\(1\\)st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\n\u0026gt; #plot iic for each individual with respect to each of the 5 items\r\u0026gt; neg_prob\u0026lt;-1-prob\r\u0026gt; information\u0026lt;-prob*neg_prob\r\u0026gt; plot(theta,information[,1], type = \u0026quot;n\u0026quot;, ylab = \u0026quot;information\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,0.3))\r\u0026gt; lines(theta,information[,1],col=\u0026quot;red\u0026quot;)\r\u0026gt; lines(theta,information[,2],col=\u0026quot;blue\u0026quot;)\r\u0026gt; lines(theta,information[,3],col=\u0026quot;orange\u0026quot;)\r\u0026gt; lines(theta,information[,4],col=\u0026quot;green\u0026quot;)\r\u0026gt; lines(theta,information[,5],col=\u0026quot;black\u0026quot;)\r\u0026gt; legend(\u0026quot;bottomleft\u0026quot;,legend = c(\u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;5\u0026quot;), lty = c(1), col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;black\u0026quot;), bty = \u0026quot;n\u0026quot;, cex = 0.5)\rSimilar to the ICCs, we see that item \\(3\\) provides the most information about high ability levels (the peak of its IIC is farthest to the right) and item \\(1\\) and \\(5\\) provides the most information about lower ability levels (the peak of its IIC is farthest to the left). We have seen that all ICCs and IICs for the items have the same shape in the 1PL model (i.e. all items are equally good at providing information about the latent trait). In the 2PL and 3PL models, we will see that this does not have to be the case.\nNext, we plot the information curve for the whole test. This is simply the sum of the individual IICs above. Ideally, we want a test which provides fairly good covereage of a wide range of latent ability levels. Otherwise, the test is only good at identifying a limited range of ability levels.\n\u0026gt; #plot iic for each individual with respect to whole test\r\u0026gt; information_test\u0026lt;-apply(information,1,sum)\r\u0026gt; plot(theta,information_test, type = \u0026quot;n\u0026quot;, ylab = \u0026quot;information (test)\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,1.5))\r\u0026gt; lines(theta,information_test,col=\u0026quot;black\u0026quot;,lty=2)\r\u0026gt; \u0026gt; summary(information_test)\rMin. 1st Qu. Median Mean 3rd Qu. Max. 0.5536 0.5699 0.7665 0.7756 0.9305 1.0791 \rWe see that this test provides the most information about low ability levels (the peak is around ability level \\(-1.5\\)), and less information about very high ability levels.\n\rAssess fit\rWe perform posterior predictive checks to test whether individual items fit the 1PLM by comparing quantities computed from the predictions of the model with those from the observed data. If these match reasonably well, then there is indication that the model has a good fit.\n\u0026gt; library(bayesplot)\r\u0026gt; library(ggplot2)\r\u0026gt; Y.rep\u0026lt;-model1_stan_par$Y_rep\r\u0026gt; \u0026gt; #Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars\r\u0026gt; ppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(\u0026quot;Item 1\u0026quot;)\r\u0026gt; ppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(\u0026quot;Item 2\u0026quot;)\r\u0026gt; ppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(\u0026quot;Item 3\u0026quot;)\r\u0026gt; ppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(\u0026quot;Item 4\u0026quot;)\r\u0026gt; ppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(\u0026quot;Item 5\u0026quot;)\r\rPlot ability scores\rWe can conclude by summarising and plotting the latent ability scores of the participants\n\u0026gt; #summary stats for theta across both iterations and individuals\r\u0026gt; summary(theta)\rMin. 1st Qu. Median Mean 3rd Qu. Max. -2.05247 -0.48303 0.07869 0.00209 0.69533 0.73909 \u0026gt; \u0026gt; #histogram and kernel density plot of theta averaged across iterations\r\u0026gt; dens.theta\u0026lt;-density(theta, bw=0.3)\r\u0026gt; hist(theta, breaks = 5, prob = T)\r\u0026gt; lines(dens.theta, lwd=2, col=\u0026quot;red\u0026quot;)\rWe see that the mean of ability scores is around \\(0\\), and the standard deviation about \\(1\\) (these are estimated ability scores are standardised).\n\r\r2 parameter logistic model (2PLM)\rIn the 2PLM, the probability that the \\(i\\)-th individual correctly answers the \\(j\\)-th item (i.e. the probability that \\(Y_{ij} = 1\\)) is assumed to have the following IRF form:\n\\[p_{ij} = P(Y_{ij}=1 \\mid \\theta_i,\\delta_j,\\alpha_j)=\\frac{1}{1+\\text{exp}(\\alpha_j(\\theta_i-\\delta_j))},\\]\nwhere \\(\\alpha_j\\) is the discrimination parameter for item \\(j\\) of the test, and is assumed to be positive and lognormally distributed according to some mean \\(\\mu_{\\alpha}\\) and standard deviation \\(\\sigma_{\\alpha}\\) which must be specified by the analyst. The item discriminability \\(\\alpha_j\\) indicates how well an item is able to discriminate between persons with different ability levels. Item discriminability is reflected in the steepness of the slope of the ICC.\nFit the model\rWe fit the 2PLM to the data using the following STAN code.\n\u0026gt; model2\u0026lt;-\u0026quot;\r+ data {\r+ int\u0026lt;lower=0\u0026gt; n;\r+ int\u0026lt;lower=0\u0026gt; p;\r+ int\u0026lt;lower=0,upper=1\u0026gt; Y[n,p];\r+ }\r+ parameters {\r+ vector[n] theta;\r+ vector\u0026lt;lower=0\u0026gt; [p] alpha;\r+ vector[p] delta;\r+ real mu_delta;\r+ real\u0026lt;lower=0\u0026gt; sigma_alpha;\r+ real\u0026lt;lower=0\u0026gt; sigma_delta;\r+ }\r+ transformed parameters{\r+ vector\u0026lt;lower=0,upper=1\u0026gt;[p] prob[n];\r+ for(i in 1:n){\r+ for (j in 1:p){\r+ prob[i,j] = inv_logit(alpha[j]*(theta[i] - delta[j]));\r+ }\r+ }\r+ }\r+ model {\r+ theta ~ normal(0,1);\r+ delta ~ normal(mu_delta,sigma_delta);\r+ mu_delta ~ normal(0,5);\r+ sigma_delta ~ cauchy(0,5);\r+ alpha ~ lognormal(0,sigma_alpha);\r+ sigma_alpha ~ cauchy(0,5);\r+ for(i in 1:n){\r+ for (j in 1:p){\r+ Y[i,j] ~ bernoulli(prob[i,j]);\r+ }\r+ }\r+ }\r+ generated quantities {\r+ vector[p] loglik_y[n];\r+ vector[p] Y_rep[n];\r+ for (i in 1: n){\r+ for (j in 1: p){\r+ loglik_y[i,j] = bernoulli_lpmf(Y[i,j] | prob[i,j]);\r+ Y_rep[i,j] = bernoulli_rng(prob[i,j]); + }\r+ }\r+ }\r+ \u0026quot;\r\u0026gt; ## write the model to a text file\r\u0026gt; writeLines(model2, con = \u0026quot;model2PLM.stan\u0026quot;)\rNext, I define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;delta\u0026quot;, \u0026quot;alpha\u0026quot;,\u0026quot;theta\u0026quot;, \u0026quot;prob\u0026quot;,\u0026quot;loglik_y\u0026quot;,\u0026quot;Y_rep\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2500 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1750\rStart the STAN model (check the model, load data into the model, specify the number of chains and compile the model). Run the STAN code via the rstan interface and the stan function.\n\u0026gt; set.seed(3456)\r\u0026gt; model2_stan\u0026lt;- stan(data = data_list, file = \u0026quot;model2PLM.stan\u0026quot;, + chains = nChains, pars = params, iter = nIter, + warmup = burnInSteps, thin = thinSteps)\r\rPlot the item characteristic curves\rItem characteristic curves (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the \\(x\\)-axis (higher values represent hight ability). Probability of a “correct” answer (\\(Y_{ij}=1\\)) to an item is plotted on the \\(y\\)-axis.\n\u0026gt; #extract parameters\r\u0026gt; model2_stan_par\u0026lt;-extract(model2_stan)\r\u0026gt; \u0026gt; discr\u0026lt;-model2_stan_par$alpha\r\u0026gt; diff\u0026lt;-model2_stan_par$delta\r\u0026gt; #see average value of item difficulty\r\u0026gt; apply(diff,2,mean)\r[1] -3.2361513 -1.3702513 -0.3117896 -1.8137234 -2.7622803\r\u0026gt; #see average value of item discriminability\r\u0026gt; apply(discr,2,mean)\r[1] 0.9062298 0.7615282 0.8492812 0.7549198 0.7976353\r\u0026gt; \u0026gt; #plot icc for each individual with respect to each of the 5 items\r\u0026gt; theta\u0026lt;-apply(model2_stan_par$theta, 2, mean)\r\u0026gt; prob\u0026lt;-apply(model2_stan_par$prob,c(2,3),mean)\r\u0026gt; plot(theta,prob[,1], type = \u0026quot;n\u0026quot;, ylab = \u0026quot;probability of correct response\u0026quot;, xlab=\u0026quot;ability\u0026quot;,xlim = c(-2.5,1), ylim = c(0,1))\r\u0026gt; lines(theta,prob[,1],col=\u0026quot;red\u0026quot;)\r\u0026gt; lines(theta,prob[,2],col=\u0026quot;blue\u0026quot;)\r\u0026gt; lines(theta,prob[,3],col=\u0026quot;orange\u0026quot;)\r\u0026gt; lines(theta,prob[,4],col=\u0026quot;green\u0026quot;)\r\u0026gt; lines(theta,prob[,5],col=\u0026quot;black\u0026quot;)\r\u0026gt; legend(\u0026quot;bottomright\u0026quot;,legend = c(\u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;5\u0026quot;), lty = c(1), col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;black\u0026quot;), bty = \u0026quot;n\u0026quot;, cex = 0.5)\rUnlike the ICCs for the 1PLM, the ICCs for the 2PLM do not all have the same shape. Item curves which are more “spread out” indicate lower discriminability (i.e. that individuals of a range of ability levels have some probability of getting the item correct). Compare this to an item with high discriminability (steep slope): for this item, we have a better estimate of the individual’s latent ability based on whether they got the question right or wrong. Because of the differing slopes, the rank-order of item difficulty changes across different latent ability levels. We can see that item \\(3\\) is still the most difficult item (i.e. lowest probability of getting correct for most latent trait values, up until about \\(\\theta=0.2\\)). Items \\(1\\) and \\(5\\) are the easiest.\n\rPlot the item information curves\rItem information curves (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the \\(1\\)st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\n\u0026gt; #plot iic for each individual with respect to each of the 5 items\r\u0026gt; neg_prob\u0026lt;-1-prob\r\u0026gt; information\u0026lt;-prob*neg_prob\r\u0026gt; information2\u0026lt;-information*(apply(discr,2,mean))^2\r\u0026gt; plot(theta,information2[,1], type = \u0026quot;n\u0026quot;, ylab = \u0026quot;information\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,0.3))\r\u0026gt; lines(theta,information2[,1],col=\u0026quot;red\u0026quot;)\r\u0026gt; lines(theta,information2[,2],col=\u0026quot;blue\u0026quot;)\r\u0026gt; lines(theta,information2[,3],col=\u0026quot;orange\u0026quot;)\r\u0026gt; lines(theta,information2[,4],col=\u0026quot;green\u0026quot;)\r\u0026gt; lines(theta,information2[,5],col=\u0026quot;black\u0026quot;)\r\u0026gt; legend(\u0026quot;bottomleft\u0026quot;,legend = c(\u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;5\u0026quot;), lty = c(1), col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;black\u0026quot;), bty = \u0026quot;n\u0026quot;, cex = 0.5)\rThe item IICs demonstrate that some items provide more information about latent ability for different ability levels. The higher the item discriminability estimate, the more information an item provides about ability levels around the point where there is a \\(50\\%\\) chance of getting the item right (i.e. the steepest point in the ICC slope). For example, item \\(3\\) (orange) clearly provides the most information at high ability levels, around \\(\\theta=-0.5\\), but almost no information about low ability levels (\\(\u0026lt; -1\\)) because the item is already too hard for those participants. In contrast, item \\(1\\) (red), which has low discriminability, doesn’t give very much information overall, but covers a wide range of ability levels.\nNext, we plot the item information curve for the whole test. This is the sum of all the item IICs above.\n\u0026gt; #plot iic for each individual with respect to whole test\r\u0026gt; information_test\u0026lt;-apply(information2,1,sum)\r\u0026gt; plot(theta,information_test, type = \u0026quot;n\u0026quot;, ylab = \u0026quot;information (test)\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,1.5))\r\u0026gt; lines(theta,information_test,col=\u0026quot;black\u0026quot;,lty=2)\r\u0026gt; \u0026gt; summary(information_test)\rMin. 1st Qu. Median Mean 3rd Qu. Max. 0.3541 0.4493 0.5176 0.5283 0.5869 0.8999 \rThe IIC for the whole test shows that the test provides the most information for slightly-lower-than average ability levels (about \\(\\theta=-1\\)), but does not provide much information about extremely high ability levels.\n\rAssess fit\rNext, we check how well the 2PLM fits the data.\n\u0026gt; Y.rep\u0026lt;-model2_stan_par$Y_rep\r\u0026gt; \u0026gt; #Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars\r\u0026gt; ppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(\u0026quot;Item 1\u0026quot;)\r\u0026gt; ppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(\u0026quot;Item 2\u0026quot;)\r\u0026gt; ppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(\u0026quot;Item 3\u0026quot;)\r\u0026gt; ppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(\u0026quot;Item 4\u0026quot;)\r\u0026gt; ppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(\u0026quot;Item 5\u0026quot;)\rWe can also compare the fit of the 1PLM and 2PLM using relative measures of fit or information criteria. These are computed based on the deviance and a penalty for model complexity called the effective number of parameters \\(p\\). Here we consider two Bayesian measures known as the Widely Applicable (WAIC) and Leave One Out (LOOIC) Information Criterion, which can be easily obtained through the functions waic and loo in the package loo.\n\u0026gt; library(loo)\r\u0026gt; #extract log-likelihood\r\u0026gt; loglik_m1\u0026lt;-model1_stan_par$loglik_y\r\u0026gt; loglik_m2\u0026lt;-model2_stan_par$loglik_y\r\u0026gt; \u0026gt; #waic\r\u0026gt; waic_m1\u0026lt;-waic(loglik_m1)\r\u0026gt; waic_m2\u0026lt;-waic(loglik_m2)\r\u0026gt; \u0026gt; #looic\r\u0026gt; looic_m1\u0026lt;-loo(loglik_m1)\r\u0026gt; looic_m2\u0026lt;-loo(loglik_m2)\r\u0026gt; \u0026gt; #compare\r\u0026gt; table_waic\u0026lt;-rbind(waic_m1$estimates[2:3,1],waic_m2$estimates[2:3,1])\r\u0026gt; table_looic\u0026lt;-rbind(looic_m1$estimates[2:3,1],looic_m2$estimates[2:3,1])\r\u0026gt; rownames(table_waic)\u0026lt;-rownames(table_looic)\u0026lt;-c(\u0026quot;1PLM\u0026quot;,\u0026quot;2PLM\u0026quot;)\r\u0026gt; knitr::kable(cbind(table_waic,table_looic), \u0026quot;pandoc\u0026quot;, align = \u0026quot;c\u0026quot;)\r\r\r\rp_waic\rwaic\rp_loo\rlooic\r\r\r\r1PLM\r1.525284\r6.519258\r1.866221\r7.201132\r\r2PLM\r1.422705\r6.455726\r1.702409\r7.015135\r\r\r\rBoth criteria suggest that the 2PLM has a slightly better fit to the data.\n\rPlot ability scores\rPlot the density curve of the estimated ability scores\n\u0026gt; #summary stats for theta across both iterations and individuals\r\u0026gt; summary(theta)\rMin. 1st Qu. Median Mean 3rd Qu. Max. -1.945004 -0.437572 0.064892 -0.000004 0.633274 0.673980 \u0026gt; \u0026gt; #histogram and kernel density plot of theta averaged across iterations\r\u0026gt; dens.theta\u0026lt;-density(theta, bw=0.3)\r\u0026gt; hist(theta, breaks = 5, prob = T)\r\u0026gt; lines(dens.theta, lwd=2, col=\u0026quot;red\u0026quot;)\rWe see that the mean of ability scores is around \\(0\\), and the standard deviation about \\(1\\) (these are estimated ability scores are standardised).\n\r\r3 parameter logistic model (3PLM)\rIn the 3PLM, the probability that the \\(i\\)-th individual correctly answers the \\(j\\)-th item (i.e. the probability that \\(Y_{ij} = 1\\)) is assumed to have the following IRF form:\n\\[p_{ij} = P(Y_{ij}=1 \\mid \\theta_i,\\delta_j,\\alpha_j,\\eta_j)=\\eta_j + (1-\\eta_j) \\frac{1}{1+\\text{exp}(\\alpha_j(\\theta_i-\\delta_j))},\\]\nwhere \\(\\eta_j\\) is the guessing parameter. Under this model, individuals with zero ability have a nonzero chance of endorsing any item, just by guessing randomly. The guessing parameter is reflected in the \\(y\\)-intercept (i.e. probability) of the ICC. The parameter is normally distributed according to some mean \\(\\mu_{\\eta}\\) and standard deviation \\(\\sigma_{\\eta}\\) which must be specified by the analyst.\nFit the model\rWe fit the 3PLM to the data using the following STAN code.\n\u0026gt; model3\u0026lt;-\u0026quot;\r+ data {\r+ int\u0026lt;lower=0\u0026gt; n;\r+ int\u0026lt;lower=0\u0026gt; p;\r+ int\u0026lt;lower=0,upper=1\u0026gt; Y[n,p];\r+ }\r+ parameters {\r+ vector[n] theta;\r+ vector\u0026lt;lower=0\u0026gt; [p] alpha;\r+ vector[p] delta;\r+ vector\u0026lt;lower=0,upper=1\u0026gt;[p] eta; //item pseudo-guessing\r+ real mu_delta;\r+ real\u0026lt;lower=0\u0026gt; sigma_alpha;\r+ real\u0026lt;lower=0\u0026gt; sigma_delta;\r+ }\r+ transformed parameters{\r+ vector\u0026lt;lower=0,upper=1\u0026gt;[p] prob_star[n];\r+ vector\u0026lt;lower=0,upper=1\u0026gt;[p] prob[n];\r+ for(i in 1:n){\r+ for (j in 1:p){\r+ prob_star[i,j] = inv_logit(alpha[j]*(theta[i] - delta[j]));\r+ prob[i, j] = eta[j] + (1-eta[j])*prob_star[i,j]; + }\r+ }\r+ }\r+ model {\r+ theta ~ normal(0,1);\r+ delta ~ normal(mu_delta,sigma_delta);\r+ mu_delta ~ normal(0,5);\r+ sigma_delta ~ cauchy(0,5);\r+ alpha ~ lognormal(0,sigma_alpha);\r+ sigma_alpha ~ cauchy(0,5);\r+ eta ~ beta(5,23);\r+ for(i in 1:n){\r+ for (j in 1:p){\r+ Y[i,j] ~ bernoulli(prob[i,j]);\r+ }\r+ }\r+ }\r+ generated quantities {\r+ vector[p] loglik_y[n];\r+ vector[p] Y_rep[n];\r+ for (i in 1: n){\r+ for (j in 1: p){\r+ loglik_y[i,j] = bernoulli_lpmf(Y[i,j] | prob[i,j]);\r+ Y_rep[i,j] = bernoulli_rng(prob[i,j]); + }\r+ }\r+ }\r+ \u0026quot;\r\u0026gt; ## write the model to a text file\r\u0026gt; writeLines(model3, con = \u0026quot;model3PLM.stan\u0026quot;)\rNext, I define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;delta\u0026quot;, \u0026quot;alpha\u0026quot;, \u0026quot;eta\u0026quot;,\u0026quot;theta\u0026quot;, \u0026quot;prob\u0026quot;,\u0026quot;loglik_y\u0026quot;,\u0026quot;Y_rep\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2500 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1750\rStart the STAN model (check the model, load data into the model, specify the number of chains and compile the model). Run the STAN code via the rstan interface and the stan function.\n\u0026gt; set.seed(3456)\r\u0026gt; model3_stan\u0026lt;- stan(data = data_list, file = \u0026quot;model3PLM.stan\u0026quot;, + chains = nChains, pars = params, iter = nIter, + warmup = burnInSteps, thin = thinSteps)\r\rPlot the item characteristic curves\rItem characteristic curves (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the \\(x\\)-axis (higher values represent hight ability). Probability of a “correct” answer (\\(Y_{ij}=1\\)) to an item is plotted on the \\(y\\)-axis.\n\u0026gt; #extract parameters\r\u0026gt; model3_stan_par\u0026lt;-extract(model3_stan)\r\u0026gt; \u0026gt; discr\u0026lt;-model3_stan_par$alpha\r\u0026gt; diff\u0026lt;-model3_stan_par$delta\r\u0026gt; gues\u0026lt;-model3_stan_par$eta\r\u0026gt; #see average value of item difficulty\r\u0026gt; apply(diff,2,mean)\r[1] -2.7984897 -0.7753931 0.1717751 -1.1931997 -2.1962556\r\u0026gt; #see average value of item discriminability\r\u0026gt; apply(discr,2,mean)\r[1] 0.9541800 0.9031630 1.0175164 0.8777688 0.8861185\r\u0026gt; #see average value of item guessing\r\u0026gt; apply(gues,2,mean)\r[1] 0.1892703 0.1891533 0.1632674 0.1933265 0.1934880\r\u0026gt; \u0026gt; #plot icc for each individual with respect to each of the 5 items\r\u0026gt; theta\u0026lt;-apply(model3_stan_par$theta, 2, mean)\r\u0026gt; prob\u0026lt;-apply(model3_stan_par$prob,c(2,3),mean)\r\u0026gt; plot(theta,prob[,1], type = \u0026quot;n\u0026quot;, ylab = \u0026quot;probability of correct response\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,1))\r\u0026gt; lines(theta,prob[,1],col=\u0026quot;red\u0026quot;)\r\u0026gt; lines(theta,prob[,2],col=\u0026quot;blue\u0026quot;)\r\u0026gt; lines(theta,prob[,3],col=\u0026quot;orange\u0026quot;)\r\u0026gt; lines(theta,prob[,4],col=\u0026quot;green\u0026quot;)\r\u0026gt; lines(theta,prob[,5],col=\u0026quot;black\u0026quot;)\r\u0026gt; legend(\u0026quot;bottomright\u0026quot;,legend = c(\u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;5\u0026quot;), lty = c(1), col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;black\u0026quot;), bty = \u0026quot;n\u0026quot;, cex = 0.5)\rThe slopes of the ICCs look very similar to those of the 2PLM. We can see that all items have \\(y\\)-intercepts greater than zero, so that even at very low ability levels, there is some chance of getting these items correct (via guessing).\n\rPlot the item information curves\rItem information curves (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the \\(1\\)st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\nHere I plot the IICs using points, rather than lines, to better display the patterns of the individuals, which vary substantially according to whether the item was correctly chosen due to chance or not.\n\u0026gt; #plot iic for each individual with respect to each of the 5 items\r\u0026gt; neg_prob\u0026lt;-1-prob\r\u0026gt; information.p1\u0026lt;-neg_prob/prob\r\u0026gt; information.p2\u0026lt;-(prob-apply(gues,2,mean))^2/(1-apply(gues,2,mean))^2\r\u0026gt; information3\u0026lt;-(apply(discr,2,mean))^2*(information.p2)*(information.p1)\r\u0026gt; plot(theta,information3[,1], type = \u0026quot;n\u0026quot;, ylab = \u0026quot;information\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,0.7))\r\u0026gt; points(theta,information3[,1],col=\u0026quot;red\u0026quot;)\r\u0026gt; points(theta,information3[,2],col=\u0026quot;blue\u0026quot;)\r\u0026gt; points(theta,information3[,3],col=\u0026quot;orange\u0026quot;)\r\u0026gt; points(theta,information3[,4],col=\u0026quot;green\u0026quot;)\r\u0026gt; points(theta,information3[,5],col=\u0026quot;black\u0026quot;)\r\u0026gt; legend(\u0026quot;bottomleft\u0026quot;,legend = c(\u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;5\u0026quot;), lty = c(1), col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;black\u0026quot;), bty = \u0026quot;n\u0026quot;, cex = 0.5)\rNext, we plot the item information curve for the whole test. This is the sum of all the item IICs above.\n\u0026gt; #plot iic for each individual with respect to whole test\r\u0026gt; information_test\u0026lt;-apply(information3,1,sum)\r\u0026gt; plot(theta,information_test, type = \u0026quot;n\u0026quot;, ylab = \u0026quot;information (test)\u0026quot;, xlab=\u0026quot;ability\u0026quot;,\r+ xlim = c(-2.5,1), ylim = c(0,1.5))\r\u0026gt; points(theta,information_test,col=\u0026quot;black\u0026quot;,lty=2)\r\u0026gt; \u0026gt; summary(information_test)\rMin. 1st Qu. Median Mean 3rd Qu. Max. 0.4119 0.4919 0.5320 0.5470 0.5817 0.7770 \r\rAssess fit\rNext, we check how well the 3PLM fits the data.\n\u0026gt; Y.rep\u0026lt;-model3_stan_par$Y_rep\r\u0026gt; \u0026gt; #Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars\r\u0026gt; ppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(\u0026quot;Item 1\u0026quot;)\r\u0026gt; ppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(\u0026quot;Item 2\u0026quot;)\r\u0026gt; ppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(\u0026quot;Item 3\u0026quot;)\r\u0026gt; ppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(\u0026quot;Item 4\u0026quot;)\r\u0026gt; ppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(\u0026quot;Item 5\u0026quot;)\rWe can also compare the fit of the 1PLM, 2PLM and 3PLM using relative measures of fit or information criteria. These are computed based on the deviance and a penalty for model complexity called the effective number of parameters \\(p\\). Here we consider two Bayesian measures known as the Widely Applicable (WAIC) and Leave One Out (LOOIC) Information Criterion, which can be easily obtained through the functions waic and loo in the package loo.\n\u0026gt; #extract log-likelihood\r\u0026gt; loglik_m3\u0026lt;-model3_stan_par$loglik_y\r\u0026gt; \u0026gt; #waic\r\u0026gt; waic_m3\u0026lt;-waic(loglik_m3)\r\u0026gt; \u0026gt; #looic\r\u0026gt; looic_m3\u0026lt;-loo(loglik_m3)\r\u0026gt; \u0026gt; #compare\r\u0026gt; table_waic\u0026lt;-rbind(waic_m1$estimates[2:3,1],waic_m2$estimates[2:3,1],waic_m3$estimates[2:3,1])\r\u0026gt; table_looic\u0026lt;-rbind(looic_m1$estimates[2:3,1],looic_m2$estimates[2:3,1],looic_m3$estimates[2:3,1])\r\u0026gt; rownames(table_waic)\u0026lt;-rownames(table_looic)\u0026lt;-c(\u0026quot;1PLM\u0026quot;,\u0026quot;2PLM\u0026quot;,\u0026quot;3PLM\u0026quot;)\r\u0026gt; knitr::kable(cbind(table_waic,table_looic), \u0026quot;pandoc\u0026quot;, align = \u0026quot;c\u0026quot;)\r\r\r\rp_waic\rwaic\rp_loo\rlooic\r\r\r\r1PLM\r1.525284\r6.519258\r1.866221\r7.201132\r\r2PLM\r1.422705\r6.455726\r1.702409\r7.015135\r\r3PLM\r1.404502\r6.428445\r1.695632\r7.010705\r\r\r\rBoth criteria suggest that both 1PLM and 2PLM have a better fit to the data than 3PLM.\n\rPlot ability scores\rPlot the density curve of the estimated ability scores\n\u0026gt; #summary stats for theta across both iterations and individuals\r\u0026gt; summary(theta)\rMin. 1st Qu. Median Mean 3rd Qu. Max. -1.8520796 -0.4391371 0.0657133 -0.0007343 0.6520041 0.7074601 \u0026gt; \u0026gt; #histogram and kernel density plot of theta averaged across iterations\r\u0026gt; dens.theta\u0026lt;-density(theta, bw=0.3)\r\u0026gt; hist(theta, breaks = 5, prob = T)\r\u0026gt; lines(dens.theta, lwd=2, col=\u0026quot;red\u0026quot;)\rWe see that the mean of ability scores is around \\(0\\), and the standard deviation about \\(1\\) (these are estimated ability scores are standardised).\n\r\rConclusions\rThe use of STAN software to estimate IRT models allows the user to alter existing\rcode to fit new variations of current models that cannot be fit in existing software packages. For example, longitudinal or multilevel data can easily be accommodated by small changes to existing STAN code. The STAN software takes care of the “grunt work” involved in estimating model parameters by constructing an MCMC algorithm to sample from the posterior distribution. Using STAN frees the user to experiment with different models that may be more appropriate for specialised data than the models that can currently be fit in other software packages. Of course, more complicated models involve more parameters than simpler models, and the analyst must specify prior distributions for these new parameters. This is a small price to pay, however, for the flexibility that the Bayesian framework and STAN software provide.\n\rReferences\rBaker, Frank B, and Seock-Ho Kim. 2004. Item Response Theory: Parameter Estimation Techniques. CRC Press.\n\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rHambleton, Ronald K, Hariharan Swaminathan, and H Jane Rogers. 1991. Fundamentals of Item Response Theory. Sage.\n\rLuo, Yong, and Hong Jiao. 2018. “Using the Stan Program for Bayesian Item Response Theory.” Educational and Psychological Measurement 78 (3): 384–408.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\r\r\r","date":1584843194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584843194,"objectID":"da3a95038444bbcccd93d53a62c2e62e","permalink":"/stan/irt-stan/irt-stan/","publishdate":"2020-03-21T21:13:14-05:00","relpermalink":"/stan/irt-stan/irt-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","item response theory"],"title":"Item Response Theory Models - STAN","type":"STAN"},{"authors":["Andrea Gabrio"],"categories":["news","website"],"content":"It has been roughtly a week and a half now since this whole shutdown started here in London and things are not going to be easy in the next few weeks. I am lucky, in that my job allows me to work remotely with limited inconveniences. Other people have to go outide for working and, if not risking thier life, at least put at risk the life of those who they care most. Last week was particularly bad in terms of supermarket products which were sold out for the most part. This week is a bit better as people may have realised that for the moment, if we just buy products as usual, we still have food and toilet paper for everyone.\nTo be honest, not much to update on my work which has slowed down due to this whole situation and also to me not feeling at my best. I hope I will have some time to look at the different projects I am involved with in the next few days. In the meantime, I worked a bit on my website with new JAGS and STAN tutorials and I have also uploaded on my GitHub page some materials (e.g. software code) related to some of the projects I did. For example, here the link to the JAGS and STAN code for the model I used to predict volleyball results\nNot sure what gif to use this time to conclude the post. So I guess I will just go for a random cat picture, which does not make any sense but which is always nice to look at.\n","date":1584662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584704043,"objectID":"6dffa30149012f71e9e6d9a4394f1cf4","permalink":"/post/update2-march/","publishdate":"2020-03-20T00:00:00Z","relpermalink":"/post/update2-march/","section":"post","summary":"It has been roughtly a week and a half now since this whole shutdown started here in London and things are not going to be easy in the next few weeks. I am lucky, in that my job allows me to work remotely with limited inconveniences. Other people have to go outide for working and, if not risking thier life, at least put at risk the life of those who they care most.","tags":["News","Academic"],"title":"Living and working at home is nice, right?","type":"post"},{"authors":["Andrea Gabrio"],"categories":["news","conference"],"content":"I have to admit, although I expected some fear to spread because of the virus which is currently and quickly infecting the world, I was surprised by the frenzy surrounding us, especially in my homecountry (Italy) and particularly in my parents\u0026rsquo; region which is at the moment under lockdown. I will also probably cancel my planned trip home for Eastern and perhaps also after that since the situations is still unclear and I may be unable to come back to the UK in the short time. This is pretty scary to all the people living in those territories, who are now forbidden to have any sort of public meeting and are strongly recommended to stay at home. I am afraid this will not be enough to stop the virus from spreading but of course it is useful as it is the only way we have if we want to contain it. The hope is that by summer time the heath will reduce the ability of the virus to spread and give us some time to come up with a possible vaccine in the next months.\nThere have already been attempts to estimate the fatality ratio of the virus using statistical methods. Here I post the tweet from Andrew Gelman\nPost Edited: Coronavirus age-specific fatality ratio, estimated using Stan, and (attempting) to account for underreporting of cases and the time delay to death. Now with data and code. And now a link to another paper (also with data and code). https://t.co/CSHnXRMtyp\n\u0026mdash; Andrew Gelman (@StatModeling) March 9, 2020  which refers to epidemiologists who tried to use STAN for achieving this objective, although an additional reference to another work based on the use of differential equation analysis is also made. However, results are still preliminary and subject to limitations for the type of data and assumptions used. From a statistical perspective I am sure this new epidemic will be interesting to study and I guess lots of funding will be devoted to analyse the upcoming data to get a better idea of the actual threat it represents for the people. I am not an epidemiologist, so I do not have a big statistical interest in this, although I am pretty much worried as any other person. Hopefully, this will become clearer as time passes and let us just hope the number of deaths will not be very high.\nSorry about talking about this here, but from time to time I would also like to highlight was is currently happening around me. As for my research, nothing has changed much for me at the moment and life as usual continues with another busy upcoming period with lots of boring meetings, reports and standard analyses to do, but hopefully I can also save some time to do some methodological work. I am also waiting for the decision about my abstract which I submitted to the EuHEA conference 2020 and which is supposed to be held in Oslo this July. I really hope I have a chance to presenting my work there as I have never been to this specific health economic conference. Fingers crossed! Of course, nobody knows what will happen from here till July and much is to be discussed also with respect to how the spreading of the virus may affect everyone\u0026rsquo;s schedule in the next months.\n","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583062443,"objectID":"d662e7aecaf5944fe50a27aadb9ec0f4","permalink":"/post/update-march/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/post/update-march/","section":"post","summary":"I have to admit, although I expected some fear to spread because of the virus which is currently and quickly infecting the world, I was surprised by the frenzy surrounding us, especially in my homecountry (Italy) and particularly in my parents\u0026rsquo; region which is at the moment under lockdown. I will also probably cancel my planned trip home for Eastern and perhaps also after that since the situations is still unclear and I may be unable to come back to the UK in the short time.","tags":["News","Academic"],"title":"Lockdown","type":"post"},{"authors":["Andrea Gabrio"],"categories":["R","JAGS","generalised linear mixed models"],"content":"\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIn some respects, Generalized Linear Mixed effects Models (GLMM) are a hierarchical extension of Generalized linear models (GLM) in a similar manner that Linear Mixed effects Models (LMM) are a hierarchical extension of Linear Models (LM). However, whilst the Gaussian (normal) distribution facilitates a relatively straight way of generating the marginal likelihood of the observed response by integrating likelihoods across all possible (and unobserved) levels of a random effect to yield parameter estimates, the same cannot be said for other distributions. Consequently various approximations have been developed to estimate the fixed and random parameters for GLMM’s:\nPenalized quasi-likelihood (PQL). This method approximates a quasi-likelihood by iterative fitting of (re)weighted linear mixed effects models based on the fit of GLM fit. Specifically, it estimates the fixed effects parameters by fitting a GLM that incorporates a correlation (variance-covariance) structure resulting from a LMM and then refits a LMM to re-estimate the variance-covariance structure by using the variance structure from the previous GLM. The cycle continues to iterate until either the fit improvement is below a threshold or a defined number of iterations has occurred. Whilst this is a relatively simple approach, that enables us to leverage methodologies for accommodating heterogeneity and spatial/temporal autocorrelation, it is known to perform poorly (estimates biased towards large variance) for Poisson distributions when the expected value is less than \\(5\\) and for binary data when the expected number of successes or failures are less than \\(5\\). Moreover, as it approximates quasi-likelihood rather than likelihood, likelihood based inference and information criterion methods (such as likelihood ratio tests and AIC) are not appropriate with this approach. Instead, Wald tests are required for inference.\n\rLaplace approximation. This approach utilises a second-order Taylor series expansion to approximate (a mathematical technique for approximating the properties of a function around a point by taking multiple derivatives of the function and summing them together) the likelihood function. If we assume that the likelihood function is approximately normal and thus a quadratic function on a log scale, we can use second-order Taylor series expansion to approximate this likelihood. Whilst this approach is considered to be more accurate than PQL, it is considerably slower and unable to accommodate alternative variance and correlation structures.\n\rGauss-Hermite quadrature (GHQ). This approach approximates the marginal likelihood by approximating the value of integrals at specific points (quadratures). This technique can be further adapted by allowing the number of quadratures and their weights to be optimized via a set of rules.\n\rMarkov-chain Monte-Carlo (MCMC). This takes a bruit force approach by recreating the likelihood by traversing the likelihood function with sequential sampling proportional to the likelihood. Although this approach is very robust (when the posteriors have converged), they are computationally very intense. Interestingly, some (including Andrew Gelman) argue that PQL, Laplace and GHQ do not yield estimates. Rather they are only approximations of estimates. By contrast, as MCMC methods are able to integrate over all levels by bruit force, the resulting parameters are indeed true estimates.\n\r\rWe will focus on the last approach which is the more general among the ones considered here and which is based on a Bayesian approach, which can be very flexible and accurate, yet very slow and complex.\n\rHierarchical Poisson regression\rThe model I will be developing is a Bayesian hierarchical Poisson regression model which I borrow from a very interesting work about modelling match results in soccer, available both as a technical report and as a series of online posts. The objective of the analysis was to model the match results from the last five seasons of La Liga, the premium Spanish football (soccer) league. In total there were \\(1900\\) rows in the dataset each with information regarding which was the home and away team, what these teams scored and what season it was. The goal outcomes of the teams are assumed to be distributed according to a Poisson distribution, while also taking into account the dependence between the goals scored by the attacking and defensive teams in each match.\n\rLoading the data\rI start by loading libraries, reading in the data and preprocessing it for JAGS. The last \\(50\\) matches have unknown outcomes and I create a new data frame d holding only matches with known outcomes. I will come back to the unknown outcomes later when it is time to use the model for prediction. I also load a R function called plotPost which was previously coded in order to facilitate the plotting of the posterior results of the model. All information about the model structure, data and functions can be found on the webpage of the original post of the author or in his technical report (Bååth (2015)).\n\u0026gt; library(R2jags)\r\u0026gt; library(coda)\r\u0026gt; library(mcmcplots)\r\u0026gt; library(stringr)\r\u0026gt; library(plyr)\r\u0026gt; library(xtable)\r\u0026gt; library(ggplot2)\r\u0026gt; source(\u0026quot;plotPost.R\u0026quot;)\r\u0026gt; set.seed(12345) # for reproducibility\r\u0026gt; \u0026gt; load(\u0026quot;laliga.RData\u0026quot;)\r\u0026gt; \u0026gt; # -1 = Away win, 0 = Draw, 1 = Home win\r\u0026gt; laliga$MatchResult \u0026lt;- sign(laliga$HomeGoals - laliga$AwayGoals)\r\u0026gt; \u0026gt; # Creating a data frame d with only the complete match results\r\u0026gt; d \u0026lt;- na.omit(laliga)\r\u0026gt; teams \u0026lt;- unique(c(d$HomeTeam, d$AwayTeam))\r\u0026gt; seasons \u0026lt;- unique(d$Season)\r\u0026gt; \u0026gt; # A list for JAGS with the data from d where the strings are coded as\r\u0026gt; # integers\r\u0026gt; data_list \u0026lt;- list(HomeGoals = d$HomeGoals, AwayGoals = d$AwayGoals, HomeTeam = as.numeric(factor(d$HomeTeam,\r+ levels = teams)), AwayTeam = as.numeric(factor(d$AwayTeam, levels = teams)),\r+ Season = as.numeric(factor(d$Season, levels = seasons)), n_teams = length(teams),\r+ n_games = nrow(d), n_seasons = length(seasons))\r\u0026gt; \u0026gt; # Convenience function to generate the type of column names Jags outputs.\r\u0026gt; col_name \u0026lt;- function(name, ...) {\r+ paste0(name, \u0026quot;[\u0026quot;, paste(..., sep = \u0026quot;,\u0026quot;), \u0026quot;]\u0026quot;)\r+ }\r\u0026gt; data_list$n_seasons\u0026lt;-NULL\r\u0026gt; data_list$Season\u0026lt;-NULL\r\rModeling Match Results\rData check\rHow are the number of goals for each team in a football match distributed? Well, let’s start by assuming that all football matches are roughly equally long, that both teams have many chances at making a goal and that each team have the same probability of making a goal each goal chance. Given these assumptions the distribution of the number of goals for each team should be well captured by a Poisson distribution. A quick and dirty comparison between the actual distribution of the number of scored goals and a Poisson distribution having the same mean number of scored goals support this notion.\n\u0026gt; par(mfcol = c(2, 1), mar = rep(2.2, 4))\r\u0026gt; hist(c(d$AwayGoals, d$HomeGoals), xlim = c(-0.5, 8), breaks = -1:9 + 0.5, main = \u0026quot;Distribution of the number of goals\\nscored by a team in a match.\u0026quot;)\r\u0026gt; mean_goals \u0026lt;- mean(c(d$AwayGoals, d$HomeGoals))\r\u0026gt; hist(rpois(9999, mean_goals), xlim = c(-0.5, 8), breaks = -1:9 + 0.5, main = \u0026quot;Random draw from a Poisson distribution with\\nthe same mean as the distribution above.\u0026quot;)\r\rModel fitting\rAll teams aren’t equally good and it will be assumed that all teams have a latent skill variable and the skill of a team minus the skill of the opposing team defines the predicted outcome of a game. As the number of goals are assumed to be Poisson distributed it is natural that the skills of the teams are on the log scale of the mean of the distribution. The distribution of the number of goals for team \\(i\\) when facing team \\(j\\) is then\n\\[ \\text{Goals} \\sim \\text{Pois}(\\lambda)\\]\nwhere \\(\\log(\\lambda)=\\text{baseline} + \\text{skill}_i - \\text{skill}_j\\). Baseline is the log average number of goals when both teams are equally good. The goal outcome of a match between home team \\(i\\) and away team \\(j\\) is modeled as:\n\\[ \\text{HomeGoals}_{ij} \\sim \\text{Pois}(\\lambda_{\\text{home},ij}),\\]\n\\[ \\text{AwayGoals}_{ij} \\sim \\text{Pois}(\\lambda_{\\text{away},ij}),\\]\nwhere\n\\[ \\log(\\lambda_{\\text{home},ij}) = \\text{baseline} + \\text{skill}_i - \\text{skill}_j, \\]\n\\[ \\log(\\lambda_{\\text{away},ij}) = \\text{baseline} + \\text{skill}_j - \\text{skill}_i. \\]\nAdd some priors to that and you’ve got a Bayesian model going! I set the prior distributions over the baseline to:\n\\[ \\text{baseline} \\sim N(0, 4^2),\\]\nand the skill of all \\(n\\) teams using a hierarchical approach to :\n\\[ \\text{skill}_{1,\\ldots,n} \\sim N(\\mu_{\\text{teams}}, \\sigma^2_{\\text{teams}}),\\]\nso that teams are assumed to have similar but not identical mean and variance parameters for thier skill parameters. These priors are made vague. For example, the prior on the baseline have a SD of 4 but since this is on the log scale of the mean number of goals it corresponds to one SD from the mean 0 covering the range of [0.02,54.6] goals. Turning this into a JAGS model requires some minor adjustments. The model have to loop over all the match results, which adds some for-loops. JAGS parameterises the normal distribution with precision (the reciprocal of the variance) instead of variance so the hyperpriors have to be converted. Finally I have to “anchor” the skill of one team to a constant otherwise the mean skill can drift away freely (conrner constraint) and the model cannot be identified. Doing these adjustments results in the following model description:\n\u0026gt; m1_string \u0026lt;- \u0026quot;model {\r+ for(i in 1:n_games) {\r+ HomeGoals[i] ~ dpois(lambda_home[HomeTeam[i],AwayTeam[i]])\r+ AwayGoals[i] ~ dpois(lambda_away[HomeTeam[i],AwayTeam[i]])\r+ }\r+ + for(home_i in 1:n_teams) {\r+ for(away_i in 1:n_teams) {\r+ lambda_home[home_i, away_i] \u0026lt;- exp(baseline + skill[home_i] - skill[away_i])\r+ lambda_away[home_i, away_i] \u0026lt;- exp(baseline + skill[away_i] - skill[home_i])\r+ }\r+ }\r+ + skill[1] \u0026lt;- 0\r+ for(j in 2:n_teams) {\r+ skill[j] ~ dnorm(group_skill, group_tau)\r+ } + + group_skill ~ dnorm(0, 0.0625)\r+ group_tau \u0026lt;- 1 / pow(group_sigma, 2)\r+ group_sigma ~ dunif(0, 3)\r+ baseline ~ dnorm(0, 0.0625)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(m1_string, con = \u0026quot;model1.txt\u0026quot;)\rNext, we define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;baseline\u0026quot;, \u0026quot;skill\u0026quot;, \u0026quot;group_skill\u0026quot;, \u0026quot;group_sigma\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10500\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Run the JAGS code via the R2jags interface and the jags function. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomisation function just to initiate the .Random.seed variable.\n\u0026gt; m1.r2jags \u0026lt;- jags(data = data_list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;model1.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 3700\rUnobserved stochastic nodes: 31\rTotal graph size: 9151\rInitializing model\r\u0026gt; \u0026gt; print(m1.r2jags)\rInference for Bugs model at \u0026quot;model1.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5%\rbaseline 0.281 0.014 0.253 0.271 0.281 0.291 0.309\rgroup_sigma 0.225 0.034 0.169 0.201 0.222 0.246 0.302\rgroup_skill 0.016 0.062 -0.104 -0.026 0.016 0.057 0.136\rskill[1] 0.000 0.000 0.000 0.000 0.000 0.000 0.000\rskill[2] 0.185 0.061 0.064 0.145 0.185 0.226 0.307\rskill[3] 0.017 0.069 -0.117 -0.030 0.017 0.064 0.154\rskill[4] -0.013 0.061 -0.132 -0.054 -0.013 0.028 0.109\rskill[5] -0.180 0.098 -0.376 -0.247 -0.179 -0.113 0.008\rskill[6] -0.048 0.063 -0.170 -0.090 -0.049 -0.007 0.081\rskill[7] -0.013 0.058 -0.128 -0.051 -0.013 0.025 0.103\rskill[8] 0.199 0.060 0.084 0.159 0.199 0.240 0.317\rskill[9] -0.077 0.063 -0.200 -0.119 -0.076 -0.034 0.046\rskill[10] -0.110 0.062 -0.230 -0.152 -0.110 -0.068 0.011\rskill[11] 0.698 0.057 0.588 0.658 0.696 0.736 0.811\rskill[12] 0.133 0.060 0.017 0.092 0.133 0.174 0.249\rskill[13] 0.017 0.059 -0.096 -0.023 0.016 0.057 0.132\rskill[14] 0.038 0.061 -0.078 -0.003 0.037 0.080 0.160\rskill[15] -0.008 0.060 -0.124 -0.048 -0.009 0.033 0.108\rskill[16] -0.117 0.099 -0.305 -0.186 -0.118 -0.051 0.080\rskill[17] 0.606 0.058 0.495 0.566 0.606 0.646 0.720\rskill[18] -0.071 0.070 -0.205 -0.118 -0.072 -0.026 0.071\rskill[19] -0.115 0.069 -0.247 -0.162 -0.115 -0.068 0.022\rskill[20] 0.075 0.064 -0.045 0.032 0.074 0.117 0.204\rskill[21] -0.104 0.065 -0.231 -0.148 -0.105 -0.060 0.027\rskill[22] -0.212 0.099 -0.403 -0.281 -0.213 -0.146 -0.017\rskill[23] -0.161 0.101 -0.360 -0.230 -0.159 -0.094 0.036\rskill[24] -0.118 0.101 -0.319 -0.186 -0.118 -0.050 0.085\rskill[25] 0.009 0.071 -0.131 -0.037 0.010 0.057 0.147\rskill[26] 0.058 0.069 -0.079 0.011 0.058 0.104 0.195\rskill[27] -0.061 0.080 -0.218 -0.115 -0.060 -0.005 0.088\rskill[28] -0.118 0.079 -0.272 -0.170 -0.119 -0.065 0.037\rskill[29] -0.059 0.105 -0.260 -0.130 -0.062 0.011 0.155\rdeviance 10912.856 7.406 10900.319 10907.610 10912.214 10917.514 10928.852\rRhat n.eff\rbaseline 1.001 15000\rgroup_sigma 1.002 1500\rgroup_skill 1.002 1600\rskill[1] 1.000 1\rskill[2] 1.005 410\rskill[3] 1.009 180\rskill[4] 1.007 670\rskill[5] 1.002 2400\rskill[6] 1.001 4400\rskill[7] 1.002 2400\rskill[8] 1.001 11000\rskill[9] 1.001 15000\rskill[10] 1.009 190\rskill[11] 1.001 4700\rskill[12] 1.005 340\rskill[13] 1.001 14000\rskill[14] 1.006 310\rskill[15] 1.002 2600\rskill[16] 1.003 12000\rskill[17] 1.001 15000\rskill[18] 1.002 2700\rskill[19] 1.003 880\rskill[20] 1.002 1800\rskill[21] 1.002 1000\rskill[22] 1.001 2800\rskill[23] 1.001 15000\rskill[24] 1.005 380\rskill[25] 1.001 15000\rskill[26] 1.002 2300\rskill[27] 1.001 15000\rskill[28] 1.001 15000\rskill[29] 1.001 13000\rdeviance 1.001 3900\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 27.4 and DIC = 10940.3\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMCMC diagnostics\rUsing the generated MCMC samples I can now look at the credible skill values of any team. Let’s look at the trace plot and the distribution of the skill parameters for FC Sevilla and FC Valencia.\n\u0026gt; team_par\u0026lt;-c(which(teams == c(\u0026quot;FC Sevilla\u0026quot;)), which(teams == \u0026quot;FC Valencia\u0026quot;))\r\u0026gt; denplot(m1.r2jags, parms = team_par, style = \u0026quot;plain\u0026quot;, main = c(\u0026quot;Sevilla\u0026quot;,\u0026quot;Valenica\u0026quot;))\r\u0026gt; traplot(m1.r2jags, parms = team_par, style = \u0026quot;plain\u0026quot;, main = c(\u0026quot;Sevilla\u0026quot;,\u0026quot;Valenica\u0026quot;))\r\rModel validation\rSeems like Sevilla and Valencia have similar skill with Valencia being slightly better. Using the MCMC samples it is not only possible to look at the distribution of parameter values but it is also straight forward to simulate matches between teams and look at the credible distribution of number of goals scored and the probability of a win for the home team, a win for the away team or a draw. The following functions simulates matches with one team as home team and one team as away team and plots the predicted result together with the actual outcomes of any matches in the laliga data set.\n\u0026gt; # Plots histograms over home_goals, away_goals, the difference in goals\r\u0026gt; # and a barplot over match results.\r\u0026gt; plot_goals \u0026lt;- function(home_goals, away_goals) {\r+ n_matches \u0026lt;- length(home_goals)\r+ goal_diff \u0026lt;- home_goals - away_goals\r+ match_result \u0026lt;- ifelse(goal_diff \u0026lt; 0, \u0026quot;away_win\u0026quot;, ifelse(goal_diff \u0026gt; 0,\r+ \u0026quot;home_win\u0026quot;, \u0026quot;equal\u0026quot;))\r+ hist(home_goals, xlim = c(-0.5, 10), breaks = (0:100) - 0.5)\r+ hist(away_goals, xlim = c(-0.5, 10), breaks = (0:100) - 0.5)\r+ hist(goal_diff, xlim = c(-6, 6), breaks = (-100:100) - 0.5)\r+ barplot(table(match_result)/n_matches, ylim = c(0, 1))\r+ }\r\u0026gt; \u0026gt; \u0026gt; plot_pred_comp1 \u0026lt;- function(home_team, away_team, ms) {\r+ # Simulates and plots game goals scores using the MCMC samples from the m1\r+ # model.\r+ par(mar=c(2,2,2,2))\r+ par(mfrow = c(2, 4))\r+ baseline \u0026lt;- ms[, \u0026quot;baseline\u0026quot;]\r+ home_skill \u0026lt;- ms[, which(teams == home_team)]\r+ away_skill \u0026lt;- ms[, which(teams == away_team)]\r+ home_goals \u0026lt;- rpois(nrow(ms), exp(baseline + home_skill - away_skill))\r+ away_goals \u0026lt;- rpois(nrow(ms), exp(baseline + away_skill - home_skill))\r+ plot_goals(home_goals, away_goals)\r+ # Plots the actual distribution of goals between the two teams\r+ home_goals \u0026lt;- d$HomeGoals[d$HomeTeam == home_team \u0026amp; d$AwayTeam == away_team]\r+ away_goals \u0026lt;- d$AwayGoals[d$HomeTeam == home_team \u0026amp; d$AwayTeam == away_team]\r+ plot_goals(home_goals, away_goals)\r+ }\rLet’s look at Valencia (home team) vs. Sevilla (away team). The graph below shows the simulation on the first row and the historical data on the second row.\n\u0026gt; ms1\u0026lt;-as.matrix(m1.r2jags$BUGSoutput$sims.matrix)\r\u0026gt; plot_pred_comp1(\u0026quot;FC Valencia\u0026quot;, \u0026quot;FC Sevilla\u0026quot;, ms1)\rHere we discover a problem with the current model. While the simulated data looks the same, except that the home team and the away team swapped places, the historical data now shows that Sevilla often wins against Valencia when being the home team. Our model doesn’t predict this because it doesn’t considers the advantage of being the home team.\n\r\rAccounting for home advantage\rThe only change to the model needed to account for the home advantage is to split the baseline into two components, a home baseline and an away baseline. The following JAGS model implements this change by splitting baseline into home_baseline and away_baseline.\nModel fitting\r\u0026gt; # model 2\r\u0026gt; m2_string \u0026lt;- \u0026quot;model {\r+ for(i in 1:n_games) {\r+ HomeGoals[i] ~ dpois(lambda_home[HomeTeam[i],AwayTeam[i]])\r+ AwayGoals[i] ~ dpois(lambda_away[HomeTeam[i],AwayTeam[i]])\r+ }\r+ + for(home_i in 1:n_teams) {\r+ for(away_i in 1:n_teams) {\r+ lambda_home[home_i, away_i] \u0026lt;- exp( home_baseline + skill[home_i] - skill[away_i])\r+ lambda_away[home_i, away_i] \u0026lt;- exp( away_baseline + skill[away_i] - skill[home_i])\r+ }\r+ }\r+ + skill[1] \u0026lt;- 0 + for(j in 2:n_teams) {\r+ skill[j] ~ dnorm(group_skill, group_tau)\r+ }\r+ + group_skill ~ dnorm(0, 0.0625)\r+ group_tau \u0026lt;- 1/pow(group_sigma, 2)\r+ group_sigma ~ dunif(0, 3)\r+ + home_baseline ~ dnorm(0, 0.0625)\r+ away_baseline ~ dnorm(0, 0.0625)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(m2_string, con = \u0026quot;model2.txt\u0026quot;)\rAnd now re-fit the model\n\u0026gt; params \u0026lt;- c(\u0026quot;home_baseline\u0026quot;, \u0026quot;away_baseline\u0026quot;, \u0026quot;skill\u0026quot;, \u0026quot;group_sigma\u0026quot;, \u0026quot;group_skill\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; m2.r2jags \u0026lt;- jags(data = data_list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;model2.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 3700\rUnobserved stochastic nodes: 32\rTotal graph size: 10863\rInitializing model\r\u0026gt; \u0026gt; print(m2.r2jags)\rInference for Bugs model at \u0026quot;model2.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75%\raway_baseline 0.081 0.022 0.038 0.067 0.082 0.096\rgroup_sigma 0.226 0.035 0.169 0.201 0.221 0.246\rgroup_skill 0.019 0.061 -0.102 -0.022 0.019 0.060\rhome_baseline 0.449 0.019 0.413 0.436 0.449 0.462\rskill[1] 0.000 0.000 0.000 0.000 0.000 0.000\rskill[2] 0.187 0.061 0.066 0.147 0.187 0.228\rskill[3] 0.017 0.068 -0.120 -0.030 0.017 0.063\rskill[4] -0.011 0.058 -0.126 -0.050 -0.011 0.028\rskill[5] -0.179 0.100 -0.375 -0.246 -0.178 -0.111\rskill[6] -0.044 0.062 -0.167 -0.087 -0.045 -0.002\rskill[7] -0.009 0.061 -0.127 -0.050 -0.009 0.033\rskill[8] 0.199 0.059 0.081 0.159 0.201 0.240\rskill[9] -0.078 0.063 -0.205 -0.121 -0.077 -0.035\rskill[10] -0.108 0.064 -0.234 -0.151 -0.108 -0.066\rskill[11] 0.699 0.057 0.583 0.661 0.699 0.737\rskill[12] 0.132 0.059 0.019 0.092 0.132 0.173\rskill[13] 0.025 0.061 -0.097 -0.016 0.025 0.065\rskill[14] 0.041 0.059 -0.074 0.000 0.041 0.081\rskill[15] -0.006 0.060 -0.125 -0.046 -0.005 0.035\rskill[16] -0.115 0.099 -0.311 -0.181 -0.116 -0.049\rskill[17] 0.611 0.059 0.494 0.571 0.612 0.653\rskill[18] -0.068 0.070 -0.204 -0.115 -0.069 -0.020\rskill[19] -0.114 0.070 -0.252 -0.162 -0.113 -0.065\rskill[20] 0.077 0.062 -0.047 0.035 0.077 0.118\rskill[21] -0.102 0.064 -0.228 -0.145 -0.101 -0.058\rskill[22] -0.202 0.098 -0.399 -0.266 -0.201 -0.138\rskill[23] -0.167 0.098 -0.361 -0.233 -0.167 -0.102\rskill[24] -0.115 0.099 -0.306 -0.183 -0.116 -0.049\rskill[25] 0.010 0.071 -0.132 -0.035 0.010 0.057\rskill[26] 0.061 0.069 -0.075 0.014 0.062 0.109\rskill[27] -0.059 0.078 -0.211 -0.112 -0.060 -0.007\rskill[28] -0.113 0.082 -0.274 -0.167 -0.113 -0.058\rskill[29] -0.051 0.105 -0.265 -0.121 -0.050 0.021\rdeviance 10742.730 7.596 10729.548 10737.288 10742.120 10747.534\r97.5% Rhat n.eff\raway_baseline 0.126 1.002 1600\rgroup_sigma 0.305 1.001 15000\rgroup_skill 0.138 1.006 330\rhome_baseline 0.486 1.001 15000\rskill[1] 0.000 1.000 1\rskill[2] 0.306 1.005 380\rskill[3] 0.149 1.006 310\rskill[4] 0.102 1.002 1900\rskill[5] 0.013 1.002 1100\rskill[6] 0.076 1.005 400\rskill[7] 0.110 1.004 440\rskill[8] 0.312 1.008 240\rskill[9] 0.045 1.003 620\rskill[10] 0.014 1.005 330\rskill[11] 0.811 1.004 580\rskill[12] 0.245 1.008 200\rskill[13] 0.144 1.003 840\rskill[14] 0.154 1.003 700\rskill[15] 0.111 1.008 210\rskill[16] 0.078 1.006 300\rskill[17] 0.722 1.005 420\rskill[18] 0.070 1.005 360\rskill[19] 0.019 1.007 290\rskill[20] 0.200 1.006 310\rskill[21] 0.025 1.010 170\rskill[22] -0.009 1.002 1600\rskill[23] 0.028 1.003 900\rskill[24] 0.078 1.006 330\rskill[25] 0.151 1.007 240\rskill[26] 0.196 1.003 770\rskill[27] 0.097 1.001 3900\rskill[28] 0.051 1.002 1800\rskill[29] 0.153 1.003 620\rdeviance 10759.025 1.004 460\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 28.8 and DIC = 10771.5\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMCMC diagnostics\rLooking at the trace plots and distributions of home_baseline and away_baseline shows that there is a considerable home advantage.\n\u0026gt; team_par\u0026lt;-c(\u0026quot;home_baseline\u0026quot;, \u0026quot;away_baseline\u0026quot;)\r\u0026gt; denplot(m2.r2jags, parms = team_par, style = \u0026quot;plain\u0026quot;, main = c(\u0026quot;home_baseline\u0026quot;,\u0026quot;away_baseline\u0026quot;))\r\u0026gt; traplot(m2.r2jags, parms = team_par, style = \u0026quot;plain\u0026quot;, main = c(\u0026quot;home_baseline\u0026quot;,\u0026quot;away_baseline\u0026quot;))\r\rModel validation\rLooking at the difference between exp(home_baseline) and exp(away_baseline) shows that the home advantage is realised as roughly \\(0.5\\) more goals for the home team.\n\u0026gt; ms2\u0026lt;-as.matrix(m2.r2jags$BUGSoutput$sims.matrix)\r\u0026gt; plotPost(exp(ms2[, \u0026quot;home_baseline\u0026quot;]) - exp(ms2[, \u0026quot;away_baseline\u0026quot;]), compVal = 0,\r+ xlab = \u0026quot;Home advantage in number of goals\u0026quot;)\r mean median mode hdiMass\rHome advantage in number of goals 0.4822046 0.4822645 0.4831489 0.95\rhdiLow hdiHigh compVal pcGTcompVal\rHome advantage in number of goals 0.4096975 0.5549439 0 1\rROPElow ROPEhigh pcInROPE\rHome advantage in number of goals NA NA NA\rComparing the DIC of the of the two models also indicates that the new model is better.\n\u0026gt; dic_m1\u0026lt;-m1.r2jags$BUGSoutput$DIC\r\u0026gt; dic_m2\u0026lt;-m2.r2jags$BUGSoutput$DIC\r\u0026gt; diff_dic\u0026lt;-dic_m1 - dic_m2\r\u0026gt; diff_dic\r[1] 168.7556\rFinally we’ll look at the simulated results for Valencia (home team) vs Sevilla (away team) using the estimates from the new model with the first row of the graph showing the predicted outcome and the second row showing the actual data.\n\u0026gt; plot_pred_comp2 \u0026lt;- function(home_team, away_team, ms) {\r+ par(mar=c(2,2,2,2))\r+ par(mfrow = c(2, 4))\r+ home_baseline \u0026lt;- ms[, \u0026quot;home_baseline\u0026quot;]\r+ away_baseline \u0026lt;- ms[, \u0026quot;away_baseline\u0026quot;]\r+ home_skill \u0026lt;- ms[, col_name(\u0026quot;skill\u0026quot;, which(teams == home_team))]\r+ away_skill \u0026lt;- ms[, col_name(\u0026quot;skill\u0026quot;, which(teams == away_team))]\r+ home_goals \u0026lt;- rpois(nrow(ms), exp(home_baseline + home_skill - away_skill))\r+ away_goals \u0026lt;- rpois(nrow(ms), exp(away_baseline + away_skill - home_skill))\r+ plot_goals(home_goals, away_goals)\r+ home_goals \u0026lt;- d$HomeGoals[d$HomeTeam == home_team \u0026amp; d$AwayTeam == away_team]\r+ away_goals \u0026lt;- d$AwayGoals[d$HomeTeam == home_team \u0026amp; d$AwayTeam == away_team]\r+ plot_goals(home_goals, away_goals)\r+ }\r\u0026gt; \u0026gt; plot_pred_comp2(\u0026quot;FC Valencia\u0026quot;, \u0026quot;FC Sevilla\u0026quot;, ms2)\rAnd similarly Sevilla (home team) vs Valencia (away team).\n\u0026gt; plot_pred_comp2(\u0026quot;FC Sevilla\u0026quot;, \u0026quot;FC Valencia\u0026quot;, ms2)\rNow the results are closer to the historical data as both Sevilla and Valencia are more likely to win when playing as the home team. At this point in the modeling process I decided to try to split the skill parameter into two components, offence skill and defense skill, thinking that some teams might be good at scoring goals but at the same time be bad at keeping the opponent from scoring. This didn’t seem to result in any better fit however, perhaps because the offensive and defensive skill of a team tend to be highly related. There is however one more thing I would like to change with the model.\n\r\rAllowing for skill variation over the season\rThe data set laliga contains data from five different seasons and an assumption of the current model is that a team has the same skill during all seasons. This is probably not a realistic assumption, teams probably differ in their year-to-year performance. And what more, some teams do not even participate in all seasons in the laliga data set, as a result of dropping out of the first division, as the following diagram shows:\nData check\r\u0026gt; qplot(Season, HomeTeam, data = d, ylab = \u0026quot;Team\u0026quot;, xlab = \u0026quot;Particicipation by Season\u0026quot;) + theme_classic()\rThe second iteration of the model was therefore modified to include the year-to-year variability in team skill. This was done by allowing each team to have one skill parameter per season but to connect the skill parameters by using a team’s skill parameter for season \\(t\\) in the prior distribution for that team’s skill parameter for season \\(t+1\\) so that\n\\[ \\text{skill}_{t+1} \\sim N(\\text{skill}_t,\\sigma^2_{\\text{season}})\\]\nfor all different \\(t\\), except the first season which is given a vague prior. Here \\(\\sigma^2_{\\text{season}}\\) is a parameter estimated using the whole data set. The home and away baselines are given the same kind of priors and below is the resulting JAGS model.\n\u0026gt; # model 3\r\u0026gt; m3_string \u0026lt;- \u0026quot;model {\r+ for(i in 1:n_games) {\r+ HomeGoals[i] ~ dpois(lambda_home[Season[i], HomeTeam[i],AwayTeam[i]])\r+ AwayGoals[i] ~ dpois(lambda_away[Season[i], HomeTeam[i],AwayTeam[i]])\r+ }\r+ + for(season_i in 1:n_seasons) {\r+ for(home_i in 1:n_teams) {\r+ for(away_i in 1:n_teams) {\r+ lambda_home[season_i, home_i, away_i] \u0026lt;- exp( home_baseline[season_i] + skill[season_i, home_i] - skill[season_i, away_i])\r+ lambda_away[season_i, home_i, away_i] \u0026lt;- exp( away_baseline[season_i] + skill[season_i, away_i] - skill[season_i, home_i])\r+ }\r+ }\r+ }\r+ + skill[1, 1] \u0026lt;- 0 + for(j in 2:n_teams) {\r+ skill[1, j] ~ dnorm(group_skill, group_tau)\r+ }\r+ + group_skill ~ dnorm(0, 0.0625)\r+ group_tau \u0026lt;- 1/pow(group_sigma, 2)\r+ group_sigma ~ dunif(0, 3)\r+ + home_baseline[1] ~ dnorm(0, 0.0625)\r+ away_baseline[1] ~ dnorm(0, 0.0625)\r+ + for(season_i in 2:n_seasons) {\r+ skill[season_i, 1] \u0026lt;- 0 + for(j in 2:n_teams) {\r+ skill[season_i, j] ~ dnorm(skill[season_i - 1, j], season_tau)\r+ }\r+ home_baseline[season_i] ~ dnorm(home_baseline[season_i - 1], season_tau)\r+ away_baseline[season_i] ~ dnorm(away_baseline[season_i - 1], season_tau)\r+ }\r+ + season_tau \u0026lt;- 1/pow(season_sigma, 2) + season_sigma ~ dunif(0, 3) + }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(m3_string, con = \u0026quot;model3.txt\u0026quot;)\rAnd now re-fit the model. These changes to the model unfortunately introduce quite a lot of autocorrelation when running the MCMC sampler. Also, I re-define the data list to include information for the season parameters.\n\u0026gt; data_list_m3 \u0026lt;- list(HomeGoals = d$HomeGoals, AwayGoals = d$AwayGoals, HomeTeam = as.numeric(factor(d$HomeTeam,\r+ levels = teams)), AwayTeam = as.numeric(factor(d$AwayTeam, levels = teams)),\r+ Season = as.numeric(factor(d$Season, levels = seasons)), n_teams = length(teams),\r+ n_games = nrow(d), n_seasons = length(seasons))\r\u0026gt; params \u0026lt;- c(\u0026quot;home_baseline\u0026quot;, \u0026quot;away_baseline\u0026quot;, \u0026quot;skill\u0026quot;, \u0026quot;season_sigma\u0026quot;, \u0026quot;group_sigma\u0026quot;, \u0026quot;group_skill\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; m3.r2jags \u0026lt;- jags(data = data_list_m3, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;model3.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 3700\rUnobserved stochastic nodes: 153\rTotal graph size: 26525\rInitializing model\r\u0026gt; \u0026gt; print(m3.r2jags)\rInference for Bugs model at \u0026quot;model3.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75%\raway_baseline[1] 0.105 0.038 0.030 0.079 0.103 0.130\raway_baseline[2] 0.078 0.030 0.018 0.059 0.079 0.099\raway_baseline[3] 0.067 0.031 0.005 0.047 0.069 0.089\raway_baseline[4] 0.064 0.032 -0.001 0.043 0.065 0.087\raway_baseline[5] 0.079 0.035 0.011 0.056 0.080 0.101\rgroup_sigma 0.217 0.035 0.158 0.193 0.214 0.238\rgroup_skill 0.018 0.060 -0.090 -0.023 0.015 0.057\rhome_baseline[1] 0.447 0.029 0.392 0.428 0.446 0.466\rhome_baseline[2] 0.437 0.026 0.383 0.421 0.438 0.454\rhome_baseline[3] 0.443 0.026 0.389 0.427 0.443 0.459\rhome_baseline[4] 0.452 0.027 0.400 0.434 0.451 0.469\rhome_baseline[5] 0.454 0.031 0.394 0.434 0.452 0.474\rseason_sigma 0.033 0.019 0.001 0.016 0.032 0.048\rskill[1,1] 0.000 0.000 0.000 0.000 0.000 0.000\rskill[2,1] 0.000 0.000 0.000 0.000 0.000 0.000\rskill[3,1] 0.000 0.000 0.000 0.000 0.000 0.000\rskill[4,1] 0.000 0.000 0.000 0.000 0.000 0.000\rskill[5,1] 0.000 0.000 0.000 0.000 0.000 0.000\rskill[1,2] 0.178 0.067 0.047 0.136 0.175 0.218\rskill[2,2] 0.169 0.065 0.039 0.129 0.167 0.207\rskill[3,2] 0.181 0.064 0.061 0.139 0.177 0.219\rskill[4,2] 0.195 0.065 0.076 0.148 0.189 0.235\rskill[5,2] 0.216 0.075 0.090 0.161 0.209 0.265\rskill[1,3] 0.015 0.074 -0.126 -0.033 0.013 0.060\rskill[2,3] 0.017 0.075 -0.127 -0.030 0.014 0.063\rskill[3,3] 0.020 0.074 -0.121 -0.027 0.017 0.066\rskill[4,3] 0.022 0.071 -0.111 -0.024 0.018 0.066\rskill[5,3] 0.027 0.075 -0.111 -0.021 0.023 0.074\rskill[1,4] -0.008 0.066 -0.122 -0.053 -0.012 0.035\rskill[2,4] -0.011 0.063 -0.118 -0.053 -0.015 0.031\rskill[3,4] -0.010 0.063 -0.120 -0.052 -0.015 0.031\rskill[4,4] -0.020 0.065 -0.136 -0.064 -0.024 0.022\rskill[5,4] -0.022 0.069 -0.151 -0.069 -0.025 0.023\rskill[1,5] -0.174 0.098 -0.370 -0.238 -0.173 -0.107\rskill[2,5] -0.174 0.104 -0.384 -0.242 -0.174 -0.104\rskill[3,5] -0.174 0.111 -0.394 -0.247 -0.174 -0.102\rskill[4,5] -0.174 0.117 -0.408 -0.251 -0.174 -0.098\rskill[5,5] -0.174 0.123 -0.420 -0.254 -0.175 -0.095\rskill[1,6] -0.034 0.074 -0.161 -0.085 -0.039 0.017\rskill[2,6] -0.048 0.068 -0.167 -0.096 -0.052 -0.004\rskill[3,6] -0.058 0.067 -0.176 -0.106 -0.060 -0.015\rskill[4,6] -0.065 0.072 -0.193 -0.117 -0.067 -0.019\rskill[5,6] -0.071 0.075 -0.207 -0.125 -0.072 -0.025\rskill[1,7] -0.006 0.069 -0.124 -0.054 -0.013 0.038\rskill[2,7] -0.014 0.065 -0.129 -0.058 -0.021 0.027\rskill[3,7] -0.009 0.064 -0.120 -0.054 -0.016 0.031\rskill[4,7] -0.006 0.066 -0.117 -0.052 -0.013 0.035\rskill[5,7] -0.001 0.071 -0.122 -0.051 -0.009 0.044\rskill[1,8] 0.200 0.067 0.069 0.155 0.198 0.242\rskill[2,8] 0.206 0.063 0.086 0.162 0.203 0.246\rskill[3,8] 0.209 0.063 0.090 0.164 0.206 0.250\rskill[4,8] 0.204 0.064 0.082 0.158 0.202 0.244\rskill[5,8] 0.195 0.069 0.059 0.151 0.194 0.239\rskill[1,9] -0.055 0.073 -0.193 -0.102 -0.057 -0.005\rskill[2,9] -0.074 0.068 -0.203 -0.119 -0.077 -0.026\rskill[3,9] -0.087 0.068 -0.219 -0.133 -0.088 -0.040\rskill[4,9] -0.105 0.074 -0.254 -0.155 -0.101 -0.057\rskill[5,9] -0.105 0.083 -0.279 -0.159 -0.100 -0.049\rskill[1,10] -0.121 0.072 -0.246 -0.175 -0.125 -0.072\rskill[2,10] -0.109 0.070 -0.224 -0.163 -0.113 -0.061\rskill[3,10] -0.102 0.071 -0.219 -0.156 -0.106 -0.051\rskill[4,10] -0.111 0.073 -0.234 -0.168 -0.116 -0.060\rskill[5,10] -0.111 0.083 -0.259 -0.173 -0.116 -0.055\rskill[1,11] 0.676 0.072 0.526 0.629 0.680 0.726\rskill[2,11] 0.696 0.063 0.571 0.654 0.699 0.738\rskill[3,11] 0.712 0.060 0.600 0.672 0.712 0.750\rskill[4,11] 0.728 0.061 0.617 0.688 0.725 0.761\rskill[5,11] 0.727 0.064 0.606 0.686 0.725 0.763\rskill[1,12] 0.146 0.064 0.025 0.106 0.144 0.186\rskill[2,12] 0.143 0.060 0.028 0.105 0.141 0.180\rskill[3,12] 0.131 0.058 0.019 0.094 0.130 0.168\rskill[4,12] 0.127 0.059 0.010 0.089 0.126 0.164\rskill[5,12] 0.127 0.064 0.002 0.086 0.126 0.166\rskill[1,13] 0.031 0.065 -0.087 -0.014 0.028 0.071\rskill[2,13] 0.035 0.062 -0.077 -0.008 0.032 0.073\rskill[3,13] 0.023 0.061 -0.091 -0.019 0.020 0.062\rskill[4,13] 0.017 0.062 -0.101 -0.026 0.015 0.057\rskill[5,13] 0.015 0.067 -0.117 -0.030 0.014 0.057\rskill[1,14] 0.028 0.064 -0.095 -0.015 0.025 0.069\rskill[2,14] 0.027 0.061 -0.091 -0.014 0.024 0.065\rskill[3,14] 0.030 0.059 -0.085 -0.010 0.027 0.067\rskill[4,14] 0.046 0.062 -0.067 0.004 0.040 0.085\rskill[5,14] 0.055 0.068 -0.064 0.008 0.049 0.099\rskill[1,15] 0.015 0.065 -0.106 -0.029 0.009 0.055\rskill[2,15] 0.019 0.063 -0.095 -0.024 0.012 0.058\rskill[3,15] -0.003 0.061 -0.115 -0.043 -0.005 0.034\rskill[4,15] -0.014 0.062 -0.132 -0.055 -0.015 0.024\rskill[5,15] -0.037 0.071 -0.182 -0.082 -0.034 0.009\rskill[1,16] -0.120 0.096 -0.304 -0.187 -0.122 -0.055\rskill[2,16] -0.121 0.103 -0.319 -0.192 -0.122 -0.051\rskill[3,16] -0.121 0.109 -0.329 -0.195 -0.121 -0.047\rskill[4,16] -0.121 0.116 -0.342 -0.197 -0.122 -0.044\rskill[5,16] -0.120 0.121 -0.357 -0.201 -0.121 -0.041\rskill[1,17] 0.543 0.083 0.372 0.491 0.544 0.600\rskill[2,17] 0.588 0.067 0.470 0.541 0.586 0.631\rskill[3,17] 0.621 0.068 0.495 0.575 0.617 0.666\rskill[4,17] 0.646 0.075 0.501 0.593 0.644 0.697\rskill[5,17] 0.640 0.077 0.497 0.586 0.637 0.694\rskill[1,18] -0.068 0.075 -0.209 -0.122 -0.069 -0.021\rskill[2,18] -0.075 0.074 -0.219 -0.126 -0.075 -0.029\rskill[3,18] -0.068 0.078 -0.218 -0.122 -0.067 -0.019\rskill[4,18] -0.061 0.081 -0.217 -0.117 -0.060 -0.011\rskill[5,18] -0.054 0.081 -0.208 -0.110 -0.054 -0.002\rskill[1,19] -0.099 0.066 -0.220 -0.144 -0.101 -0.057\rskill[2,19] -0.106 0.065 -0.227 -0.151 -0.108 -0.064\rskill[3,19] -0.122 0.070 -0.260 -0.170 -0.121 -0.074\rskill[4,19] -0.122 0.080 -0.290 -0.174 -0.119 -0.069\rskill[5,19] -0.122 0.089 -0.312 -0.176 -0.118 -0.065\rskill[1,20] 0.086 0.068 -0.034 0.038 0.083 0.130\rskill[2,20] 0.083 0.065 -0.031 0.036 0.079 0.126\rskill[3,20] 0.082 0.065 -0.033 0.034 0.078 0.125\rskill[4,20] 0.065 0.070 -0.068 0.017 0.063 0.110\rskill[5,20] 0.065 0.079 -0.094 0.014 0.064 0.115\rskill[1,21] -0.097 0.081 -0.245 -0.154 -0.103 -0.042\rskill[2,21] -0.101 0.073 -0.234 -0.152 -0.104 -0.048\rskill[3,21] -0.100 0.071 -0.230 -0.149 -0.103 -0.050\rskill[4,21] -0.107 0.070 -0.237 -0.155 -0.110 -0.059\rskill[5,21] -0.108 0.074 -0.245 -0.158 -0.114 -0.058\rskill[1,22] -0.197 0.106 -0.391 -0.269 -0.198 -0.126\rskill[2,22] -0.204 0.100 -0.389 -0.272 -0.205 -0.138\rskill[3,22] -0.204 0.107 -0.401 -0.278 -0.204 -0.134\rskill[4,22] -0.205 0.114 -0.418 -0.282 -0.205 -0.131\rskill[5,22] -0.205 0.120 -0.432 -0.287 -0.206 -0.127\rskill[1,23] -0.158 0.099 -0.343 -0.228 -0.160 -0.092\rskill[2,23] -0.164 0.094 -0.340 -0.232 -0.165 -0.102\rskill[3,23] -0.163 0.102 -0.357 -0.235 -0.165 -0.097\rskill[4,23] -0.164 0.108 -0.373 -0.239 -0.165 -0.094\rskill[5,23] -0.164 0.114 -0.386 -0.241 -0.165 -0.092\rskill[1,24] -0.102 0.105 -0.310 -0.171 -0.105 -0.033\rskill[2,24] -0.107 0.102 -0.306 -0.174 -0.108 -0.041\rskill[3,24] -0.112 0.097 -0.301 -0.177 -0.112 -0.049\rskill[4,24] -0.112 0.104 -0.317 -0.181 -0.113 -0.044\rskill[5,24] -0.112 0.111 -0.329 -0.185 -0.112 -0.041\rskill[1,25] 0.009 0.086 -0.149 -0.046 0.007 0.066\rskill[2,25] 0.009 0.081 -0.145 -0.044 0.006 0.061\rskill[3,25] 0.008 0.074 -0.140 -0.042 0.006 0.055\rskill[4,25] 0.013 0.074 -0.139 -0.038 0.011 0.060\rskill[5,25] 0.003 0.077 -0.143 -0.047 0.001 0.053\rskill[1,26] 0.048 0.088 -0.117 -0.019 0.046 0.108\rskill[2,26] 0.049 0.083 -0.104 -0.013 0.047 0.104\rskill[3,26] 0.049 0.076 -0.083 -0.009 0.047 0.099\rskill[4,26] 0.068 0.075 -0.063 0.014 0.066 0.118\rskill[5,26] 0.091 0.082 -0.057 0.034 0.091 0.145\rskill[1,27] -0.051 0.093 -0.236 -0.108 -0.049 0.000\rskill[2,27] -0.054 0.088 -0.231 -0.108 -0.050 -0.003\rskill[3,27] -0.055 0.084 -0.224 -0.108 -0.052 -0.006\rskill[4,27] -0.057 0.077 -0.213 -0.107 -0.054 -0.012\rskill[5,27] -0.053 0.079 -0.211 -0.104 -0.051 -0.007\rskill[1,28] -0.110 0.099 -0.292 -0.188 -0.112 -0.039\rskill[2,28] -0.113 0.094 -0.288 -0.188 -0.115 -0.046\rskill[3,28] -0.117 0.088 -0.282 -0.186 -0.120 -0.052\rskill[4,28] -0.121 0.081 -0.274 -0.186 -0.123 -0.063\rskill[5,28] -0.124 0.082 -0.278 -0.188 -0.127 -0.066\rskill[1,29] -0.057 0.129 -0.272 -0.154 -0.066 0.028\rskill[2,29] -0.059 0.127 -0.270 -0.155 -0.067 0.022\rskill[3,29] -0.061 0.124 -0.268 -0.154 -0.068 0.019\rskill[4,29] -0.062 0.121 -0.268 -0.152 -0.069 0.014\rskill[5,29] -0.063 0.117 -0.267 -0.149 -0.068 0.010\rdeviance 10731.636 11.981 10708.084 10723.239 10732.002 10740.556\r97.5% Rhat n.eff\raway_baseline[1] 0.185 1.056 34\raway_baseline[2] 0.135 1.004 500\raway_baseline[3] 0.123 1.001 7800\raway_baseline[4] 0.121 1.001 3300\raway_baseline[5] 0.148 1.006 280\rgroup_sigma 0.297 1.008 210\rgroup_skill 0.146 1.015 5000\rhome_baseline[1] 0.508 1.018 260\rhome_baseline[2] 0.487 1.005 15000\rhome_baseline[3] 0.493 1.004 15000\rhome_baseline[4] 0.506 1.007 390\rhome_baseline[5] 0.516 1.011 260\rseason_sigma 0.069 1.323 11\rskill[1,1] 0.000 1.000 1\rskill[2,1] 0.000 1.000 1\rskill[3,1] 0.000 1.000 1\rskill[4,1] 0.000 1.000 1\rskill[5,1] 0.000 1.000 1\rskill[1,2] 0.323 1.003 5900\rskill[2,2] 0.315 1.003 15000\rskill[3,2] 0.323 1.008 880\rskill[4,2] 0.338 1.012 290\rskill[5,2] 0.377 1.016 130\rskill[1,3] 0.170 1.017 110\rskill[2,3] 0.174 1.014 150\rskill[3,3] 0.175 1.013 180\rskill[4,3] 0.173 1.011 250\rskill[5,3] 0.185 1.004 470\rskill[1,4] 0.130 1.005 510\rskill[2,4] 0.121 1.004 680\rskill[3,4] 0.123 1.006 380\rskill[4,4] 0.117 1.003 820\rskill[5,4] 0.121 1.002 1200\rskill[1,5] 0.015 1.002 1400\rskill[2,5] 0.033 1.002 1500\rskill[3,5] 0.046 1.002 1800\rskill[4,5] 0.060 1.002 2000\rskill[5,5] 0.074 1.002 1900\rskill[1,6] 0.116 1.008 620\rskill[2,6] 0.092 1.010 15000\rskill[3,6] 0.084 1.012 3500\rskill[4,6] 0.085 1.011 1200\rskill[5,6] 0.086 1.012 710\rskill[1,7] 0.145 1.005 11000\rskill[2,7] 0.133 1.011 15000\rskill[3,7] 0.134 1.015 15000\rskill[4,7] 0.142 1.012 15000\rskill[5,7] 0.154 1.007 15000\rskill[1,8] 0.337 1.001 15000\rskill[2,8] 0.339 1.003 15000\rskill[3,8] 0.341 1.001 6700\rskill[4,8] 0.340 1.003 15000\rskill[5,8] 0.338 1.003 1400\rskill[1,9] 0.090 1.019 140\rskill[2,9] 0.059 1.010 520\rskill[3,9] 0.046 1.007 13000\rskill[4,9] 0.035 1.004 790\rskill[5,9] 0.047 1.003 790\rskill[1,10] 0.027 1.011 150\rskill[2,10] 0.032 1.016 110\rskill[3,10] 0.040 1.021 98\rskill[4,10] 0.037 1.011 170\rskill[5,10] 0.057 1.008 200\rskill[1,11] 0.815 1.035 59\rskill[2,11] 0.825 1.019 110\rskill[3,11] 0.839 1.006 320\rskill[4,11] 0.860 1.002 1100\rskill[5,11] 0.866 1.002 1000\rskill[1,12] 0.279 1.002 1900\rskill[2,12] 0.267 1.002 1300\rskill[3,12] 0.247 1.009 300\rskill[4,12] 0.244 1.012 190\rskill[5,12] 0.255 1.006 280\rskill[1,13] 0.168 1.002 4300\rskill[2,13] 0.167 1.004 15000\rskill[3,13] 0.153 1.009 350\rskill[4,13] 0.148 1.014 170\rskill[5,13] 0.152 1.012 210\rskill[1,14] 0.159 1.007 240\rskill[2,14] 0.153 1.007 240\rskill[3,14] 0.156 1.006 410\rskill[4,14] 0.176 1.001 15000\rskill[5,14] 0.202 1.002 1400\rskill[1,15] 0.155 1.001 4000\rskill[2,15] 0.154 1.001 4000\rskill[3,15] 0.122 1.008 210\rskill[4,15] 0.113 1.013 130\rskill[5,15] 0.100 1.030 58\rskill[1,16] 0.071 1.004 970\rskill[2,16] 0.084 1.002 1200\rskill[3,16] 0.095 1.002 1000\rskill[4,16] 0.108 1.002 970\rskill[5,16] 0.121 1.002 1000\rskill[1,17] 0.704 1.009 190\rskill[2,17] 0.721 1.002 15000\rskill[3,17] 0.759 1.023 100\rskill[4,17] 0.800 1.049 48\rskill[5,17] 0.796 1.039 58\rskill[1,18] 0.086 1.011 160\rskill[2,18] 0.077 1.008 220\rskill[3,18] 0.089 1.010 170\rskill[4,18] 0.099 1.011 160\rskill[5,18] 0.108 1.014 120\rskill[1,19] 0.039 1.013 130\rskill[2,19] 0.028 1.016 100\rskill[3,19] 0.017 1.028 62\rskill[4,19] 0.036 1.020 83\rskill[5,19] 0.054 1.017 98\rskill[1,20] 0.229 1.006 280\rskill[2,20] 0.221 1.006 300\rskill[3,20] 0.220 1.004 510\rskill[4,20] 0.207 1.001 15000\rskill[5,20] 0.223 1.001 15000\rskill[1,21] 0.063 1.006 320\rskill[2,21] 0.043 1.005 390\rskill[3,21] 0.042 1.007 240\rskill[4,21] 0.037 1.007 250\rskill[5,21] 0.044 1.009 190\rskill[1,22] 0.014 1.011 150\rskill[2,22] -0.001 1.012 160\rskill[3,22] 0.010 1.010 180\rskill[4,22] 0.026 1.008 220\rskill[5,22] 0.038 1.006 270\rskill[1,23] 0.043 1.003 710\rskill[2,23] 0.026 1.002 1200\rskill[3,23] 0.042 1.002 1900\rskill[4,23] 0.054 1.001 2600\rskill[5,23] 0.070 1.002 2300\rskill[1,24] 0.109 1.001 15000\rskill[2,24] 0.096 1.001 5000\rskill[3,24] 0.085 1.001 2700\rskill[4,24] 0.098 1.001 3600\rskill[5,24] 0.109 1.002 3300\rskill[1,25] 0.186 1.021 480\rskill[2,25] 0.174 1.030 410\rskill[3,25] 0.163 1.041 410\rskill[4,25] 0.169 1.041 230\rskill[5,25] 0.162 1.023 310\rskill[1,26] 0.228 1.010 240\rskill[2,26] 0.220 1.015 170\rskill[3,26] 0.203 1.026 120\rskill[4,26] 0.224 1.052 49\rskill[5,26] 0.257 1.078 31\rskill[1,27] 0.143 1.001 3900\rskill[2,27] 0.129 1.001 9900\rskill[3,27] 0.118 1.001 15000\rskill[4,27] 0.103 1.002 1600\rskill[5,27] 0.112 1.001 3800\rskill[1,28] 0.087 1.003 720\rskill[2,28] 0.068 1.004 470\rskill[3,28] 0.052 1.006 310\rskill[4,28] 0.035 1.009 190\rskill[5,28] 0.035 1.012 140\rskill[1,29] 0.235 1.058 160\rskill[2,29] 0.230 1.061 160\rskill[3,29] 0.224 1.068 150\rskill[4,29] 0.215 1.076 130\rskill[5,29] 0.204 1.085 110\rdeviance 10752.921 1.060 31\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 69.4 and DIC = 10801.0\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMCMC diagnostics\rThe following graph shows the trace plot and distribution of the season_sigma parameter.\n\u0026gt; denplot(m3.r2jags, parms = \u0026quot;season_sigma\u0026quot;, style = \u0026quot;plain\u0026quot;)\r\u0026gt; traplot(m3.r2jags, parms = \u0026quot;season_sigma\u0026quot;, style = \u0026quot;plain\u0026quot;)\rCalculating and comparing the DIC of this model with the former model show no substantial difference.\n\u0026gt; dic_m2\u0026lt;-m2.r2jags$BUGSoutput$DIC\r\u0026gt; dic_m3\u0026lt;-m3.r2jags$BUGSoutput$DIC\r\u0026gt; diff_dic\u0026lt;-dic_m2 - dic_m3\r\u0026gt; diff_dic\r[1] -29.50679\rHowever, I believe the assumptions of the current model (m3) are more reasonable so I’ll stick with this model.\n\r\rRanking the teams of La Liga\rWe’ll start by ranking the teams of La Liga using the estimated skill parameters from the 2012/2013 season. The values of the skill parameters are difficult to interpret as they are relative to the skill of the team that had its skill parameter “anchored” at zero. To put them on a more interpretable scale I’ll first zero center the skill parameters by subtracting the mean skill of all teams, I then add the home baseline and exponentiate the resulting values. These rescaled skill parameters are now on the scale of expected number of goals when playing home team. Below is a caterpillar plot of the median of the rescaled skill parameters together with the \\(68\\)% and \\(95\\)% credible intervals. The plot is ordered according to the median skill and thus also gives the ranking of the teams.\n\u0026gt; # The ranking of the teams for the 2012/13 season.\r\u0026gt; ms3\u0026lt;-m3.r2jags$BUGSoutput$sims.matrix\r\u0026gt; team_skill \u0026lt;- ms3[, str_detect(string = colnames(ms3), \u0026quot;skill\\\\[5,\u0026quot;)]\r\u0026gt; team_skill \u0026lt;- (team_skill - rowMeans(team_skill)) + ms3[, \u0026quot;home_baseline[5]\u0026quot;]\r\u0026gt; team_skill \u0026lt;- exp(team_skill)\r\u0026gt; colnames(team_skill) \u0026lt;- teams\r\u0026gt; team_skill \u0026lt;- team_skill[, order(colMeans(team_skill), decreasing = T)]\r\u0026gt; par(mar = c(2, 0.7, 0.7, 0.7), xaxs = \u0026quot;i\u0026quot;)\r\u0026gt; caterplot(team_skill, labels.loc = \u0026quot;above\u0026quot;, val.lim = c(0.7, 3.8), style = \u0026quot;plain\u0026quot;)\rTwo teams are clearly ahead of the rest, FC Barcelona and Real Madrid CF. Let’s look at the credible difference between the two teams.\n\u0026gt; plotPost(team_skill[, \u0026quot;FC Barcelona\u0026quot;] - team_skill[, \u0026quot;Real Madrid CF\u0026quot;], compVal = 0,\r+ xlab = \u0026quot;← Real Madrid vs Barcelona →\u0026quot;)\r mean median mode\r\u0026lt;U+2190\u0026gt; Real Madrid vs Barcelona \u0026lt;U+2192\u0026gt; 0.26639 0.2657129 0.2950102\rhdiMass hdiLow hdiHigh\r\u0026lt;U+2190\u0026gt; Real Madrid vs Barcelona \u0026lt;U+2192\u0026gt; 0.95 -0.1863186 0.785901\rcompVal pcGTcompVal ROPElow\r\u0026lt;U+2190\u0026gt; Real Madrid vs Barcelona \u0026lt;U+2192\u0026gt; 0 0.8660667 NA\rROPEhigh pcInROPE\r\u0026lt;U+2190\u0026gt; Real Madrid vs Barcelona \u0026lt;U+2192\u0026gt; NA NA\rFC Barcelona is the better team with a probability of \\(82\\)%\n\rPredicting the End Game of La Liga 2012/2013\rIn the laliga data set the results of the \\(50\\) last games of the 2012/2013 season was missing. Using our model we can now both predict and simulate the outcomes of these \\(50\\) games. The R code below calculates a number of measures for each game (both the games with known and unknown outcomes):\n\rThe mode of the simulated number of goals, that is, the most likely number of scored goals. If we were asked to bet on the number of goals in a game this is what we would use.\n\rThe mean of the simulated number of goals, this is our best guess of the average number of goals in a game.\n\rThe most likely match result for each game.\n\rA random sample from the distributions of credible home scores, away scores and match results. This is how La Liga actually could have played out in an alternative reality.\n\r\r\u0026gt; n \u0026lt;- nrow(ms3)\r\u0026gt; m3_pred \u0026lt;- sapply(1:nrow(laliga), function(i) {\r+ home_team \u0026lt;- which(teams == laliga$HomeTeam[i])\r+ away_team \u0026lt;- which(teams == laliga$AwayTeam[i])\r+ season \u0026lt;- which(seasons == laliga$Season[i])\r+ home_skill \u0026lt;- ms3[, col_name(\u0026quot;skill\u0026quot;, season, home_team)]\r+ away_skill \u0026lt;- ms3[, col_name(\u0026quot;skill\u0026quot;, season, away_team)]\r+ home_baseline \u0026lt;- ms3[, col_name(\u0026quot;home_baseline\u0026quot;, season)]\r+ away_baseline \u0026lt;- ms3[, col_name(\u0026quot;away_baseline\u0026quot;, season)]\r+ + home_goals \u0026lt;- rpois(n, exp(home_baseline + home_skill - away_skill))\r+ away_goals \u0026lt;- rpois(n, exp(away_baseline + away_skill - home_skill))\r+ home_goals_table \u0026lt;- table(home_goals)\r+ away_goals_table \u0026lt;- table(away_goals)\r+ match_results \u0026lt;- sign(home_goals - away_goals)\r+ match_results_table \u0026lt;- table(match_results)\r+ + mode_home_goal \u0026lt;- as.numeric(names(home_goals_table)[ which.max(home_goals_table)])\r+ mode_away_goal \u0026lt;- as.numeric(names(away_goals_table)[ which.max(away_goals_table)])\r+ match_result \u0026lt;- as.numeric(names(match_results_table)[which.max(match_results_table)])\r+ rand_i \u0026lt;- sample(seq_along(home_goals), 1)\r+ + c(mode_home_goal = mode_home_goal, mode_away_goal = mode_away_goal, match_result = match_result,\r+ mean_home_goal = mean(home_goals), mean_away_goal = mean(away_goals),\r+ rand_home_goal = home_goals[rand_i], rand_away_goal = away_goals[rand_i],\r+ rand_match_result = match_results[rand_i])\r+ })\r\u0026gt; m3_pred \u0026lt;- t(m3_pred)\rFirst let’s compare the distribution of the number of goals in the data with the predicted mode, mean and randomised number of goals for all the games (focusing on the number of goals for the home team). First the actual distribution of the number of goals for the home teams.\n\u0026gt; hist(laliga$HomeGoals, breaks = (-1:10) + 0.5, xlim = c(-0.5, 10), main = \u0026quot;Distribution of the number of goals\\nscored by a home team in a match.\u0026quot;, xlab = \u0026quot;\u0026quot;)\rThis next plot shows the distribution of the modes from the predicted distribution of home goals from each game. That is, what is the most probable outcome, for the home team, in each game.\n\u0026gt; hist(m3_pred[, \u0026quot;mode_home_goal\u0026quot;], breaks = (-1:10) + 0.5, xlim = c(-0.5, 10),\r+ main = \u0026quot;Distribution of predicted most\\nprobable scoreby a home team in\\na match.\u0026quot;, xlab = \u0026quot;\u0026quot;)\rFor almost all games the single most likely number of goals is one. Actually, if you know nothing about a La Liga game betting on one goal for the home team is \\(78\\)% of the times the best bet. Lest instead look at the distribution of the predicted mean number of home goals in each game.\n\u0026gt; hist(m3_pred[, \u0026quot;mean_home_goal\u0026quot;], breaks = (-1:10) + 0.5, xlim = c(-0.5, 10),\r+ main = \u0026quot;Distribution of predicted mean \\n score by a home team in a match.\u0026quot;, xlab = \u0026quot;\u0026quot;)\rFor most games the expected number of goals are \\(2\\). That is, even if your safest bet is one goal you would expect to see around two goals. The distribution of the mode and the mean number of goals doesn’t look remotely like the actual number of goals. This was not to be expected, we would however expect the distribution of randomized goals (where for each match the number of goals has been randomly drawn from that match’s predicted home goal distribution) to look similar to the actual number of home goals. Looking at the histogram below, this seems to be the case.\n\u0026gt; hist(m3_pred[, \u0026quot;rand_home_goal\u0026quot;], breaks = (-1:10) + 0.5, xlim = c(-0.5, 10),\r+ main = \u0026quot;Distribution of randomly draw \\n score by a home team in a match.\u0026quot;, xlab = \u0026quot;\u0026quot;)\rWe can also look at how well the model predicts the data. This should probably be done using cross validation, but as the number of effective parameters are much smaller than the number of data points a direct comparison should at least give an estimated prediction accuracy in the right ballpark.\n\u0026gt; mean(laliga$HomeGoals == m3_pred[, \u0026quot;mode_home_goal\u0026quot;], na.rm = T)\r[1] 0.3318919\r\u0026gt; \u0026gt; mean((laliga$HomeGoals - m3_pred[, \u0026quot;mean_home_goal\u0026quot;])^2, na.rm = T)\r[1] 1.457061\rSo on average the model predicts the correct number of home goals \\(34\\)% of the time and guesses the average number of goals with a mean squared error of \\(1.45\\). Now we’ll look at the actual and predicted match outcomes. The graph below shows the match outcomes in the data with \\(1\\) being a home win, \\(0\\) being a draw and \\(-1\\) being a win for the away team.\n\u0026gt; hist(laliga$MatchResult, breaks = (-2:1) + 0.5, main = \u0026quot;Actual match results.\u0026quot;, xlab = \u0026quot;\u0026quot;)\rNow looking at the most probable outcomes of the matches according to the model.\n\u0026gt; hist(m3_pred[, \u0026quot;match_result\u0026quot;], breaks = (-2:1) + 0.5, main = \u0026quot;Predicted match results.\u0026quot;, xlab = \u0026quot;\u0026quot;)\rFor almost all matches the safest bet is to bet on the home team. While draws are not uncommon it is never the safest bet. As in the case with the number of home goals, the randomized match outcomes have a distribution similar to the actual match outcomes:\n\u0026gt; hist(m3_pred[, \u0026quot;rand_match_result\u0026quot;], breaks = (-2:1) + 0.5, main = \u0026quot;Randomized match results.\u0026quot;, xlab = \u0026quot;\u0026quot;)\r\u0026gt; mean(laliga$MatchResult == m3_pred[, \u0026quot;match_result\u0026quot;], na.rm = T)\r[1] 0.5637838\rThe model predicts the correct match outcome \\(56\\)% of the time. Pretty good! Now that we’ve checked that the model reasonably predicts the La Liga history let’s predict the La Liga endgame! The code below displays the predicted mode and mean number of goals for the endgame and the predicted winner of each game.\n\u0026gt; laliga_forecast \u0026lt;- laliga[is.na(laliga$HomeGoals), c(\u0026quot;Season\u0026quot;, \u0026quot;Week\u0026quot;, \u0026quot;HomeTeam\u0026quot;,\r+ \u0026quot;AwayTeam\u0026quot;)]\r\u0026gt; m3_forecast \u0026lt;- m3_pred[is.na(laliga$HomeGoals), ]\r\u0026gt; laliga_forecast$mean_home_goals \u0026lt;- round(m3_forecast[, \u0026quot;mean_home_goal\u0026quot;], 1)\r\u0026gt; laliga_forecast$mean_away_goals \u0026lt;- round(m3_forecast[, \u0026quot;mean_away_goal\u0026quot;], 1)\r\u0026gt; laliga_forecast$mode_home_goals \u0026lt;- m3_forecast[, \u0026quot;mode_home_goal\u0026quot;]\r\u0026gt; laliga_forecast$mode_away_goals \u0026lt;- m3_forecast[, \u0026quot;mode_away_goal\u0026quot;]\r\u0026gt; laliga_forecast$predicted_winner \u0026lt;- ifelse(m3_forecast[, \u0026quot;match_result\u0026quot;] ==\r+ 1, laliga_forecast$HomeTeam, ifelse(m3_forecast[, \u0026quot;match_result\u0026quot;] == -1,\r+ laliga_forecast$AwayTeam, \u0026quot;Draw\u0026quot;))\r\u0026gt; \u0026gt; rownames(laliga_forecast) \u0026lt;- NULL\r\u0026gt; knitr::kable(laliga_forecast, \u0026quot;pandoc\u0026quot;, align = \u0026quot;c\u0026quot;)\r\r\rSeason\rWeek\rHomeTeam\rAwayTeam\rmean_home_goals\rmean_away_goals\rmode_home_goals\rmode_away_goals\rpredicted_winner\r\r\r\r2012/13\r34\rCelta Vigo\rAthletic Club Bilbao\r1.5\r1.2\r1\r1\rCelta Vigo\r\r2012/13\r34\rDeportivo de La CoruÃ±a\rAtlÃ©tico Madrid\r1.2\r1.4\r1\r1\rAtlÃ©tico Madrid\r\r2012/13\r34\rFC Barcelona\rBetis Sevilla\r3.2\r0.5\r3\r0\rFC Barcelona\r\r2012/13\r34\rFC Sevilla\rEspanyol Barcelona\r1.8\r1.0\r1\r0\rFC Sevilla\r\r2012/13\r34\rFC Valencia\rCA Osasuna\r2.0\r0.9\r2\r0\rFC Valencia\r\r2012/13\r34\rGetafe CF\rReal Sociedad San Sebastian\r1.5\r1.2\r1\r1\rGetafe CF\r\r2012/13\r34\rGranada CF\rMÃ¡laga CF\r1.3\r1.3\r1\r1\rGranada CF\r\r2012/13\r34\rRCD Mallorca\rLevante U.D.\r1.5\r1.1\r1\r1\rRCD Mallorca\r\r2012/13\r34\rReal Madrid CF\rReal Valladolid\r3.2\r0.5\r3\r0\rReal Madrid CF\r\r2012/13\r34\rReal Zaragoza\rRayo Vallecano\r1.5\r1.1\r1\r1\rReal Zaragoza\r\r2012/13\r35\rAthletic Club Bilbao\rRCD Mallorca\r1.6\r1.0\r1\r1\rAthletic Club Bilbao\r\r2012/13\r35\rAtlÃ©tico Madrid\rFC Barcelona\r1.0\r1.8\r0\r1\rFC Barcelona\r\r2012/13\r35\rBetis Sevilla\rCelta Vigo\r1.7\r1.0\r1\r1\rBetis Sevilla\r\r2012/13\r35\rCA Osasuna\rGetafe CF\r1.5\r1.1\r1\r1\rCA Osasuna\r\r2012/13\r35\rEspanyol Barcelona\rReal Madrid CF\r0.8\r2.1\r0\r2\rReal Madrid CF\r\r2012/13\r35\rLevante U.D.\rReal Zaragoza\r1.8\r1.0\r1\r0\rLevante U.D.\r\r2012/13\r35\rMÃ¡laga CF\rFC Sevilla\r1.5\r1.2\r1\r1\rMÃ¡laga CF\r\r2012/13\r35\rRayo Vallecano\rFC Valencia\r1.2\r1.4\r1\r1\rFC Valencia\r\r2012/13\r35\rReal Sociedad San Sebastian\rGranada CF\r2.0\r0.9\r2\r0\rReal Sociedad San Sebastian\r\r2012/13\r35\rReal Valladolid\rDeportivo de La CoruÃ±a\r1.6\r1.1\r1\r1\rReal Valladolid\r\r2012/13\r36\rCelta Vigo\rAtlÃ©tico Madrid\r1.2\r1.5\r1\r1\rAtlÃ©tico Madrid\r\r2012/13\r36\rDeportivo de La CoruÃ±a\rEspanyol Barcelona\r1.5\r1.2\r1\r1\rDeportivo de La CoruÃ±a\r\r2012/13\r36\rFC Barcelona\rReal Valladolid\r3.5\r0.5\r3\r0\rFC Barcelona\r\r2012/13\r36\rFC Sevilla\rReal Sociedad San Sebastian\r1.6\r1.0\r1\r1\rFC Sevilla\r\r2012/13\r36\rGetafe CF\rFC Valencia\r1.3\r1.3\r1\r1\rGetafe CF\r\r2012/13\r36\rGranada CF\rCA Osasuna\r1.4\r1.2\r1\r1\rGranada CF\r\r2012/13\r36\rLevante U.D.\rRayo Vallecano\r1.7\r1.0\r1\r1\rLevante U.D.\r\r2012/13\r36\rRCD Mallorca\rBetis Sevilla\r1.5\r1.2\r1\r1\rRCD Mallorca\r\r2012/13\r36\rReal Madrid CF\rMÃ¡laga CF\r2.9\r0.6\r2\r0\rReal Madrid CF\r\r2012/13\r36\rReal Zaragoza\rAthletic Club Bilbao\r1.4\r1.2\r1\r1\rReal Zaragoza\r\r2012/13\r37\rAthletic Club Bilbao\rLevante U.D.\r1.6\r1.1\r1\r1\rAthletic Club Bilbao\r\r2012/13\r37\rAtlÃ©tico Madrid\rRCD Mallorca\r2.0\r0.8\r1\r0\rAtlÃ©tico Madrid\r\r2012/13\r37\rBetis Sevilla\rReal Zaragoza\r1.8\r1.0\r1\r0\rBetis Sevilla\r\r2012/13\r37\rCA Osasuna\rFC Sevilla\r1.4\r1.3\r1\r1\rCA Osasuna\r\r2012/13\r37\rEspanyol Barcelona\rFC Barcelona\r0.8\r2.3\r0\r2\rFC Barcelona\r\r2012/13\r37\rFC Valencia\rGranada CF\r2.2\r0.8\r2\r0\rFC Valencia\r\r2012/13\r37\rGetafe CF\rRayo Vallecano\r1.7\r1.0\r1\r0\rGetafe CF\r\r2012/13\r37\rMÃ¡laga CF\rDeportivo de La CoruÃ±a\r1.8\r0.9\r1\r0\rMÃ¡laga CF\r\r2012/13\r37\rReal Sociedad San Sebastian\rReal Madrid CF\r0.9\r1.9\r0\r1\rReal Madrid CF\r\r2012/13\r37\rReal Valladolid\rCelta Vigo\r1.6\r1.1\r1\r1\rReal Valladolid\r\r2012/13\r38\rCelta Vigo\rEspanyol Barcelona\r1.5\r1.2\r1\r1\rCelta Vigo\r\r2012/13\r38\rDeportivo de La CoruÃ±a\rReal Sociedad San Sebastian\r1.3\r1.3\r1\r1\rDeportivo de La CoruÃ±a\r\r2012/13\r38\rFC Barcelona\rMÃ¡laga CF\r3.1\r0.6\r3\r0\rFC Barcelona\r\r2012/13\r38\rFC Sevilla\rFC Valencia\r1.5\r1.2\r1\r1\rFC Sevilla\r\r2012/13\r38\rGranada CF\rGetafe CF\r1.4\r1.2\r1\r1\rGranada CF\r\r2012/13\r38\rLevante U.D.\rBetis Sevilla\r1.5\r1.1\r1\r1\rLevante U.D.\r\r2012/13\r38\rRCD Mallorca\rReal Valladolid\r1.6\r1.1\r1\r1\rRCD Mallorca\r\r2012/13\r38\rRayo Vallecano\rAthletic Club Bilbao\r1.5\r1.2\r1\r1\rRayo Vallecano\r\r2012/13\r38\rReal Madrid CF\rCA Osasuna\r3.1\r0.6\r2\r0\rReal Madrid CF\r\r2012/13\r38\rReal Zaragoza\rAtlÃ©tico Madrid\r1.1\r1.5\r1\r1\rAtlÃ©tico Madrid\r\r\r\rWhile these predictions are good if you want to bet on the likely winner they do not reflect how the actual endgame will play out, e.g., there is not a single draw in the predicted_winner column. So at last let’s look at a possible version of the La Liga endgame by displaying the simulated match results calculated earlier.\n\u0026gt; laliga_sim \u0026lt;- laliga[is.na(laliga$HomeGoals), c(\u0026quot;Season\u0026quot;, \u0026quot;Week\u0026quot;, \u0026quot;HomeTeam\u0026quot;,\r+ \u0026quot;AwayTeam\u0026quot;)]\r\u0026gt; laliga_sim$home_goals \u0026lt;- m3_forecast[, \u0026quot;rand_home_goal\u0026quot;]\r\u0026gt; laliga_sim$away_goals \u0026lt;- m3_forecast[, \u0026quot;rand_away_goal\u0026quot;]\r\u0026gt; laliga_sim$winner \u0026lt;- ifelse(m3_forecast[, \u0026quot;rand_match_result\u0026quot;] == 1, laliga_forecast$HomeTeam,\r+ ifelse(m3_forecast[, \u0026quot;rand_match_result\u0026quot;] == -1, laliga_forecast$AwayTeam,\r+ \u0026quot;Draw\u0026quot;))\r\u0026gt; \u0026gt; rownames(laliga_sim) \u0026lt;- NULL\r\u0026gt; knitr::kable(laliga_sim, \u0026quot;pandoc\u0026quot;, align = \u0026quot;c\u0026quot;)\r\r\rSeason\rWeek\rHomeTeam\rAwayTeam\rhome_goals\raway_goals\rwinner\r\r\r\r2012/13\r34\rCelta Vigo\rAthletic Club Bilbao\r0\r0\rDraw\r\r2012/13\r34\rDeportivo de La CoruÃ±a\rAtlÃ©tico Madrid\r1\r5\rAtlÃ©tico Madrid\r\r2012/13\r34\rFC Barcelona\rBetis Sevilla\r5\r0\rFC Barcelona\r\r2012/13\r34\rFC Sevilla\rEspanyol Barcelona\r1\r0\rFC Sevilla\r\r2012/13\r34\rFC Valencia\rCA Osasuna\r1\r2\rCA Osasuna\r\r2012/13\r34\rGetafe CF\rReal Sociedad San Sebastian\r1\r0\rGetafe CF\r\r2012/13\r34\rGranada CF\rMÃ¡laga CF\r0\r2\rMÃ¡laga CF\r\r2012/13\r34\rRCD Mallorca\rLevante U.D.\r0\r1\rLevante U.D.\r\r2012/13\r34\rReal Madrid CF\rReal Valladolid\r2\r1\rReal Madrid CF\r\r2012/13\r34\rReal Zaragoza\rRayo Vallecano\r0\r2\rRayo Vallecano\r\r2012/13\r35\rAthletic Club Bilbao\rRCD Mallorca\r2\r0\rAthletic Club Bilbao\r\r2012/13\r35\rAtlÃ©tico Madrid\rFC Barcelona\r1\r3\rFC Barcelona\r\r2012/13\r35\rBetis Sevilla\rCelta Vigo\r2\r1\rBetis Sevilla\r\r2012/13\r35\rCA Osasuna\rGetafe CF\r2\r1\rCA Osasuna\r\r2012/13\r35\rEspanyol Barcelona\rReal Madrid CF\r2\r3\rReal Madrid CF\r\r2012/13\r35\rLevante U.D.\rReal Zaragoza\r2\r0\rLevante U.D.\r\r2012/13\r35\rMÃ¡laga CF\rFC Sevilla\r0\r1\rFC Sevilla\r\r2012/13\r35\rRayo Vallecano\rFC Valencia\r2\r4\rFC Valencia\r\r2012/13\r35\rReal Sociedad San Sebastian\rGranada CF\r0\r3\rGranada CF\r\r2012/13\r35\rReal Valladolid\rDeportivo de La CoruÃ±a\r3\r2\rReal Valladolid\r\r2012/13\r36\rCelta Vigo\rAtlÃ©tico Madrid\r1\r1\rDraw\r\r2012/13\r36\rDeportivo de La CoruÃ±a\rEspanyol Barcelona\r1\r0\rDeportivo de La CoruÃ±a\r\r2012/13\r36\rFC Barcelona\rReal Valladolid\r4\r1\rFC Barcelona\r\r2012/13\r36\rFC Sevilla\rReal Sociedad San Sebastian\r2\r0\rFC Sevilla\r\r2012/13\r36\rGetafe CF\rFC Valencia\r1\r0\rGetafe CF\r\r2012/13\r36\rGranada CF\rCA Osasuna\r3\r3\rDraw\r\r2012/13\r36\rLevante U.D.\rRayo Vallecano\r3\r0\rLevante U.D.\r\r2012/13\r36\rRCD Mallorca\rBetis Sevilla\r3\r0\rRCD Mallorca\r\r2012/13\r36\rReal Madrid CF\rMÃ¡laga CF\r1\r1\rDraw\r\r2012/13\r36\rReal Zaragoza\rAthletic Club Bilbao\r1\r0\rReal Zaragoza\r\r2012/13\r37\rAthletic Club Bilbao\rLevante U.D.\r2\r0\rAthletic Club Bilbao\r\r2012/13\r37\rAtlÃ©tico Madrid\rRCD Mallorca\r1\r1\rDraw\r\r2012/13\r37\rBetis Sevilla\rReal Zaragoza\r0\r6\rReal Zaragoza\r\r2012/13\r37\rCA Osasuna\rFC Sevilla\r2\r0\rCA Osasuna\r\r2012/13\r37\rEspanyol Barcelona\rFC Barcelona\r1\r3\rFC Barcelona\r\r2012/13\r37\rFC Valencia\rGranada CF\r1\r0\rFC Valencia\r\r2012/13\r37\rGetafe CF\rRayo Vallecano\r0\r1\rRayo Vallecano\r\r2012/13\r37\rMÃ¡laga CF\rDeportivo de La CoruÃ±a\r0\r0\rDraw\r\r2012/13\r37\rReal Sociedad San Sebastian\rReal Madrid CF\r1\r4\rReal Madrid CF\r\r2012/13\r37\rReal Valladolid\rCelta Vigo\r2\r0\rReal Valladolid\r\r2012/13\r38\rCelta Vigo\rEspanyol Barcelona\r1\r1\rDraw\r\r2012/13\r38\rDeportivo de La CoruÃ±a\rReal Sociedad San Sebastian\r1\r3\rReal Sociedad San Sebastian\r\r2012/13\r38\rFC Barcelona\rMÃ¡laga CF\r2\r0\rFC Barcelona\r\r2012/13\r38\rFC Sevilla\rFC Valencia\r3\r1\rFC Sevilla\r\r2012/13\r38\rGranada CF\rGetafe CF\r0\r1\rGetafe CF\r\r2012/13\r38\rLevante U.D.\rBetis Sevilla\r1\r2\rBetis Sevilla\r\r2012/13\r38\rRCD Mallorca\rReal Valladolid\r1\r0\rRCD Mallorca\r\r2012/13\r38\rRayo Vallecano\rAthletic Club Bilbao\r2\r0\rRayo Vallecano\r\r2012/13\r38\rReal Madrid CF\rCA Osasuna\r3\r0\rReal Madrid CF\r\r2012/13\r38\rReal Zaragoza\rAtlÃ©tico Madrid\r0\r1\rAtlÃ©tico Madrid\r\r\r\rNow we see a number of games resulting in a draw. We also see that Malaga manages to beat Real Madrid in week \\(36\\), against all odds, even though playing as the away team.\n\rCalculating the Predicted Payout for Sevilla vs Valencia, 2013-06-01\rAt the time when this model was developed (2013-05-28) most of the matches in the 2012/2013 season had been played and Barcelona was already the winner (and the most skilled team as predicted by my model). There were however some matches left, for example, Sevilla (home team) vs Valencia (away team) at the \\(1\\)st of June, 2013. One of the powers with using Bayesian modeling and MCMC sampling is that once you have the MCMC samples of the parameters it is straight forward to calculate any quantity resulting from these estimates while still retaining the uncertainty of the parameter estimates. So let’s look at the predicted distribution of the number of goals for the Sevilla vs Valencia game and see if I can use my model to make some money. I’ll start by using the MCMC samples to calculate the distribution of the number of goals for Sevilla and Valencia.\n\u0026gt; n \u0026lt;- nrow(ms3)\r\u0026gt; home_team \u0026lt;- which(teams == \u0026quot;FC Sevilla\u0026quot;)\r\u0026gt; away_team \u0026lt;- which(teams == \u0026quot;FC Valencia\u0026quot;)\r\u0026gt; season \u0026lt;- which(seasons == \u0026quot;2012/13\u0026quot;)\r\u0026gt; home_skill \u0026lt;- ms3[, col_name(\u0026quot;skill\u0026quot;, season, home_team)]\r\u0026gt; away_skill \u0026lt;- ms3[, col_name(\u0026quot;skill\u0026quot;, season, away_team)]\r\u0026gt; home_baseline \u0026lt;- ms3[, col_name(\u0026quot;home_baseline\u0026quot;, season)]\r\u0026gt; away_baseline \u0026lt;- ms3[, col_name(\u0026quot;away_baseline\u0026quot;, season)]\r\u0026gt; \u0026gt; home_goals \u0026lt;- rpois(n, exp(home_baseline + home_skill - away_skill))\r\u0026gt; away_goals \u0026lt;- rpois(n, exp(away_baseline + away_skill - home_skill))\rLooking at summary of these two distributions shows that it will be a close game but with a slight advantage for the home team Sevilla.\n\u0026gt; par(mfrow = c(2, 2), mar = rep(2.2, 4))\r\u0026gt; plot_goals(home_goals, away_goals)\rWhen developing the model (2013-05-28) the author got the specific payouts (that is, how much would I get back if my bet was successful) for betting on the outcome of this game on a betting site. Using my simulated distribution of the number of goals I can calculate the predicted payouts of my model.\n\u0026gt; 1/c(Sevilla = mean(home_goals \u0026gt; away_goals), Draw = mean(home_goals == away_goals),\r+ Valencia = mean(home_goals \u0026lt; away_goals))\rSevilla Draw Valencia 2.281369 3.841229 3.318584 \rI should clearly bet on Sevilla as my model predicts a payout of \\(2.24\\) (that is, a likely win for Sevilla) while betsson.com gives me the much higher payout of \\(3.2\\). It is also possible to bet on the final goal outcome so let’s calculate what payouts my model predicts for different goal outcomes.\n\u0026gt; goals_payout \u0026lt;- laply(0:6, function(home_goal) {\r+ laply(0:6, function(away_goal) {\r+ 1/mean(home_goals == home_goal \u0026amp; away_goals == away_goal)\r+ })\r+ })\r\u0026gt; \u0026gt; colnames(goals_payout) \u0026lt;- paste(\u0026quot;Valencia\u0026quot;, 0:6, sep = \u0026quot; - \u0026quot;)\r\u0026gt; rownames(goals_payout) \u0026lt;- paste(\u0026quot;Sevilla\u0026quot;, 0:6, sep = \u0026quot; - \u0026quot;)\r\u0026gt; goals_payout \u0026lt;- round(goals_payout, 1)\r\u0026gt; knitr::kable(goals_payout, \u0026quot;pandoc\u0026quot;, align = \u0026quot;c\u0026quot;)\r\r\r\rValencia - 0\rValencia - 1\rValencia - 2\rValencia - 3\rValencia - 4\rValencia - 5\rValencia - 6\r\r\r\rSevilla - 0\r13.8\r11.9\r21.5\r47.3\r161.3\r714.3\r2500.0\r\rSevilla - 1\r9.5\r8.0\r13.4\r36.9\r122.0\r441.2\r1666.7\r\rSevilla - 2\r12.8\r11.5\r19.4\r56.4\r176.5\r625.0\r1875.0\r\rSevilla - 3\r26.5\r22.9\r39.8\r97.4\r500.0\r1363.6\rInf\r\rSevilla - 4\r81.5\r58.8\r101.4\r306.1\r1071.4\r3000.0\rInf\r\rSevilla - 5\r211.3\r223.9\r319.1\r1000.0\r2500.0\r15000.0\rInf\r\rSevilla - 6\r750.0\r483.9\r1500.0\r3000.0\r7500.0\rInf\rInf\r\r\r\rThe most likely result is 1 - 1 with a predicted payout of \\(8.4\\) and the beeting site agrees with this also offering their lowest payout for this bet, \\(5.3\\). Not good enough! Looking at the payouts at the beeting site I can see that Sevilla - Valencia: 2 - 0 gives me a payout of \\(16.0\\), that’s much better than my predicted payout of \\(13.1\\). I’ll go for that!\n\rConclusions\rI believe the model has a lot things going for it. It is conceptually quite simple and easy to understand, implement and extend. It captures the patterns in and distribution of the data well. It allows me to easily calculate the probability of any outcome, from a game with whichever teams from any La Liga season. Want to calculate the probability that RCD Mallorca (home team) vs Malaga CF (away team) in the Season 2009/2010 would result in a draw? Easy! What’s the probability of the total number of goals in Granada CF vs Athletic Club Bilbao being a prime number? No problemo! What if Real Madrid from 2008/2009 met Barcelona from 2012/2013 in 2010/2011 and both teams had the home advantage? Well, that’s possible. There are also a couple of things that could be improved (many which are not too hard to address). Currently there is assumed to be no dependency between the goal distributions of the home and away teams, but this might not be realistic. Maybe if one team have scored more goals the other team “looses steam” (a negative correlation between the teams’ scores) or instead maybe the other team tries harder (a positive correlation). Such dependencies could maybe be added to the model using copulas. * One of the advantages of Bayesian statistics is the possibility to used informative priors. As I have no knowledge of football I’ve been using vague priors but with the help of a more knowledgeable football fan the model could be given more informative priors. Also, the predictive performance of the model has not been as thoroughly examined and this could be remedied with a healthy dose of cross validation.\n\rReferences\rBååth, Rasmus. 2015. “Modeling Match Results in Soccer Using a Hierarchical Bayesian Poisson Model.” Technical Report LUCS minor 18, Lund University Cognitive Science, Lund, Sweden.\n\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1581819194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581819194,"objectID":"ec9cd11fe7f76583de68b2a72996124d","permalink":"/jags/glmm-jags/glmm-jags/","publishdate":"2020-02-15T21:13:14-05:00","relpermalink":"/jags/glmm-jags/glmm-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","generalised linear mixed models"],"title":"Generalised Linear Mixed Models - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","STAN","generalised linear mixed models"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nOverview\rIn some respects, Generalized Linear Mixed effects Models (GLMM) are a hierarchical extension of Generalized linear models (GLM) in a similar manner that Linear Mixed effects Models (LMM) are a hierarchical extension of Linear Models (LM). However, whilst the Gaussian (normal) distribution facilitates a relatively straight way of generating the marginal likelihood of the observed response by integrating likelihoods across all possible (and unobserved) levels of a random effect to yield parameter estimates, the same cannot be said for other distributions. Consequently various approximations have been developed to estimate the fixed and random parameters for GLMM’s:\nPenalized quasi-likelihood (PQL). This method approximates a quasi-likelihood by iterative fitting of (re)weighted linear mixed effects models based on the fit of GLM fit. Specifically, it estimates the fixed effects parameters by fitting a GLM that incorporates a correlation (variance-covariance) structure resulting from a LMM and then refits a LMM to re-estimate the variance-covariance structure by using the variance structure from the previous GLM. The cycle continues to iterate until either the fit improvement is below a threshold or a defined number of iterations has occurred. Whilst this is a relatively simple approach, that enables us to leverage methodologies for accommodating heterogeneity and spatial/temporal autocorrelation, it is known to perform poorly (estimates biased towards large variance) for Poisson distributions when the expected value is less than \\(5\\) and for binary data when the expected number of successes or failures are less than \\(5\\). Moreover, as it approximates quasi-likelihood rather than likelihood, likelihood based inference and information criterion methods (such as likelihood ratio tests and AIC) are not appropriate with this approach. Instead, Wald tests are required for inference.\n\rLaplace approximation. This approach utilises a second-order Taylor series expansion to approximate (a mathematical technique for approximating the properties of a function around a point by taking multiple derivatives of the function and summing them together) the likelihood function. If we assume that the likelihood function is approximately normal and thus a quadratic function on a log scale, we can use second-order Taylor series expansion to approximate this likelihood. Whilst this approach is considered to be more accurate than PQL, it is considerably slower and unable to accommodate alternative variance and correlation structures.\n\rGauss-Hermite quadrature (GHQ). This approach approximates the marginal likelihood by approximating the value of integrals at specific points (quadratures). This technique can be further adapted by allowing the number of quadratures and their weights to be optimized via a set of rules.\n\rMarkov-chain Monte-Carlo (MCMC). This takes a bruit force approach by recreating the likelihood by traversing the likelihood function with sequential sampling proportional to the likelihood. Although this approach is very robust (when the posteriors have converged), they are computationally very intense. Interestingly, some (including Andrew Gelman) argue that PQL, Laplace and GHQ do not yield estimates. Rather they are only approximations of estimates. By contrast, as MCMC methods are able to integrate over all levels by bruit force, the resulting parameters are indeed true estimates.\n\r\rWe will focus on the last approach which is the more general among the ones considered here and which is based on a Bayesian approach, which can be very flexible and accurate, yet very slow and complex.\n\rHierarchical Poisson regression\rThe model I will be developing is a Bayesian hierarchical Poisson regression model which I used for the modelling of match results in volleyball, also available as a published paper. The objective of the analysis is to model the match results from the season 2017-2018 of Serie A1, the premium Italian female volleyball league. In total there were \\(136\\) games (rows) in the dataset each with information regarding which was the home and away team, what these teams scored, whether the match ended with \\(4\\) or \\(5\\) sets, and additional in-game statistics such as number of attacks, digs, serves, and blocks for each team in each match. The point outcomes of the teams in the games are assumed to be distributed according to a Poisson distribution. This model is similar to other models used for the modelling of football results, where the points scored by the home and away teams are defined as a function of latent attacking and defensive skills of the teams estimated across the games.\nThe peculiar (and novel) aspect of the model is that it takes into account the features of volleyball to generate plausible game results and league points scored by the teams across the season. In particular, for modelling purposes it is important to accout for the following aspects. 1) according to the current scoring system, matches are played until a team wins a total of three sets, with each set typically going to \\(25\\) points. However, if the two teams won two sets each, the third set then goes only up to \\(15\\) points. 2) a team must win each set by \\(2\\) points. If the score is tied with even numbers, both teams have to continue playing the set until a \\(2\\)-point lead is obtained. Otherwise, points keep accumulating until one team wins with a margin of victory of \\(2\\) points, even if the score is greater than \\(25\\) or \\(15\\) points. 3) in professional volleyball leagues, the teams get points according to set numbers at the end of all matches in the league. More specifically, the team points are awarded as follows: if the match is won \\(3–0\\) or \\(3–1\\), \\(3\\) points are assigned to the winner and \\(0\\) points to the loser; if the match is won \\(3–2\\), \\(2\\) points are assigned to the winner and \\(1\\) point to the loser.\nThe model is a Bayesian hierarchical analysis of volleyball data which allows to jointly predict match results and team rankings in national leagues. I use data from the women’s volleyball Italian Serie A1 \\(2017–2018\\) season as a motivating example to implement and validate the model.\n\rLoading the data\rI start by loading libraries, reading in the data and preprocessing it for STAN. In the original verison of the analysis I used JAGS and a slightly more complicated model which I do not consider here, but that you may consult in my paper if interested (Gabrio (2020)).\n\u0026gt; library(plyr)\r\u0026gt; library(dplyr)\r\u0026gt; library(rstan)\r\u0026gt; library(bayesplot)\r\u0026gt; library(boot)\r\u0026gt; library(ggplot2)\r\u0026gt; library(scales)\r\u0026gt; library(RColorBrewer)\r\u0026gt; library(viridis)\r\u0026gt; set.seed(12345) # for reproducibility\r\u0026gt; \u0026gt; #load data\r\u0026gt; data\u0026lt;-read.csv(file = \u0026quot;Volley_prepro_v1.csv\u0026quot;,header = T)\r\u0026gt; data$home.team\u0026lt;-as.factor(data$home.team)\r\u0026gt; data$away.team\u0026lt;-as.factor(data$away.team)\r\u0026gt; data$bloeff1\u0026lt;-(data$bloper1-data$bloinv1)/(data$blo1+data$bloper1)\r\u0026gt; data$bloeff2\u0026lt;-(data$bloper2-data$bloinv2)/(data$blo2+data$bloper2)\r\u0026gt; data_stat1\u0026lt;-ddply(data, .(h), summarise, meanatteff1=mean(atteff1),meansereff1=mean(sereff1),\r+ meandefeff1=mean(defeff1),meanbloeff1=mean(bloeff1))\r\u0026gt; data_stat2\u0026lt;-ddply(data, .(a), summarise, meanatteff2=mean(atteff2),meansereff2=mean(sereff2),\r+ meandefeff2=mean(defeff2),meanbloeff2=mean(bloeff2))\r\u0026gt; #center cov grand mean\r\u0026gt; atteff1.cen\u0026lt;-data$atteff1-mean(data$atteff1)\r\u0026gt; sereff1.cen\u0026lt;-data$sereff1-mean(data$sereff1)\r\u0026gt; bloeff1.cen\u0026lt;-data$bloeff1-mean(data$bloeff1)\r\u0026gt; defeff1.cen\u0026lt;-data$defeff1-mean(data$defeff1)\r\u0026gt; \u0026gt; atteff2.cen\u0026lt;-data$atteff2-mean(data$atteff2)\r\u0026gt; sereff2.cen\u0026lt;-data$sereff2-mean(data$sereff2)\r\u0026gt; bloeff2.cen\u0026lt;-data$bloeff2-mean(data$bloeff2)\r\u0026gt; defeff2.cen\u0026lt;-data$defeff2-mean(data$defeff2)\rData check\rHow are the number of points for each team in a volleyball match distributed? Well, let’s start by assuming that both teams have many chances at making a point and that each team have the same probability of scoring each point chance. Given these assumptions the distribution of the number of points for each team should be well captured by a Poisson distribution. A quick and dirty comparison between the actual distribution of the number of scored goals and a Poisson distribution having the same mean number of scored goals support this notion.\n\u0026gt; par(mfcol = c(2, 1), mar = rep(2.2, 4))\r\u0026gt; hist(c(data$y2, data$y1), xlim = c(40, 120), breaks = 8,main = \u0026quot;Distribution of the number of points\\nscored by a team in a match.\u0026quot;)\r\u0026gt; mean_goals \u0026lt;- mean(c(data$y2, data$y1))\r\u0026gt; hist(rpois(9999, mean_goals), xlim = c(40, 120), breaks = 8 ,main = \u0026quot;Random draw from a Poisson distribution with\\nthe same mean as the distribution above.\u0026quot;)\r\rModel fitting\rAll teams aren’t equally good and it will be assumed that all teams have a latent skill variable and the skill of a team minus the skill of the opposing team defines the predicted outcome of a game. As the number of goals are assumed to be Poisson distributed it is natural that the skills of the teams are on the log scale of the mean of the distribution. The distribution of the number of goals for the home team \\(i\\) when facing the away team \\(j\\) is then\n\\[ \\text{Points} \\sim \\text{Pois}(\\lambda)\\]\nwhere \\(\\log(\\lambda)=\\mu + \\text{home} + \\text{skill}_i - \\text{skill}_j\\). \\(\\mu\\) is a constant, while home is the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The point outcome of a match between home team \\(i\\) and away team \\(j\\) is modeled as:\n\\[ \\text{HomePoins}_{ij} \\sim \\text{Pois}(\\lambda_{\\text{home},ij}),\\]\n\\[ \\text{AwayPoints}_{ij} \\sim \\text{Pois}(\\lambda_{\\text{away},ij}),\\]\nwhere\n\\[ \\log(\\lambda_{\\text{home},ij}) = \\mu + \\text{home} + \\text{skill}_i - \\text{skill}_j, \\]\n\\[ \\log(\\lambda_{\\text{away},ij}) = \\mu + \\text{skill}_j - \\text{skill}_i. \\]\nThe skill parameters for the home and away teams are specified as a function of a set of attack and defense skills, which in turn are a linear function of different in-game statistics including the number of attacks, digs, serves and blocks for each team:\n\\[ \\text{skill}_i = \\alpha_{0i} + \\alpha_{1i}\\text{attacks} + \\alpha_{2i}\\text{serves} + , \\]\n\\[ \\text{skill}_j = \\beta_{0j} + \\beta_{1j}\\text{digs} + \\beta_{2j}\\text{blocks}, \\]\nThe distribution of two indicator variables, related to whether or not the fifth set was played (\\(d^s\\)) and whether the home team was the winner (\\(d^g\\)) in each match, are aslo included in the model. These are modelled as:\n\\[ d^s \\sim \\text{Bernoulli}(\\pi^s) \\;\\;\\; \\text{and} \\;\\;\\; d^g \\sim \\text{Bernoulli}(\\pi^g), \\]\nwhere the corresponding probabilities are specified as\n\\[ \\text{logit}(\\pi^s) = \\gamma_{0} + \\gamma_1\\text{HomePoins}_{i} + \\gamma_2\\text{AwayPoins}_{j}, \\]\n\\[ \\text{logit}(\\pi^g) = \\delta_{0} + \\delta_1\\text{HomePoins}_{i} + \\delta_2\\text{AwayPoins}_{j} + \\delta_3d^g. \\]\nI set the prior distributions over \\(\\mu\\) and \\(\\text{home}\\) to:\n\\[ \\text{home} \\sim N(0, 10000) \\;\\;\\; \\text{and} \\;\\;\\; \\mu \\sim N(0, 10000),\\]\nweakly informative priors on \\(\\boldsymbol \\gamma\\), \\(\\boldsymbol \\delta\\), and set the priors on the skill of all \\(n\\) teams using a hierarchical approach to :\n\\[ \\text{skill}_{1,\\ldots,n} \\sim N(\\mu_{\\text{teams}}, \\sigma^2_{\\text{teams}}),\\]\nso that teams are assumed to have similar but not identical mean and variance parameters for thier skill parameters. Turning this into a STAN model requires some minor adjustments. I have to “anchor” the sum of team skills to a constant otherwise the mean skill can drift away freely (sum to zero constraint) and the model cannot be identified. Doing these adjustments results in the following model description:\n\u0026gt; rstanString\u0026lt;-\u0026quot;\r+ data{\r+ int\u0026lt;lower=1\u0026gt; nteams; // number of teams\r+ int\u0026lt;lower=1\u0026gt; ngames; // number of games\r+ int\u0026lt;lower=1, upper=nteams\u0026gt; home_team[ngames]; // home team ID (1, ..., 12)\r+ int\u0026lt;lower=1, upper=nteams\u0026gt; away_team[ngames]; // away team ID (1, ..., 12)\r+ vector [ngames] att_eff1; // in game statistics for number of attacks (home)\r+ vector [ngames] ser_eff1; // in game statistics for number of serves (home)\r+ vector [ngames] def_eff2; // in game statistics for number of digs (away)\r+ vector [ngames] blo_eff2; // in game statistics for number of blocks (away)\r+ vector [ngames] att_eff2; // in game statistics for number of attacks (away)\r+ vector [ngames] ser_eff2; // in game statistics for number of serves (away)\r+ vector [ngames] def_eff1; // in game statistics for number of digs (home)\r+ vector [ngames] blo_eff1; // in game statistics for number of blocks (home)\r+ int\u0026lt;lower=0\u0026gt; y1[ngames]; // number of points scored by home team\r+ int\u0026lt;lower=0\u0026gt; y2[ngames]; // number of points scored by away team\r+ int\u0026lt;lower=0, upper=1\u0026gt; ds[ngames]; // indicator for number of sets played (3/4 or 5)\r+ int\u0026lt;lower=0, upper=1\u0026gt; dg[ngames]; // indicator for winning of the match for home team\r+ }\r+ parameters{\r+ real home;\r+ real mu;\r+ real mu0_att;\r+ real mu0_def;\r+ real\u0026lt;lower=0\u0026gt; sigma0_att;\r+ real\u0026lt;lower=0\u0026gt; sigma0_def;\r+ real mu1_att;\r+ real mu1_def;\r+ real\u0026lt;lower=0\u0026gt; sigma1_att;\r+ real\u0026lt;lower=0\u0026gt; sigma1_def;\r+ real mu1_ser;\r+ real mu1_blo;\r+ real\u0026lt;lower=0\u0026gt; sigma1_ser;\r+ real\u0026lt;lower=0\u0026gt; sigma1_blo;\r+ vector [nteams] beta0_att_star;\r+ vector [nteams] beta1_att_star;\r+ vector [nteams] beta1_ser_star;\r+ vector [nteams] beta0_def_star;\r+ vector [nteams] beta1_def_star;\r+ vector [nteams] beta1_blo_star;\r+ real gamma[3];\r+ real delta[4];\r+ }\r+ transformed parameters{\r+ //Trick to code the sum-to-zero constraint\r+ vector [nteams] beta0_att;\r+ vector [nteams] beta1_att;\r+ vector [nteams] beta1_ser;\r+ vector [nteams] beta0_def;\r+ vector [nteams] beta1_def;\r+ vector [nteams] beta1_blo;\r+ vector\u0026lt;lower=0\u0026gt;[ngames] theta1;\r+ vector\u0026lt;lower=0\u0026gt;[ngames] theta2;\r+ beta0_att = beta0_att_star - mean(beta0_att_star);\r+ beta1_att = beta1_att_star - mean(beta1_att_star);\r+ beta1_ser = beta1_ser_star - mean(beta1_ser_star);\r+ beta0_def = beta0_def_star - mean(beta0_def_star);\r+ beta1_def = beta1_def_star - mean(beta1_def_star);\r+ beta1_blo = beta1_blo_star - mean(beta1_blo_star);\r+ for (g in 1:ngames) {\r+ theta1[g] = exp(home + mu + beta0_att[home_team[g]] + beta1_att[home_team[g]]*att_eff1[g] + beta1_ser[home_team[g]]*ser_eff1[g] + + beta0_def[away_team[g]] + beta1_def[away_team[g]]*def_eff2[g] + beta1_blo[away_team[g]]*blo_eff2[g]); + theta2[g] = exp(home + mu + beta0_att[away_team[g]] + beta1_att[away_team[g]]*att_eff1[g] + beta1_ser[away_team[g]]*ser_eff1[g] + + beta0_def[home_team[g]] + beta1_def[home_team[g]]*def_eff2[g] + beta1_blo[home_team[g]]*blo_eff2[g]); + }\r+ }\r+ model {\r+ //priors\r+ home ~ normal(0, 100);\r+ mu ~ normal(0, 100);\r+ mu0_att ~ normal(0, 100);\r+ mu0_def ~ normal(0, 100);\r+ sigma0_att ~ uniform(0, 100);\r+ sigma0_def ~ uniform(0, 100);\r+ mu1_att ~ normal(0, 100);\r+ mu1_def ~ normal(0, 100);\r+ sigma1_att ~ uniform(0, 100);\r+ sigma1_def ~ uniform(0, 100);\r+ mu1_ser ~ normal(0, 100);\r+ mu1_blo ~ normal(0, 100);\r+ sigma1_ser ~ uniform(0, 100);\r+ sigma1_blo ~ uniform(0, 100);\r+ gamma ~ normal(0, 100);\r+ delta ~ normal(0, 100);\r+ beta0_att_star ~ normal(mu0_att, sigma0_att);\r+ beta0_def_star ~ normal(mu0_def, sigma0_def);\r+ beta1_att_star ~ normal(mu1_att, sigma1_att);\r+ beta1_def_star ~ normal(mu1_def, sigma1_def);\r+ beta1_ser_star ~ normal(mu1_ser, sigma1_ser);\r+ beta1_blo_star ~ normal(mu1_blo, sigma1_blo);\r+ // likelihood\r+ for (g in 1:ngames) {\r+ y1[g] ~ poisson(theta1[g]);\r+ y2[g] ~ poisson(theta2[g]);\r+ ds[g] ~ bernoulli_logit(gamma[1] + gamma[2]*y1[g] + gamma[3]*y2[g]);\r+ dg[g] ~ bernoulli_logit(delta[1] + delta[2]*y1[g] + delta[3]*y2[g] + delta[4]*ds[g]);\r+ }\r+ }\r+ generated quantities{\r+ // loglikelihood + vector[ngames] loglik_y1;\r+ vector[ngames] loglik_y2;\r+ vector[ngames] loglik_ds;\r+ vector[ngames] loglik_dg;\r+ for (g in 1:ngames) {\r+ loglik_y1[g] = poisson_lpmf(y1[g]| theta1[g]);\r+ loglik_y2[g] = poisson_lpmf(y2[g]| theta2[g]);\r+ loglik_ds[g] = bernoulli_logit_lpmf(ds[g]| gamma[1] + gamma[2]*y1[g] + gamma[3]*y2[g]);\r+ loglik_dg[g] = bernoulli_logit_lpmf(dg[g]| delta[1] + delta[2]*y1[g] + delta[3]*y2[g] + delta[4]*ds[g]);\r+ }\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstanString, con = \u0026quot;Modelbasic.stan\u0026quot;)\rNext, we put the data into a list to be passed to STAN, define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; #prepare data\r\u0026gt; y1\u0026lt;-data$y1\r\u0026gt; y2\u0026lt;-data$y2\r\u0026gt; ngames\u0026lt;-max(data$Game)\r\u0026gt; nteams\u0026lt;-max(data$h)\r\u0026gt; home_team\u0026lt;-data$h\r\u0026gt; away_team\u0026lt;-data$a\r\u0026gt; \u0026gt; att_eff1\u0026lt;-data$atteff1\r\u0026gt; att_eff2\u0026lt;-data$atteff2\r\u0026gt; ser_eff1\u0026lt;-data$sereff1\r\u0026gt; ser_eff2\u0026lt;-data$sereff2\r\u0026gt; blo_eff1\u0026lt;-data$bloeff1\r\u0026gt; blo_eff2\u0026lt;-data$bloeff2\r\u0026gt; def_eff1\u0026lt;-data$defeff1\r\u0026gt; def_eff2\u0026lt;-data$defeff2\r\u0026gt; \u0026gt; att_eff1\u0026lt;-atteff1.cen\r\u0026gt; att_eff2\u0026lt;-atteff2.cen\r\u0026gt; ser_eff1\u0026lt;-sereff1.cen\r\u0026gt; ser_eff2\u0026lt;-sereff2.cen\r\u0026gt; blo_eff1\u0026lt;-bloeff1.cen\r\u0026gt; blo_eff2\u0026lt;-bloeff2.cen\r\u0026gt; def_eff1\u0026lt;-defeff1.cen\r\u0026gt; def_eff2\u0026lt;-defeff2.cen\r\u0026gt; \u0026gt; ds\u0026lt;-ifelse(data$settot==5,1,0)\r\u0026gt; dg\u0026lt;-ifelse(data$set1\u0026gt;data$set2,1,0)\r\u0026gt; \u0026gt; #pre-processing\r\u0026gt; datalist \u0026lt;- list(y1=y1,y2=y2,ngames=ngames,nteams=nteams,home_team=home_team,away_team=away_team,att_eff1=att_eff1,att_eff2=att_eff2,def_eff1=def_eff1,def_eff2=def_eff2,ser_eff1=ser_eff1,ser_eff2=ser_eff2,blo_eff1=blo_eff1,blo_eff2=blo_eff2,ds=ds, dg=dg)\r\u0026gt; params \u0026lt;- c(\u0026quot;mu\u0026quot;,\u0026quot;home\u0026quot;,\u0026quot;gamma\u0026quot;,\u0026quot;delta\u0026quot;,\u0026quot;beta0_att\u0026quot;,\u0026quot;beta0_def\u0026quot;,\r+ \u0026quot;beta1_att\u0026quot;,\u0026quot;beta1_def\u0026quot;,\u0026quot;beta1_ser\u0026quot;,\u0026quot;beta1_blo\u0026quot;,\u0026quot;theta1\u0026quot;,\r+ \u0026quot;theta2\u0026quot;,\u0026quot;loglik_y1\u0026quot;,\u0026quot;loglik_y2\u0026quot;,\u0026quot;loglik_ds\u0026quot;,\u0026quot;loglik_dg\u0026quot;)\r\u0026gt; burnInSteps = 500\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 2000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\rStart the STAN model (check the model, load data into the model, specify the number of chains and compile the model). Run the STAN code via the rstan package and the stan function.\n\u0026gt; model_stan\u0026lt;- stan(data = datalist, file = \u0026quot;Modelbasic.stan\u0026quot;, + chains = nChains, pars = params, iter = nIter, + warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;Modelbasic\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0.001 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup)\rChain 1: Iteration: 100 / 1000 [ 10%] (Warmup)\rChain 1: Iteration: 200 / 1000 [ 20%] (Warmup)\rChain 1: Iteration: 300 / 1000 [ 30%] (Warmup)\rChain 1: Iteration: 400 / 1000 [ 40%] (Warmup)\rChain 1: Iteration: 500 / 1000 [ 50%] (Warmup)\rChain 1: Iteration: 501 / 1000 [ 50%] (Sampling)\rChain 1: Iteration: 600 / 1000 [ 60%] (Sampling)\rChain 1: Iteration: 700 / 1000 [ 70%] (Sampling)\rChain 1: Iteration: 800 / 1000 [ 80%] (Sampling)\rChain 1: Iteration: 900 / 1000 [ 90%] (Sampling)\rChain 1: Iteration: 1000 / 1000 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 72.275 seconds (Warm-up)\rChain 1: 81.918 seconds (Sampling)\rChain 1: 154.193 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;Modelbasic\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup)\rChain 2: Iteration: 100 / 1000 [ 10%] (Warmup)\rChain 2: Iteration: 200 / 1000 [ 20%] (Warmup)\rChain 2: Iteration: 300 / 1000 [ 30%] (Warmup)\rChain 2: Iteration: 400 / 1000 [ 40%] (Warmup)\rChain 2: Iteration: 500 / 1000 [ 50%] (Warmup)\rChain 2: Iteration: 501 / 1000 [ 50%] (Sampling)\rChain 2: Iteration: 600 / 1000 [ 60%] (Sampling)\rChain 2: Iteration: 700 / 1000 [ 70%] (Sampling)\rChain 2: Iteration: 800 / 1000 [ 80%] (Sampling)\rChain 2: Iteration: 900 / 1000 [ 90%] (Sampling)\rChain 2: Iteration: 1000 / 1000 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 65.843 seconds (Warm-up)\rChain 2: 80.38 seconds (Sampling)\rChain 2: 146.223 seconds (Total)\rChain 2: \u0026gt; print(model_stan, pars =c(\u0026quot;mu\u0026quot;,\u0026quot;home\u0026quot;,\u0026quot;gamma\u0026quot;,\u0026quot;delta\u0026quot;,\u0026quot;beta0_att\u0026quot;,\r+ \u0026quot;beta0_def\u0026quot;,\u0026quot;beta1_att\u0026quot;,\u0026quot;beta1_def\u0026quot;,\r+ \u0026quot;beta1_ser\u0026quot;,\u0026quot;beta1_blo\u0026quot;))\rInference for Stan model: Modelbasic.\r2 chains, each with iter=1000; warmup=500; thin=1; post-warmup draws per chain=500, total post-warmup draws=1000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff\rmu 6.88 40.69 42.08 -49.79 -32.01 1.64 45.81 71.34 1\rhome -2.43 40.69 42.08 -66.90 -41.36 2.80 36.45 54.23 1\rgamma[1] -81.77 7.94 19.69 -124.06 -94.10 -77.67 -67.84 -47.99 6\rgamma[2] 0.42 0.06 0.14 0.20 0.32 0.40 0.49 0.75 7\rgamma[3] 0.42 0.03 0.09 0.24 0.36 0.42 0.49 0.61 11\rdelta[1] -9.40 1.07 3.79 -16.54 -12.13 -9.31 -6.82 -2.42 13\rdelta[2] 0.44 0.02 0.07 0.32 0.38 0.44 0.50 0.56 16\rdelta[3] -0.33 0.01 0.05 -0.43 -0.37 -0.32 -0.29 -0.21 20\rdelta[4] -2.33 0.28 1.02 -4.57 -2.99 -2.32 -1.46 -0.66 13\rbeta0_att[1] 0.00 0.00 0.02 -0.04 -0.02 0.00 0.01 0.04 465\rbeta0_att[2] 0.06 0.00 0.02 0.02 0.04 0.05 0.07 0.10 120\rbeta0_att[3] 0.02 0.00 0.02 -0.03 0.01 0.02 0.03 0.06 534\rbeta0_att[4] 0.03 0.00 0.02 -0.01 0.02 0.03 0.04 0.07 178\rbeta0_att[5] -0.05 0.00 0.02 -0.09 -0.06 -0.05 -0.03 -0.01 59\rbeta0_att[6] -0.09 0.00 0.02 -0.14 -0.11 -0.09 -0.08 -0.05 29\rbeta0_att[7] 0.04 0.00 0.02 0.00 0.03 0.04 0.06 0.08 314\rbeta0_att[8] 0.04 0.00 0.02 0.00 0.03 0.04 0.05 0.08 291\rbeta0_att[9] -0.03 0.00 0.02 -0.07 -0.04 -0.03 -0.01 0.02 524\rbeta0_att[10] 0.00 0.00 0.02 -0.03 -0.01 0.00 0.02 0.04 353\rbeta0_att[11] -0.01 0.00 0.02 -0.05 -0.02 -0.01 0.01 0.03 625\rbeta0_att[12] -0.01 0.00 0.02 -0.05 -0.02 -0.01 0.00 0.03 701\rbeta0_def[1] 0.07 0.00 0.02 0.02 0.05 0.07 0.08 0.11 172\rbeta0_def[2] 0.02 0.00 0.02 -0.03 0.00 0.02 0.04 0.06 135\rbeta0_def[3] 0.08 0.00 0.02 0.04 0.07 0.08 0.10 0.12 417\rbeta0_def[4] -0.10 0.00 0.03 -0.14 -0.11 -0.10 -0.08 -0.04 173\rbeta0_def[5] 0.02 0.00 0.02 -0.03 0.00 0.02 0.03 0.06 78\rbeta0_def[6] 0.03 0.00 0.02 -0.02 0.01 0.03 0.04 0.07 520\rbeta0_def[7] 0.02 0.00 0.02 -0.02 0.01 0.02 0.04 0.06 348\rbeta0_def[8] -0.05 0.00 0.02 -0.10 -0.06 -0.05 -0.03 0.00 267\rbeta0_def[9] -0.01 0.00 0.02 -0.06 -0.03 -0.01 0.01 0.04 208\rbeta0_def[10] 0.04 0.00 0.02 0.00 0.02 0.04 0.05 0.08 250\rbeta0_def[11] 0.02 0.00 0.02 -0.02 0.01 0.02 0.04 0.07 217\rbeta0_def[12] -0.14 0.00 0.02 -0.19 -0.16 -0.14 -0.13 -0.10 80\rbeta1_att[1] -0.69 0.05 0.31 -1.32 -0.89 -0.70 -0.52 -0.05 41\rbeta1_att[2] -0.76 0.08 0.40 -1.63 -0.99 -0.75 -0.51 0.10 28\rbeta1_att[3] 0.24 0.06 0.32 -0.40 0.04 0.24 0.45 0.88 30\rbeta1_att[4] 0.47 0.04 0.23 0.05 0.31 0.47 0.63 0.91 39\rbeta1_att[5] -0.69 0.06 0.34 -1.39 -0.91 -0.68 -0.46 -0.05 31\rbeta1_att[6] -0.21 0.04 0.30 -0.82 -0.40 -0.22 -0.02 0.39 57\rbeta1_att[7] -0.48 0.05 0.32 -1.11 -0.69 -0.49 -0.26 0.13 51\rbeta1_att[8] 0.69 0.05 0.35 -0.07 0.48 0.71 0.91 1.28 43\rbeta1_att[9] -0.32 0.05 0.33 -0.97 -0.56 -0.32 -0.09 0.28 42\rbeta1_att[10] 0.31 0.07 0.34 -0.29 0.07 0.29 0.56 0.98 27\rbeta1_att[11] 1.20 0.14 0.39 0.34 1.00 1.24 1.46 1.91 8\rbeta1_att[12] 0.26 0.06 0.31 -0.36 0.05 0.26 0.46 0.85 28\rbeta1_def[1] -0.09 0.02 0.12 -0.33 -0.19 -0.08 0.00 0.13 50\rbeta1_def[2] 0.19 0.02 0.13 -0.07 0.09 0.18 0.27 0.47 76\rbeta1_def[3] -0.02 0.02 0.14 -0.29 -0.11 -0.02 0.07 0.29 60\rbeta1_def[4] 0.21 0.02 0.16 -0.09 0.11 0.22 0.32 0.49 56\rbeta1_def[5] -0.25 0.03 0.16 -0.57 -0.36 -0.24 -0.13 0.05 34\rbeta1_def[6] 0.03 0.02 0.16 -0.27 -0.08 0.04 0.15 0.34 44\rbeta1_def[7] -0.12 0.01 0.13 -0.37 -0.21 -0.12 -0.04 0.13 78\rbeta1_def[8] -0.42 0.02 0.14 -0.70 -0.52 -0.42 -0.31 -0.16 50\rbeta1_def[9] -0.20 0.02 0.13 -0.48 -0.29 -0.19 -0.12 0.06 55\rbeta1_def[10] 0.16 0.01 0.12 -0.06 0.08 0.16 0.24 0.37 66\rbeta1_def[11] -0.08 0.02 0.14 -0.33 -0.19 -0.07 0.01 0.18 33\rbeta1_def[12] 0.59 0.06 0.18 0.23 0.46 0.60 0.72 0.93 8\rbeta1_ser[1] -0.05 0.08 0.47 -0.88 -0.38 -0.08 0.23 0.94 38\rbeta1_ser[2] -1.56 0.11 0.54 -2.63 -1.93 -1.58 -1.21 -0.48 22\rbeta1_ser[3] 0.61 0.11 0.54 -0.65 0.29 0.61 0.97 1.66 23\rbeta1_ser[4] 0.27 0.06 0.35 -0.38 0.03 0.25 0.54 0.90 38\rbeta1_ser[5] 0.70 0.11 0.52 -0.41 0.35 0.65 1.06 1.69 22\rbeta1_ser[6] -0.46 0.12 0.63 -1.74 -0.88 -0.42 -0.02 0.72 30\rbeta1_ser[7] -1.13 0.08 0.43 -1.97 -1.41 -1.11 -0.81 -0.38 30\rbeta1_ser[8] 2.01 0.10 0.47 1.10 1.66 2.02 2.34 2.89 22\rbeta1_ser[9] -0.65 0.13 0.55 -1.80 -1.00 -0.59 -0.27 0.33 18\rbeta1_ser[10] -0.59 0.07 0.37 -1.31 -0.86 -0.56 -0.33 0.05 32\rbeta1_ser[11] 1.81 0.15 0.54 0.84 1.38 1.81 2.21 2.88 14\rbeta1_ser[12] -0.95 0.07 0.42 -1.70 -1.26 -0.97 -0.61 -0.16 33\rbeta1_blo[1] -0.05 0.01 0.08 -0.23 -0.10 -0.04 0.00 0.09 46\rbeta1_blo[2] 0.11 0.04 0.10 -0.05 0.04 0.10 0.16 0.35 6\rbeta1_blo[3] -0.07 0.01 0.07 -0.21 -0.12 -0.07 -0.03 0.05 24\rbeta1_blo[4] -0.03 0.01 0.05 -0.14 -0.06 -0.03 0.00 0.06 66\rbeta1_blo[5] 0.02 0.01 0.07 -0.09 -0.02 0.02 0.07 0.17 85\rbeta1_blo[6] 0.02 0.01 0.06 -0.09 -0.02 0.02 0.06 0.12 79\rbeta1_blo[7] 0.08 0.01 0.06 -0.01 0.04 0.08 0.12 0.21 58\rbeta1_blo[8] 0.07 0.01 0.08 -0.06 0.02 0.07 0.12 0.24 44\rbeta1_blo[9] -0.01 0.01 0.07 -0.14 -0.06 -0.01 0.03 0.12 45\rbeta1_blo[10] -0.05 0.01 0.05 -0.15 -0.08 -0.05 -0.02 0.03 83\rbeta1_blo[11] -0.05 0.01 0.06 -0.18 -0.10 -0.05 -0.01 0.08 81\rbeta1_blo[12] -0.03 0.01 0.06 -0.17 -0.07 -0.03 0.01 0.10 59\rRhat\rmu 4.07\rhome 4.07\rgamma[1] 1.55\rgamma[2] 1.57\rgamma[3] 1.14\rdelta[1] 1.11\rdelta[2] 1.20\rdelta[3] 1.09\rdelta[4] 1.20\rbeta0_att[1] 1.01\rbeta0_att[2] 1.04\rbeta0_att[3] 1.01\rbeta0_att[4] 1.02\rbeta0_att[5] 1.05\rbeta0_att[6] 1.11\rbeta0_att[7] 1.00\rbeta0_att[8] 1.01\rbeta0_att[9] 1.00\rbeta0_att[10] 1.01\rbeta0_att[11] 1.00\rbeta0_att[12] 1.00\rbeta0_def[1] 1.01\rbeta0_def[2] 1.01\rbeta0_def[3] 1.01\rbeta0_def[4] 1.00\rbeta0_def[5] 1.07\rbeta0_def[6] 1.00\rbeta0_def[7] 1.01\rbeta0_def[8] 1.01\rbeta0_def[9] 1.03\rbeta0_def[10] 1.00\rbeta0_def[11] 1.01\rbeta0_def[12] 1.03\rbeta1_att[1] 1.06\rbeta1_att[2] 1.01\rbeta1_att[3] 1.06\rbeta1_att[4] 1.05\rbeta1_att[5] 1.10\rbeta1_att[6] 1.02\rbeta1_att[7] 1.02\rbeta1_att[8] 1.07\rbeta1_att[9] 1.04\rbeta1_att[10] 1.09\rbeta1_att[11] 1.22\rbeta1_att[12] 1.09\rbeta1_def[1] 1.01\rbeta1_def[2] 1.03\rbeta1_def[3] 1.05\rbeta1_def[4] 1.05\rbeta1_def[5] 1.10\rbeta1_def[6] 1.01\rbeta1_def[7] 1.03\rbeta1_def[8] 1.12\rbeta1_def[9] 1.03\rbeta1_def[10] 1.05\rbeta1_def[11] 1.05\rbeta1_def[12] 1.22\rbeta1_ser[1] 1.04\rbeta1_ser[2] 1.06\rbeta1_ser[3] 1.07\rbeta1_ser[4] 1.06\rbeta1_ser[5] 1.09\rbeta1_ser[6] 1.00\rbeta1_ser[7] 1.07\rbeta1_ser[8] 1.24\rbeta1_ser[9] 1.13\rbeta1_ser[10] 1.16\rbeta1_ser[11] 1.17\rbeta1_ser[12] 1.05\rbeta1_blo[1] 1.02\rbeta1_blo[2] 1.22\rbeta1_blo[3] 1.10\rbeta1_blo[4] 1.07\rbeta1_blo[5] 1.02\rbeta1_blo[6] 1.02\rbeta1_blo[7] 1.06\rbeta1_blo[8] 1.06\rbeta1_blo[9] 1.09\rbeta1_blo[10] 1.05\rbeta1_blo[11] 1.02\rbeta1_blo[12] 1.04\rSamples were drawn using NUTS(diag_e) at Wed Mar 25 14:32:21 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\rThe diagnostic results are not perfect, perhaps I should run the algorithm a bit longer to get better results as a total of \\(1000\\) iterations does not seem enough. Here I do not bother to save time. To get an idea about posterior results I can plot at the average marginal mean offensive and defensive skills for each of the \\(12\\) teams in the league using the following code.\n\u0026gt; #plot att vs def effext by team\r\u0026gt; model_stan_par\u0026lt;-extract(model_stan)\r\u0026gt; \u0026gt; beta0.att\u0026lt;-apply(model_stan_par$beta0_att, 2, mean)\r\u0026gt; beta0.def\u0026lt;-apply(model_stan_par$beta0_def, 2, mean)\r\u0026gt; names\u0026lt;-unique(data.frame(data$home.team,data$h)[order(data$h),][,1])\r\u0026gt; \u0026gt; plot(beta0.att,beta0.def, main = \u0026quot;\u0026quot;, type = \u0026quot;n\u0026quot;, xlab = \u0026quot;Mean attack effect\u0026quot;, ylab = \u0026quot;Mean defence effect\u0026quot;, xlim=c(-0.13,0.08), ylim=c(-0.18,0.1))\r\u0026gt; points(beta0.att,beta0.def, pch=16, col=\u0026quot;red\u0026quot;,cex = 1.2)\r\u0026gt; abline(v=0)\r\u0026gt; abline(h=0)\r\u0026gt; text(beta0.att,beta0.def,names,cex = 0.8, adj = c(0.4,1.3))\rDifferent clusters of teams can be easily detected and suggest a different performance of the teams based on their average offensive and defensive skills. Those associated with higher offensive (to the right) and lower defensive (to the bottom) effects are the ones with the best performance across the season.\n\rMCMC diagnostics\rUsing the generated MCMC samples I can now look at some diagnostic measures. For example, we can assess convergence of the chains using the function mcmc_combo in the package bayesplot which provides a summary of convergence diagnostics using different graphics. We consider the marginal skill parameters for four teams to give an example.\n\u0026gt; mcmc_combo(model_stan, pars=c(\u0026quot;beta0_att[1]\u0026quot;,\u0026quot;beta0_att[2]\u0026quot;,\r+ \u0026quot;beta0_att[3]\u0026quot;,\u0026quot;beta0_att[4]\u0026quot;))\r\u0026gt; mcmc_combo(model_stan, pars=c(\u0026quot;beta0_def[1]\u0026quot;,\u0026quot;beta0_def[2]\u0026quot;,\r+ \u0026quot;beta0_def[3]\u0026quot;,\u0026quot;beta0_def[4]\u0026quot;))\r\rModel validation\rWe can finally assess the fit of the model to the data by computing posterior predictive checks, where we use the posterior values of the parameters to sample a large number of replications for the data. We then use these to generate different types of results and compare them with the actual results to detect possible misfits of the model.\n\u0026gt; #obtain parameters to generate replications\r\u0026gt; y1.pred\u0026lt;-y2.pred\u0026lt;-matrix(NA,length(model_stan_par$home),132)\r\u0026gt; ds.pred\u0026lt;-dg.pred\u0026lt;-matrix(NA,length(model_stan_par$home),132)\r\u0026gt; pi.s\u0026lt;-pi.g\u0026lt;-matrix(NA,length(model_stan_par$home),132)\r\u0026gt; y1.mat\u0026lt;-y2.mat\u0026lt;-ds.mat\u0026lt;-matrix(NA,length(model_stan_par$home),132)\r\u0026gt; for(i in 1:length(model_stan_par$home)){\r+ y1.mat[i,]\u0026lt;-y1\r+ y2.mat[i,]\u0026lt;-y2\r+ ds.mat[i,]\u0026lt;-ds\r+ pi.s[i,]\u0026lt;-inv.logit(model_stan_par$gamma[i,1] + model_stan_par$gamma[i,2]*y1.mat[i,] + model_stan_par$gamma[i,3]*y2.mat[i,])\r+ pi.g[i,]\u0026lt;-inv.logit(model_stan_par$delta[i,1] + model_stan_par$delta[i,2]*y1.mat[i,] + model_stan_par$delta[i,3]*y2.mat[i,] + model_stan_par$delta[i,4]*ds.mat[i,])\r+ }\r\u0026gt; \u0026gt; #generate predictions\r\u0026gt; set.seed(3456)\r\u0026gt; for(i in 1:length(model_stan_par$home)){\r+ y1.pred[i,]\u0026lt;-rpois(n=132,lambda = model_stan_par$theta1[i,])\r+ y2.pred[i,]\u0026lt;-rpois(n=132,lambda = model_stan_par$theta2[i,])\r+ ds.pred[i,]\u0026lt;-rbinom(n=132, size = 1,prob = pi.s[i,])\r+ dg.pred[i,]\u0026lt;-rbinom(n=132, size = 1,prob = pi.g[i,])\r+ }\r\u0026gt; \u0026gt; #compute prediction points\r\u0026gt; results\u0026lt;-list()\r\u0026gt; for(i in 1:1000){\r+ results[[i]]\u0026lt;-data.frame(y1.pred[i,],y2.pred[i,],ds.pred[i,],dg.pred[i,],data$h,data$a) + results[[i]]$points.home\u0026lt;-ifelse(results[[i]]$ds.pred.i...==0 \u0026amp; results[[i]]$dg.pred.i...==1,3,0)\r+ results[[i]]$points.home\u0026lt;-ifelse(results[[i]]$ds.pred.i...==1 \u0026amp; results[[i]]$dg.pred.i...==1,2,results[[i]]$points.home)\r+ results[[i]]$points.home\u0026lt;-ifelse(results[[i]]$ds.pred.i...==1 \u0026amp; results[[i]]$dg.pred.i...==0,1,results[[i]]$points.home)\r+ + results[[i]]$points.away\u0026lt;-ifelse(results[[i]]$ds.pred.i...==0 \u0026amp; results[[i]]$dg.pred.i...==0,3,0)\r+ results[[i]]$points.away\u0026lt;-ifelse(results[[i]]$ds.pred.i...==1 \u0026amp; results[[i]]$dg.pred.i...==0,2,results[[i]]$points.away)\r+ results[[i]]$points.away\u0026lt;-ifelse(results[[i]]$ds.pred.i...==1 \u0026amp; results[[i]]$dg.pred.i...==1,1,results[[i]]$points.away)\r+ }\r\u0026gt; \u0026gt; #compare results for scores by team\r\u0026gt; tot.y1.list\u0026lt;-tot.y2.list\u0026lt;-list()\r\u0026gt; for(i in 1:1000){\r+ tot.y1.list[[i]]\u0026lt;-ddply(results[[i]], .(data.h), summarise, totscorehome=sum(y1.pred.i...))\r+ tot.y2.list[[i]]\u0026lt;-ddply(results[[i]], .(data.a), summarise, totscoreaway=sum(y2.pred.i...))\r+ }\r\u0026gt; tot.y1.list.neg\u0026lt;-tot.y2.list.neg\u0026lt;-list()\r\u0026gt; for(i in 1:1000){\r+ tot.y1.list.neg[[i]]\u0026lt;-ddply(results[[i]], .(data.h), summarise, totscorehomeneg=sum(y2.pred.i...))\r+ tot.y2.list.neg[[i]]\u0026lt;-ddply(results[[i]], .(data.a), summarise, totscoreawayneg=sum(y1.pred.i...))\r+ }\r\u0026gt; \u0026gt; \u0026gt; tot.y1.list.mat\u0026lt;-tot.y2.list.mat\u0026lt;-matrix(NA,1000,12)\r\u0026gt; tot.y1.list.neg.mat\u0026lt;-tot.y2.list.neg.mat\u0026lt;-matrix(NA,1000,12)\r\u0026gt; for(i in 1:1000){\r+ tot.y1.list.mat[i,]\u0026lt;-tot.y1.list[[i]][,2]\r+ tot.y2.list.mat[i,]\u0026lt;-tot.y2.list[[i]][,2]\r+ tot.y1.list.neg.mat[i,]\u0026lt;-tot.y1.list.neg[[i]][,2]\r+ tot.y2.list.neg.mat[i,]\u0026lt;-tot.y2.list.neg[[i]][,2]\r+ }\r\u0026gt; tot.y1.obs\u0026lt;-ddply(data, .(h), summarise, totscorehomeobs=sum(y1))\r\u0026gt; tot.y2.obs\u0026lt;-ddply(data, .(a), summarise, totscorehomeobs=sum(y2))\r\u0026gt; tot.y1.obs.neg\u0026lt;-ddply(data, .(h), summarise, totscorehomeobs=sum(y2))\r\u0026gt; tot.y2.obs.neg\u0026lt;-ddply(data, .(a), summarise, totscorehomeobs=sum(y1))\r\u0026gt; \u0026gt; #compute prediction points\r\u0026gt; results\u0026lt;-list()\r\u0026gt; for(i in 1:1000){\r+ results[[i]]\u0026lt;-data.frame(y1.pred[i,],y2.pred[i,],ds.pred[i,],dg.pred[i,],data$h,data$a) + results[[i]]$points.home\u0026lt;-ifelse(results[[i]]$ds.pred.i...==0 \u0026amp; results[[i]]$dg.pred.i...==1,3,0)\r+ results[[i]]$points.home\u0026lt;-ifelse(results[[i]]$ds.pred.i...==1 \u0026amp; results[[i]]$dg.pred.i...==1,2,results[[i]]$points.home)\r+ results[[i]]$points.home\u0026lt;-ifelse(results[[i]]$ds.pred.i...==1 \u0026amp; results[[i]]$dg.pred.i...==0,1,results[[i]]$points.home)\r+ + results[[i]]$points.away\u0026lt;-ifelse(results[[i]]$ds.pred.i...==0 \u0026amp; results[[i]]$dg.pred.i...==0,3,0)\r+ results[[i]]$points.away\u0026lt;-ifelse(results[[i]]$ds.pred.i...==1 \u0026amp; results[[i]]$dg.pred.i...==0,2,results[[i]]$points.away)\r+ results[[i]]$points.away\u0026lt;-ifelse(results[[i]]$ds.pred.i...==1 \u0026amp; results[[i]]$dg.pred.i...==1,1,results[[i]]$points.away)\r+ }\r\u0026gt; \u0026gt; #compare results for scores by team\r\u0026gt; tot.y1.list\u0026lt;-tot.y2.list\u0026lt;-list()\r\u0026gt; for(i in 1:1000){\r+ tot.y1.list[[i]]\u0026lt;-ddply(results[[i]], .(data.h), summarise, totscorehome=sum(y1.pred.i...))\r+ tot.y2.list[[i]]\u0026lt;-ddply(results[[i]], .(data.a), summarise, totscoreaway=sum(y2.pred.i...))\r+ }\r\u0026gt; tot.y1.list.neg\u0026lt;-tot.y2.list.neg\u0026lt;-list()\r\u0026gt; for(i in 1:1000){\r+ tot.y1.list.neg[[i]]\u0026lt;-ddply(results[[i]], .(data.h), summarise, totscorehomeneg=sum(y2.pred.i...))\r+ tot.y2.list.neg[[i]]\u0026lt;-ddply(results[[i]], .(data.a), summarise, totscoreawayneg=sum(y1.pred.i...))\r+ }\r\u0026gt; \u0026gt; \u0026gt; tot.y1.list.mat\u0026lt;-tot.y2.list.mat\u0026lt;-matrix(NA,1000,12)\r\u0026gt; tot.y1.list.neg.mat\u0026lt;-tot.y2.list.neg.mat\u0026lt;-matrix(NA,1000,12)\r\u0026gt; for(i in 1:1000){\r+ tot.y1.list.mat[i,]\u0026lt;-tot.y1.list[[i]][,2]\r+ tot.y2.list.mat[i,]\u0026lt;-tot.y2.list[[i]][,2]\r+ tot.y1.list.neg.mat[i,]\u0026lt;-tot.y1.list.neg[[i]][,2]\r+ tot.y2.list.neg.mat[i,]\u0026lt;-tot.y2.list.neg[[i]][,2]\r+ }\r\u0026gt; tot.y1.obs\u0026lt;-ddply(data, .(h), summarise, totscorehomeobs=sum(y1))\r\u0026gt; tot.y2.obs\u0026lt;-ddply(data, .(a), summarise, totscorehomeobs=sum(y2))\r\u0026gt; tot.y1.obs.neg\u0026lt;-ddply(data, .(h), summarise, totscorehomeobs=sum(y2))\r\u0026gt; tot.y2.obs.neg\u0026lt;-ddply(data, .(a), summarise, totscorehomeobs=sum(y1))\r\u0026gt; \u0026gt; #scored\r\u0026gt; tot.y.obs\u0026lt;-tot.y1.obs[,2]+tot.y2.obs[,2]\r\u0026gt; tot.y.pred\u0026lt;-apply(tot.y1.list.mat,2,median)+apply(tot.y2.list.mat,2,median)\r\u0026gt; res.y\u0026lt;-cbind(tot.y.obs,tot.y.pred)\r\u0026gt; rownames(res.y)\u0026lt;-names\r\u0026gt; res.y\u0026lt;-round(res.y,digits = 0)\r\u0026gt; \u0026gt; #conceded\r\u0026gt; tot.y.obs.neg\u0026lt;-tot.y1.obs.neg[,2]+tot.y2.obs.neg[,2]\r\u0026gt; tot.y.pred.neg\u0026lt;-apply(tot.y1.list.neg.mat,2,median)+apply(tot.y2.list.neg.mat,2,median)\r\u0026gt; res.y.neg\u0026lt;-cbind(tot.y.obs.neg,tot.y.pred.neg)\r\u0026gt; rownames(res.y.neg)\u0026lt;-names\r\u0026gt; res.y.neg\u0026lt;-round(res.y.neg,digits = 0)\r\u0026gt; \u0026gt; #compare results for points\r\u0026gt; data.points.list\u0026lt;-list()\r\u0026gt; for(i in 1:1000){\r+ data.points.list[[i]]\u0026lt;-data.frame(data$Game)\r+ data.points.list[[i]]$Game\u0026lt;-data$Game\r+ data.points.list[[i]]$h\u0026lt;-data$h\r+ data.points.list[[i]]$a\u0026lt;-data$a\r+ data.points.list[[i]]$points.home\u0026lt;-results[[i]]$points.home\r+ data.points.list[[i]]$points.away\u0026lt;-results[[i]]$points.away\r+ }\r\u0026gt; tot.home.list\u0026lt;-tot.away.list\u0026lt;-tot.team.list\u0026lt;-list()\r\u0026gt; for(i in 1:1000){\r+ tot.home.list[[i]]\u0026lt;-ddply(data.points.list[[i]], .(h), summarise, totpointhome=sum(points.home))\r+ tot.away.list[[i]]\u0026lt;-ddply(data.points.list[[i]], .(a), summarise, totpointaway=sum(points.away))\r+ }\r\u0026gt; \u0026gt; for(i in 1:1000){\r+ tot.team.list[[i]]\u0026lt;-data.frame(levels(data$home.team), tot.home.list[[i]]$h)\r+ tot.team.list[[i]]$tot.team\u0026lt;-tot.home.list[[i]]$totpointhome + tot.away.list[[i]]$totpointaway\r+ tot.team.list[[i]]$true.team\u0026lt;-c(19, 39, 23, 50, 19, 11, 37, 51, 32, 33, 27, 50)\r+ tot.team.list[[i]]\u0026lt;-tot.team.list[[i]][order(tot.team.list[[i]]$tot.team, decreasing = TRUE),]\r+ tot.team.list[[i]]$rank\u0026lt;-rep(1:12)\r+ }\r\u0026gt; \u0026gt; #plot total scores by team\r\u0026gt; tot.scores\u0026lt;-matrix(NA,1000,12)\r\u0026gt; colnames(tot.scores)\u0026lt;-names\r\u0026gt; for(i in 1:1000){\r+ for(j in 1:12){\r+ tot.scores[i,j]\u0026lt;-tot.team.list[[i]]$tot.team[tot.team.list[[i]]$tot.home.list..i...h==j] + }\r+ }\r\u0026gt; tot.scores.obs\u0026lt;-c(19, 39, 23, 50, 19, 11, 37, 51, 32, 33, 27, 50)\r\u0026gt; tot.scores.med\u0026lt;-apply(tot.scores, 2, median)\r\u0026gt; tot.scores.final\u0026lt;-cbind(tot.scores.obs,tot.scores.med)\r\u0026gt; \u0026gt; #plot total wins \u0026gt; tot.wins\u0026lt;-matrix(NA,1000,12)\r\u0026gt; colnames(tot.wins)\u0026lt;-names\r\u0026gt; for(i in 1:1000){\r+ for(j in 1:12){\r+ tot.wins[i,j]\u0026lt;-length(data.points.list[[i]]$points.home[data.points.list[[i]]$points.home\u0026gt;1 \u0026amp; + data.points.list[[i]]$h==j]) + length(data.points.list[[i]]$points.away[data.points.list[[i]]$points.away\u0026gt;1 \u0026amp; + data.points.list[[i]]$a==j])\r+ }\r+ }\r\u0026gt; tot.wins.obs\u0026lt;-c(7,12,6,17,7,5,13,17,10,12,8,18)\r\u0026gt; tot.wins.prop\u0026lt;-tot.wins/22\r\u0026gt; tot.wins.obs.prop\u0026lt;-tot.wins.obs/22\r\u0026gt; tot.wins.med\u0026lt;-apply(tot.wins, 2, median)\r\u0026gt; tot.wins.final\u0026lt;-cbind(tot.wins.obs,tot.wins.med)\rHere, for each team, we compare the predicted and observed total number of points scored in the matches, the number of won/lost matches, and the league points scored based on a replicated and the original season results.\n\u0026gt; #summarise pred results\r\u0026gt; res.final.obs\u0026lt;-cbind(res.y[,1],res.y.neg[,1],tot.wins.final[,1],tot.scores.final[,1])\r\u0026gt; res.final.pred\u0026lt;-cbind(res.y[,2],res.y.neg[,2],tot.wins.final[,2],tot.scores.final[,2])\r\u0026gt; res.final\u0026lt;-cbind(res.final.obs,res.final.pred)\r\u0026gt; colnames(res.final)\u0026lt;-c(\u0026quot;scored\u0026quot;,\u0026quot;conc\u0026#39;d\u0026quot;,\u0026quot;wins\u0026quot;,\u0026quot;points\u0026quot;,\u0026quot;scored\u0026quot;,\u0026quot;conc\u0026#39;d\u0026quot;,\u0026quot;wins\u0026quot;,\u0026quot;points\u0026quot;)\r\u0026gt; knitr::kable(res.final, \u0026quot;pandoc\u0026quot;, align = \u0026quot;c\u0026quot;)\r\r\r\rscored\rconc’d\rwins\rpoints\rscored\rconc’d\rwins\rpoints\r\r\r\rBergamo\r1848\r2025\r7\r19\r1851\r2013\r7\r20\r\rBusto Arsizio\r1999\r1927\r12\r39\r1969\r1926\r12\r37\r\rCasalmaggiore\r1922\r2051\r6\r23\r1912\r2031\r7\r23\r\rConegliano\r1960\r1696\r17\r50\r1944\r1719\r18\r50\r\rFilottrano\r1781\r1961\r7\r19\r1806\r1952\r6\r18\r\rLegnano\r1642\r1903\r5\r11\r1686\r1892\r4\r16\r\rMonza\r2003\r1943\r13\r37\r1979\r1934\r13\r38\r\rNovara\r1987\r1776\r17\r51\r1970\r1787\r17\r51\r\rPesaro\r1776\r1820\r10\r32\r1791\r1819\r11\r32\r\rPiacenza\r1888\r1939\r12\r33\r1884\r1928\r9\r30\r\rSan Casciano\r1807\r1881\r8\r27\r1809\r1874\r9\r29\r\rScandicci\r1865\r1556\r18\r50\r1869\r1591\r18\r51\r\r\r\rPredicted results (the last four columns in the table) are not so bad, especially when looking at the number of league points scored by the teams between the replicated and observed season which are quite similar. These results suggest that the model seems to predict in a reasonable way the league results. To further assess this aspect, we consider two plots. The first compares the ranking of the teams across a large number of replications of the season based on the number of wins, losses and league points gained for each team.\n\u0026gt; #summarise pred results\r\u0026gt; res.matrix\u0026lt;-matrix(NA,length(tot.team.list),12)\r\u0026gt; colnames(res.matrix)\u0026lt;-names\r\u0026gt; for(i in 1:1000){\r+ res.matrix[i,1]\u0026lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==1]\r+ res.matrix[i,2]\u0026lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==2]\r+ res.matrix[i,3]\u0026lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==3]\r+ res.matrix[i,4]\u0026lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==4]\r+ res.matrix[i,5]\u0026lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==5]\r+ res.matrix[i,6]\u0026lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==6]\r+ res.matrix[i,7]\u0026lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==7]\r+ res.matrix[i,8]\u0026lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==8]\r+ res.matrix[i,9]\u0026lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==9]\r+ res.matrix[i,10]\u0026lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==10]\r+ res.matrix[i,11]\u0026lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==11]\r+ res.matrix[i,12]\u0026lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==12]\r+ }\r\u0026gt; \u0026gt; #create stacked barplot of results\r\u0026gt; data.barplot\u0026lt;-data.frame(rep(1:c(1000*12)))\r\u0026gt; names(data.barplot)\u0026lt;-c(\u0026quot;Game\u0026quot;)\r\u0026gt; data.barplot$position\u0026lt;-as.factor(c(res.matrix[,1],res.matrix[,2],res.matrix[,3],res.matrix[,4],\r+ res.matrix[,5],res.matrix[,6],res.matrix[,7],res.matrix[,8],\r+ res.matrix[,9],res.matrix[,10],res.matrix[,11],res.matrix[,12]))\r\u0026gt; data.barplot$team\u0026lt;-rep(NA,1000*12)\r\u0026gt; data.barplot$team[1:1000]\u0026lt;-rep(paste(names[1]),1000)\r\u0026gt; data.barplot$team[1001:2000]\u0026lt;-rep(paste(names[2]),1000)\r\u0026gt; data.barplot$team[2001:3000]\u0026lt;-rep(paste(names[3]),1000)\r\u0026gt; data.barplot$team[3001:4000]\u0026lt;-rep(paste(names[4]),1000)\r\u0026gt; data.barplot$team[4001:5000]\u0026lt;-rep(paste(names[5]),1000)\r\u0026gt; data.barplot$team[5001:6000]\u0026lt;-rep(paste(names[6]),1000)\r\u0026gt; data.barplot$team[6001:7000]\u0026lt;-rep(paste(names[7]),1000)\r\u0026gt; data.barplot$team[7001:8000]\u0026lt;-rep(paste(names[8]),1000)\r\u0026gt; data.barplot$team[8001:9000]\u0026lt;-rep(paste(names[9]),1000)\r\u0026gt; data.barplot$team[9001:10000]\u0026lt;-rep(paste(names[10]),1000)\r\u0026gt; data.barplot$team[10001:11000]\u0026lt;-rep(paste(names[11]),1000)\r\u0026gt; data.barplot$team[11001:12000]\u0026lt;-rep(paste(names[12]),1000)\r\u0026gt; #data.barplot$team\u0026lt;-as.factor(data.barplot$team)\r\u0026gt; data.barplot$team \u0026lt;-factor(data.barplot$team, levels = c(\u0026quot;Novara\u0026quot;, \u0026quot;Scandicci\u0026quot;,\u0026quot;Conegliano\u0026quot;, \u0026quot;Monza\u0026quot;,\u0026quot;Busto Arsizio\u0026quot;,\r+ \u0026quot;Pesaro\u0026quot;,\u0026quot;Piacenza\u0026quot;, \u0026quot;San Casciano\u0026quot;,\u0026quot;Casalmaggiore\u0026quot;, \u0026quot;Bergamo\u0026quot;,\r+ \u0026quot;Filottrano\u0026quot;, \u0026quot;Legnano\u0026quot;))\r\u0026gt; \u0026gt; \u0026gt; \u0026gt; data.barplot$match\u0026lt;-c(rep(1,1000),rep(2,1000),rep(3,1000),rep(4,1000),rep(5,1000),rep(6,1000),\r+ rep(7,1000),rep(8,1000),rep(9,1000),rep(10,1000),rep(11,1000),rep(12,1000))\r\u0026gt; data.barplot$match\u0026lt;-as.factor(data.barplot$match)\r\u0026gt; data.barplot$Game\u0026lt;-rep(1,nrow(data.barplot))\r\u0026gt; data.barplot$area\u0026lt;-ifelse(data.barplot$position==1|data.barplot$position==2|data.barplot$position==3,\u0026quot;high\u0026quot;,\u0026quot;middle\u0026quot;)\r\u0026gt; data.barplot$area\u0026lt;-ifelse(data.barplot$position==12|data.barplot$position==11|data.barplot$position==10,\u0026quot;low\u0026quot;,data.barplot$area)\r\u0026gt; data.barplot$area\u0026lt;-as.factor(data.barplot$area)\r\u0026gt; data.barplot$area\u0026lt;-ordered(data.barplot$area,levels=c(\u0026quot;low\u0026quot;,\u0026quot;middle\u0026quot;,\u0026quot;high\u0026quot;))\r\u0026gt; data.barplot$team\u0026lt;-factor(data.barplot$team,levels = rev(levels(data.barplot$team)))\r\u0026gt; \u0026gt; df.summary1\u0026lt;-ddply(data.barplot,.(team,position),summarise,count=sum(Game), percent=sum(Game)/1000)\r\u0026gt; df.summary2\u0026lt;-ddply(data.barplot,.(team,area),summarise,count=sum(Game), percent=sum(Game)/1000)\r\u0026gt; \u0026gt; ggplot(df.summary1, aes(x=team, y=percent, fill=position)) +\r+ geom_bar(stat=\u0026quot;identity\u0026quot;, width = 0.7, colour=\u0026quot;black\u0026quot;, lwd=0.1) +\r+ geom_text(aes(label=ifelse(percent \u0026gt;= 0.1, paste0(sprintf(\u0026quot;%.0f\u0026quot;, percent*100),\u0026quot;%\u0026quot;),\u0026quot;\u0026quot;)),\r+ position=position_stack(vjust=0.5), colour=\u0026quot;white\u0026quot;) +\r+ coord_flip() + scale_y_continuous(labels = percent_format()) +\r+ labs(y=\u0026quot;\u0026quot;, x=\u0026quot;\u0026quot;) + scale_fill_viridis(discrete = T) + + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill = \u0026quot;white\u0026quot;),\r+ axis.line = element_line(colour = \u0026quot;black\u0026quot;))\rThe mean rankings of the teams are in line with those observed in the actual season, while also providing uncertainty about the chance of each team to end in a particular position in the league (only percentages above \\(10\\%\\) are shown for clarity).\nThe second plot compares the points trend throughout the season for each team with respect to the trend predicted by the model based on the replicated results for each match in the season.\n\u0026gt; #########plot for cumlative points over simulated season\r\u0026gt; points.list\u0026lt;-cum.points.list\u0026lt;-list()\r\u0026gt; for(i in 1:nrow(res.matrix)){\r+ points.list[[i]]\u0026lt;-matrix(NA,22,12)\r+ cum.points.list[[i]]\u0026lt;-matrix(NA,22,12)\r+ for(j in 1:12){\r+ points.list[[i]][,j]\u0026lt;-c(data.points.list[[i]]$points.home[data.points.list[[i]]$h==j],data.points.list[[i]]$points.away[data.points.list[[i]]$a==j])\r+ }\r+ colnames(points.list[[i]])\u0026lt;-unique(levels(data$home.team))\r+ rownames(points.list[[i]])\u0026lt;-rep(1:22)\r+ cum.points.list[[i]]\u0026lt;-apply(points.list[[i]], 2, cumsum)\r+ }\r\u0026gt; \u0026gt; #########plot for cumlative points over simulated season\r\u0026gt; points.list\u0026lt;-cum.points.list\u0026lt;-list()\r\u0026gt; for(i in 1:nrow(res.matrix)){\r+ points.list[[i]]\u0026lt;-matrix(NA,22,12)\r+ cum.points.list[[i]]\u0026lt;-matrix(NA,22,12)\r+ for(j in 1:12){\r+ points.list[[i]][,j]\u0026lt;-c(data.points.list[[i]]$points.home[data.points.list[[i]]$h==j],data.points.list[[i]]$points.away[data.points.list[[i]]$a==j])\r+ }\r+ colnames(points.list[[i]])\u0026lt;-unique(levels(data$home.team))\r+ rownames(points.list[[i]])\u0026lt;-rep(1:22)\r+ cum.points.list[[i]]\u0026lt;-apply(points.list[[i]], 2, cumsum)\r+ }\r\u0026gt; \u0026gt; #plot cumulative points obs vs pred\r\u0026gt; obs.cum.points\u0026lt;-matrix(NA,22,12)\r\u0026gt; colnames(obs.cum.points)\u0026lt;-unique(levels(data$home.team))\r\u0026gt; rownames(obs.cum.points)\u0026lt;-rep(1:22)\r\u0026gt; obs.cum.points[1,]\u0026lt;-c(0,1,0,3,3,3,0,2,0,0,3,3)\r\u0026gt; obs.cum.points[2,]\u0026lt;-c(0,3,0,3,0,0,3,3,0,3,0,3)\r\u0026gt; obs.cum.points[3,]\u0026lt;-c(0,2,3,3,0,0,0,3,3,1,0,3)\r\u0026gt; obs.cum.points[4,]\u0026lt;-c(0,3,0,3,0,0,3,3,1,2,0,3)\r\u0026gt; obs.cum.points[5,]\u0026lt;-c(0,3,0,3,0,3,0,3,3,0,0,3)\r\u0026gt; obs.cum.points[6,]\u0026lt;-c(0,3,1,3,0,0,0,3,1,3,2,2)\r\u0026gt; obs.cum.points[7,]\u0026lt;-c(2,1,3,2,0,1,1,3,2,2,1,0)\r\u0026gt; obs.cum.points[8,]\u0026lt;-c(0,2,1,1,0,0,3,2,3,2,1,3)\r\u0026gt; obs.cum.points[9,]\u0026lt;-c(3,3,1,3,0,2,2,1,0,0,3,0)\r\u0026gt; obs.cum.points[10,]\u0026lt;-c(1,2,1,2,1,2,1,1,2,2,1,2)\r\u0026gt; obs.cum.points[11,]\u0026lt;-c(0,1,0,3,0,0,3,3,0,3,3,2)\r\u0026gt; obs.cum.points[12,]\u0026lt;-c(3,0,0,3,0,0,3,3,3,0,0,3)\r\u0026gt; obs.cum.points[13,]\u0026lt;-c(0,1,3,3,2,0,3,2,1,3,0,0)\r\u0026gt; obs.cum.points[14,]\u0026lt;-c(2,3,1,3,0,0,0,3,3,0,0,3)\r\u0026gt; obs.cum.points[15,]\u0026lt;-c(2,1,0,3,2,0,3,1,0,3,0,3)\r\u0026gt; obs.cum.points[16,]\u0026lt;-c(0,3,0,3,0,0,0,3,3,0,3,3)\r\u0026gt; obs.cum.points[17,]\u0026lt;-c(2,0,3,0,0,3,3,1,0,3,0,3)\r\u0026gt; obs.cum.points[18,]\u0026lt;-c(0,0,0,3,3,1,2,3,1,2,3,0)\r\u0026gt; obs.cum.points[19,]\u0026lt;-c(3,3,3,2,0,0,3,0,0,1,0,3)\r\u0026gt; obs.cum.points[20,]\u0026lt;-c(0,0,2,0,3,1,0,3,3,0,3,3)\r\u0026gt; obs.cum.points[21,]\u0026lt;-c(0,3,0,1,3,0,2,2,0,1,3,3)\r\u0026gt; obs.cum.points[22,]\u0026lt;-c(1,1,1,0,2,0,2,3,3,2,1,2)\r\u0026gt; \u0026gt; obs.cum.points\u0026lt;-apply(obs.cum.points, 2, cumsum)\r\u0026gt; \u0026gt; par(mar=c(2.1, 3.1, 3.1, 3.1))\r\u0026gt; par(mfrow=c(4,3), mai = c(0.4, 0.4, 0.1, 0.2))\r\u0026gt; for(i in 1:12){\r+ plot(rep(1:22),obs.cum.points[,i], axes = F, type = \u0026quot;n\u0026quot;, xlab = \u0026quot;games\u0026quot;, ylab = \u0026quot;points\u0026quot;,xlim = c(0,23),ylim = c(0,54))\r+ axis(1,at=c(0,5,10,15,20,25),labels = c(0,5,10,15,20,25))\r+ axis(2,at=c(0,10,20,30,40,50),labels = c(0,10,20,30,40,50))\r+ lines(rep(1:22),obs.cum.points[,i], lty=1,lwd=1,col=\u0026quot;black\u0026quot;)\r+ lines(rep(1:22), cum.points.list[[7]][,i], lty=1,lwd=1,col=\u0026quot;red\u0026quot;)\r+ text(5,50,unique(levels(names))[i],pos = 1, cex = 1)\r+ }\rThe observed trends (black lines) are pretty much matched by the trends predicted by the model (red lines) for most of the teams with few exceptions. This suggests that, although the model seems to have a good predictive ability, in some cases there are still margins of improvement. In my original paper I have improved the model using a different parameterisation where I tried to account for the dependence between the latent skill parameters using a scaled inverse Wishart distribution for the covariance matrix of the random effects. Here I do not consider this model here, which lead to some improvements compared with the basic Poisson model.\n\r\rConclusions\rOne potential limitation of the framework is that only match-specific statistics are\rused for estimation and prediction purposes. Ideally, the use of set-specific statistics\rcould improve the predictive power of the model. However, this would introduce additional problems related to the choice of the distributions for modelling the number of points scored by the opposing teams in a set, which is subject to specific constraints. Finally, the flexibility of the proposed framework allows the extension of the model in many ways. For example, additional types of in-game statistics (e.g. number of passes), if available, could be incorporated to further improve the predictions of the model; alternative distributions could also be specified to model the total number of scores \\(y\\) during the season (e.g. Negative Binomial) which may result in a better fit to the data.\n\rReferences\rGabrio, Andrea. 2020. “Bayesian Hierarchical Models for the Prediction of Volleyball Results.” Journal of Applied Statistics, 1–21.\n\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\r\r\r","date":1581819194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581819194,"objectID":"0ad88fdd80ead02bc0969eb9838287a3","permalink":"/stan/glmm-stan/glmm-stan/","publishdate":"2020-02-15T21:13:14-05:00","relpermalink":"/stan/glmm-stan/glmm-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","generalised linear mixed models"],"title":"Generalised Linear Mixed Models - STAN","type":"STAN"},{"authors":["Andrea Gabrio"],"categories":["R","JAGS","generalised linear models"],"content":"\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rWhilst in many instances, count data can be approximated reasonably well by a normal distribution (particularly if the counts are all above zero and the mean count is greater than about \\(20\\)), more typically, when count data are modelled via normal distribution certain undesirable characteristics arise that are a consequence of the nature of discrete non-negative data.\n\rExpected (predicted) values and confidence bands less than zero are illogical, yet these are entirely possible from a normal distribution\n\rThe distribution of count data are often skewed when their mean is low (in part because the distribution is truncated to the left by zero) and variance usually increases with increasing mean (variance is typically proportional to mean in count data). By contrast, the Gaussian (normal) distribution assumes that mean and variance are unrelated and thus estimates (particularly of standard error) might well be reasonable inaccurate.\n\r\rPoisson regression is a type of generalised linear model (GLM) in which a non-negative integer (natural number) response is modelled against a linear predictor via a specific link function. The linear predictor is typically a linear combination of effects parameters. The role of the link function is to transform the expected values of the response y (which is on the scale of (\\(0;\\infty\\)), as is the Poisson distribution from which expectations are drawn) into the scale of the linear predictor (which is \\(-\\infty;\\infty\\)).\nAs implied in the name of this group of analyses, a Poisson rather than Gaussian (normal) distribution is used to represent the errors (residuals). Like count data (number of individuals, species etc), the Poisson distribution encapsulates positive integers and is bound by zero at one end. Consequently, the degree of variability is directly related the expected value (equivalent to the mean of a Gaussian distribution). Put differently, the variance is a function of the mean. Repeated observations from a Poisson distribution located close to zero will yield a much smaller spread of observations than will samples drawn from a Poisson distribution located a greater distance from zero. In the Poisson distribution, the variance has a 1:1 relationship with the mean. The canonical link function for the Poisson distribution is a log-link function.\nWhilst the expectation that the mean=variance (\\(\\mu=\\sigma\\)) is broadly compatible with actual count data (that variance increases at the same rate as the mean), under certain circumstances, this might not be the case. For example, when there are other unmeasured influences on the response variable, the distribution of counts might be somewhat clumped which can result in higher than expected variability (that is \\(\\sigma \u0026gt; \\mu\\)). The variance increases more rapidly than does the mean. This is referred to as overdispersion. The degree to which the variability is greater than the mean (and thus the expected degree of variability) is called dispersion. Effectively, the Poisson distribution has a dispersion parameter (or scaling factor) of \\(1\\).\nIt turns out that overdispersion is very common for count data and it typically underestimates variability, standard errors and thus deflated p-values. There are a number of ways of overcoming this limitation, the effectiveness of which depend on the causes of overdispersion.\nQuasi-Poisson models - these introduce the dispersion parameter (\\(\\phi\\)) into the model. This approach does not utilize an underlying error distribution to calculate the maximum likelihood (there is no quasi-Poisson distribution). Instead, if the Newton-Ralphson iterative reweighting least squares algorithm is applied using a direct specification of the relationship between mean and variance (\\(\\text{var}(y)=\\phi\\mu\\), the estimates of the regression coefficients are identical to those of the maximum likelihood estimates from the Poisson model. This is analogous to fitting ordinary least squares on symmetrical, yet not normally distributed data - the parameter estimates are the same, however they won’t necessarily be as efficient. The standard errors of the coefficients are then calculated by multiplying the Poisson model coefficient standard errors by \\(\\sqrt{\\phi}\\). Unfortunately, because the quasi-poisson model is not estimated via maximum likelihood, properties such as AIC and log-likelihood cannot be derived. Consequently, quasi-poisson and Poisson model fits cannot be compared via either AIC or likelihood ratio tests (nor can they be compared via deviance as uasi-poisson and Poisson models have the same residual deviance). That said, quasi-likelihood can be obtained by dividing the likelihood from the Poisson model by the dispersion (scale) factor.\nNegative binomial model - technically, the negative binomial distribution is a probability distribution for the number of successes before a specified number of failures. However, the negative binomial can also be defined (parameterised) in terms of a mean (\\(\\mu\\)) and scale factor (\\(\\omega\\)),\n\\[ p(y_i) = \\frac{\\Gamma(y_i+\\omega)}{\\Gamma(\\omega)y!} \\times \\frac{\\mu^{y_i}_i\\omega^\\omega}{(\\mu_i+\\omega)^{\\mu_i+\\omega}},\\]\nwhere the expectected value of the values \\(y_i\\) (the means) are (\\(\\mu_i\\)) and the variance is \\(y_i=\\frac{\\mu_i+\\mu^2_i}{\\omega}\\). In this way, the negative binomial is a two-stage hierarchical process in which the response is modeled against a Poisson distribution whose expected count is in turn modeled by a Gamma distribution with a mean of \\(\\mu\\) and constant scale parameter (\\(\\omega\\)). Strictly, the negative binomial is not an exponential family distribution (unless \\(\\omega\\) is fixed as a constant), and thus negative binomial models cannot be fit via the usual GLM iterative reweighting algorithm. Instead estimates of the regression parameters along with the scale factor (\\(\\omega\\)) are obtained via maximum likelihood. The negative binomial model is useful for accommodating overdispersal when it is likely caused by clumping (due to the influence of other unmeasured factors) within the response.\nZero-inflated Poisson model - overdispersion can also be caused by the presence of a greater number of zero’s than would otherwise be expected for a Poisson distribution. There are potentially two sources of zero counts - genuine zeros and false zeros. Firstly, there may genuinely be no individuals present. This would be the number expected by a Poisson distribution. Secondly, individuals may have been present yet not detected or may not even been possible. These are false zero’s and lead to zero inflated data (data with more zeros than expected). For example, the number of joeys accompanying an adult koala could be zero because the koala has no offspring (true zero) or because the koala is male or infertile (both of which would be examples of false zeros). Similarly, zero counts of the number of individual in a transect are due either to the absence of individuals or the inability of the observer to detect them. Whilst in the former example, the latent variable representing false zeros (sex or infertility) can be identified and those individuals removed prior to analysis, this is not the case for the latter example. That is, we cannot easily partition which counts of zero are due to detection issues and which are a true indication of the natural state.\nConsistent with these two sources of zeros, zero-inflated models combine a binary logistic regression model (that models count membership according to a latent variable representing observations that can only be zeros - not detectable or male koalas) with a Poisson regression (that models count membership according to a latent variable representing observations whose values could be 0 or any positive integer - fertile female koalas).\n\rPoisson regression\rThe following equations are provided since in Bayesian modelling, it is occasionally necessary to directly define the log-likelihood calculations (particularly for zero-inflated models and other mixture models).\n\\[ f(y \\mid \\lambda) = \\frac{\\lambda^ye^{-\\lambda}}{y!},\\]\nwhere \\(E[Y]=Var(Y)=\\lambda\\) and \\(\\lambda\\) is the mean.\n\rData generation\rLets say we wanted to model the abundance of an item (\\(y\\)) against a continuous predictor (\\(x\\)). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(8)\r\u0026gt; #The number of samples\r\u0026gt; n.x \u0026lt;- 20\r\u0026gt; #Create x values that at uniformly distributed throughout the rate of 1 to 20\r\u0026gt; x \u0026lt;- sort(runif(n = n.x, min = 1, max =20))\r\u0026gt; mm \u0026lt;- model.matrix(~x)\r\u0026gt; intercept \u0026lt;- 0.6\r\u0026gt; slope=0.1\r\u0026gt; #The linear predictor\r\u0026gt; linpred \u0026lt;- mm %*% c(intercept,slope)\r\u0026gt; #Predicted y values\r\u0026gt; lambda \u0026lt;- exp(linpred)\r\u0026gt; #Add some noise and make binomial\r\u0026gt; y \u0026lt;- rpois(n=n.x, lambda=lambda)\r\u0026gt; dat \u0026lt;- data.frame(y,x)\rWith these sort of data, we are primarily interested in investigating whether there is a relationship between the binary response variable and the linear predictor (linear combination of one or more continuous or categorical predictors).\n\rExploratory data analysis\rThere are at least five main potential models we could consider fitting to these data:\n\rOrdinary least squares regression (general linear model) - assumes normality of residuals\n\rPoisson regression - assumes mean=variance (dispersion\\(=1\\))\n\rQuasi-poisson regression - a general solution to overdispersion. Assumes variance is a function of mean, dispersion estimated, however likelihood based statistics unavailable\n\rNegative binomial regression - a specific solution to overdispersion caused by clumping (due to an unmeasured latent variable). Scaling factor (\\(\\omega\\)) is estimated along with the regression parameters.\n\rZero-inflation model - a specific solution to overdispersion caused by excessive zeros (due to an unmeasured latent variable). Mixture of binomial and Poisson models.\n\r\rWhen counts are all very large (not close to \\(0\\)) and their ranges do not span orders of magnitude, they take on very Gaussian properties (symmetrical distribution and variance independent of the mean). Given that models based on the Gaussian distribution are more optimized and recognized than Generalized Linear Models, it can be prudent to adopt Gaussian models for such data. Hence it is a good idea to first explore whether a Poisson model is likely to be more appropriate than a standard Gaussian model. The potential for overdispersion can be explored by adding a rug to boxplot. The rug is simply tick marks on the inside of an axis at the position corresponding to an observation. As multiple identical values result in tick marks drawn over one another, it is typically a good idea to apply a slight amount of jitter (random displacement) to the values used by the rug.\n\u0026gt; hist(dat$x)\r\u0026gt; \u0026gt; boxplot(dat$y, horizontal=TRUE)\r\u0026gt; rug(jitter(dat$y), side=1)\rThere is definitely signs of non-normality that would warrant Poisson models. The rug applied to the boxplots does not indicate a series degree of clumping and there appears to be few zero. Thus overdispersion is unlikely to be an issue. Lets now explore linearity by creating a histogram of the predictor variable (\\(x\\)) and a scatterplot of the relationship between the response (\\(y\\)) and the predictor (\\(x\\))\n\u0026gt; #now for the scatterplot\r\u0026gt; plot(y~x, dat, log=\u0026quot;y\u0026quot;)\r\u0026gt; with(dat, lines(lowess(y~x)))\rConclusions: the predictor (\\(x\\)) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the scatterplot does not display major deviations from a straight line and thus linearity is satisfied. Violations of linearity could be addressed by either:\n\rdefine a non-linear linear predictor (such as a polynomial, spline or other non-linear function).\n\rtransform the scale of the predictor variables.\n\r\rAlthough we have already established that there are few zeros in the data (and thus overdispersion is unlikely to be an issue), we can also explore this by comparing the number of zeros in the data to the number of zeros that would be expected from a Poisson distribution with a mean equal to the mean count of the data.\n\u0026gt; #proportion of 0\u0026#39;s in the data\r\u0026gt; dat.tab\u0026lt;-table(dat$y==0)\r\u0026gt; dat.tab/sum(dat.tab)\rFALSE 1 \u0026gt; \u0026gt; #proportion of 0\u0026#39;s expected from a Poisson distribution\r\u0026gt; mu \u0026lt;- mean(dat$y)\r\u0026gt; cnts \u0026lt;- rpois(1000, mu)\r\u0026gt; dat.tab \u0026lt;- table(cnts == 0)\r\u0026gt; dat.tab/sum(dat.tab)\rFALSE TRUE 0.997 0.003 \rIn the above, the value under FALSE is the proportion of non-zero values in the data and the value under TRUE is the proportion of zeros in the data. In this example, there are no zeros in the observed data which corresponds closely to the very low proportion expected (\\(0.003\\)).\n\rModel fitting\r\\[ y_i \\sim \\text{Pois}(\\lambda_i),\\]\nwhere \\(\\log(\\lambda_i)=\\eta_i\\), with \\(\\eta_i=\\beta_0+\\beta_1x_{i}\\) and \\(\\beta_0,\\beta_1 \\sim N(0, 10000)\\).\n\u0026gt; dat.list \u0026lt;- with(dat,list(Y=y, X=x,N=nrow(dat)))\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ for (i in 1:N) {\r+ Y[i] ~ dpois(lambda[i])\r+ log(lambda[i]) \u0026lt;- beta0 + beta1*X[i]\r+ }\r+ beta0 ~ dnorm(0,1.0E-06)\r+ beta1 ~ dnorm(0,1.0E-06)\r+ } + \u0026quot;\r\u0026gt; writeLines(modelString, con=\u0026#39;modelpois.txt\u0026#39;)\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; library(R2jags)\r\u0026gt; dat.P.jags \u0026lt;- jags(data=dat.list,model.file=\u0026#39;modelpois.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 2\rTotal graph size: 105\rInitializing model\r\rModel evaluation\r\u0026gt; library(mcmcplots)\r\u0026gt; denplot(dat.P.jags, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;))\r\u0026gt; traplot(dat.P.jags, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;))\r\u0026gt; \u0026gt; raftery.diag(as.mcmc(dat.P.jags))\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 10 10830 3746 2.89 beta1 12 12612 3746 3.37 deviance 3 4410 3746 1.18 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 12 14878 3746 3.97 beta1 10 11942 3746 3.19 deviance 2 3995 3746 1.07 \u0026gt; \u0026gt; autocorr.diag(as.mcmc(dat.P.jags))\rbeta0 beta1 deviance\rLag 0 1.00000000 1.0000000000 1.000000000\rLag 1 0.83977964 0.8111423616 0.508803232\rLag 5 0.43884918 0.3859514845 0.118732714\rLag 10 0.22584100 0.1883831873 0.029775648\rLag 50 -0.01164622 -0.0003926876 -0.007507996\rOne very important model validation procedure is to examine a plot of residuals against predicted or fitted values (the residual plot). Ideally, residual plots should show a random scatter of points without outliers. That is, there should be no patterns in the residuals. Patterns suggest inappropriate linear predictor (or scale) and/or inappropriate residual distribution/link function. The residuals used in such plots should be standardized (particularly if the model incorporated any variance-covariance structures - such as an autoregressive correlation structure) Pearsons’s residuals standardize residuals by division with the square-root of the variance. We can generate Pearson’s residuals within the JAGS model. Alternatively, we could use the parameters to generate the residuals outside of JAGS. Pearson’s residuals are calculated according to:\n\\[ \\epsilon = \\frac{y_i - \\mu}{\\sqrt{\\text{var}(y)}},\\]\nwhere \\(\\mu\\) is the expected value of \\(Y\\) (\\(=\\lambda\\) for Poisson) and var(\\(y\\)) is the variance of \\(Y\\) (\\(=\\lambda\\) for Poisson).\n\u0026gt; #extract the samples for the two model parameters\r\u0026gt; coefs \u0026lt;- dat.P.jags$BUGSoutput$sims.matrix[,1:2]\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data=dat)\r\u0026gt; #expected values on a log scale\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; #expected value on response scale\r\u0026gt; lambda \u0026lt;- exp(eta)\r\u0026gt; #Expected value and variance are both equal to lambda\r\u0026gt; expY \u0026lt;- varY \u0026lt;- lambda\r\u0026gt; #sweep across rows and then divide by lambda\r\u0026gt; Resid \u0026lt;- -1*sweep(expY,2,dat$y,\u0026#39;-\u0026#39;)/sqrt(varY)\r\u0026gt; #plot residuals vs expected values\r\u0026gt; plot(apply(Resid,2,mean)~apply(eta,2,mean))\rNow we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Poisson distribution matching that estimated by the model. Essentially this is estimating how well the Poisson distribution, the log-link function and the linear model approximates the observed data.\n\u0026gt; SSres\u0026lt;-apply(Resid^2,1,sum)\r\u0026gt; \u0026gt; #generate a matrix of draws from a poisson distribution\r\u0026gt; # the matrix is the same dimensions as lambda and uses the probabilities of lambda\r\u0026gt; YNew \u0026lt;- matrix(rpois(length(lambda),lambda=lambda),nrow=nrow(lambda))\r\u0026gt; \u0026gt; Resid1\u0026lt;-(lambda - YNew)/sqrt(lambda)\r\u0026gt; SSres.sim\u0026lt;-apply(Resid1^2,1,sum)\r\u0026gt; mean(SSres.sim\u0026gt;SSres, na.rm = T)\r[1] 0.4697\r\rGoodness of fit\r\u0026gt; dat.list1 \u0026lt;- with(dat,list(Y=y, X=x,N=nrow(dat)))\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ for (i in 1:N) {\r+ #likelihood function\r+ Y[i] ~ dpois(lambda[i])\r+ eta[i] \u0026lt;- beta0+beta1*X[i] #linear predictor\r+ log(lambda[i]) \u0026lt;- eta[i] #link function\r+ + #E(Y) and var(Y)\r+ expY[i] \u0026lt;- lambda[i]\r+ varY[i] \u0026lt;- lambda[i]\r+ + # Calculate RSS\r+ Resid[i] \u0026lt;- (Y[i] - expY[i])/sqrt(varY[i])\r+ RSS[i] \u0026lt;- pow(Resid[i],2)\r+ + #Simulate data from a Poisson distribution\r+ Y1[i] ~ dpois(lambda[i])\r+ #Calculate RSS for simulated data\r+ Resid1[i] \u0026lt;- (Y1[i] - expY[i])/sqrt(varY[i])\r+ RSS1[i] \u0026lt;-pow(Resid1[i],2) + }\r+ #Priors\r+ beta0 ~ dnorm(0,1.0E-06)\r+ beta1 ~ dnorm(0,1.0E-06)\r+ #Bayesian P-value\r+ Pvalue \u0026lt;- mean(sum(RSS1)\u0026gt;sum(RSS))\r+ } + \u0026quot;\r\u0026gt; \u0026gt; writeLines(modelString, con=\u0026#39;modelpois_gof.txt\u0026#39;)\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;, \u0026#39;Pvalue\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.P.jags1 \u0026lt;- jags(data=dat.list,model.file=\u0026#39;modelpois_gof.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 22\rTotal graph size: 272\rInitializing model\r\u0026gt; \u0026gt; print(dat.P.jags1)\rInference for Bugs model at \u0026quot;modelpois_gof.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rPvalue 0.478 0.500 0.000 0.000 0.000 1.000 1.000 1.001 10000\rbeta0 0.546 0.254 0.015 0.381 0.559 0.719 1.013 1.001 10000\rbeta1 0.112 0.018 0.077 0.099 0.111 0.124 0.149 1.001 3200\rdeviance 88.372 3.041 86.373 86.883 87.671 89.075 93.868 1.006 2000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 4.6 and DIC = 93.0\rDIC is an estimate of expected predictive error (lower deviance is better).\rConclusions: the Bayesian p-value is approximately \\(0.5\\), suggesting that there is a good fit of the model to the data.\nUnfortunately, unlike with linear models (Gaussian family), the expected distribution of data (residuals) varies over the range of fitted values for numerous (often competing) ways that make diagnosing (and attributing causes thereof) miss-specified generalized linear models from standard residual plots very difficult. The use of standardized (Pearson) residuals or deviance residuals can partly address this issue, yet they still do not offer completely consistent diagnoses across all issues (miss-specified model, over-dispersion, zero-inflation). An alternative approach is to use simulated data from the model posteriors to calculate an empirical cumulative density function from which residuals are are generated as values corresponding to the observed data along the density function. Now we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Bernoulli distribution matching that estimated by the model. Essentially this is estimating how well the Bernoulli distribution and linear model approximates the observed data.\n\u0026gt; #extract the samples for the two model parameters\r\u0026gt; coefs \u0026lt;- dat.P.jags$BUGSoutput$sims.matrix[,1:2]\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data=dat)\r\u0026gt; #expected values on a log scale\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; #expected value on response scale\r\u0026gt; lambda \u0026lt;- exp(eta)\r\u0026gt; \u0026gt; simRes \u0026lt;- function(lambda, data,n=250, plot=T, family=\u0026#39;poisson\u0026#39;) {\r+ require(gap)\r+ N = nrow(data)\r+ sim = switch(family,\r+ \u0026#39;poisson\u0026#39; = matrix(rpois(n*N,apply(lambda,2,mean)),ncol=N, byrow=TRUE)\r+ )\r+ a = apply(sim + runif(n,-0.5,0.5),2,ecdf)\r+ resid\u0026lt;-NULL\r+ for (i in 1:nrow(data)) resid\u0026lt;-c(resid,a[[i]](data$y[i] + runif(1 ,-0.5,0.5)))\r+ if (plot==T) {\r+ par(mfrow=c(1,2))\r+ gap::qqunif(resid,pch = 2, bty = \u0026quot;n\u0026quot;,\r+ logscale = F, col = \u0026quot;black\u0026quot;, cex = 0.6, main = \u0026quot;QQ plot residuals\u0026quot;,\r+ cex.main = 1, las=1)\r+ plot(resid~apply(lambda,2,mean), xlab=\u0026#39;Predicted value\u0026#39;, ylab=\u0026#39;Standardized residual\u0026#39;, las=1)\r+ }\r+ resid\r+ }\r\u0026gt; \u0026gt; simRes(lambda,dat, family=\u0026#39;poisson\u0026#39;)\r [1] 0.220 0.544 0.532 0.344 0.812 0.980 0.048 0.592 0.548 0.728 0.164 0.492\r[13] 0.856 0.096 0.240 0.292 0.876 0.880 0.148 0.748\rThe trend (black symbols) in the qq-plot does not appear to be overly non-linear (matching the ideal red line well), suggesting that the model is not overdispersed. The spread of standardized (simulated) residuals in the residual plot do not appear overly non-uniform. That is there is not trend in the residuals. Furthermore, there is not a concentration of points close to \\(1\\) or \\(0\\) (which would imply overdispersion).\nRecall that the Poisson regression model assumes that variance=mean (var=μϕ where \\(\\phi=1\\)) and thus dispersion (\\(\\phi=\\frac{\\text{var}}{\\mu}=1)\\)). However, we can also calculate approximately what the dispersion factor would be by using sum square of the residuals as a measure of variance and the model residual degrees of freedom as a measure of the mean (since the expected value of a Poisson distribution is the same as its degrees of freedom).\n\\[ \\phi = \\frac{RSS}{df},\\]\nwhere \\(df=n−k\\) and \\(k\\) is the number of estimated model coefficients.\n\u0026gt; Resid \u0026lt;- -1*sweep(lambda,2,dat$y,\u0026#39;-\u0026#39;)/sqrt(lambda)\r\u0026gt; RSS\u0026lt;-apply(Resid^2,1,sum)\r\u0026gt; (df\u0026lt;-nrow(dat)-ncol(coefs))\r[1] 18\r\u0026gt; \u0026gt; Disp \u0026lt;- RSS/df\r\u0026gt; data.frame(Median=median(Disp), Mean=mean(Disp), HPDinterval(as.mcmc(Disp)),\r+ HPDinterval(as.mcmc(Disp),p=0.5))\rMedian Mean lower upper lower.1 upper.1\rvar1 1.053527 1.110853 0.9299722 1.449502 0.9300381 1.053579\rWe can incorporate the dispersion statistic directly into JAGS.\n\u0026gt; dat.list \u0026lt;- with(dat,list(Y=y, X=x,N=nrow(dat)))\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ for (i in 1:N) {\r+ Y[i] ~ dpois(lambda[i])\r+ eta[i] \u0026lt;- beta0 + beta1*X[i]\r+ log(lambda[i]) \u0026lt;- eta[i]\r+ expY[i] \u0026lt;- lambda[i]\r+ varY[i] \u0026lt;- lambda[i]\r+ Resid[i] \u0026lt;- (Y[i] - expY[i])/sqrt(varY[i]) + }\r+ beta0 ~ dnorm(0,1.0E-06)\r+ beta1 ~ dnorm(0,1.0E-06)\r+ RSS \u0026lt;- sum(pow(Resid,2))\r+ df \u0026lt;- N-2\r+ phi \u0026lt;- RSS/df\r+ } + \u0026quot;\r\u0026gt; \u0026gt; writeLines(modelString, con=\u0026#39;modelpois_disp.txt\u0026#39;)\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;,\u0026#39;phi\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.P.jags \u0026lt;- jags(data=dat.list,model.file=\u0026#39;modelpois_disp.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 2\rTotal graph size: 171\rInitializing model\r\u0026gt; \u0026gt; print(dat.P.jags)\rInference for Bugs model at \u0026quot;modelpois_disp.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 0.552 0.256 0.039 0.382 0.557 0.724 1.042 1.001 10000\rbeta1 0.111 0.019 0.074 0.099 0.112 0.124 0.147 1.001 2800\rphi 1.105 0.246 0.934 0.977 1.048 1.169 1.581 1.001 10000\rdeviance 88.354 2.633 86.368 86.896 87.709 89.074 93.897 1.002 4300\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.5 and DIC = 91.8\rDIC is an estimate of expected predictive error (lower deviance is better).\rThe dispersion statistic is close to \\(1\\) and thus there is no evidence that the data were overdispersed. The Poisson distribution was therefore appropriate.\n\rExploring the model parameters\rIf there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics.\n\u0026gt; library(coda)\r\u0026gt; print(dat.P.jags)\rInference for Bugs model at \u0026quot;modelpois_disp.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 0.552 0.256 0.039 0.382 0.557 0.724 1.042 1.001 10000\rbeta1 0.111 0.019 0.074 0.099 0.112 0.124 0.147 1.001 2800\rphi 1.105 0.246 0.934 0.977 1.048 1.169 1.581 1.001 10000\rdeviance 88.354 2.633 86.368 86.896 87.709 89.074 93.897 1.002 4300\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.5 and DIC = 91.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; library(plyr)\r\u0026gt; adply(dat.P.jags$BUGSoutput$sims.matrix[,1:2], 2, function(x) {\r+ data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\r+ })\rX1 Median Mean lower upper lower.1 upper.1\r1 beta0 0.5570252 0.5517525 0.03871735 1.0423628 0.39092213 0.7317579\r2 beta1 0.1115363 0.1113176 0.07499903 0.1484004 0.09893134 0.1239861\r\u0026gt; \u0026gt; #on original scale\r\u0026gt; adply(exp(dat.P.jags$BUGSoutput$sims.matrix[,1:2]), 2, function(x) {\r+ data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\r+ })\rX1 Median Mean lower upper lower.1 upper.1\r1 beta0 1.745472 1.793464 0.9803783 2.734057 1.423789 2.013575\r2 beta1 1.117994 1.117948 1.0778831 1.159977 1.101510 1.129458\rConclusions: We would reject the null hypothesis of no effect of \\(x\\) on \\(y\\). An increase in x is associated with a significant linear increase (positive slope) in the abundance of y. Every \\(1\\) unit increase in \\(x\\) results in a log \\(0.11\\) unit increase in \\(y\\). We usually express this in terms of abundance rather than log abundance, so every \\(1\\) unit increase in \\(x\\) results in a ($e^{ 0.11} = 1.12 $) \\(1.12\\) unit increase in the abundance of \\(y\\).\n\rExplorations of the trends\rA measure of the strength of the relationship can be obtained according to:\n\\[ R^2 = 1 - \\frac{\\text{RSS}_{model}}{\\text{RSS}_{null}}\\]\n\u0026gt; Xmat \u0026lt;- model.matrix(~x, data=dat)\r\u0026gt; #expected values on a log scale\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; #expected value on response scale\r\u0026gt; lambda \u0026lt;- exp(eta)\r\u0026gt; #calculate the raw SS residuals\r\u0026gt; SSres \u0026lt;- apply((-1*(sweep(lambda,2,dat$y,\u0026#39;-\u0026#39;)))^2,1,sum)\r\u0026gt; SSres.null \u0026lt;- sum((dat$y - mean(dat$y))^2)\r\u0026gt; #OR \u0026gt; SSres.null \u0026lt;- crossprod(dat$y - mean(dat$y))\r\u0026gt; #calculate the model r2\r\u0026gt; 1-mean(SSres)/SSres.null\r[,1]\r[1,] 0.6569594\rConclusions: \\(65\\)% of the variation in \\(y\\) abundance can be explained by its relationship with \\(x\\). We can also do it directly into JAGS.\n\u0026gt; dat.list \u0026lt;- with(dat,list(Y=y, X=x,N=nrow(dat)))\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ for (i in 1:N) {\r+ Y[i] ~ dpois(lambda[i])\r+ eta[i] \u0026lt;- beta0 + beta1*X[i]\r+ log(lambda[i]) \u0026lt;- eta[i]\r+ res[i] \u0026lt;- Y[i] - lambda[i]\r+ resnull[i] \u0026lt;- Y[i] - meanY\r+ }\r+ meanY \u0026lt;- mean(Y)\r+ beta0 ~ dnorm(0,1.0E-06)\r+ beta1 ~ dnorm(0,1.0E-06)\r+ RSS \u0026lt;- sum(res^2)\r+ RSSnull \u0026lt;- sum(resnull^2)\r+ r2 \u0026lt;- 1-RSS/RSSnull\r+ } + \u0026quot;\r\u0026gt; \u0026gt; writeLines(modelString, con=\u0026#39;modelpois_disp_r2.txt\u0026#39;)\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;,\u0026#39;r2\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.P.jags \u0026lt;- jags(data=dat.list,model.file=\u0026#39;modelpois_disp_r2.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 2\rTotal graph size: 150\rInitializing model\r\u0026gt; \u0026gt; print(dat.P.jags)\rInference for Bugs model at \u0026quot;modelpois_disp_r2.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 0.547 0.257 0.024 0.379 0.556 0.721 1.032 1.001 3900\rbeta1 0.112 0.019 0.077 0.100 0.112 0.125 0.150 1.001 7000\rr2 0.655 0.057 0.510 0.640 0.672 0.690 0.701 1.001 10000\rdeviance 88.383 2.776 86.372 86.904 87.733 89.122 93.692 1.003 6200\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.9 and DIC = 92.2\rDIC is an estimate of expected predictive error (lower deviance is better).\rFinally, we will create a summary plot.\n\u0026gt; par(mar = c(4, 5, 0, 0))\r\u0026gt; plot(y ~ x, data = dat, type = \u0026quot;n\u0026quot;, ann = F, axes = F)\r\u0026gt; points(y ~ x, data = dat, pch = 16)\r\u0026gt; xs \u0026lt;- seq(min(dat$x,na.rm=TRUE),max(dat$x,na.rm=TRUE), l = 1000)\r\u0026gt; Xmat \u0026lt;- model.matrix(~xs)\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; ys \u0026lt;- exp(eta)\r\u0026gt; library(plyr)\r\u0026gt; library(coda)\r\u0026gt; data.tab \u0026lt;- adply(ys,2,function(x) {\r+ data.frame(Median=median(x), HPDinterval(as.mcmc(x)))\r+ })\r\u0026gt; data.tab \u0026lt;- cbind(x=xs,data.tab)\r\u0026gt; points(Median ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;)\r\u0026gt; lines(lower ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;, lty = 2)\r\u0026gt; lines(upper ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;, lty = 2)\r\u0026gt; \u0026gt; axis(1)\r\u0026gt; mtext(\u0026quot;X\u0026quot;, 1, cex = 1.5, line = 3)\r\u0026gt; axis(2, las = 2)\r\u0026gt; mtext(\u0026quot;Abundance of Y\u0026quot;, 2, cex = 1.5, line = 3)\r\u0026gt; box(bty = \u0026quot;l\u0026quot;)\r\rFull log-likelihood function\rNow lets try it by specifying log-likelihood and the zero trick. When applying this trick, we need to manually calculate the deviance as the inbuilt deviance will be based on the log-likelihood of estimating the zeros (as part of the zero trick) rather than the deviance of the intended model. The one advantage of the zero trick is that the Deviance and thus DIC, AIC provided by R2jags will be incorrect. Hence, they too need to be manually defined within JAGS I suspect that the AIC calculation I have used is incorrect.\n\u0026gt; Xmat \u0026lt;- model.matrix(~x, dat)\r\u0026gt; nX \u0026lt;- ncol(Xmat)\r\u0026gt; dat.list2 \u0026lt;- with(dat,list(Y=y, X=Xmat,N=nrow(dat), mu=rep(0,nX),\r+ Sigma=diag(1.0E-06,nX), zeros=rep(0,nrow(dat)), C=10000))\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ for (i in 1:N) {\r+ zeros[i] ~ dpois(zeros.lambda[i])\r+ zeros.lambda[i] \u0026lt;- -ll[i] + C + ll[i] \u0026lt;- Y[i]*log(lambda[i]) - lambda[i] - loggam(Y[i]+1)\r+ eta[i] \u0026lt;- inprod(beta[], X[i,])\r+ log(lambda[i]) \u0026lt;- eta[i]\r+ llm[i] \u0026lt;- Y[i]*log(meanlambda) - meanlambda - loggam(Y[i]+1)\r+ }\r+ meanlambda \u0026lt;- mean(lambda)\r+ beta ~ dmnorm(mu[],Sigma[,])\r+ dev \u0026lt;- sum(-2*ll)\r+ pD \u0026lt;- mean(dev)-sum(-2*llm)\r+ AIC \u0026lt;- min(dev+(2*pD))\r+ } + \u0026quot;\r\u0026gt; \u0026gt; writeLines(modelString, con=\u0026#39;modelpois_ll.txt\u0026#39;)\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta\u0026#39;,\u0026#39;dev\u0026#39;,\u0026#39;AIC\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.P.jags3 \u0026lt;- jags(data=dat.list2,model.file=\u0026#39;modelpois_ll.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 1\rTotal graph size: 353\rInitializing model\r\u0026gt; \u0026gt; print(dat.P.jags3)\rInference for Bugs model at \u0026quot;modelpois_ll.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75%\rAIC 13.728 4.725 9.624 10.548 11.952 15.158\rbeta[1] 0.481 0.259 -0.079 0.319 0.506 0.669\rbeta[2] 0.116 0.019 0.084 0.103 0.114 0.128\rdev 88.382 2.009 86.361 86.883 87.731 89.265\rdeviance 400088.382 2.009 400086.361 400086.883 400087.731 400089.265\r97.5% Rhat n.eff\rAIC 26.878 1.016 180\rbeta[1] 0.922 1.037 49\rbeta[2] 0.155 1.029 60\rdev 94.071 1.009 300\rdeviance 400094.071 1.000 1\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 2.0 and DIC = 400090.4\rDIC is an estimate of expected predictive error (lower deviance is better).\r\r\rNegative binomial\rThe following equations are provided since in Bayesian modelling, it is occasionally necessary to directly define the log-likelihood calculations (particularly for zero-inflated models and other mixture models).\n\\[ f(y \\mid r, p) = \\frac{\\Gamma(y+r)}{\\Gamma(r)\\Gamma(y+1)}p^r(1-p)^y,\\]\nwhere \\(p\\) is the probability of \\(y\\) successes until \\(r\\) failures. If, we make \\(p=\\frac{\\text{size}}{\\text{size}+\\mu}\\), then we can define the function in terms of \\(\\mu\\):\n\\[ \\mu = \\frac{r(1-p)}{p},\\]\nwhere \\(E[Y]=\\mu\\), \\(Var(Y)=\\mu + \\frac{\\mu^2}{r}\\).\nData generation\rLets say we wanted to model the abundance of an item (\\(y\\)) against a continuous predictor (\\(x\\)). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(37) #16 #35\r\u0026gt; #The number of samples\r\u0026gt; n.x \u0026lt;- 20\r\u0026gt; #Create x values that at uniformly distributed throughout the rate of 1 to 20\r\u0026gt; x \u0026lt;- sort(runif(n = n.x, min = 1, max =20))\r\u0026gt; mm \u0026lt;- model.matrix(~x)\r\u0026gt; intercept \u0026lt;- 0.6\r\u0026gt; slope=0.1\r\u0026gt; #The linear predictor\r\u0026gt; linpred \u0026lt;- mm %*% c(intercept,slope)\r\u0026gt; #Predicted y values\r\u0026gt; lambda \u0026lt;- exp(linpred)\r\u0026gt; #Add some noise and make binomial\r\u0026gt; y \u0026lt;- rnbinom(n=n.x, mu=lambda, size=1)\r\u0026gt; dat.nb \u0026lt;- data.frame(y,x)\rWhen counts are all very large (not close to \\(0\\)) and their ranges do not span orders of magnitude, they take on very Gaussian properties (symmetrical distribution and variance independent of the mean). Given that models based on the Gaussian distribution are more optimized and recognized than Generalized Linear Models, it can be prudent to adopt Gaussian models for such data. Hence it is a good idea to first explore whether a Poisson or Negative Binomial model is likely to be more appropriate than a standard Gaussian model. Recall from Poisson regression, there are five main potential models that we could consider fitting to these data.\n\u0026gt; hist(dat$x)\r\u0026gt; \u0026gt; #now for the scatterplot\r\u0026gt; plot(y~x, dat.nb, log=\u0026quot;y\u0026quot;)\r\u0026gt; with(dat.nb, lines(lowess(y~x)))\rConclusions: the predictor (\\(x\\)) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the scatterplot does not display major deviations from a straight line and thus linearity is satisfied. Violations of linearity could be addressed by either:\n\rdefine a non-linear linear predictor (such as a polynomial, spline or other non-linear function).\n\rtransform the scale of the predictor variables.\n\r\rAlthough we have already established that there are few zeros in the data (and thus overdispersion is unlikely to be an issue), we can also explore this by comparing the number of zeros in the data to the number of zeros that would be expected from a Poisson distribution with a mean equal to the mean count of the data.\n\u0026gt; #proportion of 0\u0026#39;s in the data\r\u0026gt; dat.nb.tab\u0026lt;-table(dat.nb$y==0)\r\u0026gt; dat.nb.tab/sum(dat.nb.tab)\rFALSE TRUE 0.95 0.05 \u0026gt; \u0026gt; #proportion of 0\u0026#39;s expected from a Poisson distribution\r\u0026gt; mu \u0026lt;- mean(dat.nb$y)\r\u0026gt; cnts \u0026lt;- rpois(1000, mu)\r\u0026gt; dat.nb.tabE \u0026lt;- table(cnts == 0)\r\u0026gt; dat.nb.tabE/sum(dat.nb.tabE)\rFALSE 1 \rIn the above, the value under FALSE is the proportion of non-zero values in the data and the value under TRUE is the proportion of zeros in the data. In this example, the proportion of zeros observed is similar to the proportion expected. Indeed, there was only a single zero observed. Hence it is likely that if there is overdispersion it is unlikely to be due to excessive zeros.\n\rModel fitting\r\\[ y_i \\sim \\text{NegBin}(p_i,r),\\]\nwhere \\(p_i=\\frac{r}{r+\\lambda_i}\\), with \\(\\log(\\lambda_i)=\\beta_0+\\beta_1x_{i}\\) and \\(\\beta_0,\\beta_1 \\sim N(0, 10000)\\), \\(r \\sim \\text{Unif}(0.001,1000)\\).\n\u0026gt; dat.nb.list \u0026lt;- with(dat.nb,list(Y=y, X=x,N=nrow(dat.nb)))\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ for (i in 1:N) {\r+ Y[i] ~ dnegbin(p[i],size)\r+ p[i] \u0026lt;- size/(size+lambda[i])\r+ log(lambda[i]) \u0026lt;- beta0 + beta1*X[i]\r+ }\r+ beta0 ~ dnorm(0,1.0E-06)\r+ beta1 ~ dnorm(0,1.0E-06)\r+ size ~ dunif(0.001,1000)\r+ theta \u0026lt;- pow(1/mean(p),2)\r+ scaleparam \u0026lt;- mean((1-p)/p) + } + \u0026quot;\r\u0026gt; writeLines(modelString, con=\u0026#39;modelnbin.txt\u0026#39;)\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;, \u0026#39;size\u0026#39;,\u0026#39;theta\u0026#39;,\u0026#39;scaleparam\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.NB.jags \u0026lt;- jags(data=dat.nb.list,model.file=\u0026#39;modelnbin.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 3\rTotal graph size: 157\rInitializing model\r\rModel evaluation\r\u0026gt; denplot(dat.NB.jags, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;,\u0026quot;size\u0026quot;))\r\u0026gt; traplot(dat.NB.jags, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;,\u0026quot;size\u0026quot;))\r\u0026gt; \u0026gt; raftery.diag(as.mcmc(dat.NB.jags))\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 16 17518 3746 4.68 beta1 24 28713 3746 7.66 deviance 3 4198 3746 1.12 scaleparam 16 16290 3746 4.35 size 4 5038 3746 1.34 theta 16 16244 3746 4.34 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 18 20025 3746 5.35 beta1 24 21072 3746 5.63 deviance 3 4267 3746 1.14 scaleparam 18 19920 3746 5.32 size 3 4375 3746 1.17 theta 20 20682 3746 5.52 \u0026gt; \u0026gt; autocorr.diag(as.mcmc(dat.NB.jags))\rbeta0 beta1 deviance scaleparam size\rLag 0 1.000000000 1.000000000 1.000000000 1.00000000 1.000000000\rLag 1 0.855250119 0.856542892 0.566377262 0.33360033 0.684520361\rLag 5 0.519024321 0.521535488 0.163024546 0.07618281 0.220180993\rLag 10 0.276801196 0.280283232 0.025179110 0.02814049 0.039259726\rLag 50 -0.008060569 -0.004454124 -0.003876422 -0.01103395 0.006904325\rtheta\rLag 0 1.00000000\rLag 1 0.26024619\rLag 5 0.05872969\rLag 10 0.02940084\rLag 50 -0.01349378\rWe now explore the goodness of fit of the models via the residuals and deviance. We could calculate the Pearsons’s residuals within the JAGS model. Alternatively, we could use the parameters to generate the residuals outside of JAGS.\n\u0026gt; #extract the samples for the two model parameters\r\u0026gt; coefs \u0026lt;- dat.NB.jags$BUGSoutput$sims.matrix[,1:2]\r\u0026gt; size \u0026lt;- dat.NB.jags$BUGSoutput$sims.matrix[,\u0026#39;size\u0026#39;]\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data=dat.nb)\r\u0026gt; #expected values on a log scale\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; #expected value on response scale\r\u0026gt; lambda \u0026lt;- exp(eta)\r\u0026gt; varY \u0026lt;- lambda + (lambda^2)/size\r\u0026gt; #sweep across rows and then divide by lambda\r\u0026gt; Resid \u0026lt;- -1*sweep(lambda,2,dat.nb$y,\u0026#39;-\u0026#39;)/sqrt(varY)\r\u0026gt; #plot residuals vs expected values\r\u0026gt; plot(apply(Resid,2,mean)~apply(eta,2,mean))\rNow we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Negative binomial distribution matching that estimated by the model. Essentially this is estimating how well the Negative binomial distribution, the log-link function and the linear model approximates the observed data.\n\u0026gt; SSres\u0026lt;-apply(Resid^2,1,sum)\r\u0026gt; \u0026gt; #generate a matrix of draws from a negative binomial distribution\r\u0026gt; # the matrix is the same dimensions as pi and uses the probabilities of pi\r\u0026gt; YNew \u0026lt;- matrix(rnbinom(length(lambda),mu=lambda, size=size),nrow=nrow(lambda))\r\u0026gt; Resid1\u0026lt;-(lambda - YNew)/sqrt(varY)\r\u0026gt; SSres.sim\u0026lt;-apply(Resid1^2,1,sum)\r\u0026gt; mean(SSres.sim\u0026gt;SSres, na.rm = T)\r[1] 0.4163\rConclusions: the Bayesian p-value is approximately \\(0.5\\), suggesting that there is a good fit of the model to the data.\nUnfortunately, unlike with linear models (Gaussian family), the expected distribution of data (residuals) varies over the range of fitted values for numerous (often competing) ways that make diagnosing (and attributing causes thereof) miss-specified generalized linear models from standard residual plots very difficult. The use of standardized (Pearson) residuals or deviance residuals can partly address this issue, yet they still do not offer completely consistent diagnoses across all issues (miss-specified model, over-dispersion, zero-inflation). An alternative approach is to use simulated data from the model posteriors to calculate an empirical cumulative density function from which residuals are are generated as values corresponding to the observed data along the density function.\n\u0026gt; #extract the samples for the two model parameters\r\u0026gt; coefs \u0026lt;- dat.NB.jags$BUGSoutput$sims.matrix[,1:2]\r\u0026gt; size \u0026lt;- dat.NB.jags$BUGSoutput$sims.matrix[,\u0026#39;size\u0026#39;]\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data=dat.nb)\r\u0026gt; #expected values on a log scale\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; #expected value on response scale\r\u0026gt; lambda \u0026lt;- exp(eta)\r\u0026gt; \u0026gt; simRes \u0026lt;- function(lambda, data,n=250, plot=T, family=\u0026#39;negbin\u0026#39;, size=NULL) {\r+ require(gap)\r+ N = nrow(data)\r+ sim = switch(family,\r+ \u0026#39;poisson\u0026#39; = matrix(rpois(n*N,apply(lambda,2,mean)),ncol=N, byrow=TRUE),\r+ \u0026#39;negbin\u0026#39; = matrix(MASS:::rnegbin(n*N,apply(lambda,2,mean),size),ncol=N, byrow=TRUE)\r+ )\r+ a = apply(sim + runif(n,-0.5,0.5),2,ecdf)\r+ resid\u0026lt;-NULL\r+ for (i in 1:nrow(data)) resid\u0026lt;-c(resid,a[[i]](data$y[i] + runif(1 ,-0.5,0.5)))\r+ if (plot==T) {\r+ par(mfrow=c(1,2))\r+ gap::qqunif(resid,pch = 2, bty = \u0026quot;n\u0026quot;,\r+ logscale = F, col = \u0026quot;black\u0026quot;, cex = 0.6, main = \u0026quot;QQ plot residuals\u0026quot;,\r+ cex.main = 1, las=1)\r+ plot(resid~apply(lambda,2,mean), xlab=\u0026#39;Predicted value\u0026#39;, ylab=\u0026#39;Standardized residual\u0026#39;, las=1)\r+ }\r+ resid\r+ }\r\u0026gt; \u0026gt; simRes(lambda,dat.nb, family=\u0026#39;negbin\u0026#39;, size=mean(size))\r [1] 0.368 0.944 0.456 0.788 0.148 0.928 0.136 0.704 0.164 0.800 0.500 0.464\r[13] 0.100 0.216 0.680 0.212 0.000 0.676 0.924 0.852\rThe trend (black symbols) in the qq-plot does not appear to be overly non-linear (matching the ideal red line well), suggesting that the model is not overdispersed. The spread of standardized (simulated) residuals in the residual plot do not appear overly non-uniform. That is there is not trend in the residuals. Furthermore, there is not a concentration of points close to \\(1\\) or \\(0\\) (which would imply overdispersion).\n\rExploring the model parameters\rIf there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics. As with most Bayesian models, it is best to base conclusions on medians rather than means.\n\u0026gt; print(dat.NB.jags)\rInference for Bugs model at \u0026quot;modelnbin.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 0.731 0.395 -0.023 0.470 0.717 0.984 1.534 1.001 10000\rbeta1 0.097 0.032 0.034 0.077 0.098 0.118 0.158 1.001 10000\rscaleparam 2.787 1.756 0.704 1.670 2.412 3.444 7.089 1.001 10000\rsize 3.255 2.190 1.055 1.941 2.697 3.853 9.050 1.001 10000\rtheta 12.548 12.474 2.669 5.892 9.157 14.790 43.249 1.001 10000\rdeviance 113.053 2.691 110.093 111.115 112.352 114.190 120.305 1.002 2000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.6 and DIC = 116.7\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; adply(dat.NB.jags$BUGSoutput$sims.matrix, 2, function(x) {\r+ data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\r+ })\rX1 Median Mean lower upper lower.1\r1 beta0 0.71693048 0.73121205 -0.05129743 1.5032427 0.4523583\r2 beta1 0.09800852 0.09730699 0.03509028 0.1591976 0.0789372\r3 deviance 112.35178835 113.05255254 109.86520971 118.4814291 110.0898498\r4 scaleparam 2.41198253 2.78665197 0.33094006 5.9583607 1.2865037\r5 size 2.69653197 3.25545915 0.68960555 7.2146030 1.4202953\r6 theta 9.15704708 12.54776430 1.61632232 32.9116959 3.6489231\rupper.1\r1 0.9610028\r2 0.1201659\r3 112.4668566\r4 2.8677393\r5 2.9988148\r6 10.3646959\r\u0026gt; \u0026gt; #on original scale\r\u0026gt; adply(exp(dat.NB.jags$BUGSoutput$sims.matrix[,1:2]), 2, function(x) {\r+ data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\r+ })\rX1 Median Mean lower upper lower.1 upper.1\r1 beta0 2.048137 2.249960 0.8335273 4.249614 1.340384 2.309801\r2 beta1 1.102972 1.102753 1.0357132 1.172570 1.080463 1.125944\rConclusions: We would reject the null hypothesis of no effect of \\(x\\) on \\(y\\). An increase in x is associated with a significant linear increase (positive slope) in the abundance of \\(y\\). Every \\(1\\) unit increase in \\(x\\) results in a log \\(0.09\\) unit increase in \\(y\\). We usually express this in terms of abundance rather than log abundance, so every \\(1\\) unit increase in \\(x\\) results in a ($e^{ 0.09} = 1.02 $) \\(1.02\\) unit increase in the abundance of \\(y\\).\n\rExplorations of the trends\rA measure of the strength of the relationship can be obtained according to:\n\\[ R^2 = 1 - \\frac{\\text{RSS}_{model}}{\\text{RSS}_{null}}\\]\n\u0026gt; Xmat \u0026lt;- model.matrix(~x, data=dat.nb)\r\u0026gt; #expected values on a log scale\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; #expected value on response scale\r\u0026gt; lambda \u0026lt;- exp(eta)\r\u0026gt; #calculate the raw SS residuals\r\u0026gt; SSres \u0026lt;- apply((-1*(sweep(lambda,2,dat.nb$y,\u0026#39;-\u0026#39;)))^2,1,sum)\r\u0026gt; SSres.null \u0026lt;- sum((dat.nb$y - mean(dat.nb$y))^2)\r\u0026gt; #OR \u0026gt; SSres.null \u0026lt;- crossprod(dat.nb$y - mean(dat.nb$y))\r\u0026gt; #calculate the model r2\r\u0026gt; 1-mean(SSres)/SSres.null\r[,1]\r[1,] 0.270553\rConclusions: \\(27\\)% of the variation in \\(y\\) abundance can be explained by its relationship with \\(x\\). We can also do it directly into JAGS.\nFinally, we will create a summary plot.\n\u0026gt; par(mar = c(4, 5, 0, 0))\r\u0026gt; plot(y ~ x, data = dat.nb, type = \u0026quot;n\u0026quot;, ann = F, axes = F)\r\u0026gt; points(y ~ x, data = dat.nb, pch = 16)\r\u0026gt; xs \u0026lt;- seq(min(dat.nb$x,na.rm=TRUE),max(dat.nb$x,na.rm=TRUE), l = 1000)\r\u0026gt; Xmat \u0026lt;- model.matrix(~xs)\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; ys \u0026lt;- exp(eta)\r\u0026gt; library(plyr)\r\u0026gt; library(coda)\r\u0026gt; data.tab \u0026lt;- adply(ys,2,function(x) {\r+ data.frame(Median=median(x), HPDinterval(as.mcmc(x)))\r+ })\r\u0026gt; data.tab \u0026lt;- cbind(x=xs,data.tab)\r\u0026gt; points(Median ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;)\r\u0026gt; lines(lower ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;, lty = 2)\r\u0026gt; lines(upper ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;, lty = 2)\r\u0026gt; \u0026gt; axis(1)\r\u0026gt; mtext(\u0026quot;X\u0026quot;, 1, cex = 1.5, line = 3)\r\u0026gt; axis(2, las = 2)\r\u0026gt; mtext(\u0026quot;Abundance of Y\u0026quot;, 2, cex = 1.5, line = 3)\r\u0026gt; box(bty = \u0026quot;l\u0026quot;)\r\rFull log-likelihood function\rNow lets try it by specifying log-likelihood and the zero trick. When applying this trick, we need to manually calculate the deviance as the inbuilt deviance will be based on the log-likelihood of estimating the zeros (as part of the zero trick) rather than the deviance of the intended model. The one advantage of the zero trick is that the Deviance and thus DIC, AIC provided by R2jags will be incorrect. Hence, they too need to be manually defined within JAGS I suspect that the AIC calculation I have used is incorrect.\n\u0026gt; Xmat \u0026lt;- model.matrix(~x, dat.nb)\r\u0026gt; nX \u0026lt;- ncol(Xmat)\r\u0026gt; dat.nb.list2 \u0026lt;- with(dat.nb,list(Y=y, X=Xmat,N=nrow(dat.nb), mu=rep(0,nX),\r+ Sigma=diag(1.0E-06,nX), zeros=rep(0,nrow(dat.nb)), C=10000))\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ for (i in 1:N) {\r+ zeros[i] ~ dpois(zeros.lambda[i])\r+ zeros.lambda[i] \u0026lt;- -ll[i] + C + ll[i] \u0026lt;- loggam(Y[i]+size) - loggam(Y[i]+1) - loggam(size) + size*(log(p[i]) - log(p[i]+1)) - + Y[i]*log(p[i]+1)\r+ p[i] \u0026lt;- size/lambda[i]\r+ eta[i] \u0026lt;- inprod(beta[], X[i,])\r+ log(lambda[i]) \u0026lt;- eta[i]\r+ }\r+ beta ~ dmnorm(mu[],Sigma[,])\r+ size ~ dunif(0.001,1000)\r+ dev \u0026lt;- sum(-2*ll)\r+ } + \u0026quot;\r\u0026gt; \u0026gt; writeLines(modelString, con=\u0026#39;modelnbin_ll.txt\u0026#39;)\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta\u0026#39;,\u0026#39;dev\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.NB.jags3 \u0026lt;- jags(data=dat.nb.list2,model.file=\u0026#39;modelnbin_ll.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 2\rTotal graph size: 453\rInitializing model\r\u0026gt; \u0026gt; print(dat.NB.jags3)\rInference for Bugs model at \u0026quot;modelnbin_ll.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75%\rbeta[1] 0.739 0.386 0.039 0.484 0.726 0.968\rbeta[2] 0.096 0.031 0.034 0.077 0.096 0.116\rdev 112.830 2.548 110.074 111.037 112.105 113.842\rdeviance 400112.830 2.548 400110.074 400111.037 400112.105 400113.842\r97.5% Rhat n.eff\rbeta[1] 1.536 1.015 160\rbeta[2] 0.153 1.010 230\rdev 119.701 1.002 1200\rdeviance 400119.701 1.000 1\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.2 and DIC = 400116.1\rDIC is an estimate of expected predictive error (lower deviance is better).\r\r\rZero inflated Poisson\rZero-Inflation Poisson (ZIP) mixture model is defined as:\n\\[ p(y_i \\mid \\theta, \\lambda) = \\begin{cases}\r\\theta + (1-\\theta) \\times \\text{Pois}(0 \\mid \\lambda) \u0026amp; \\text{if } y_i = 0\\\\ (1-\\theta) \\times \\text{Pois}(y_i \\mid \\lambda) \u0026amp; \\text{if } y_i \u0026gt; 0, \\end{cases}\\]\nwhere \\(\\theta\\) is the probability of false values (zeros). Hence there is essentially two models coupled together (a mixture model) to yield an overall probability:\n\rwhen an observed response is zero (\\(y_i=0\\)), it is the probability of getting a false value (zero) plus the probability of a true value multiplied probability of drawing a value of zero from a Poisson distribution of lambda.\n\rwhen an observed response is greater than \\(0\\), it is the probability of a true value multiplied probability of drawing that value from a Poisson distribution of lambda\n\r\rThe above formulation indicates the same \\(\\lambda\\) for both the zeros and non-zeros components. In the model of zero values, we are essentially investigating whether the likelihood of false zeros is related to the linear predictor and then the greater than zero model investigates whether the counts are related to the linear predictor. However, we are typically less interested in modelling determinants of false zeros. Indeed, it is better that the likelihood of false zeros be unrelated to the linear predictor. For example, if excess (false zeros) are due to issues of detectability (individuals are present, just not detected), it is better that the detectability is not related to experimental treatments. Ideally, any detectability issues should be equal across all treatment levels. The expected value of \\(Y\\) and the variance in \\(Y\\) for a ZIP model are:\n\\[ E[y_i] = \\lambda \\times (1-\\theta),\\]\n\\[ \\text{Var}(y_i) = \\lambda \\times (1-\\theta) \\times (1+\\theta \\times \\lambda^2)\\]\nData generation\rLets say we wanted to model the abundance of an item (\\(y\\)) against a continuous predictor (\\(x\\)). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(9) #34.5 #4 #10 #16 #17 #26\r\u0026gt; #The number of samples\r\u0026gt; n.x \u0026lt;- 20\r\u0026gt; #Create x values that at uniformly distributed throughout the rate of 1 to 20\r\u0026gt; x \u0026lt;- sort(runif(n = n.x, min = 1, max =20))\r\u0026gt; mm \u0026lt;- model.matrix(~x)\r\u0026gt; intercept \u0026lt;- 0.6\r\u0026gt; slope=0.1\r\u0026gt; #The linear predictor\r\u0026gt; linpred \u0026lt;- mm %*% c(intercept,slope)\r\u0026gt; #Predicted y values\r\u0026gt; lambda \u0026lt;- exp(linpred)\r\u0026gt; #Add some noise and make binomial\r\u0026gt; library(gamlss.dist)\r\u0026gt; #fixed latent binomial\r\u0026gt; y\u0026lt;- rZIP(n.x,lambda, 0.4)\r\u0026gt; #latent binomial influenced by the linear predictor \u0026gt; #y\u0026lt;- rZIP(n.x,lambda, 1-exp(linpred)/(1+exp(linpred)))\r\u0026gt; dat.zip \u0026lt;- data.frame(y,x)\r\u0026gt; \u0026gt; summary(glm(y~x, dat.zip, family=\u0026quot;poisson\u0026quot;))\rCall:\rglm(formula = y ~ x, family = \u0026quot;poisson\u0026quot;, data = dat.zip)\rDeviance Residuals: Min 1Q Median 3Q Max -4.6803 -2.0343 0.2895 1.2767 2.1153 Coefficients:\rEstimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 0.30200 0.25247 1.196 0.232 x 0.10691 0.01847 5.789 7.09e-09 ***\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r(Dispersion parameter for poisson family taken to be 1)\rNull deviance: 111.495 on 19 degrees of freedom\rResidual deviance: 79.118 on 18 degrees of freedom\rAIC: 126.64\rNumber of Fisher Scoring iterations: 5\r\u0026gt; \u0026gt; plot(glm(y~x, dat.zip, family=\u0026quot;poisson\u0026quot;))\r\u0026gt; \u0026gt; library(pscl)\r\u0026gt; summary(zeroinfl(y ~ x | 1, dist = \u0026quot;poisson\u0026quot;, data = dat.zip))\rCall:\rzeroinfl(formula = y ~ x | 1, data = dat.zip, dist = \u0026quot;poisson\u0026quot;)\rPearson residuals:\rMin 1Q Median 3Q Max -1.1625 -0.9549 0.1955 0.8125 1.4438 Count model coefficients (poisson with log link):\rEstimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 0.88696 0.28825 3.077 0.00209 ** x 0.09374 0.02106 4.450 8.58e-06 ***\rZero-inflation model coefficients (binomial with logit link):\rEstimate Std. Error z value Pr(\u0026gt;|z|)\r(Intercept) -0.4581 0.4725 -0.97 0.332\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Number of iterations in BFGS optimization: 8 Log-likelihood: -38.58 on 3 Df\r\u0026gt; \u0026gt; plot(resid(zeroinfl(y ~ x | 1, dist = \u0026quot;poisson\u0026quot;, data = dat.zip))~fitted(zeroinfl(y ~ x | 1, dist = \u0026quot;poisson\u0026quot;)))\r\u0026gt; \u0026gt; library(gamlss)\r\u0026gt; summary(gamlss(y~x,data=dat.zip, family=ZIP))\rGAMLSS-RS iteration 1: Global Deviance = 77.8434 GAMLSS-RS iteration 2: Global Deviance = 77.1603 GAMLSS-RS iteration 3: Global Deviance = 77.1598 ******************************************************************\rFamily: c(\u0026quot;ZIP\u0026quot;, \u0026quot;Poisson Zero Inflated\u0026quot;) Call: gamlss(formula = y ~ x, family = ZIP, data = dat.zip) Fitting method: RS() ------------------------------------------------------------------\rMu link function: log\rMu Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 0.88620 0.28819 3.075 0.006862 ** x 0.09387 0.02105 4.458 0.000345 ***\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r------------------------------------------------------------------\rSigma link function: logit\rSigma Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|)\r(Intercept) -0.4582 0.4725 -0.97 0.346\r------------------------------------------------------------------\rNo. of observations in the fit: 20 Degrees of Freedom for the fit: 3\rResidual Deg. of Freedom: 17 at cycle: 3 Global Deviance: 77.15981 AIC: 83.15981 SBC: 86.14701 ******************************************************************\r\u0026gt; \u0026gt; predict(gamlss(y~x,data=dat.zip, family=ZIP), se.fit=TRUE, what=\u0026quot;mu\u0026quot;)\rGAMLSS-RS iteration 1: Global Deviance = 77.8434 GAMLSS-RS iteration 2: Global Deviance = 77.1603 GAMLSS-RS iteration 3: Global Deviance = 77.1598 $fit\r1 2 3 4 5 6 7 8 0.9952647 1.0233409 1.1897115 1.2189891 1.3490911 1.3644351 1.3748867 1.5164069 9 10 11 12 13 14 15 16 1.6184170 1.6379917 1.6760055 1.6962694 1.7705249 1.8559090 1.8578379 1.8718850 17 18 19 20 2.1712345 2.5536059 2.7205304 2.7472964 $se.fit\r1 2 3 4 5 6 7 8 0.3826655 0.3724115 0.3131865 0.3031078 0.2601025 0.2552658 0.2520053 0.2112310 9 10 11 12 13 14 15 16 0.1872286 0.1833232 0.1765049 0.1733169 0.1646100 0.1610460 0.1610499 0.1611915 17 18 19 20 0.2055555 0.3248709 0.3848647 0.3947072 \r\rExploratory data analysis\rCheck the distribution of the \\(y\\) abundances.\n\u0026gt; hist(dat.zip$y)\r\u0026gt; \u0026gt; boxplot(dat.zip$y, horizontal=TRUE)\r\u0026gt; rug(jitter(dat.zip$y))\rThere is definitely signs of non-normality that would warrant Poisson models. Further to that, there appears to be a large number of zeros that are likely to be the cause of overdispersion A zero-inflated Poisson model is likely to be one of the most effective for modeling these data. Lets now explore linearity by creating a histogram of the predictor variable (\\(x\\)). Note, it is difficult to directly assess issues of linearity. Indeed, a scatterplot with lowess smoother will be largely influenced by the presence of zeros. One possible way of doing so is to explore the trend in the non-zero data.\n\u0026gt; hist(dat.zip$x)\r\u0026gt; \u0026gt; #now for the scatterplot\r\u0026gt; plot(y~x, dat.zip)\r\u0026gt; with(subset(dat.zip,y\u0026gt;0), lines(lowess(y~x)))\rConclusions: the predictor (\\(x\\)) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the non-zero data cloud does not display major deviations from a straight line and thus linearity is likely to be satisfied. Violations of linearity (whilst difficult to be certain about due to the unknown influence of the zeros) could be addressed by either:\n\rdefine a non-linear linear predictor (such as a polynomial, spline or other non-linear function).\n\rtransform the scale of the predictor variables.\n\r\rAlthough we have already established that there are few zeros in the data (and thus overdispersion is unlikely to be an issue), we can also explore this by comparing the number of zeros in the data to the number of zeros that would be expected from a Poisson distribution with a mean equal to the mean count of the data.\n\u0026gt; #proportion of 0\u0026#39;s in the data\r\u0026gt; dat.zip.tab\u0026lt;-table(dat.zip$y==0)\r\u0026gt; dat.zip.tab/sum(dat.zip.tab)\rFALSE TRUE 0.6 0.4 \u0026gt; \u0026gt; #proportion of 0\u0026#39;s expected from a Poisson distribution\r\u0026gt; mu \u0026lt;- mean(dat.zip$y)\r\u0026gt; cnts \u0026lt;- rpois(1000, mu)\r\u0026gt; dat.zip.tabE \u0026lt;- table(cnts == 0)\r\u0026gt; dat.zip.tabE/sum(dat.zip.tabE)\rFALSE TRUE 0.982 0.018 \rIn the above, the value under FALSE is the proportion of non-zero values in the data and the value under TRUE is the proportion of zeros in the data. In this example, the proportion of zeros observed (\\(45\\)%) far exceeds that that would have been expected (\\(7.9\\)%). Hence it is highly likely that any models will be zero-inflated.\n\rModel fitting\r\\[ y_i \\sim \\text{ZIP}(\\lambda_i, \\theta),\\]\nwhere \\(\\text{logit}(\\theta) = \\gamma_0\\), \\(\\log(\\lambda_i)=\\eta_i\\), with \\(\\eta_i=\\beta_0+\\beta_1x_{i}\\) and \\(\\beta_0,\\beta_1,\\gamma_0 \\sim N(0, 10000)\\).\n\u0026gt; dat.zip.list \u0026lt;- with(dat.zip,list(Y=y, X=x,N=nrow(dat.nb), z=ifelse(y==0,0,1)))\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ for (i in 1:N) {\r+ z[i] ~ dbern(one.minus.theta)\r+ Y[i] ~ dpois(lambda[i])\r+ lambda[i] \u0026lt;- z[i]*eta[i]\r+ log(eta[i]) \u0026lt;- beta0 + beta1*X[i]\r+ }\r+ one.minus.theta \u0026lt;- 1-theta\r+ logit(theta) \u0026lt;- gamma0\r+ beta0 ~ dnorm(0,1.0E-06)\r+ beta1 ~ dnorm(0,1.0E-06)\r+ gamma0 ~ dnorm(0,1.0E-06)\r+ } + \u0026quot;\r\u0026gt; writeLines(modelString, con=\u0026#39;modelzip.txt\u0026#39;)\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;, \u0026#39;gamma0\u0026#39;,\u0026#39;theta\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.zip.jags \u0026lt;- jags(data=dat.zip.list,model.file=\u0026#39;modelzip.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 40\rUnobserved stochastic nodes: 3\rTotal graph size: 149\rInitializing model\r\rModel evaluation\r\u0026gt; denplot(dat.zip.jags, parms = c(\u0026#39;beta\u0026#39;, \u0026#39;gamma0\u0026#39;))\r\u0026gt; traplot(dat.zip.jags, parms = c(\u0026#39;beta\u0026#39;, \u0026#39;gamma0\u0026#39;))\r\u0026gt; \u0026gt; raftery.diag(as.mcmc(dat.zip.jags))\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 20 20276 3746 5.41 beta1 22 24038 3746 6.42 deviance 4 4636 3746 1.24 gamma0 5 5908 3746 1.58 theta 5 5908 3746 1.58 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 20 21336 3746 5.70 beta1 20 22636 3746 6.04 deviance 3 4267 3746 1.14 gamma0 5 6078 3746 1.62 theta 5 6078 3746 1.62 \u0026gt; \u0026gt; autocorr.diag(as.mcmc(dat.zip.jags))\rbeta0 beta1 deviance gamma0 theta\rLag 0 1.00000000 1.00000000 1.00000000 1.000000000 1.000000000\rLag 1 0.88627108 0.88426590 0.51799594 0.232408997 0.227686735\rLag 5 0.58998005 0.59775827 0.19471855 0.002321179 0.001571686\rLag 10 0.35846288 0.35888205 0.06697926 0.017561785 0.015598223\rLag 50 -0.01753582 -0.01936659 -0.01212528 0.022040872 0.021016755\r\rGoodness of fit\r\u0026gt; #extract the samples for the two model parameters\r\u0026gt; coefs \u0026lt;- dat.zip.jags$BUGSoutput$sims.matrix[,1:2]\r\u0026gt; theta \u0026lt;- dat.zip.jags$BUGSoutput$sims.matrix[,\u0026#39;theta\u0026#39;]\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data=dat.zip)\r\u0026gt; #expected values on a log scale\r\u0026gt; lambda\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; #expected value on response scale\r\u0026gt; eta \u0026lt;- exp(lambda)\r\u0026gt; expY \u0026lt;- sweep(eta,1,(1-theta),\u0026quot;*\u0026quot;)\r\u0026gt; varY \u0026lt;- eta+sweep(eta^2,1,theta,\u0026quot;*\u0026quot;)\r\u0026gt; varY \u0026lt;- sweep(varY,1,(1-theta),\u0026#39;*\u0026#39;)\r\u0026gt; #sweep across rows and then divide by lambda\r\u0026gt; Resid \u0026lt;- -1*sweep(expY,2,dat.zip$y,\u0026#39;-\u0026#39;)/sqrt(varY)\r\u0026gt; #plot residuals vs expected values\r\u0026gt; plot(apply(Resid,2,mean)~apply(eta,2,mean))\rNow we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Poisson distribution matching that estimated by the model. Essentially this is estimating how well the Poisson distribution, the log-link function and the linear model approximates the observed data. When doing so, we need to consider the expected value and variance of the zero-inflated poisson.\n\u0026gt; SSres\u0026lt;-apply(Resid^2,1,sum, na.rm=T)\r\u0026gt; \u0026gt; #generate a matrix of draws from a zero-inflated poisson (ZIP) distribution\r\u0026gt; # the matrix is the same dimensions as lambda\r\u0026gt; library(gamlss.dist)\r\u0026gt; #YNew \u0026lt;- matrix(rZIP(length(lambda),eta, theta),nrow=nrow(lambda))\r\u0026gt; lambda \u0026lt;- sweep(eta,1,ifelse(dat.zip$y==0,0,1),\u0026#39;*\u0026#39;)\r\u0026gt; YNew \u0026lt;- matrix(rpois(length(lambda),lambda),nrow=nrow(lambda))\r\u0026gt; Resid1\u0026lt;-(expY - YNew)/sqrt(varY)\r\u0026gt; SSres.sim\u0026lt;-apply(Resid1^2,1,sum)\r\u0026gt; mean(SSres.sim\u0026gt;SSres, na.rm = T)\r[1] 0.5619\rSince it is difficult to diagnose many issues from the typical residuals we will now explore simulated residuals.\n\u0026gt; #extract the samples for the two model parameters\r\u0026gt; coefs \u0026lt;- dat.zip.jags$BUGSoutput$sims.matrix[,1:2]\r\u0026gt; theta \u0026lt;- dat.zip.jags$BUGSoutput$sims.matrix[,\u0026#39;theta\u0026#39;]\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data=dat.zip)\r\u0026gt; #expected values on a log scale\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; #expected value on response scale\r\u0026gt; lambda \u0026lt;- exp(eta)\r\u0026gt; \u0026gt; simRes \u0026lt;- function(lambda, data,n=250, plot=T, family=\u0026#39;negbin\u0026#39;, size=NULL,theta=NULL) {\r+ require(gap)\r+ N = nrow(data)\r+ sim = switch(family,\r+ \u0026#39;poisson\u0026#39; = matrix(rpois(n*N,apply(lambda,2,mean)),ncol=N, byrow=TRUE),\r+ \u0026#39;negbin\u0026#39; = matrix(MASS:::rnegbin(n*N,apply(lambda,2,mean),size),ncol=N, byrow=TRUE),\r+ \u0026#39;zip\u0026#39; = matrix(gamlss.dist:::rZIP(n*N,apply(lambda,2,mean),theta),ncol=N, byrow=TRUE)\r+ )\r+ a = apply(sim + runif(n,-0.5,0.5),2,ecdf)\r+ resid\u0026lt;-NULL\r+ for (i in 1:nrow(data)) resid\u0026lt;-c(resid,a[[i]](data$y[i] + runif(1 ,-0.5,0.5)))\r+ if (plot==T) {\r+ par(mfrow=c(1,2))\r+ gap::qqunif(resid,pch = 2, bty = \u0026quot;n\u0026quot;,\r+ logscale = F, col = \u0026quot;black\u0026quot;, cex = 0.6, main = \u0026quot;QQ plot residuals\u0026quot;,\r+ cex.main = 1, las=1)\r+ plot(resid~apply(lambda,2,mean), xlab=\u0026#39;Predicted value\u0026#39;, ylab=\u0026#39;Standardized residual\u0026#39;, las=1)\r+ }\r+ resid\r+ }\r\u0026gt; \u0026gt; simRes(lambda,dat.zip, family=\u0026#39;zip\u0026#39;,theta=theta)\r [1] 0.718 0.212 0.106 0.050 0.476 0.778 0.248 0.060 0.878 0.704 0.090 0.890\r[13] 0.416 0.764 0.282 0.752 0.602 0.848 0.154 0.656\rThe trend (black symbols) in the qq-plot does not appear to be overly non-linear (matching the ideal red line well), suggesting that the model is not overdispersed. The spread of standardized (simulated) residuals in the residual plot do not appear overly non-uniform. That is there is not trend in the residuals. Furthermore, there is not a concentration of points close to \\(1\\) or \\(0\\) (which would imply overdispersion). Hence, once zero-inflation is accounted for, the model does not display overdispersion. Although there is a slight hint of non-linearity in that the residuals are high for low and high fitted values and lower in the middle, this might well be an artifact of the small data set size. By change, most of the observed values in the middle range of the predictor were zero.\n\rExploring the model parameters\rIf there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics. As with most Bayesian models, it is best to base conclusions on medians rather than means.\n\u0026gt; print(dat.zip.jags)\rInference for Bugs model at \u0026quot;modelzip.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 0.930 0.282 0.365 0.742 0.933 1.128 1.468 1.003 860\rbeta1 0.090 0.021 0.049 0.076 0.090 0.104 0.132 1.002 1400\rgamma0 -0.420 0.458 -1.349 -0.722 -0.417 -0.110 0.459 1.001 10000\rtheta 0.401 0.105 0.206 0.327 0.397 0.472 0.613 1.001 7600\rdeviance 80.674 2.501 77.856 78.867 80.008 81.801 87.064 1.001 10000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.1 and DIC = 83.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; adply(dat.zip.jags$BUGSoutput$sims.matrix, 2, function(x) {\r+ data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\r+ })\rX1 Median Mean lower upper lower.1\r1 beta0 0.93334635 0.92997411 0.37821529 1.4774189 0.7530788\r2 beta1 0.09005537 0.09005981 0.04864163 0.1313802 0.0749821\r3 deviance 80.00841187 80.67383245 77.63946567 85.5798539 77.8273778\r4 gamma0 -0.41667356 -0.41996110 -1.34903991 0.4586909 -0.7013225\r5 theta 0.39731301 0.40136809 0.19516803 0.6012233 0.3173026\rupper.1\r1 1.13629442\r2 0.10333030\r3 80.10544007\r4 -0.09258569\r5 0.46170971\r\u0026gt; \u0026gt; #on original scale\r\u0026gt; adply(exp(dat.zip.jags$BUGSoutput$sims.matrix[,1:2]), 2, function(x) {\r+ data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\r+ })\rX1 Median Mean lower upper lower.1 upper.1\r1 beta0 2.543005 2.636434 1.280362 4.056990 1.911083 2.853498\r2 beta1 1.094235 1.094483 1.049844 1.140401 1.077865 1.108858\rConclusions: We would reject the null hypothesis of no effect of \\(x\\) on \\(y\\). An increase in \\(x\\) is associated with a significant linear increase (positive slope) in the abundance of \\(y\\). Every \\(1\\) unit increase in \\(x\\) results in a log \\(0.09\\) unit increase in \\(y\\). We usually express this in terms of abundance rather than log abundance, so every \\(1\\) unit increase in \\(x\\) results in a (\\(e^{0.09}=1.1\\)) \\(1.1\\) unit increase in the abundance of \\(y\\).\n\rExplorations of the trends\rA measure of the strength of the relationship can be obtained according to:\n\\[ R^2 = 1 - \\frac{\\text{RSS}_{model}}{\\text{RSS}_{null}}\\]\nAlternatively, we could use McFadden’s psuedo\n\\[ R^2 = 1- \\frac{LL(Model_{full})}{LL(Model_{reduced}}\\]\n\u0026gt; Xmat \u0026lt;- model.matrix(~x, dat=dat.zip)\r\u0026gt; #expected values on a log scale\r\u0026gt; neta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; #expected value on response scale\r\u0026gt; eta \u0026lt;- exp(neta)\r\u0026gt; lambda \u0026lt;- sweep(eta,2,ifelse(dat.zip$y==0,0,1),\u0026#39;*\u0026#39;)\r\u0026gt; theta \u0026lt;- dat.zip.jags$BUGSoutput$sims.matrix[,\u0026#39;theta\u0026#39;]\r\u0026gt; expY \u0026lt;- sweep(lambda,2,1-theta,\u0026#39;*\u0026#39;)\r\u0026gt; #calculate the raw SS residuals\r\u0026gt; SSres \u0026lt;- apply((-1*(sweep(expY,2,dat.zip$y,\u0026#39;-\u0026#39;)))^2,1,sum)\r\u0026gt; mean(SSres)\r[1] 168.3814\r\u0026gt; \u0026gt; SSres.null \u0026lt;- sum((dat.zip$y - mean(dat.zip$y))^2)\r\u0026gt; #calculate the model r2\r\u0026gt; 1-mean(SSres)/SSres.null\r[1] 0.5977029\rConclusions: \\(50\\)% of the variation in \\(y\\) abundance can be explained by its relationship with \\(x\\). Finally, we will create a summary plot.\n\u0026gt; par(mar = c(4, 5, 0, 0))\r\u0026gt; plot(y ~ x, data = dat.zip, type = \u0026quot;n\u0026quot;, ann = F, axes = F)\r\u0026gt; points(y ~ x, data = dat.zip, pch = 16)\r\u0026gt; xs \u0026lt;- seq(min(dat.zip$x,na.rm=TRUE),max(dat.zip$x,na.rm=TRUE), l = 1000)\r\u0026gt; Xmat \u0026lt;- model.matrix(~xs)\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; ys \u0026lt;- exp(eta)\r\u0026gt; library(plyr)\r\u0026gt; library(coda)\r\u0026gt; data.tab \u0026lt;- adply(ys,2,function(x) {\r+ data.frame(Median=median(x), HPDinterval(as.mcmc(x)))\r+ })\r\u0026gt; data.tab \u0026lt;- cbind(x=xs,data.tab)\r\u0026gt; points(Median ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;)\r\u0026gt; lines(lower ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;, lty = 2)\r\u0026gt; lines(upper ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;, lty = 2)\r\u0026gt; \u0026gt; axis(1)\r\u0026gt; mtext(\u0026quot;X\u0026quot;, 1, cex = 1.5, line = 3)\r\u0026gt; axis(2, las = 2)\r\u0026gt; mtext(\u0026quot;Abundance of Y\u0026quot;, 2, cex = 1.5, line = 3)\r\u0026gt; box(bty = \u0026quot;l\u0026quot;)\r\rFull log-likelihood function\rNow lets try it by specifying log-likelihood and the zero trick. When applying this trick, we need to manually calculate the deviance as the inbuilt deviance will be based on the log-likelihood of estimating the zeros (as part of the zero trick) rather than the deviance of the intended model. The one advantage of the zero trick is that the Deviance and thus DIC, AIC provided by R2jags will be incorrect. Hence, they too need to be manually defined within JAGS I suspect that the AIC calculation I have used is incorrect.\n\u0026gt; Xmat \u0026lt;- model.matrix(~x, dat.zip)\r\u0026gt; nX \u0026lt;- ncol(Xmat)\r\u0026gt; dat.zip.list2 \u0026lt;- with(dat.zip,list(Y=y, X=Xmat,N=nrow(dat.zip), mu=rep(0,nX),\r+ Sigma=diag(1.0E-06,nX), zeros=rep(0,nrow(dat)), C=10000))\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ for (i in 1:N) {\r+ zeros[i] ~ dpois(zeros.lambda[i])\r+ zeros.lambda[i] \u0026lt;- -ll[i] + C + ll[i] \u0026lt;- Y[i]*log(lambda[i]) - lambda[i] - loggam(Y[i]+1)\r+ eta[i] \u0026lt;- inprod(beta[], X[i,])\r+ log(lambda[i]) \u0026lt;- eta[i]\r+ llm[i] \u0026lt;- Y[i]*log(meanlambda) - meanlambda - loggam(Y[i]+1)\r+ }\r+ meanlambda \u0026lt;- mean(lambda)\r+ beta ~ dmnorm(mu[],Sigma[,])\r+ dev \u0026lt;- sum(-2*ll)\r+ pD \u0026lt;- mean(dev)-sum(-2*llm)\r+ AIC \u0026lt;- min(dev+(2*pD))\r+ } + \u0026quot;\r\u0026gt; \u0026gt; writeLines(modelString, con=\u0026#39;modelzip_ll.txt\u0026#39;)\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta\u0026#39;,\u0026#39;dev\u0026#39;,\u0026#39;AIC\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.ZIP.jags3 \u0026lt;- jags(data=dat.zip.list2,model.file=\u0026#39;modelzip_ll.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 1\rTotal graph size: 328\rInitializing model\r\u0026gt; \u0026gt; print(dat.ZIP.jags3 )\rInference for Bugs model at \u0026quot;modelzip_ll.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75%\rAIC 61.488 3.844 57.991 58.846 60.144 62.785\rbeta[1] 0.329 0.225 -0.122 0.176 0.331 0.475\rbeta[2] 0.104 0.017 0.070 0.093 0.104 0.116\rdev 124.472 1.801 122.700 123.170 123.871 125.221\rdeviance 400124.472 1.801 400122.700 400123.170 400123.871 400125.221\r97.5% Rhat n.eff\rAIC 72.257 1.089 35\rbeta[1] 0.785 1.071 67\rbeta[2] 0.137 1.051 69\rdev 129.054 1.042 53\rdeviance 400129.054 1.000 1\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 1.6 and DIC = 400126.1\rDIC is an estimate of expected predictive error (lower deviance is better).\r\r\rZero inflated Negative Binomial\rData generation\rLets say we wanted to model the abundance of an item (\\(y\\)) against a continuous predictor (\\(x\\)). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(37) #34.5 #4 #10 #16 #17 #26\r\u0026gt; #The number of samples\r\u0026gt; n.x \u0026lt;- 20\r\u0026gt; #Create x values that at uniformly distributed throughout the rate of 1 to 20\r\u0026gt; x \u0026lt;- sort(runif(n = n.x, min = 1, max =20))\r\u0026gt; mm \u0026lt;- model.matrix(~x)\r\u0026gt; intercept \u0026lt;- 0.6\r\u0026gt; slope=0.1\r\u0026gt; #The linear predictor\r\u0026gt; linpred \u0026lt;- mm %*% c(intercept,slope)\r\u0026gt; #Predicted y values\r\u0026gt; lambda \u0026lt;- exp(linpred)\r\u0026gt; #Add some noise and make binomial\r\u0026gt; library(gamlss.dist)\r\u0026gt; #fixed latent binomial\r\u0026gt; y\u0026lt;- rZINBI(n.x,lambda, 0.4)\r\u0026gt; #latent binomial influenced by the linear predictor \u0026gt; #y\u0026lt;- rZINB(n.x,lambda, 1-exp(linpred)/(1+exp(linpred)))\r\u0026gt; dat.zinb \u0026lt;- data.frame(y,x)\r\u0026gt; \u0026gt; summary(dat.glm.nb\u0026lt;-glm.nb(y~x, dat.zinb))\rCall:\rglm.nb(formula = y ~ x, data = dat.zinb, init.theta = 0.4646673144, link = log)\rDeviance Residuals: Min 1Q Median 3Q Max -1.3578 -1.3455 -0.5069 0.3790 1.1809 Coefficients:\rEstimate Std. Error z value Pr(\u0026gt;|z|)\r(Intercept) 0.914191 0.796804 1.147 0.251\rx 0.009149 0.067713 0.135 0.893\r(Dispersion parameter for Negative Binomial(0.4647) family taken to be 1)\rNull deviance: 20.303 on 19 degrees of freedom\rResidual deviance: 20.282 on 18 degrees of freedom\rAIC: 90.365\rNumber of Fisher Scoring iterations: 1\rTheta: 0.465 Std. Err.: 0.218 2 x log-likelihood: -84.365 \u0026gt; \u0026gt; plot(glm.nb(y~x, dat.zinb))\r\u0026gt; \u0026gt; library(pscl)\r\u0026gt; summary(dat.zeroinfl\u0026lt;-zeroinfl(y ~ x | 1, dist = \u0026quot;negbin\u0026quot;, data = dat.zinb))\rCall:\rzeroinfl(formula = y ~ x | 1, data = dat.zinb, dist = \u0026quot;negbin\u0026quot;)\rPearson residuals:\rMin 1Q Median 3Q Max -0.9609 -0.9268 -0.4446 1.0425 1.7556 Count model coefficients (negbin with log link):\rEstimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 0.92733 0.32507 2.853 0.00433 **\rx 0.06870 0.02755 2.494 0.01263 * Log(theta) 3.36066 3.59739 0.934 0.35020 Zero-inflation model coefficients (binomial with logit link):\rEstimate Std. Error z value Pr(\u0026gt;|z|)\r(Intercept) -0.2250 0.4559 -0.494 0.622\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Theta = 28.8082 Number of iterations in BFGS optimization: 17 Log-likelihood: -38.54 on 4 Df\r\u0026gt; \u0026gt; plot(resid(zeroinfl(y ~ x | 1, dist = \u0026quot;negbin\u0026quot;, data = dat.zinb))~fitted(zeroinfl(y ~ x | 1, dist = \u0026quot;negbin\u0026quot;)))\r\u0026gt; \u0026gt; vuong(dat.glm.nb, dat.zeroinfl)\rVuong Non-Nested Hypothesis Test-Statistic: (test-statistic is asymptotically distributed N(0,1) under the\rnull that the models are indistinguishible)\r-------------------------------------------------------------\rVuong z-statistic H_A p-value\rRaw -1.2809521 model2 \u0026gt; model1 0.10011\rAIC-corrected -0.9296587 model2 \u0026gt; model1 0.17627\rBIC-corrected -0.7547616 model2 \u0026gt; model1 0.22520\r\u0026gt; \u0026gt; library(gamlss)\r\u0026gt; summary(gamlss(y~x, data=dat.zinb, family=\u0026#39;ZINBI\u0026#39;))\rGAMLSS-RS iteration 1: Global Deviance = 81.436 GAMLSS-RS iteration 2: Global Deviance = 78.1917 GAMLSS-RS iteration 3: Global Deviance = 77.0798 GAMLSS-RS iteration 4: Global Deviance = 77.0726 GAMLSS-RS iteration 5: Global Deviance = 77.0725 ******************************************************************\rFamily: c(\u0026quot;ZINBI\u0026quot;, \u0026quot;Zero inflated negative binomial type I\u0026quot;) Call: gamlss(formula = y ~ x, family = \u0026quot;ZINBI\u0026quot;, data = dat.zinb) Fitting method: RS() ------------------------------------------------------------------\rMu link function: log\rMu Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 0.92653 0.32502 2.851 0.0116 *\rx 0.06880 0.02753 2.499 0.0237 *\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r------------------------------------------------------------------\rSigma link function: log\rSigma Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|)\r(Intercept) -3.363 3.603 -0.933 0.365\r------------------------------------------------------------------\rNu link function: logit Nu Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|)\r(Intercept) -0.2250 0.4559 -0.494 0.628\r------------------------------------------------------------------\rNo. of observations in the fit: 20 Degrees of Freedom for the fit: 4\rResidual Deg. of Freedom: 16 at cycle: 5 Global Deviance: 77.0725 AIC: 85.0725 SBC: 89.05543 ******************************************************************\r\u0026gt; \u0026gt; summary(gamlss(y~x, nu.fo=y~x,data=dat.zinb, family=\u0026#39;ZINBI\u0026#39;))\rGAMLSS-RS iteration 1: Global Deviance = 78.2478 GAMLSS-RS iteration 2: Global Deviance = 74.2622 GAMLSS-RS iteration 3: Global Deviance = 73.8329 GAMLSS-RS iteration 4: Global Deviance = 73.8305 GAMLSS-RS iteration 5: Global Deviance = 73.8305 ******************************************************************\rFamily: c(\u0026quot;ZINBI\u0026quot;, \u0026quot;Zero inflated negative binomial type I\u0026quot;) Call: gamlss(formula = y ~ x, nu.formula = y ~ x, family = \u0026quot;ZINBI\u0026quot;, data = dat.zinb) Fitting method: RS() ------------------------------------------------------------------\rMu link function: log\rMu Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 0.84246 0.35267 2.389 0.0305 *\rx 0.07481 0.02933 2.550 0.0222 *\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r------------------------------------------------------------------\rSigma link function: log\rSigma Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|)\r(Intercept) -2.982 2.844 -1.048 0.311\r------------------------------------------------------------------\rNu link function: logit Nu Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|)\r(Intercept) -2.4988 1.8283 -1.367 0.192\rx 0.1996 0.1417 1.408 0.179\r------------------------------------------------------------------\rNo. of observations in the fit: 20 Degrees of Freedom for the fit: 5\rResidual Deg. of Freedom: 15 at cycle: 5 Global Deviance: 73.83046 AIC: 83.83046 SBC: 88.80912 ******************************************************************\r\rExploratory data analysis\rCheck the distribution of the \\(y\\) abundances.\n\u0026gt; hist(dat.zinb$y)\r\u0026gt; \u0026gt; boxplot(dat.zinb$y, horizontal=TRUE)\r\u0026gt; rug(jitter(dat.zinb$y))\rThere is definitely signs of non-normality that would warrant Poisson or negative binomial models. Further to that, there appears to be a large number of zeros and a possible clumpiness that are likely to be the cause of overdispersion A zero-inflated negative binomial model is likely to be one of the most effective for modeling these data. Lets now explore linearity by creating a histogram of the predictor variable (\\(x\\)). Note, it is difficult to directly assess issues of linearity. Indeed, a scatterplot with lowess smoother will be largely influenced by the presence of zeros. One possible way of doing so is to explore the trend in the non-zero data.\n\u0026gt; hist(dat.zinb$x)\r\u0026gt; \u0026gt; #now for the scatterplot\r\u0026gt; plot(y~x, dat.zinb, log=\u0026quot;y\u0026quot;)\r\u0026gt; with(subset(dat.zinb,y\u0026gt;0), lines(lowess(y~x)))\rConclusions: the predictor (\\(x\\)) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the non-zero data cloud does not display major deviations from a straight line and thus linearity is likely to be satisfied. Violations of linearity (whilst difficult to be certain about due to the unknown influence of the zeros) could be addressed by either:\n\rdefine a non-linear linear predictor (such as a polynomial, spline or other non-linear function).\n\rtransform the scale of the predictor variables.\n\r\rAlthough we have already established that there are few zeros in the data (and thus overdispersion is unlikely to be an issue), we can also explore this by comparing the number of zeros in the data to the number of zeros that would be expected from a Poisson distribution with a mean equal to the mean count of the data.\n\u0026gt; #proportion of 0\u0026#39;s in the data\r\u0026gt; dat.zinb.tab\u0026lt;-table(dat.zinb$y==0)\r\u0026gt; dat.zinb.tab/sum(dat.zinb.tab)\rFALSE TRUE 0.55 0.45 \u0026gt; \u0026gt; #proportion of 0\u0026#39;s expected from a Poisson distribution\r\u0026gt; mu \u0026lt;- mean(dat.zinb$y)\r\u0026gt; v \u0026lt;- var(dat.zinb$y)\r\u0026gt; size \u0026lt;- mu + (mu^2)/v\r\u0026gt; cnts \u0026lt;- rnbinom(1000, mu=mu, size=size)\r\u0026gt; dat.zinb.tabE \u0026lt;- table(cnts == 0)\r\u0026gt; dat.zinb.tabE/sum(dat.zinb.tabE)\rFALSE TRUE 0.861 0.139 \rIn the above, the value under FALSE is the proportion of non-zero values in the data and the value under TRUE is the proportion of zeros in the data. In this example, the proportion of zeros observed (\\(45\\)%) far exceeds that that would have been expected (\\(14\\)%). Hence it is highly likely that any models will be zero-inflated.\n\rModel fitting\r\\[ y_i \\sim \\text{ZINB}(\\lambda_i, \\theta),\\]\nwhere \\(\\text{logit}(\\theta) = \\gamma_0\\), \\(\\log(\\lambda_i)=\\eta_i\\), with \\(\\eta_i=\\beta_0+\\beta_1x_{i}\\) and \\(\\beta_0,\\beta_1,\\gamma_0 \\sim N(0, 10000)\\).\n\u0026gt; dat.zinb.list \u0026lt;- with(dat.zinb,list(Y=y, X=x,N=nrow(dat.zinb),z=ifelse(y==0,0,1)))\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ for (i in 1:N) {\r+ z[i] ~ dbern(psi.min)\r+ Y[i] ~ dnegbin(p[i],size)\r+ p[i] \u0026lt;- size/(size+mu.eff[i])\r+ mu.eff[i] \u0026lt;- z[i]*mu[i]\r+ eta[i] \u0026lt;- beta0 + beta1*X[i]\r+ log(mu[i]) \u0026lt;- eta[i]\r+ }\r+ gamma ~ dnorm(0,0.001)\r+ psi.min \u0026lt;- min(0.9999, max(0.00001, (1-psi)))\r+ logit(psi) \u0026lt;- max(-20, min(20, gamma))\r+ size ~ dunif(0.001, 5)\r+ theta \u0026lt;- pow(1/mean(p),2)\r+ beta0 ~ dnorm(0,1.0E-06)\r+ beta1 ~ dnorm(0,1.0E-06)\r+ } + \u0026quot;\r\u0026gt; writeLines(modelString, con=\u0026#39;modelzinb.txt\u0026#39;)\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;, \u0026#39;size\u0026#39;, \u0026#39;theta\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.zinb.jags \u0026lt;- jags(data=dat.zinb.list,model.file=\u0026#39;modelzinb.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 40\rUnobserved stochastic nodes: 4\rTotal graph size: 205\rInitializing model\r\u0026gt; \u0026gt; print(dat.zinb.jags)\rInference for Bugs model at \u0026quot;modelzinb.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 0.971 0.460 0.055 0.678 0.963 1.273 1.868 1.007 250\rbeta1 0.067 0.042 -0.016 0.039 0.066 0.094 0.151 1.008 200\rsize 3.501 1.015 1.389 2.763 3.644 4.351 4.935 1.001 10000\rtheta 2.200 0.367 1.721 1.937 2.115 2.371 3.145 1.001 3300\rdeviance 82.769 2.843 79.139 80.663 82.139 84.254 89.891 1.001 10000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 4.0 and DIC = 86.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rModel evaluation\r\u0026gt; denplot(dat.zinb.jags, parms = c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;, \u0026#39;size\u0026#39;, \u0026#39;theta\u0026#39;))\r\u0026gt; traplot(dat.zinb.jags, parms = c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;, \u0026#39;size\u0026#39;, \u0026#39;theta\u0026#39;))\r\u0026gt; \u0026gt; raftery.diag(as.mcmc(dat.zinb.jags))\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 15 16236 3746 4.33 beta1 14 15725 3746 4.20 deviance 3 4484 3746 1.20 size 5 5771 3746 1.54 theta 3 4338 3746 1.16 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 27 27564 3746 7.36 beta1 18 21057 3746 5.62 deviance 3 4410 3746 1.18 size 5 5771 3746 1.54 theta 2 3995 3746 1.07 \u0026gt; \u0026gt; autocorr.diag(as.mcmc(dat.zinb.jags))\rbeta0 beta1 deviance size theta\rLag 0 1.00000000 1.00000000 1.00000000 1.000000000 1.000000000\rLag 1 0.82187294 0.82300377 0.55172222 0.391115803 0.387289605\rLag 5 0.44679310 0.44494849 0.13559995 0.045297725 0.067379632\rLag 10 0.19928140 0.20123773 0.05371302 0.008341721 0.013304093\rLag 50 -0.04037202 -0.04473554 -0.02496182 0.011474420 0.007333003\r\rGoodness of fit\r\u0026gt; #extract the samples for the two model parameters\r\u0026gt; coefs \u0026lt;- dat.zinb.jags$BUGSoutput$sims.matrix[,1:2]\r\u0026gt; theta \u0026lt;- dat.zinb.jags$BUGSoutput$sims.matrix[,\u0026#39;theta\u0026#39;]\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data=dat.zinb)\r\u0026gt; #expected values on a log scale\r\u0026gt; lambda\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; #expected value on response scale\r\u0026gt; eta \u0026lt;- exp(lambda)\r\u0026gt; expY \u0026lt;- sweep(eta,1,(1-theta),\u0026quot;*\u0026quot;)\r\u0026gt; varY \u0026lt;- eta+sweep(eta^2,1,theta,\u0026quot;*\u0026quot;)\r\u0026gt; head(varY)\r1 2 3 4 5 6 7 8\r[1,] 10.844323 13.189501 15.47499 15.73287 18.21519 26.87133 28.14742 29.27065\r[2,] 71.832694 61.952112 54.97632 54.30484 48.72535 36.70113 35.49495 34.51074\r[3,] 24.764991 24.273552 23.88302 23.84316 23.49392 22.60135 22.49799 22.41131\r[4,] 6.397443 8.786249 11.40150 11.71375 14.89149 28.26188 30.51610 32.55772\r[5,] 27.048585 28.561484 29.85015 29.98628 31.21706 34.70423 35.14294 35.51685\r[6,] 32.911549 36.163316 39.03708 39.34619 42.18864 50.70280 51.82155 52.78337\r9 10 11 12 13 14 15\r[1,] 32.48606 39.45733 45.43874 50.02645 59.31437 71.66019 73.00520\r[2,] 32.02933 27.89750 25.25717 23.61192 20.97369 18.40930 18.17586\r[3,] 22.18262 21.76433 21.46712 21.26761 20.92017 20.54281 20.50616\r[4,] 38.69312 53.42044 67.53693 79.24793 105.19992 144.10581 148.63604\r[5,] 36.53055 38.49254 39.97741 41.01961 42.92731 45.14296 45.36660\r[6,] 55.42928 60.70818 64.84042 67.81058 73.39529 80.11924 80.81199\r16 17 18 19 20\r[1,] 89.37583 90.29710 93.03697 99.45044 121.73297\r[2,] 15.83105 15.72115 15.40547 14.72560 12.85289\r[3,] 20.11263 20.09293 20.03565 19.90863 19.52937\r[4,] 208.15726 211.74132 222.54395 248.66128 348.12988\r[5,] 47.86864 47.99890 48.38049 49.24196 51.94528\r[6,] 88.73695 89.15824 90.39740 93.22189 102.32692\r\u0026gt; \u0026gt; #varY \u0026lt;- sweep(varY,1,(1-theta),\u0026#39;*\u0026#39;)\r\u0026gt; #sweep across rows and then divide by lambda\r\u0026gt; Resid \u0026lt;- -1*sweep(expY,2,dat.zinb$y,\u0026#39;-\u0026#39;)/sqrt(varY)\r\u0026gt; #plot residuals vs expected values\r\u0026gt; plot(apply(Resid,2,mean)~apply(eta,2,mean))\rNow we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Poisson distribution matching that estimated by the model. Essentially this is estimating how well the Poisson distribution, the log-link function and the linear model approximates the observed data. When doing so, we need to consider the expected value and variance of the zero-inflated poisson.\n\u0026gt; SSres\u0026lt;-apply(Resid^2,1,sum, na.rm=T)\r\u0026gt; \u0026gt; #generate a matrix of draws from a zero-inflated poisson (ZINB) distribution\r\u0026gt; # the matrix is the same dimensions as lambda\r\u0026gt; library(gamlss.dist)\r\u0026gt; #YNew \u0026lt;- matrix(rZINB(length(lambda),eta, theta),nrow=nrow(lambda))\r\u0026gt; lambda \u0026lt;- sweep(eta,1,ifelse(dat.zinb$y==0,0,1),\u0026#39;*\u0026#39;)\r\u0026gt; YNew \u0026lt;- matrix(rpois(length(lambda),lambda),nrow=nrow(lambda))\r\u0026gt; Resid1\u0026lt;-(expY - YNew)/sqrt(varY)\r\u0026gt; SSres.sim\u0026lt;-apply(Resid1^2,1,sum)\r\u0026gt; mean(SSres.sim\u0026gt;SSres, na.rm = T)\r[1] 0.5212\r\rExploring the model parameters\rIf there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics. As with most Bayesian models, it is best to base conclusions on medians rather than means.\n\u0026gt; print(dat.zinb.jags)\rInference for Bugs model at \u0026quot;modelzinb.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 0.971 0.460 0.055 0.678 0.963 1.273 1.868 1.007 250\rbeta1 0.067 0.042 -0.016 0.039 0.066 0.094 0.151 1.008 200\rsize 3.501 1.015 1.389 2.763 3.644 4.351 4.935 1.001 10000\rtheta 2.200 0.367 1.721 1.937 2.115 2.371 3.145 1.001 3300\rdeviance 82.769 2.843 79.139 80.663 82.139 84.254 89.891 1.001 10000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 4.0 and DIC = 86.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; adply(dat.zinb.jags$BUGSoutput$sims.matrix, 2, function(x) {\r+ data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\r+ })\rX1 Median Mean lower upper lower.1\r1 beta0 0.96339931 0.97060322 0.05196655 1.8646388 0.68771701\r2 beta1 0.06565837 0.06658472 -0.01850221 0.1478933 0.03570031\r3 deviance 82.13938661 82.76912313 78.75619568 88.5891138 79.69050804\r4 size 3.64385931 3.50054311 1.63847682 4.9995959 3.62583688\r5 theta 2.11463918 2.19954948 1.65289052 2.9565781 1.83698278\rupper.1\r1 1.28169341\r2 0.09027889\r3 82.76458253\r4 4.98121591\r5 2.20696839\r\u0026gt; \u0026gt; #on original scale\r\u0026gt; adply(exp(dat.zinb.jags$BUGSoutput$sims.matrix[,1:2]), 2, function(x) {\r+ data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\r+ })\rX1 Median Mean lower upper lower.1 upper.1\r1 beta0 2.620590 2.935935 0.7910564 5.701997 1.628127 3.096623\r2 beta1 1.067862 1.069796 0.9816679 1.159389 1.036345 1.094479\rConclusions: We would reject the null hypothesis of no effect of \\(x\\) on \\(y\\). An increase in \\(x\\) is associated with a significant linear increase (positive slope) in the abundance of \\(y\\). Every \\(1\\) unit increase in \\(x\\) results in a log \\(0.06\\) unit increase in \\(y\\). We usually express this in terms of abundance rather than log abundance, so every \\(1\\) unit increase in \\(x\\) results in a (\\(e^{0.06}=1.07\\)) \\(1.07\\) unit increase in the abundance of \\(y\\).\n\r\rReferences\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1581732794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581732794,"objectID":"2b32f0b9d7527a4c47cff044f33742b2","permalink":"/jags/glm2-jags/glm2-jags/","publishdate":"2020-02-14T21:13:14-05:00","relpermalink":"/jags/glm2-jags/glm2-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","generalised linear models"],"title":"Generalised Linear Models part II - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","JAGS","generalised linear models"],"content":"\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rBefore discussing generalised linear models, we will first revise a couple of fundamental aspects of general linear models and in particular, how they restrict the usefulness of these models in clinical applications. General linear models provide a set of well adopted and recognised procedures for relating response variables to a linear combination of one or more continuous or categorical predictors (hence the “general”). Nevertheless, the reliability and applicability of such models are restricted by the degree to which the residuals conform to normality and the mean and variance are independent of one another. The general linear model essentially comprises three components.\n\\[ E[Y] = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p + \\epsilon.\\]\nThe Random (Stochastic) component that specifies the conditional distribution (Normal or Gaussian distribution) of the response variable. Whilst the mean of the normal distribution is assumed to vary as a function of the linear predictors (Systematic component - the regression equation), the variance is assumed to remain constant. Denoted \\(\\epsilon\\) in the above equation, the random component is more formally defined as \\(Y_i \\sim N(0, \\sigma^2)\\). That is, each value of \\(Y\\) (the response) is assumed to be drawn from a normal distribution with different means (\\(\\mu_i\\)) yet fixed variance (\\(\\sigma^2\\)).\n\rThe Systematic component that represents the linear combination of predictors (which can be categorical, continuous, polynomial or other contrasts) for a linear predictor. The linear predictor describes (predict) the “expected” mean and variability of the response(s) (which are assumed to follow normal distribution(s)).\n\rThe Link function which links the expected values of the response (Random component) to the linear combination of predictors (systematic component). For the normal (Gaussian) distribution, the link function is a the “identity” link (\\(\\mu_i\\)). That is:\n\r\r\\[ \\mu_i = \\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_px_{ip}\\]\nThere are many real situations for which the assumptions imposed by the normal distribution are unlikely to be satisfied. For example, if the measured response to a predictor treatment (such as nest parasite load) can only be binary (such as abandoned or not), then the differences between the observed and expected values (residuals) are unlikely to follow a normal distribution. Instead, in this case, they should follow a binomial distribution.\nOften response variables have a restricted range. For example a species may be either present or not present and thus the response is restricted to either \\(1\\) (present) or absent (\\(0\\)). Values less than \\(0\\) or greater than \\(1\\) are not logical. Similarly, the abundance of a species in a quadrat is bounded by a minimum value of zero - it is not possible to have fewer than zero individuals. Proportional abundances are also restricted to between \\(0\\) and \\(1\\) (or \\(100\\)). The normal distribution however, is valid for the range between positive and negative infinity (ie not restricted) and thus expected values of the linear predictor can be outside of the restricted range that naturally operates on the response variable. Hence, the normal distribution might not always represent a sensible probability model as it can predict values outside the logical range of the data. Furthermore, the as a result of these range restrictions, variance can be tied to the mean in that expected probabilities towards the extremes of the restricted range tend to have lower variability (as the lower or upper bounds of the probabilities are trunctated).\n\rData types\rResponse data can generally be classified into one of four levels\n\rNominal - responses are those that represent un-ordered categories For example, we could record the ‘preferred’ food choice of an animal as either “Fruit”, “Meat”, “Seeds” or “Leaves”. The spacing between categories is undetermined and responses are restricted to those options.\n\rOrdinal - responses are those that represent categories with sensible orders, yet undetermined spacing between categories. Likert scale questionnaire responses to questions such as “Rate the quality of your experience… on a scale of \\(1\\) to \\(5\\)” are a classic example. Categorized levels of a response (“High”, “Medium”,“Low”) would also be another example of an ordinal variable\n\rInterval - responses are those for which both the order and scale (spacing) are meaningful, yet multiplication is meaningless due to the arbitrary scale of the data (where zero does not refer to nothing). Temperature in degrees C is a good example of such a response (consider whether \\(-28\\) degrees \\(^\\star-1 = 28\\) degrees has a sensible interpretation).\n\rRatio - responses are those for which order, scale and zero are meaningful. For example a measurement scale such as length in millimeters or mass in grams.\n\r\r\rGLMs\rGeneralized linear models (GLM’s) extend the application range of linear modelling by accommodating non-stable variances as well as alternative exponential residual distributions (such as the binomial and Poisson distributions). GLMs have the same three components as general linear models (of which the systematic component is identical), yet a broader range of Random components are accommodated and thus alternative Link functions must also be possible.\n\rRandom component defines the exponential distribution (Gaussian, Poisson, binomial, gamma, and inverse Gaussian distributions) from which the responses are assumed to be drawn. These distributions are characterised by some function of the mean (canonical or location parameter) and a function of the variance (dispersion parameter). Note that for binomial and Poisson distributions, the dispersion parameter is \\(1\\), whereas for the Guassian (normal) distribution the dispersion parameter is the error variance and is assumed to be independent of the mean. The negative binomial distribution can also be treated as an exponential distribution if the dispersion parameter is fixed as a constant.\n\rSystematic component again defines the linear combination of predictors\n\rLink function, \\(g(\\mu)\\) links the systematic and random components. Although there are many commonly employed link functions, typically the exact form of the link function depends on the nature of the random response distribution. Some of the canonical (natural choice) link functions and distribution pairings that are suitable for different forms of generalized linear models are listed in the following table. The only real restriction on a link function is that it must preserve the order of values such that larger values are always larger than smaller values (be monotonic) and must yield derivatives that are legal throughout the entire range of the data.\n\r\r\rLink functions\rIn contrast to fitting linear models to transformations of the raw data, the link functions transform the curve predicted by the systematic component into a scale approximating that of the response.\n\rLogit. Log odds-ratio The slope parameter represents the rate of change in log odds-ratio per unit increase in a predictor.\n\rProbit. The probit transformation is the inverse cumulative distribution for the standard normal distribution and is useful when the response is likely to be a categorization of an otherwise continuous scale. So whilst measurements might be recorded on a categorical scale (either for convenience or because that is how they manifest), these measurements are a proxy for an underlying variable (latent variable) that is actually continuous. So if the purpose of the linear modeling is to predict the underlying latent variable, then probit regression is likely to be appropriate. The slope parameter represents the rate of change in response probability per unit increase in a predictor.\n\rComplementary log-log. The log-log transformation is useful for extremely asymmetrical distributions (notably survival analyses).\n\r\r\rEstimation\rThe generalized nature of GLM’s makes them incompatible with ordinary least squares model fitting procedures. Instead, parameter estimates and model fitting are typically achieved by maximum likelihood methods based on an iterative re-weighting algorithm (such as the Newton-Raphson algorithm). Essentially, the Newton-Raphson algorithm (also known as a scoring algorithm) fits a linear model to an adjusted response variable (transformed via the link function) using a set of weights and then iteratively re-fits the model with new sets of weights recalculated according to the fit of the previous iteration. For canonical link-distribution pairs (see the table above), the Newton-Raphson algorithm usually converges (arrives at a common outcome or equilibrium) very efficiently and reliably. The Newton-Raphson algorithm facilitates a unifying model fitting procedure across the family of exponential probability distributions thereby providing a means by which binary and count data can be incorporated into the suit of regular linear model designs. In fact, linear regression (including ANOVA, ANCOVA and other general linear models) can be considered a special form of GLM that features a normal distribution and identity link function and for which the maximum likelihood procedure has an exact solution. Notably, when variance is stable, both maximum likelihood and ordinary least squares yield very similar parameter estimates.\nTypical distributions used for GLMs include:\n\rGaussian.\n\rBinomial. Represents the number of successes out of \\(n\\) independent trials each with a set probability (typically \\(0.5\\))\n\rPoisson.\n\rNegative Binomial. Represents the number of failures out of a sequence of n independent trials before a success is obtained each with a set probability. Alternatively, a negative binomial can be defined in terms of its mean (\\(\\mu\\)) and dispersion parameter. The dispersion parameter can be used to adjust the variances independent of the mean and is therefore useful as an alternative to the Poisson distribution when there is evidence of overdispersion (dispersion parameter \\(\u0026gt;1\\)).\n\r\r\rDispersion\rThe variance of binomial or Poisson distributions is assumed to be related to the sample size and mean respectively, and thus, there is not a variance parameter in their definitions. In fact, the variance (or dispersion) parameter is fixed to \\(1\\). As a result, logistic/probit regression as well as Poisson regression and log-linear modelling assume that sample variances conform to the respective distribution definitions. However, it is common for individual sampling units (e.g. individuals) to co-vary such that other, unmeasured influences, increase (or less commonly, decrease) variability. For example, although a population sex ratio might be 1:1, male to female ratios within a clutch might be highly skewed towards one or other sex. Positive correlations cause greater variance (overdispersion) and result in deflated standard errors (and thus exaggerated levels of precision and higher Type I errors). Additionally, count data (for example number of fish per transect) can be overdispersed as a result of an unexpectedly high number of zero’s (zero inflated). In this case, the zeros arise for two reasons.\nGenuine zero values - zero fish counted because there were non present.\n\rFalse zeros - there were fish present, yet not detected (and thus not recorded).\n\r\rThe dispersion parameter (degree of variance inflation or over-dispersion) can be estimated by dividing either the Pearsons \\(\\chi^2\\) or the Deviance by the degrees of freedom, where \\(n\\) is the number of observations in p parameters). As a general rule, dispersion parameters approaching \\(2\\) (or \\(0.5\\)) indicate possible violations of this assumption (although large overdispersion parameters can also be the result of a poorly specified model or outliers). Where over (or under) dispersion is suspected to be an issue, the following options are available:\n\ruse quasibinomial and quasipoisson families can be used as alternatives to model the dispersion. These quasi-likelihood models derive the dispersion parameter (function of the variance) from the observed data and are useful when overdispersion is suspected to be caused by positive correlations or other unobserved sources of variance. Rather than assuming that the variance is fixed, quasi- models assume that variance is a linear (multiplicative) function of the mean. Test statistics from such models should be based on F-tests rather than chi-squared tests.\n\rfor count data, use a negative binomial as an alternative to a Poisson distribution. The negative binomial distribution also estimates the dispersion parameter and assumes that the variance is a quadratic function of the mean.\n\ruse zero-inflated binomial (ZIB) and zero-inflated poisson (ZIP) when overdispersion is suspected to be caused by excessive numbers of zeros.\n\r\r\rBinary data - logistic regression\rLogistic regression is a form of GLM that employs the logit-binomial link distribution canonical pairing to model the effects of one or more continuous or categorical (with dummy coding) predictor variables on a binary (dead/alive, presence/absence, etc) response variable. For example, we could investigate the relationship between salinity levels (salt concentration) and mortality of frogs. Similarly, we could model the presence of a species of bird as a function of habitat patch size, or nest predation (predated or not) as a function of the distance from vegetative cover. Consider the fictitious data presented in the following figure. Clearly, a regular simple linear model is inappropriate for modelling the probability of presence. Note that at very low and high levels of \\(X\\), the predicted probabilities (probabilities or proportions of the population) are less than zero and greater than one respectively - logically impossible outcomes. Note also, that the residuals cannot be drawn from a normal distribution, since for any value of \\(X\\), there are only two possible outcomes (\\(1\\) or \\(0\\)).\nThe logistic model (Figure c above) relating the probability (\\(\\pi(x)\\)) that the response (\\(y_i\\)) equals one (present) for a given level of \\(x_i\\) (patch size) is defined as:\n\\[ \\pi(x) = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}\\]\nAppropriately, since \\(e^{\\beta_0+\\beta_1x}\\) (the “natural constant” raised to a simple linear model) must evaluate to between 0 and infinity, the logistic model must asymptote towards (and is thus bounded by) zero and one. Alternatively (as described briefly above), the logit link function can be used to transform \\(\\pi(x)\\) such that the logistic model is expressed as the log odds (probability of one state relative to the alternative) against a familiar linear combination of the explanatory variables (as is linear regression).\n\\[ ln \\left( \\frac{\\pi(x)}{1-\\pi(x)} \\right) = \\beta_0 + \\beta_1x_i\\]\nAlthough the \\(\\beta_0\\) (\\(y\\)-intercept) parameter is interpreted similar to that of linear regression (albeit of little clinical interest), this is not the case for the slope parameter (\\(\\beta_1\\)). Rather than representing the rate of change in the response for a given change in the predictor, in logistic regression, \\(\\beta_1\\) represents the rate of change in the odds ratio (ratio of odds of an event at two different levels of a predictor) for a given unit change in the predictor. The exponentiated slope represents the odds ratio (\\(\\theta=e^{\\beta_1}\\)), the proportional rate at which the predicted odds change for a given unit change of the predictor.\nNull hypotheses\rAs with linear regression, a separate \\(H_0\\) is tested for each of the estimated model parameters:\n\r\\(H_0:\\beta_1=0\\) (the population slope - proportional rate of change in odds ratio). This test examines whether the log odds of an occurrence are independent of the predictor variable and thus whether or not there is likely to be a relationship between the response and predictor.\n\r\\(H_0:\\beta_0=0\\) (the population intercept equals zero). As stated previously, this is typically of little clinical interest.\n\r\rSimilar to linear regression, there are two ways of testing the main null hypotheses:\nParameter estimation approach. Maximum likelihood estimates of the parameters and their asymptoticd standard errors (\\(S_{b1}\\)) are used to calculate the Wald \\(t\\) (or \\(t\\)-ratio) statistic \\(W=\\frac{b_1}{S_{b1}}\\), which approximately follows a standard \\(z\\) distribution when the null hypothesis is true. The reliability of Wald tests diminishes substantially with small sample sizes. For such cases, the second option is therefore more appropriate.\n\r(log)-likelihood ratio tests approach. This approach essentially involves comparing the fit of models with (full) and without (reduced) the term of interest:\n\r\r\\[ \\text{logit}(\\pi) = \\beta_0 + \\beta_1x_1 \\;\\;\\; (\\text{full model})\\]\n\\[ \\text{logit}(\\pi) = \\beta_0 \\;\\;\\; (\\text{reduced model})\\]\nThe fit of any given model is measured via log-likelihood and the differences between the fit of two models is described by a likelihood ratio statistic (G2 \\(= 2\\)(log-likelihood reduced model - log-likelihood full model)). The G2 quantity is also known as deviance and is analogous to the residual sums of squares in a linear model. When the null hypothesis is true, the G2 statistic approximately follows a \\(\\chi^2\\) distribution with one degree of freedom. An analogue of the linear model \\(r^2\\) measure can be calculated as:\n\\[ r^2 = 1- \\frac{G^2_0}{G^2_1},\\]\nwhere \\(G^2_0\\) and \\(G^2_1\\) are the deviances due to the intercept and slope terms respectively. Analogous to the ANOVA table that partitions the total variation into components explained by each of the model terms (and the unexplained error), it is possible to construct a analysis of deviance table that partitions the deviance into components explained by each of the model terms.\n\r\rCount data - Poisson and log-linear models\rAnother form of data for which scale transformations are often unsuitable or unsuccessful are count data. Count data tend to follow a Poisson distribution (see here) and consequently, the mean and variance are usually related. Generalized linear models provide appropriate means to model count data according to two design contexts:\nas an alternative to linear regression for modeling count data against a linear combination of continuous and/or categorical predictor variables (Poisson regression)\n\ras an alternative to contingency tables in which the associations between categorical variables are explored (log-linear modelling)\n\r\rPoisson regression\nThe Poisson regression model is\n\\[ \\log(\\mu)=\\beta_0 + \\beta_1x_1+ \\ldots + \\beta_px_p,\\]\nwhere \\(\\log(\\mu)\\) is the link function used to link the mean of the Poisson response variable to the linear combination of predictor variables. Poisson regression otherwise shares null hypotheses, parameter estimation, model fitting and selection with logistic regression.\nLog-linear modelling\nContingency tables were introduced along with caveats regarding the reliability and interoperability of such analyses (particularly when expected proportions are small or for multi-way tables). In contrast to logistic and Poisson regression, all variables in a log-linear model do not empirically distinguish between response and predictor variables. Nevertheless, as in contingency tables, causality can be implied when logical and justified by interpretation. The saturated (or full) log-linear model resembles a multiway ANOVA model. The full and reduced log-linear models for a two factor design are:\n\\[ \\log(f_{ij}) = \\mu + \\gamma^A_i + \\gamma^B_j + \\gamma^{AB}_{ij} \\;\\;\\; (\\text{full model}),\\]\n\\[ \\log(f_{ij}) = \\mu + \\gamma^A_i + \\gamma^B_j \\;\\;\\; (\\text{reduced model})\\]\nwhere \\(\\log(f_{ij}\\) is the log link function, \\(\\mu\\) is the mean of the (log) of expected frequencies (\\(f_{ij}\\)) and \\(\\gamma^A_i\\) is the effect of the ith category of the variable (A), \\(\\gamma^B_j\\) is the effect of the \\(j\\)-th category of B and \\(\\gamma^{AB}_{ij}\\) is the interactive effect of each category combination on the (log) expected frequencies. Reduced models differ from full models in the absence of all higher order interaction terms. Comparing the fit of full and reduced models therefore provides a means of assessing the effect of the interaction. Whilst two-way tables contain only a single interaction term (and thus a single full and reduced model), multiway tables have multiple interactions. For example, a three-way table has a three way interaction (ABC) as well as three two-way interactions (AB, AC, BC). Consequently, there are numerous full and reduced models, each appropriate for different interaction terms. The following table indicates the association between null hypothesis and fitted models.\nNull hypothese\rConsistent with contingency table analysis, log-linear models test the null hypothesis (\\(H_0\\)) that the categorical variables are independent of (not associated with) one another. Such null hypotheses are tested by comparing the fit (deviance, G2) of full and reduced models. The G2 is compared to a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in degrees of freedom of the full and reduced models. Thereafter, odds ratios are useful for interpreting any lack of independence. For multi-way tables, there are multiple full and reduced models:\n\rComplete dependence: \\(H_0: ABC = 0\\). No three way interaction. Either no association (conditional independence) between each pair of variables, or else the patterns of associations (conditional dependencies) are the same for each level of the third. If this null hypothesis is rejected (\\(ABC \\neq 0\\)), the causes of lack of independence can be explored by examining the residuals or odds ratios. Alternatively, main effects tests (testing the effects of two-way interactions separately at each level of the third) can be performed. If the three-way interaction is not rejected (no three-way association), lower order interactions can be explored.\n\rConditional independence/dependence: if the three-way interaction is not rejected (no three-way association), lower order interactions can be explored.\n\r\\(H_0: AB=0\\) - A and B conditionally independent (not associated) within each level of C.\r\\(H_0: AC=0\\) - A and C conditionally independent (not associated) within each level of B.\r\\(H_0: BC=0\\) - B and C conditionally independent (not associated) within each level of A.\r\rMarginal independence:\n\r\\(H_0: AB=0\\) - no association between A and B pooling over C.\r\\(H_0: AC=0\\) - no association between A and C pooling over B.\r\\(H_0: BC=0\\) - no association between B and C pooling over A.\r\rComplete independence: If none of the two-way interactions are rejected (no two-way associations), complete independence (all two-way interactions equal zero) can be explored.\n\r\\(H_0: AB=AC=BC=0\\) - Each of the variables are completely independent of all the other variables.\r\r\rAnalysis of designs with more than three factors proceed similarly, starting with tests of higher order interactions and progressing to lower order interactions only in the absence of higher order interactions.\n\r\rAssumptions\rCompared to general linear models, the requirements of generalised linear models are less stringent. In particular, neither normality nor homoscedasticity are assumed. Nevertheless, to maximize the reliability of null hypotheses tests, the following assumptions do apply:\n\rall observations should be independent to ensure that the samples provide an unbiased estimate of the intended population.\n\rit is important to establish that no observations are overly influential. Most linear model influence (and outlier) diagnostics extend to generalized linear models and are taken from the final iteration of the weighted least squares algorithm. Useful diagnoses include:\nResiduals - there are numerous forms of residuals that have been defined for generalized linear models, each essentially being a variant on the difference between observed and predicted (influence in \\(y\\)-space) theme. Note that the residuals from logistic regression are difficult to interpret.\n\rLeverage - a measure of outlyingness and influence in \\(x\\)-space.\n\rDfbeta - an analogue of Cook’s D statistic which provides a standardized measure of the overall influence of observations on the parameter estimates and model fit.\n\r\ralthough linearity between the response and predictors is not assumed, the relationship between each of the predictors and the link function is assumed to be linear. This linearity can be examined via the following:\ngoodness-of-fit. For log-linear models, \\(\\chi^2\\) contingency tables can be performed, however due to the low reliability of such tests with small sample sizes, this is not an option for logistic regression with continuous predictor(s) (since each combination is typically unique and thus the expected values are always \\(1\\)).\n\rHosmer-Lemeshow (\\(\\hat{C}\\)). Data are aggregated into \\(10\\) groups or bins (either by cutting the data according to the predictor range or equal frequencies in each group) such that goodness-of-fit test is more reliable. Nevertheless, the Hosmer-Lemeshow statistic has low power and relies on the somewhat arbitrary bin sizes.\n\rle Cessie-van Houwelingen-Copas omnibus test. This is a goodness-of-fit test for binary data based on the smoothing of residuals.\n\rcomponent + residual (partial residual) plots. Non-linearity is diagnosed as a substantial deviation from a linear trend.\n\r\r\rNon-linearity can be dealt with either by transformation (of the predictor variable(s), fitting polynomial terms or via splines/generalised additive modelling (GAM) depending on the degree and nature of the non-linearity.\n\r(over or under) dispersion.\r\r\r\rData generation\rLogistic regression is a type of generalised linear model (GLM) that models a binary response against a linear predictor via a specific link function. The linear predictor is the typically a linear combination of effects parameters (e.g. \\(\\beta_0+\\beta_1x_1\\)). The role of the link function is to transform the expected values of the response \\(y\\) (which is on the scale of (\\(0,1\\)), as is the binomial distribution from which expectations are drawn) into the scale of the linear predictor (which is \\(-\\infty;\\infty\\)). GLM’s transform the expected values (via a link) whereas LM’s transform the observed data. Thus while GLM’s operate on the scale of the original data and yet also on a scale appropriate of the residuals, LM’s do neither. There are many ways (transformations) that can map values on the (\\(0,1\\)) scale into values on the (\\(-\\infty;\\infty\\)) scale, however, the three most common are:\n\rlogit: \\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right)\\) - log odds ratio.\n\rprobit: \\(\\phi^{-1}(\\pi)\\) where \\(\\phi^{-1}\\) is an inverse normal cumulative density function.\n\rcomplimentary log-log: \\(\\log(−\\log(1−\\pi))\\).\n\r\rLets say we wanted to model the presence/absence of an item (\\(y\\)) against a continuous predictor (\\(x\\)) As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(865)\r\u0026gt; #The number of samples\r\u0026gt; n.x \u0026lt;- 20\r\u0026gt; #Create x values that at uniformly distributed throughout the rate of 1 to 20\r\u0026gt; x \u0026lt;- sort(runif(n = n.x, min = 1, max =20))\r\u0026gt; #The slope is the rate of change in log odds ratio for each unit change in x\r\u0026gt; # the smaller the slope, the slower the change (more variability in data too)\r\u0026gt; slope=0.5\r\u0026gt; #Inflection point is where the slope of the line is greatest\r\u0026gt; #this is also the LD50 point\r\u0026gt; inflect \u0026lt;- 10\r\u0026gt; #Intercept (no interpretation)\r\u0026gt; intercept \u0026lt;- -1*(slope*inflect)\r\u0026gt; #The linear predictor\r\u0026gt; linpred \u0026lt;- intercept+slope*x\r\u0026gt; #Predicted y values\r\u0026gt; y.pred \u0026lt;- exp(linpred)/(1+exp(linpred))\r\u0026gt; #Add some noise and make binomial\r\u0026gt; n.y \u0026lt;-rbinom(n=n.x,20,p=0.9)\r\u0026gt; y\u0026lt;- rbinom(n = n.x,size=1, prob = y.pred)\r\u0026gt; dat \u0026lt;- data.frame(y,x)\rWith these sort of data, we are primarily interested in investigating whether there is a relationship between the binary response variable and the linear predictor (linear combination of one or more continuous or categorical predictors).\nExploratory data analysis\rSo lets explore linearity by creating a histogram of the predictor variable (\\(x\\)) and a scatterplot of the relationship between the response (\\(y\\)) and the predictor (\\(x\\))\n\u0026gt; hist(dat$x)\r\u0026gt; \u0026gt; #now for the scatterplot\r\u0026gt; plot(y~x, dat)\r\u0026gt; with(dat, lines(lowess(y~x)))\rConclusions: the predictor (\\(x\\)) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the scatterplot does not display major deviations from a standard sigmoidal curve and thus linearity is satisfied. Violations of linearity could be addressed by either:\n\rdefine a non-linear linear predictor (such as a polynomial, spline or other non-linear function).\n\rtransform the scale of the predictor variables.\n\r\r\rModel fitting\rEffects model\nNote that in order to prevent arithmetic overflows (particularly with the clog-log model, I am going to constrain the estimated linear predictor to between \\(-20\\) and \\(20\\). Values outside of this on a inverse-log scale are extremely small and huge respectively.\rI will demonstrate logistic regression with a range of possible link functions (each of which yield different parameter interpretations). Consider first the logit function:\n\\[ y \\sim \\text{Bern}(\\pi),\\]\nwhere \\(\\text{logit}(\\pi)=\\beta_0+\\beta_1x_1\\) and \\(\\beta_0,\\beta_1 \\sim N(0, 10000)\\).\n\u0026gt; modelString=\u0026quot;\r+ model{\r+ for (i in 1:N) {\r+ y[i] ~ dbern(p[i])\r+ logit(p[i]) \u0026lt;- max(-20,min(20,beta0+beta1*x[i]))\r+ }\r+ beta0 ~ dnorm(0,1.0E-06)\r+ beta1 ~ dnorm(0,1.0E-06)\r+ }\r+ \u0026quot;\r\u0026gt; writeLines(modelString, con=\u0026#39;modellogit.txt\u0026#39;)\r\u0026gt; \u0026gt; dat.list \u0026lt;- with(dat, list(y=y, x=x, N=nrow(dat)))\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; library(R2jags)\r\u0026gt; dat.logit.jags \u0026lt;- jags(data=dat.list,model.file=\u0026#39;modellogit.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 2\rTotal graph size: 147\rInitializing model\rSecond, we consider the probit function:\n\\[ y \\sim \\text{Bern}(\\pi),\\]\nwhere \\(\\text{probit}(\\pi)=\\beta_0+\\beta_1x_1\\) and \\(\\beta_0,\\beta_1 \\sim N(0, 10000)\\).\n\u0026gt; modelString2=\u0026quot;\r+ model{\r+ for (i in 1:N) {\r+ y[i] ~ dbern(p[i])\r+ probit(p[i]) \u0026lt;- max(-20,min(20,beta0+beta1*x[i]))\r+ }\r+ beta0 ~ dnorm(0,1.0E-06)\r+ beta1 ~ dnorm(0,1.0E-06)\r+ }\r+ \u0026quot;\r\u0026gt; writeLines(modelString2, con=\u0026#39;modelprobit.txt\u0026#39;)\r\u0026gt; \u0026gt; dat.list \u0026lt;- with(dat, list(y=y, x=x, N=nrow(dat)))\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.probit.jags \u0026lt;- jags(data=dat.list,model.file=\u0026#39;modelprobit.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 2\rTotal graph size: 147\rInitializing model\rFinally, the complementary log-log\n\\[ y \\sim \\text{Bern}(\\pi),\\]\nwhere \\(\\text{probit}(\\pi)=\\beta_0+\\beta_1x_1\\) and \\(\\beta_0,\\beta_1 \\sim N(0, 10000)\\).\n\u0026gt; modelString3=\u0026quot;\r+ model{\r+ for (i in 1:N) {\r+ y[i] ~ dbern(p[i])\r+ cloglog(p[i]) \u0026lt;- max(-20,min(20,beta0+beta1*x[i]))\r+ }\r+ beta0 ~ dnorm(0,1.0E-06)\r+ beta1 ~ dnorm(0,1.0E-06)\r+ }\r+ \u0026quot;\r\u0026gt; writeLines(modelString3, con=\u0026#39;modelcloglog.txt\u0026#39;)\r\u0026gt; \u0026gt; dat.list \u0026lt;- with(dat, list(y=y, x=x, N=nrow(dat)))\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.cloglog.jags \u0026lt;- jags(data=dat.list,model.file=\u0026#39;modelcloglog.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 2\rTotal graph size: 147\rInitializing model\rPrior to exploring the model parameters, it is prudent to confirm that the model did indeed fit the assumptions and was an appropriate fit to the data as well as that the MCMC sampling chain was adequately mixed and the retained samples independent. Whilst I will only demonstrate this for the logit model, the procedure would be identical for exploring the probit and clog-log models.\n\rModel evaluation\r\u0026gt; library(mcmcplots)\r\u0026gt; denplot(dat.logit.jags, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;))\r\u0026gt; traplot(dat.logit.jags, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;))\r\u0026gt; \u0026gt; raftery.diag(as.mcmc(dat.logit.jags))\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 50 54338 3746 14.50 beta1 36 39555 3746 10.60 deviance 4 4955 3746 1.32 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 30 31743 3746 8.47 beta1 40 52860 3746 14.10 deviance 8 10336 3746 2.76 \u0026gt; \u0026gt; autocorr.diag(as.mcmc(dat.logit.jags))\rbeta0 beta1 deviance\rLag 0 1.0000000 1.0000000 1.0000000\rLag 1 0.9816715 0.9811729 0.5841946\rLag 5 0.9190319 0.9197111 0.4477029\rLag 10 0.8458674 0.8477906 0.3948904\rLag 50 0.4300407 0.4306464 0.2065881\rIt seems that the level of auto-correlation at the nominated lag of \\(10\\) is extremely high. Ideally, the level of auto-correlation should be less than \\(0.1\\). To achieve this, we need a lag of \\(1000\\). Consequently, we will resample at a lag of \\(1000\\) and obviously we are going to need more iterations to ensure that we retain a large enough sample from which to derive estimates. In order to support a thinning rate of \\(1000\\), the number of iterations is going to need to be very high. Hence, the following might take considerable time to run.\n\u0026gt; dat.logit.jags \u0026lt;- jags(data=dat.list,model.file=\u0026#39;modellogit.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=100)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 2\rTotal graph size: 147\rInitializing model\r\u0026gt; \u0026gt; print(dat.logit.jags)\rInference for Bugs model at \u0026quot;modellogit.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded), n.thin = 100\rn.sims = 100 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 -16.51 9.652 -40.133 -20.118 -14.170 -9.587 -5.463 1.040 58\rbeta1 1.66 0.973 0.458 0.979 1.427 2.032 3.926 1.026 100\rdeviance 9.94 2.764 7.457 8.161 8.942 10.678 16.848 1.024 100\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.8 and DIC = 13.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; autocorr.diag(as.mcmc(dat.logit.jags))\rbeta0 beta1 deviance\rLag 0 1.0000000 1.0000000 1.0000000\rLag 100 0.4435502 0.4390086 0.1529258\rLag 500 0.1102886 0.1246140 0.1950554\rLag 1000 -0.1091505 -0.1008427 -0.1582021\rConclusions: the samples are now less auto-correlated and the chains are arguably mixed better. We now explore the goodness of fit of the models via the residuals and deviance. We could calculate the Pearsons’s residuals within the JAGS model. Alternatively, we could use the parameters to generate the residuals outside of JAGS.\n\u0026gt; library(boot)\r\u0026gt; coefs \u0026lt;- dat.logit.jags$BUGSoutput$sims.matrix[,1:2]\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data=dat)\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; pi \u0026lt;- inv.logit(eta)\r\u0026gt; #sweep across rows and then divide by pi\r\u0026gt; Resid \u0026lt;- -1*sweep(pi,2,dat$y,\u0026#39;-\u0026#39;)/sqrt(pi*(1-pi))\r\u0026gt; plot(apply(Resid,2,mean)~apply(eta,2,mean))\rNow we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Bernoulli distribution matching that estimated by the model. Essentially this is estimating how well the Bernoulli distribution and linear model approximates the observed data.\n\u0026gt; SSres\u0026lt;-apply(Resid^2,1,sum)\r\u0026gt; \u0026gt; #generate a matrix of draws from a binomial distribution\r\u0026gt; # the matrix is the same dimensions as pi and uses the probabilities of pi\r\u0026gt; YNew \u0026lt;- matrix(rbinom(length(pi),prob=pi,size=1),nrow=nrow(pi))\r\u0026gt; \u0026gt; Resid1\u0026lt;-(pi - YNew)/sqrt(pi*(1-pi))\r\u0026gt; SSres.sim\u0026lt;-apply(Resid1^2,1,sum)\r\u0026gt; mean(SSres.sim\u0026gt;SSres, na.rm = T)\r[1] 0.21875\rAlternatively, we could generate the new samples and calculate the sums squares of residuals etc all within JAGS.\n\u0026gt; dat.list \u0026lt;- with(dat, list(y=y, x=x, N=nrow(dat)))\r\u0026gt; modelString=\u0026quot;\r+ model{\r+ for (i in 1:N) {\r+ y[i] ~ dbern(p[i])\r+ logit(p[i]) \u0026lt;- max(-20,min(20,eta[i]))\r+ eta[i] \u0026lt;- beta0+beta1*x[i]\r+ YNew[i] ~dbern(p[i])\r+ varY[i] \u0026lt;- p[i]*(1-p[i])\r+ PRes[i] \u0026lt;- (y[i] - p[i]) / sqrt(varY[i])\r+ PResNew[i] \u0026lt;- (YNew[i] - p[i]) / sqrt(varY[i])\r+ D[i] \u0026lt;- pow(PRes[i],2)\r+ DNew[i] \u0026lt;- pow(PResNew[i],2)\r+ }\r+ Fit \u0026lt;- sum(D[1:N])\r+ FitNew \u0026lt;-sum(DNew[1:N]) + beta0 ~ dnorm(0,1.0E-06)\r+ beta1 ~ dnorm(0,1.0E-06)\r+ pvalue \u0026lt;- mean(FitNew\u0026gt;Fit)\r+ }\r+ \u0026quot;\r\u0026gt; writeLines(modelString, con=\u0026#39;modellogit_v2.txt\u0026#39;)\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;,\u0026#39;Fit\u0026#39;,\u0026#39;FitNew\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.logit.jags1 \u0026lt;- jags(data=dat.list,model.file=\u0026#39;modellogit_v2.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 20\rUnobserved stochastic nodes: 22\rTotal graph size: 343\rInitializing model\r\u0026gt; \u0026gt; print(dat.logit.jags1)\rInference for Bugs model at \u0026quot;modellogit_v2.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rFit 38.470 339.283 6.122 8.237 12.604 24.120 186.621 1.013 540\rFitNew 15.837 230.080 0.395 2.025 3.780 8.372 63.411 1.001 3800\rbeta0 -15.507 7.665 -35.657 -19.433 -13.831 -10.040 -4.873 1.024 660\rbeta1 1.570 0.791 0.501 0.991 1.390 1.979 3.656 1.017 10000\rdeviance 9.678 2.175 7.482 8.070 9.018 10.581 15.412 1.013 220\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 2.4 and DIC = 12.0\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; out \u0026lt;- dat.logit.jags1$BUGSoutput\r\u0026gt; mean(out$sims.list$FitNew \u0026gt; out$sims.list$Fit)\r[1] 0.1947\rConclusions: although the Bayesian p-value is quite a bit lower than \\(0.5\\), suggesting that there is more variability in the data than should be expected from this simple logistic regression model, this value is not any closer to \\(0\\) (a value that would indicate that the model does not fit the data at all well. Thus we might conclude that whilst not ideal, the model is adequate.\n\rExploring the model parameters\rIf there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics.\n\u0026gt; library(coda)\r\u0026gt; print(dat.logit.jags)\rInference for Bugs model at \u0026quot;modellogit.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded), n.thin = 100\rn.sims = 100 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 -16.51 9.652 -40.133 -20.118 -14.170 -9.587 -5.463 1.040 58\rbeta1 1.66 0.973 0.458 0.979 1.427 2.032 3.926 1.026 100\rdeviance 9.94 2.764 7.457 8.161 8.942 10.678 16.848 1.024 100\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.8 and DIC = 13.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; library(plyr)\r\u0026gt; adply(dat.logit.jags$BUGSoutput$sims.matrix[,1:2], 2, function(x) {\r+ data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\r+ })\rX1 Median Mean lower upper lower.1 upper.1\r1 beta0 -14.169526 -16.510277 -38.4322729 -2.190571 -15.809670 -6.767604\r2 beta1 1.427161 1.660376 0.3019023 3.728819 0.866335 1.791501\rConclusions: We would reject the null hypothesis (p\\(\u0026lt;0.05\\)). An increase in \\(x\\) is associated with a significant linear increase (positive slope) in log odds of y success. Every \\(1\\) unit increase in \\(x\\) results in a \\(0.86\\) unit increase in log odds-ratio. We usually express this in terms of odds-ratio rather than log odds-ratio, so every \\(1\\) unit increase in \\(x\\) results in a (\\(e^{0.86}=2.36\\)) \\(2.36\\) unit increase in odds-ratio.\n\rExplorations of the trends\rWe might also be interested in the LD50 - the value of \\(x\\) where the probability switches from favoring \\(1\\) to favoring \\(0\\). LD50 is calculated as:\n\\[ LD50 = - \\frac{\\text{intercept}}{\\text{slope}}\\]\n\u0026gt; summary(as.mcmc(-coefs[,1]/coefs[,2]))\rIterations = 1:100\rThinning interval = 1 Number of chains = 1 Sample size per chain = 100 1. Empirical mean and standard deviation for each variable,\rplus standard error of the mean:\rMean SD Naive SE Time-series SE 9.92488 0.84980 0.08498 0.06916 2. Quantiles for each variable:\r2.5% 25% 50% 75% 97.5% 7.737 9.460 9.894 10.448 11.538 \u0026gt; \u0026gt; #OR\r\u0026gt; LD50 \u0026lt;- -coefs[,1]/coefs[,2]\r\u0026gt; data.frame(Median=median(LD50), Mean=mean(LD50), HPDinterval(as.mcmc(LD50)), HPDinterval(as.mcmc(LD50),p=0.5))\rMedian Mean lower upper lower.1 upper.1\rvar1 9.894002 9.924877 7.930942 11.59808 9.547373 10.50285\rConclusions: the LD50 is \\(10.5\\). Finally, we will create a summary plot.\n\u0026gt; par(mar = c(4, 5, 0, 0))\r\u0026gt; plot(y ~ x, data = dat, type = \u0026quot;n\u0026quot;, ann = F, axes = F)\r\u0026gt; points(y ~ x, data = dat, pch = 16)\r\u0026gt; xs \u0026lt;- seq(0, 20, l = 1000)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~xs)\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; ys \u0026lt;- inv.logit(eta)\r\u0026gt; library(plyr)\r\u0026gt; data.tab \u0026lt;- adply(ys,2,function(x) {\r+ data.frame(Median=median(x), HPDinterval(as.mcmc(x)))\r+ })\r\u0026gt; data.tab \u0026lt;- cbind(x=xs,data.tab)\r\u0026gt; \u0026gt; points(Median ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;)\r\u0026gt; lines(lower ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;, lty = 2)\r\u0026gt; lines(upper ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;, lty = 2)\r\u0026gt; axis(1)\r\u0026gt; mtext(\u0026quot;X\u0026quot;, 1, cex = 1.5, line = 3)\r\u0026gt; axis(2, las = 2)\r\u0026gt; mtext(\u0026quot;Y\u0026quot;, 2, cex = 1.5, line = 3)\r\u0026gt; box(bty = \u0026quot;l\u0026quot;)\r\r\rGrouped binary data\rIn the previous demonstration, the response variable represented the state of a single item per level of the predictor variable (\\(x\\)). That single item could be observed having a value of either \\(1\\) or \\(0\\). Another common situation is to observe the number of items in one of two states (typically dead or alive) for each level of a treatment. For example, you could tally up the number of germinated and non-germinated seeds out of a bank of \\(10\\) seeds at each of \\(8\\) temperature or nutrient levels. Recall that the binomial distribution represents the density (probability) of all possible successes (germinations) out of a total of \\(N\\) items (seeds). Hence the binomial distribution is also a suitable error distribution for such grouped binary data. For this demonstration, we will model the number of successes against a uniformly distributed predictor (\\(x\\)). The number of trials in each group (level of the predictor) will vary slightly (yet randomly) so as to mimick complications that inevadably occur in real experiments.\n\u0026gt; set.seed(876)\r\u0026gt; #The number of levels of x\r\u0026gt; n.x \u0026lt;- 10\r\u0026gt; #Create x values that at uniformly distributed throughout the rate of 10 to 20\r\u0026gt; x \u0026lt;- sort(runif(n = n.x, min = 10, max =20))\r\u0026gt; #The slope is the rate of change in log odds ratio for each unit change in x\r\u0026gt; # the smaller the slope, the slower the change (more variability in data too)\r\u0026gt; slope=-.25\r\u0026gt; #Inflection point is where the slope of the line is greatest\r\u0026gt; #this is also the LD50 point\r\u0026gt; inflect \u0026lt;- 15\r\u0026gt; #Intercept (no interpretation)\r\u0026gt; intercept \u0026lt;- -1*(slope*inflect)\r\u0026gt; #The linear predictor\r\u0026gt; linpred \u0026lt;- intercept+slope*x\r\u0026gt; #Predicted y values\r\u0026gt; y.pred \u0026lt;- exp(linpred)/(1+exp(linpred))\r\u0026gt; #Add some noise and make binary (0\u0026#39;s and 1\u0026#39;s)\r\u0026gt; n.trial \u0026lt;- rbinom(n=n.x,20, prob=0.9)\r\u0026gt; success \u0026lt;- rbinom(n = n.x, size = n.trial,prob = y.pred)\r\u0026gt; failure \u0026lt;- n.trial - success\r\u0026gt; dat \u0026lt;- data.frame(success,failure,x)\rExploratory data analysis\rSo lets explore linearity by creating a histogram of the predictor variable (\\(x\\)) and a scatterplot of the relationship between the either the number of successes (success) or the number of (failures) and the predictor (\\(x\\)). Note, that this will not account for the differences in trial size per group and so a scatterplot of the relationship between the number of successes (success) or the number of (failures) divided by the total number of trials against the predictor (\\(x\\)) might be more appropriate.\n\u0026gt; hist(dat$x)\r\u0026gt; \u0026gt; #now for the scatterplot\r\u0026gt; plot(success~x, dat)\r\u0026gt; with(dat, lines(lowess(success~x)))\r\u0026gt; \u0026gt; #scatterplot standardised for trial size\r\u0026gt; plot(success/(success+failure)~x, dat)\r\u0026gt; with(dat, lines(lowess(success/(success+failure)~x)))\rConclusions: the predictor (\\(x\\)) does not display any skewness (although it is not all that uniform - random data) or other issues that might lead to non-linearity. The lowess smoother on either scatterplot does not display major deviations from a standard sigmoidal curve and thus linearity is likely to be satisfied. Violations of linearity could be addressed by either:\n\rdefine a non-linear linear predictor (such as a polynomial, spline or other non-linear function).\n\rtransform the scale of the predictor variables.\n\r\r\rModel fitting\rClearly the number of successes is also dependent on the number of trials. Larger numbers of trials might be expected to yeild higher numbers of successes.\n\u0026gt; dat.list \u0026lt;- with(dat, list(success=success, total=success+failure, x=x, N=nrow(dat)))\r\u0026gt; modelString=\u0026quot;\r+ model{\r+ for (i in 1:N) {\r+ success[i] ~ dbin(p[i],total[i])\r+ logit(p[i]) \u0026lt;- max(-20,min(20,beta0+beta1*x[i]))\r+ }\r+ beta0 ~ dnorm(0,1.0E-06)\r+ beta1 ~ dnorm(0,1.0E-06)\r+ }\r+ \u0026quot;\r\u0026gt; writeLines(modelString, con=\u0026#39;modelgbin.txt\u0026#39;)\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta0\u0026#39;,\u0026#39;beta1\u0026#39;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 20000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; dat.logit.jags \u0026lt;- jags(data=dat.list,model.file=\u0026#39;modelgbin.txt\u0026#39;, param=params,\r+ n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 10\rUnobserved stochastic nodes: 2\rTotal graph size: 87\rInitializing model\rAs with the logistic regression presented earlier, we could alternatively use probit or clog-log link functions.\n\rModel evaluation\r\u0026gt; denplot(dat.logit.jags, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;))\r\u0026gt; traplot(dat.logit.jags, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;))\r\u0026gt; \u0026gt; raftery.diag(as.mcmc(dat.logit.jags))\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 46 50468 3746 13.50 beta1 90 98698 3746 26.30 deviance 6 8920 3746 2.38 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 84 103188 3746 27.50 beta1 52 58312 3746 15.60 deviance 8 9488 3746 2.53 \u0026gt; \u0026gt; autocorr.diag(as.mcmc(dat.logit.jags))\rbeta0 beta1 deviance\rLag 0 1.0000000 1.0000000 1.00000000\rLag 1 0.9830416 0.9831425 0.56062724\rLag 5 0.9248140 0.9256704 0.42678260\rLag 10 0.8543024 0.8555131 0.36633408\rLag 50 0.4631353 0.4636323 0.07250394\rLets explore the diagnostics - particularly the residuals.\n\u0026gt; inv.logit \u0026lt;- binomial()$linkinv\r\u0026gt; #Calculate residuals\r\u0026gt; coefs \u0026lt;- dat.logit.jags$BUGSoutput$sims.matrix[,1:2]\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data=dat)\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; pi \u0026lt;- inv.logit(eta)\r\u0026gt; #sweep across rows and then divide by pi\r\u0026gt; Resid \u0026lt;- -1*sweep(pi,2,dat$success/(dat$success+dat$failure),\u0026#39;-\u0026#39;)/sqrt(pi*(1-pi))\r\u0026gt; plot(apply(Resid,2,mean)~apply(eta,2,mean))\r\u0026gt; lines(lowess(apply(Resid,2,mean)~apply(eta,2,mean)))\rConclusions: there is no obvious patterns in the residuals, or at least there are no obvious trends remaining that would be indicative of non-linearity.\nNow we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Bernoulli distribution matching that estimated by the model. Essentially this is estimating how well the Bernoulli distribution and linear model approximates the observed data.\n\u0026gt; SSres\u0026lt;-apply(Resid^2,1,sum)\r\u0026gt; \u0026gt; #generate a matrix of draws from a binomial distribution\r\u0026gt; #the matrix is the same dimensions as pi and uses the probabilities of pi\r\u0026gt; YNew \u0026lt;- matrix(rbinom(length(pi),prob=pi,size=(dat$success+dat$failure)),nrow=nrow(pi))\r\u0026gt; Resid1 \u0026lt;- 1*(pi-YNew/(dat$success+dat$failure))/sqrt(pi*(1-pi))\r\u0026gt; SSres.sim\u0026lt;-apply(Resid1^2,1,sum)\r\u0026gt; mean(SSres.sim\u0026gt;SSres, na.rm=T)\r[1] 0.4559\rConclusions: this Bayesian p-value is reasonably close to \\(0.5\\). Therefore we would conclude that there was no strong evidence for a lack of fit of the model.\n\rExplorations of the trends\rWe might also be interested in the LD50 - the value of \\(x\\) where the probability switches from favoring \\(1\\) to favoring \\(0\\). LD50 is calculated as:\n\\[ LD50 = - \\frac{\\text{intercept}}{\\text{slope}}\\]\n\u0026gt; summary(as.mcmc(-coefs[,1]/coefs[,2]))\rIterations = 1:10000\rThinning interval = 1 Number of chains = 1 Sample size per chain = 10000 1. Empirical mean and standard deviation for each variable,\rplus standard error of the mean:\rMean SD Naive SE Time-series SE 12.80838 6.30455 0.06305 0.05732 2. Quantiles for each variable:\r2.5% 25% 50% 75% 97.5% 10.09 12.45 13.08 13.58 14.41 \u0026gt; \u0026gt; #OR\r\u0026gt; LD50 \u0026lt;- -coefs[,1]/coefs[,2]\r\u0026gt; data.frame(Median=median(LD50), Mean=mean(LD50), HPDinterval(as.mcmc(LD50)), HPDinterval(as.mcmc(LD50),p=0.5))\rMedian Mean lower upper lower.1 upper.1\rvar1 13.08204 12.80838 10.76017 14.74202 12.71997 13.79013\rConclusions: the LD50 is \\(13.1\\). Finally, we will create a summary plot.\n\u0026gt; par(mar = c(4, 5, 0, 0))\r\u0026gt; plot(success/(success+failure) ~ x, data = dat, type = \u0026quot;n\u0026quot;, ann = F, axes = F)\r\u0026gt; points(success/(success+failure) ~ x, data = dat, pch = 16)\r\u0026gt; xs \u0026lt;- seq(min(dat$x, na.rm=TRUE),max(dat$x, na.rm=TRUE), l = 1000)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~xs)\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; ys \u0026lt;- inv.logit(eta)\r\u0026gt; data.tab \u0026lt;- adply(ys,2,function(x) {\r+ data.frame(Median=median(x), HPDinterval(as.mcmc(x)))\r+ })\r\u0026gt; data.tab \u0026lt;- cbind(x=xs,data.tab)\r\u0026gt; \u0026gt; points(Median ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;)\r\u0026gt; with(data.tab,polygon(c(x,rev(x)),c(lower,rev(upper)), col=\u0026quot;#0000ff60\u0026quot;, border=NA))\r\u0026gt; #lines(lower ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;, lty = 2)\r\u0026gt; #lines(upper ~ x, data=data.tab,col = \u0026quot;black\u0026quot;, type = \u0026quot;l\u0026quot;, lty = 2)\r\u0026gt; axis(1)\r\u0026gt; mtext(\u0026quot;X\u0026quot;, 1, cex = 1.5, line = 3)\r\u0026gt; axis(2, las = 2)\r\u0026gt; mtext(\u0026quot;Probability of success\u0026quot;, 2, cex = 1.5, line = 3)\r\u0026gt; box(bty = \u0026quot;l\u0026quot;)\r\u0026gt; \u0026gt; #or via ggplot\r\u0026gt; \u0026gt; xs \u0026lt;- seq(min(dat$x, na.rm=TRUE),max(dat$x, na.rm=TRUE), l = 1000)\r\u0026gt; Xmat \u0026lt;- model.matrix(~xs)\r\u0026gt; eta\u0026lt;-coefs %*% t(Xmat)\r\u0026gt; library(boot)\r\u0026gt; ys \u0026lt;- inv.logit(eta)\r\u0026gt; library(plyr)\r\u0026gt; data.tab \u0026lt;- adply(ys,2,function(x) {\r+ data.frame(Median=median(x), HPDinterval(as.mcmc(x)))\r+ })\r\u0026gt; data.tab \u0026lt;- cbind(x=xs,data.tab)\r\u0026gt; \u0026gt; library(ggplot2)\r\u0026gt; library(grid)\r\u0026gt; dat$p \u0026lt;- with(dat, success/(success+failure))\r\u0026gt; p1 \u0026lt;- ggplot(data.tab,aes(y=Median, x=x)) + geom_point(data=dat,aes(y=p, x=x),color=\u0026quot;gray40\u0026quot;)+\r+ geom_smooth(aes(ymin=lower, ymax=upper), stat=\u0026quot;identity\u0026quot;)+\r+ scale_x_continuous(\u0026quot;X\u0026quot;)+scale_y_continuous(\u0026quot;Probability of success\u0026quot;)\r\u0026gt; p1+theme(panel.grid.major=element_blank(),\r+ panel.grid.minor=element_blank(),\r+ panel.border=element_blank(),\r+ panel.background=element_blank(),\r+ axis.title.y=element_text(size=15,vjust=0,angle=90),\r+ axis.title.x=element_text(size=15,vjust=-1),\r+ axis.text.y=element_text(size=12),\r+ axis.text.x=element_text(size=12),\r+ axis.line=element_line(),\r+ plot.margin=unit(c(0.5,0.5,2,2), \u0026quot;lines\u0026quot;))\r\r\rReferences\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1581646394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581646394,"objectID":"3c192d0d395f713d5fc7a90821d16341","permalink":"/jags/glm-jags/glm-jags/","publishdate":"2020-02-13T21:13:14-05:00","relpermalink":"/jags/glm-jags/glm-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","generalised linear models"],"title":"Generalised Linear Models - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","JAGS","goodness of fit tests","frequency analysis"],"content":"\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rThe analyses described in previous tutorials have all involved response variables that implicitly represent normally distributed and continuous population responses. In this context, continuous indicates that (at least in theory), any value of measurement down to an infinite number of decimal places is possible. Population responses can also be categorical such that the values could be logically or experimentally constrained to a set number of discrete possibilities. For example, individuals in a population can be categorized as either male or female, reaches in a stream could be classified as either riffles, runs or pools and salinity levels of sites might be categorized as either high, medium or low. Typically, categorical response variables are tallied up to generate the frequency of replicates in each of the possible categories. From above, we would tally up the frequency of males and females, the number of riffles, runs and pools and the high, medium and low salinity sites. Hence, rather than model data in which a response was measured from each replicate in the sample (as was the case for previous analyses in this series), frequency analyses model data on the frequency of replicates in each possible category. Furthermore, frequency data follow a Poisson distribution rather than a normal distribution. The Poisson distribution is a symmetrical distribution in which only discrete integer values are possible and whose variance is equal to its mean.\nSince the mean and variance of a Poisson distribution are equal, distributions with higher expected values are shorter and wider than those with smaller means. Note that a Poisson distribution with an expected less than less than \\(5\\) will be obviously asymmetrical as a Poisson distribution is bounded to the left by zero. This has important implications for the reliability of frequency analyses when sample sizes are low. The frequencies expected for each category are determined by the size of the sample and the nature of the (null) hypothesis. For example, if the null hypothesis is that there are three times as many females as males in a population (ratio of \\(3:1\\)), then a sample of \\(110\\) individuals would be expected to yield \\(0.75\\times110=82.5\\) females and \\(0.25\\times110=27.5\\) males.\n\rThe Chi-square statistic\rThe degree of difference between the observed (o) and expected (e) sample category frequencies is represented by the chi-square (\\(\\chi^2\\)) statistic.\n\\[ \\chi^2=\\sum\\frac{(o-e)^2}{e}.\\]\nThis is a relative measure that is standardised by the magnitude of the expected frequencies. When the null hypothesis is true (typically this represents the situation when there are no effects or patterns of interest in the population response category frequencies), and we have sampled in an unbiased manner, we might expect the observed category frequencies in the sample to be very similar (if not equal) to the expected frequencies and thus, the chi-square value should be close to zero. Likewise, repeated sampling from such a population is likely to yield chi-square values close to zero and large chi-square values should be relatively rare. As such, the chi-square statistic approximately follows a \\(\\chi^2\\) distribution, a mathematical probability distribution representing the frequency (and thus probability) of all possible ranges of chi-square statistics that could result when the null hypothesis is true.\nThe \\(\\chi^2\\) distribution is defined as:\n\\[ p(x) = \\frac{1}{2^{\\frac{n}{2}}\\gamma(\\frac{n}{2})}x^{\\frac{n}{2-1}}e^{-\\frac{x}{2}}.\\]\nNote that the location AND shape are both determined by a single parameter (the sample size, n which is also equal to the degrees of freedom \\(+ 1\\)). The \\(\\chi^2\\) distribution is an asymmetrical distribution bounded by zero and infinity and whose exact shape is determined by the degrees of freedom (calculated as the total number of categories minus \\(1\\)). Note also that the peak of a chi-square distribution is not actually at zero (although it does approach it when the degrees of freedom is equal to zero). Initially, this might seem counter intuitive. We might expect that when a null hypothesis is true, the most common chi-square value will be zero. However, the \\(\\chi^2\\) distribution takes into account the expected natural variability in a population as well as the nature of sampling (in which multiple samples should yield slightly different results). The more categories there are, the more likely that the observed and expected values will differ. It could be argued that when there are a large number of categories, samples in which all the observed frequencies are very close to the expected frequencies are a little suspicious and may represent dishonesty on the part of the researcher (Indeed the extraordinary conformity of Gregor Mendelâ€™s pea experiments have been subjected to such skepticism).\nBy comparing any given sample chi-square statistic to its appropriate \\(\\chi^2\\) distribution, the probability that the observed category frequencies could have be collected from a population with a specific ratio of frequencies (for example \\(3:1\\)) can be estimated. As is the case for most hypothesis tests, probabilities lower than \\(0.05\\) (\\(5\\)%) are considered unlikely and suggest that the sample is unlikely to have come from a population characterized by the null hypothesis. Chi-squared tests are typically one-tailed tests focusing on the right-hand tail as we are primarily interested in the probability of obtaining large chi-square values. Nevertheless, it is also possible to focus on the left-hand tail so as to investigate whether the observed values are “too good to be true”.\n\rAssumptions\rA chi-square statistic will follow a \\(\\chi^2\\) distribution approximately provided that:\n\rAll observations are classified independently of one another. The classification of one replicate should not be influenced by or related to the classification of other replicates. Random sampling should address this.\n\rNo more than \\(20\\)% of the expected frequencies are less than five. \\(\\chi^2\\) distributions do not reliably approximate the distribution of all possible chi-square values under those circumstances (Expected frequencies less than five result in asymmetrical sampling distributions (since they must be truncated at zero) and thus potentially unrepresentative χ2 distributions). Since the expected values are a function of sample sizes, meeting this assumption is a matter of ensuring sufficient replication. When sample sizes or other circumstances beyond control lead to a violation of this assumption, numerous options are available.\n\r\r\rGoodness of fit tests\rHomogeneous frequencies tests\nHomogeneous frequencies tests (often referred to as goodness of fit tests) are used to test null hypotheses that the category frequencies observed within a single variable could arise from a population displaying a specific ratio of frequencies. The null hypothesis (\\(H_0\\)) is that the observed frequencies come from a population with a specific ratio of frequencies.\nDistributional conformity - Kolmogorov-Smirnov tests\nStrictly, goodness of fit tests are used to examine whether a frequency/sampling distribution is homogeneous with some declared distribution. For example, we might use a goodness of fit test to formally investigate whether the distribution of a response variable deviates substantially from a normal distribution. In this case, frequencies of responses in a set of pre-defined bin ranges are compared to those frequencies expected according to the mathematical model of a normal distribution. Since calculations of these expected frequencies also involve estimates of population mean and variance (both required to determine the mathematical formula), a two degree of freedom loss is incurred (hence \\(df=n−2\\)).\n\rContingency tables\rContingency tables are used to investigate the associations between two or more categorical variables. That is, they test whether the patterns of frequencies in one categorical variable differ between different levels of other categorical variable(s) or ould the variables be independent of another. In this way, they are analogous to interactions in factorial linear models (such as factorial ANOVA). Contingency tables test the null hypothesis (\\(H_0\\)) that the categorical variables are independent of (not associated with) one another. Note that analyses of contingency tables do not empirically distinguish between response and predictor variables (analogous to correlation), yet causality can be implied when logical and justified by interpretation. As an example, contingency tables could be used to investigate whether incidences of hair and eye color in a population are associated with one another (is one hair color type more commonly observed with a certain eye color). In this case, neither hair color nor eye color influence one another, their incidences are both controlled by a separate set of unmeasured factors. By contrast, an association between the presence or absence of a species of frog and the level of salinity (high, medium or low) could imply that salinity effects the distribution of that species of frog - but not vice versa. Sample replicates are cross-classified according to the levels (categories) of multiple categorical variables. The data are conceptualized as a table (hence the name) with the rows representing the levels of one variable and the column the levels of the other variable(s) such that the cells represent the category combinations. The expected frequency of any given cell is calculated as:\n\\[ \\frac{\\text{(row total)} \\times \\text{(column total)}}{\\text{(grand total)}}.\\]\nThereafter, the chi-square calculations are calculated as described above and the chi-square value is compared to a \\(\\chi^2\\) distribution with \\((r−1)(c−1)\\) degrees of freedom. Contingency tables involving more than two variables have multiple interaction levels and thus multiple potential sources of independence. For example, in a three-way contingency table between variables A, B and C, there are four interactions (A:B, A:C, B:C and A:B:C). Such designs are arguably more appropriately analysed using log-linear models.\nOdds ratios\nThe chi-square test provides an indication of whether or not the occurrences in one set of categories are likely to be associated with other sets of categories (an interaction between two or more categorical variables), yet does not provide any indication of how strongly the variables are associated (magnitude of the effect). Furthermore, for variables with more than two categories (e.g. high, medium, low), there is no indication of which category combinations contribute most to the associations. This role is provided by odds ratios which are essentially a measure of effect size. Odds refer the likelihood of a specific event or outcome occurring (such as the odds of a species being present) versus the odds of it not occurring (and thus the occurrence of an alternative outcome) and are calculated as \\(\\frac{\\pi_j}{(1-\\pi_j)}\\) where \\(\\pi_j\\) refers to the probability of the event occurring. For example, we could calculate the odds of frogs being present in highly saline habitats as the probability of frogs being present divided by the probability of them being absent. Similarly, we could calculate the likelihood of frog presence (odds) within low salinity habitats. The ratio of two of these likelihoods (odds ratio) can then be used to compare whether the likelihood of one outcome (frog presence) is the same for both categories (salinity levels). For example, is the likelihood of frogs being present in highly saline habitats the same as the probability of them being present in habitats with low levels of salinity. In so doing, the odds ratio is a measure of effect size that describes the strength of an association between pairs of cross-classification levels. Although odds and thus odds ratios (\\(\\theta\\)) are technically derived from probabilities, they can also be estimated using cell frequencies (\\(n\\)).\n\\[ \\theta = \\frac{n_{11}n_{22}}{n_{12}n_{21}}\\]\nor alternatively\n\\[ \\theta = \\frac{(n_{11}+0.5)(n_{22}+0.5)}{(n_{12} + 0.5)(n_{21} + 0.5)}\\]\nwhere \\(0.5\\) is a small constant added to prevent division by zero. An odds ratio of one indicates that the event or occurrence (presence of frogs) is equally likely in both categories (high and low salinity habitats). Odds ratios greater than one signify that the event or occurrence is more likely in the first than second category and vice verse for odds ratios less than one. For example, when comparing the presence/absence of frogs in low versus high salinity habitats, an odds ratio of \\(5.8\\) would suggest that frogs are \\(5.8\\) times more likely to be present in low salinity habitats than those that highly saline. The distribution of odds ratios (which range from \\(0\\) to \\(\\infty\\)) is not symmetrical around the null position (\\(1\\)) thereby precluding confidence interval and standard error calculations. Instead, these measures are calculated from log transformed (natural log) odds ratios (the distribution of which is a standard normal distribution centered around \\(0\\)) and then converted back into a linear scale by anti-logging. Odds ratios can only be calculated between category pairs from two variables and therefore \\(2 \\times 2\\) contingency tables (tables with only two rows and two columns). However, tables with more rows and columns can be accommodate by splitting the table up into partial tables of specific category pair combinations. Odds ratios (and confidence intervals) are then calculated from each pairing, notwithstanding their lack of independence. For example, if there were three levels of salinity (high, medium and low), the odds ratios from three partial tables (high vs medium, high vs low, medium vs low) could be calculated.\nSince odds ratios only explore pairwise patterns within two-way interactions, odds ratios for multi-way (three or more variables) tables are considerably more complex to calculate and interpret. Partial tables between two of the variables (e.g frog presence/absence and high/low salinity) are constructed for each level of a third (season: summer/winter). This essentially removes the effect of the third variable by holding it constant. Associations in partial tables are therefore referred to as conditional associations - since the outcomes (associated or independent) from each partial table are explicitly conditional on the level of the third variable at which they were tested.\nSpecific contributions to a lack of independence (significant associations) can also be investigated by exploring the residuals. Recall that residuals are the difference between the observed values (frequencies) and those predicted or expected when the null hypothesis is true (no association between variables). Hence the magnitude of each residual indicates how much each of the cross classification combinations differs from what is expected. The residuals are typically standardized (by dividing by the square of the expected frequencies) to enable individual residuals to be compared relative to one another. Large residuals (in magnitude) indicate large deviations from what is expected when the null hypothesis is true and thus also indicate large influences (contributions) to the overall association. The sign (\\(+\\) or \\(-\\)) of the residual indicates whether the frequencies were higher or lower than expected.\n\rG tests\rAn alternative to the chi-square test for goodness of fit and contingency table analyses is the G-test. The G-test is based on a log likelihood-ratio test. A log likelihood ratio is a ratio of maximum likelihoods of the alternative and null hypotheses. More simply, a log likelihood ratio test essentially examines how likely (the probability) the alternative hypothesis (representing an effect) is compared to how likely the null hypothesis (no effect) is given the collected data. The G2 statistic is calculated as:\n\\[ G^2 = 2 \\sum o \\; ln\\frac{o}{e}\\]\nwhere o and e are the observed and expected sample category frequencies respectively and ln denotes the natural logarithm (base e). When the null hypothesis is true, the G2 statistic approximately follows a theoretical \\(\\chi^2\\) distribution with the same degrees of freedom as the corresponding chi-square statistic. The G2 statistic (which is twice the value of the log-likelihood ratio) is arguably more appropriate than the chi-square statistic as it is closely aligned with the theoretical basis of the χ2 distribution (for which the chi-squared statistic is a convenient approximation). For large sample sizes, G2 and \\(\\chi^2\\) statistics are equivalent, however the former is a better approximation of the theoretical chi2 distribution when the difference between the observed and expected is less than the expected frequencies (ie \\(|o−e|\u0026lt;e\\)). Nevertheless, G-tests operate under the same assumptions are the chi-square statistic and thus very small sample sizes (expected values less than \\(5\\)) are still problematic. G-tests have the additional advantage that they can be used additively with more complex designs and a thus more extensible than the chi-squared statistic.\n\rSmall sample sizes\rAs discussed previously, both the \\(\\chi^2\\) and G2 statistics are poor approximations of theoretical \\(\\chi^2\\) distributions when sample sizes are very small. Under these circumstances a number of alternative options are available:\n\rIf the issue has arisen due to a large number of category levels in one or more of the variables, some categories could be combined together.\n\rFishers exact test which essentially calculates the probability of obtaining the cell frequencies given the observed marginal totals in \\(2 \\times 2\\) tables. The calculations involved in such tests are extremely tedious as they involve calculating probabilities from hypergeometric distributions (discrete distributions describing the number of successes from sequences of samples drawn with out replacement) for all combinations of cell values that result in the given marginal totals.\n\rYates’ continuity correction calculates the test statistic after adding and subtracting \\(0.5\\) from observed values less than and greater than expected values respectively. Yates’ correction can only be applied to designs with a single degree of freedom (goodness-of-fit designs with two categories or \\(2 \\times 2\\) tables) and for goodness-of-fit tests provide p-values that are closer to those of an exact binomial. However, they typically yield over inflated p-values in contingency tables and so have gone out of favour.\n\rWilliams’ correction is applied by dividing the test statistic by \\(1+(p2−1)6nv\\), where \\(p\\) is the number of categories, \\(n\\) is the total sample size (total of observed frequencies) and \\(v\\) is the number of degrees of freedom \\((p−1)\\). Williams’ corrections can be applied to designs with greater than one degree of freedom, and are considered marginally more appropriate than Yates’ corrections if corrections are insisted.\n\rRandomisation tests in which the sample test statistic (either \\(\\chi^2\\) or G2) is compared to a probability distribution generated by repeatedly calculating the test statistic from an equivalent number of observations drawn from a population (sampling with replacement) with the specific ratio of category frequencies defined by the null hypothesis. Significance is thereafter determined by the proportion of the randomised test statistic values that are greater than or equal to the value of the statistic that is based on observed data.\n\rLog-linear modelling (as a form of generalized linear model)\n\r\r\r\rData generation\rGoodness of fit tests are concerned with comparing the observed frequencies with those expected on the basis of a specific null hypothesis. So lets now fabricate a motivating scenario and some data. We will create a scenario that involves items classified into one of three groups (A, B and C). The number of items in each classification group are then tallied up. Out of a total of \\(47\\) items, \\(15\\) where of type A, \\(9\\) where of type B and \\(23\\) where of type C. We could evaluate a parity (a \\(1:1:1\\) ratio from these data. In a frequentist context, this might involve testing a null hypothesis that the observed data could have come from a population with a \\(1:1\\) item ratio. In this case the probability would be the probability of obtaining the observed ratio of frequencies when the null hypothesis is true. In a Bayesian context, there are numerous ways that we could tackle these data. We would be evaluating the evidence for the null hypothesis (\\(1:1:1\\) item ratio) given the observed by estimating the degree of freedom from a chi-square distribution. Alternatively, we could estimate the value of the three population fractions which are expected to be \\(1/3, 1/3, 1/3\\) when \\(1:1:1\\). We will explore this option first and then explore the chi-square approach second. To extend the example, lets also explore a \\(1:1:2\\) ratio. We start by generating the observed data:\n\u0026gt; #the observed frequences of A and B\r\u0026gt; obs \u0026lt;- c(15,9,23)\r\u0026gt; obs\r[1] 15 9 23\r\rEstimating population fractions - binomial distribution\rThe binomial distribution represents the distribution of possible densities (probabilities) for the number of successes p out of a total of n independent trials. In this case, it can be used to model the number of items of each group (A, B and C) out of a total of \\(47\\) items. The prior distribution for \\(p_i\\) would be a beta distribution (values range from \\(0\\) to \\(1\\)) with shape parameters a and b the hyperpriors of which follow vague (flat, imprecise) gamma distributions.\n\\[ obs_i \\sim \\text{Bin}(p_i,n_i),\\]\nwhere \\(p_i\\sim \\text{Beta}(a,b)\\) and \\(a,b \\sim \\text{Gamma}(1,0.01)\\).\nExploratory data analysis\rThe data should logically follow a binomial distribution (since the observations are counts of positive events out of a total).\n\rModel fitting\rWe now translate the likelihood model into JAGS code and store the code in an external file.\n\u0026gt; modelString=\u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:nGroups) {\r+ obs[i] ~ dbin(p[i],n[i])\r+ p[i] ~ dbeta(a[i],b[i])\r+ a[i] ~ dgamma(1,0.01)\r+ b[i] ~ dgamma(1,0.01)\r+ }\r+ }\r+ \u0026quot;\r\u0026gt; ## write the model to a text file \u0026gt; writeLines(modelString,con=\u0026quot;chi2model.txt\u0026quot;)\r\rThe likelihood model indicates that the observed counts are modeled by a binomial distribution with a probability of p (fraction) from n trials (items).\n\rThe prior on each p is defined as a beta distribution with shape parameters a and b\n\rThe hyperpriors for each a and b are drawn from imprecise (vague, flat) gamma distributions.\n\r\rDefine the data list. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; # The observed item frequencies\r\u0026gt; obs \u0026lt;- c(15, 9, 23)\r\u0026gt; data.list \u0026lt;- list(obs = obs, n = c(47, 47, 47), nGroups = 3)\r\u0026gt; data.list\r$obs\r[1] 15 9 23\r$n\r[1] 47 47 47\r$nGroups\r[1] 3\rDefine the parameters to monitor and the chain details\n\u0026gt; params \u0026lt;- c(\u0026quot;p\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 1000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 5000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\rFit the model in JAGS using the function jags in the package R2jags (which should be loaded first).\n\u0026gt; library(R2jags)\r\u0026gt; # Fit the model for the 1:1:1 ratio\r\u0026gt; data.r2jags \u0026lt;- jags(data = data.list, inits = NULL, parameters.to.save = params, + model.file = \u0026quot;chi2model.txt\u0026quot;,n.chains = nChains, n.iter = nIter, + n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 3\rUnobserved stochastic nodes: 9\rTotal graph size: 18\rInitializing model\r\u0026gt; \u0026gt; print(data.r2jags)\rInference for Bugs model at \u0026quot;chi2model.txt\u0026quot;, fit using jags,\r2 chains, each with 2500 iterations (first 1000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rp[1] 0.323 0.064 0.205 0.276 0.321 0.365 0.449 1.003 570\rp[2] 0.204 0.053 0.117 0.168 0.199 0.234 0.322 1.047 38\rp[3] 0.496 0.073 0.350 0.448 0.497 0.545 0.636 1.001 3000\rdeviance 15.119 2.384 12.535 13.373 14.490 16.130 21.782 1.005 320\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 2.8 and DIC = 18.0\rDIC is an estimate of expected predictive error (lower deviance is better).\rConclusions: Initially, we should focus our attention on the Rhat and n.eff columns. These are the scale reduction and number of effective samples respectively and they provide an indication of the degree of mixing or coverage of the samples. Ideally, the n.eff values should be approximately equal to the number of saved samples (in this case \\(4701\\)), and the Rhat values should be approximately \\(1\\) (complete convergence). Whilst the actual values are likely to differ substantially from run to run (due to the stochastic nature of the way the chains traverse the posterior distribution), on this occasion, the n.eff of the first two probability parameters (p[1] and p[2]) are substantially lower than \\(4700\\). Hence, the samples of these parameters may not accurately reflect the posterior distribution. We might consider altering one or more of the chain behavioural paramters (such as the thinning rate), alter the model definition (or priors) itself.\n\rModel evaluation\r\u0026gt; library(mcmcplots)\r\u0026gt; denplot(data.r2jags, parms = c(\u0026quot;p\u0026quot;))\r\u0026gt; traplot(data.r2jags, parms = c(\u0026quot;p\u0026quot;))\r\u0026gt; \u0026gt; raftery.diag(as.mcmc(data.r2jags))\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r[[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r\u0026gt; \u0026gt; autocorr.diag(as.mcmc(data.r2jags))\rdeviance p[1] p[2] p[3]\rLag 0 1.00000000 1.00000000 1.00000000 1.00000000\rLag 1 0.67862022 0.79329782 0.75266787 0.79501632\rLag 5 0.22316853 0.37163355 0.31066379 0.35879998\rLag 10 0.05470517 0.15225282 0.11155263 0.17106406\rLag 50 0.02781881 -0.00722609 -0.09133568 -0.03525663\rConclusions: Minimum required number of MCMC samples to ensure that sufficient samples had been collected to achieve good accuracy is \\(3746\\). We had \\(5000\\) per chain (\\(5000\\times3=15000\\)).\n\u0026gt; params \u0026lt;- c(\u0026quot;p\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 1000\r\u0026gt; thinSteps = 50\r\u0026gt; numSavedSteps = 5000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; data.r2jags \u0026lt;- jags(data = data.list, inits = NULL, parameters.to.save = params, + model.file = \u0026quot;chi2model.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 3\rUnobserved stochastic nodes: 9\rTotal graph size: 18\rInitializing model\r\u0026gt; \u0026gt; print(data.r2jags)\rInference for Bugs model at \u0026quot;chi2model.txt\u0026quot;, fit using jags,\r2 chains, each with 125000 iterations (first 1000 discarded), n.thin = 50\rn.sims = 4960 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rp[1] 0.325 0.067 0.200 0.279 0.322 0.369 0.459 1.001 5000\rp[2] 0.204 0.056 0.105 0.164 0.201 0.240 0.324 1.002 2100\rp[3] 0.491 0.070 0.353 0.443 0.490 0.539 0.630 1.001 5000\rdeviance 15.223 2.356 12.524 13.457 14.647 16.360 21.253 1.002 1700\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 2.8 and DIC = 18.0\rDIC is an estimate of expected predictive error (lower deviance is better).\rConclusions: Rhat and n.eff are now much better for the probability parameters. The estimated fractions for A, B and C are:\n\rA: 0.327 (0.207, 0.466)\n\rB: 0.200 (0.104, 0.323)\n\rC: 0.491 (0.355, 0.625)\n\r\rCollectively, the fractions of 1/3, 1/3 and 1/3 do not fall within these ranges. However, collectively the fractions 1/4, 1/4, 2/4 do fall comfortably within these ranges. This suggests that the population ratio is more likely to be 1:1:2 than 1:1:1.\n\r\rChi-square\rAn appropriate test statistic for comparing an observed (o) frequency ratio to an expected (e) frequency ratio is the chi-square \\(\\chi^2\\) statistic. In effect, the chi-square statistic (which incorporates the variability in the data in to measure of the difference between observed and expected) becomes the input for the likelihood model. Whilst we could simply pass JAGS the chi-square statistic, by parsing the observed and expected values and having the chi-square value calculated within JAGS data, the resulting JAGS code is more complete and able to accommodate other scenarios. So if, chisq is the chi-square statistic and \\(k\\) is the degrees of freedom (and thus expected value of the \\(\\chi^2\\) distribution), then the likelihood model is:\n\\[ \\text{chisq} \\sim \\chi^2(k),\\]\nwhere \\(k \\sim \\text{Unif}(0.01,100)\\).\nExploratory data analysis\rSo lets calculate the expected frequencies as a means to evaluate this assumption. The expected values are calculated as:\n\\[ e=\\text{total counts} \\times \\text{expected fraction}.\\]\nIt is clear that in neither case are any of the expected frequencies less than \\(5\\). Therefore, we would conclude that probabilities derived from the \\(\\chi^2\\) distribution are likely to be reliable.\n\rModel fitting\rWe now translate the likelihood model into JAGS code and store the code in an external file.\n\u0026gt; modelString2=\u0026quot;\r+ data {\r+ for (i in 1:n){\r+ resid[i] \u0026lt;- pow(obs[i]-exp[i],2)/exp[i]\r+ }\r+ chisq \u0026lt;- sum(resid)\r+ }\r+ model {\r+ #Likelihood\r+ chisq ~ dchisqr(k)\r+ #Priors\r+ k ~ dunif(0.01,100)\r+ }\r+ \u0026quot;\r\u0026gt; ## write the model to a text file \u0026gt; writeLines(modelString2,con=\u0026quot;chi2model2.txt\u0026quot;)\r\rFirst of all, the standardized residuals and chi-square statistic are calculated according to the formula listed above.\n\rThe likelihood model indicates that the chi-squared statistic can be modeled by a \\(\\chi^2\\) distribution with a centrality parameter of \\(k\\).\n\rThe prior on \\(k\\) is defined as a uniform (thus vague) flat prior whose values could range from \\(0.01\\) to \\(100\\) (all with equal probability).\n\r\rDefine the data list. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; # The observed item frequencies\r\u0026gt; obs \u0026lt;- c(15, 9, 23)\r\u0026gt; # The expected item frequencies (for a 1:1:1 ratio)\r\u0026gt; exp \u0026lt;- rep(sum(obs) * 1/3, 3)\r\u0026gt; data.list \u0026lt;- list(obs = obs, exp = exp, n = 3)\r\u0026gt; data.list\r$obs\r[1] 15 9 23\r$exp\r[1] 15.66667 15.66667 15.66667\r$n\r[1] 3\r\u0026gt; \u0026gt; # The expected item frequencies (for a 1:1:2 ratio)\r\u0026gt; exp \u0026lt;- sum(obs) * c(1/4, 1/4, 2/4)\r\u0026gt; data.list1 \u0026lt;- list(obs = obs, exp = exp, n = 3)\r\u0026gt; data.list1\r$obs\r[1] 15 9 23\r$exp\r[1] 11.75 11.75 23.50\r$n\r[1] 3\rDefine the parameters to monitor and the chain details\n\u0026gt; params \u0026lt;- c(\u0026quot;chisq\u0026quot;, \u0026quot;resid\u0026quot;, \u0026quot;k\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 1000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 5000\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\rFit the model in JAGS using the function jags in the package R2jags (which should be loaded first).\n\u0026gt; # Fit the model for the 1:1:1 ratio\r\u0026gt; data.r2jags2 \u0026lt;- jags(data = data.list, inits = NULL, parameters.to.save = params, + model.file = \u0026quot;chi2model2.txt\u0026quot;,n.chains = nChains, n.iter = nIter, + n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling data graph\rResolving undeclared variables\rAllocating nodes\rInitializing\rReading data back into data table\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 1\rUnobserved stochastic nodes: 1\rTotal graph size: 14\rInitializing model\r\u0026gt; \u0026gt; print(data.r2jags2)\rInference for Bugs model at \u0026quot;chi2model2.txt\u0026quot;, fit using jags,\r2 chains, each with 2500 iterations (first 1000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rchisq 6.298 0.000 6.298 6.298 6.298 6.298 6.298 1.000 1\rk 8.293 3.611 2.306 5.686 7.915 10.559 16.165 1.001 3000\rresid[1] 0.028 0.000 0.028 0.028 0.028 0.028 0.028 1.000 1\rresid[2] 2.837 0.000 2.837 2.837 2.837 2.837 2.837 1.000 1\rresid[3] 3.433 0.000 3.433 3.433 3.433 3.433 3.433 1.000 1\rdeviance 5.338 1.428 4.346 4.444 4.809 5.647 9.405 1.001 3000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 1.0 and DIC = 6.4\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; # Fit the model for the 1:1:2 ratio\r\u0026gt; data.r2jags2.1 \u0026lt;- jags(data = data.list1, inits = NULL, parameters.to.save = params, + model.file = \u0026quot;chi2model2.txt\u0026quot;,n.chains = nChains, n.iter = nIter, + n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling data graph\rResolving undeclared variables\rAllocating nodes\rInitializing\rReading data back into data table\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 1\rUnobserved stochastic nodes: 1\rTotal graph size: 14\rInitializing model\r\u0026gt; \u0026gt; print(data.r2jags2.1)\rInference for Bugs model at \u0026quot;chi2model2.txt\u0026quot;, fit using jags,\r2 chains, each with 2500 iterations (first 1000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rchisq 1.553 0.000 1.553 1.553 1.553 1.553 1.553 1.000 1\rk 3.455 1.909 0.549 2.007 3.185 4.612 7.908 1.002 3000\rresid[1] 0.899 0.000 0.899 0.899 0.899 0.899 0.899 1.000 1\rresid[2] 0.644 0.000 0.644 0.644 0.644 0.644 0.644 1.000 1\rresid[3] 0.011 0.000 0.011 0.011 0.011 0.011 0.011 1.000 1\rdeviance 3.883 1.426 2.870 2.976 3.334 4.230 8.006 1.003 920\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 1.0 and DIC = 4.9\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rModel evaluation\r\u0026gt; denplot(data.r2jags2, parms = c(\u0026quot;k\u0026quot;))\r\u0026gt; traplot(data.r2jags2, parms = c(\u0026quot;k\u0026quot;))\r\u0026gt; \u0026gt; raftery.diag(as.mcmc(data.r2jags2))\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r[[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r\u0026gt; \u0026gt; autocorr.diag(as.mcmc(data.r2jags2))\rchisq deviance k resid[1] resid[2] resid[3]\rLag 0 NaN 1.00000000 1.00000000 NaN NaN NaN\rLag 1 NaN 0.42415918 0.27295298 NaN NaN NaN\rLag 5 NaN -0.01961156 -0.01627609 NaN NaN NaN\rLag 10 NaN -0.03086926 -0.01043329 NaN NaN NaN\rLag 50 NaN -0.01409259 -0.02172076 NaN NaN NaN\rConclusions: The trace plots show what appears to be “random noise” about the parameter value. There is no real suggestion of a step or dramatic change in the trend direction along the length of the sampling chain. The samples seem relatively stable. Thus it would seem that the chains are well mixed and have converged. The density plot (for \\(k\\)) is not symmetrical. This suggests that the mean is not a good point estimate for this parameter - the median would be better.\n\rExploring model parameters\r\u0026gt; print(data.r2jags2)\rInference for Bugs model at \u0026quot;chi2model2.txt\u0026quot;, fit using jags,\r2 chains, each with 2500 iterations (first 1000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rchisq 6.298 0.000 6.298 6.298 6.298 6.298 6.298 1.000 1\rk 8.293 3.611 2.306 5.686 7.915 10.559 16.165 1.001 3000\rresid[1] 0.028 0.000 0.028 0.028 0.028 0.028 0.028 1.000 1\rresid[2] 2.837 0.000 2.837 2.837 2.837 2.837 2.837 1.000 1\rresid[3] 3.433 0.000 3.433 3.433 3.433 3.433 3.433 1.000 1\rdeviance 5.338 1.428 4.346 4.444 4.809 5.647 9.405 1.001 3000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 1.0 and DIC = 6.4\rDIC is an estimate of expected predictive error (lower deviance is better).\rConclusions: The median degrees of freedom (\\(k\\)) was \\(8.00\\) with a \\(95\\)% spread of \\(2.31-16.16\\). This interval does not include the value of \\(2\\) (expected value of the chi2 distribution for this hypothesis). Hence there is evidence that the population ratio deviates from a 1:1:1 ratio.\n\u0026gt; print(data.r2jags2.1)\rInference for Bugs model at \u0026quot;chi2model2.txt\u0026quot;, fit using jags,\r2 chains, each with 2500 iterations (first 1000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rchisq 1.553 0.000 1.553 1.553 1.553 1.553 1.553 1.000 1\rk 3.455 1.909 0.549 2.007 3.185 4.612 7.908 1.002 3000\rresid[1] 0.899 0.000 0.899 0.899 0.899 0.899 0.899 1.000 1\rresid[2] 0.644 0.000 0.644 0.644 0.644 0.644 0.644 1.000 1\rresid[3] 0.011 0.000 0.011 0.011 0.011 0.011 0.011 1.000 1\rdeviance 3.883 1.426 2.870 2.976 3.334 4.230 8.006 1.003 920\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 1.0 and DIC = 4.9\rDIC is an estimate of expected predictive error (lower deviance is better).\rConclusions: The median degrees of freedom (\\(k\\)) was \\(3.31\\) with a \\(95\\)% spread of \\(0.57-7.61\\). This interval comfortably includes the value of \\(2\\) (expected value of the chi2 distribution for this hypothesis). Hence there is no evidence that the population ratio deviates from a 1:1:2 ratio.\n\rExploration of the trends\rThere are a number of avenues we could take in order to explore the data and models further. One thing we could do is calculate the probability that \\(k\\) is greater than $24 (the expected value) for each hypothesis. This can be done either by modifying the JAGS code to include a derivative that uses the step function, or we can derive it within R from the \\(k\\) samples. Lets explore the latter.\n\u0026gt; k \u0026lt;- data.r2jags2$BUGSoutput$sims.matrix[, \u0026quot;k\u0026quot;]\r\u0026gt; pr \u0026lt;- sum(k \u0026gt; 2)/length(k)\r\u0026gt; pr\r[1] 0.9813333\r\u0026gt; \u0026gt; k \u0026lt;- data.r2jags2.1$BUGSoutput$sims.matrix[, \u0026quot;k\u0026quot;]\r\u0026gt; pr1 \u0026lt;- sum(k \u0026gt; 2)/length(k)\r\u0026gt; pr1\r[1] 0.7513333\rConclusions: the probability that the expected value exceeds \\(2\\) for the 1:1:1 hypothesis is \\(0.982\\) (\\(98.2\\)%). There is an \\(98.2\\)% likelihood that the population is not 1:1:1.\rWe could also compare the two alternative hypotheses. The 1:1:2 hypothesis has lower DIC and is therefore considered a better fit (\\(4.7\\) vs \\(6.4\\)). This is a difference in DIC of around \\(1.7\\) units. So the data have higher support for a 1:1:2 population ratio than a 1:1:1 ratio.\n\r\rReferences\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1581559994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581559994,"objectID":"c3f9f106b1ef109fb9c512a73397a3a1","permalink":"/jags/gof-tests-jags/gof-tests-jags/","publishdate":"2020-02-12T21:13:14-05:00","relpermalink":"/jags/gof-tests-jags/gof-tests-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","goodness of fit tests","frequency analysis"],"title":"Goodness of fit tests - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","anova","JAGS","split-plot","repeated measures"],"content":"\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rSplit-plot designs (plots refer to agricultural field plots for which these designs were originally devised) extend unreplicated factorial (randomised complete block and simple repeated measures) designs by incorporating an additional factor whose levels are applied to entire blocks. Similarly, complex repeated measures designs are repeated measures designs in which there are different types of subjects. Consider the example of a randomised complete block. Blocks of four treatments (representing leaf packs subject to different aquatic taxa) were secured in numerous locations throughout a potentially heterogeneous stream. If some of those blocks had been placed in riffles, some in runs and some in pool habitats of the stream, the design becomes a split-plot design incorporating a between block factor (stream region: runs, riffles or pools) and a within block factor (leaf pack exposure type: microbial, macro invertebrate or vertebrate). Furthermore, the design would enable us to investigate whether the roles that different organism scales play on the breakdown of leaf material in stream are consistent across each of the major regions of a stream (interaction between region and exposure type). Alternatively (or in addition), shading could be artificially applied to half of the blocks, thereby introducing a between block effect (whether the block is shaded or not). Extending the repeated measures examples from Tutorial 9.3a, there might have been different populations (such as different species or histories) of rats or sharks. Any single subject (such as an individual shark or rat) can only be of one of the populations types and thus this additional factor represents a between subject effect.\n\rLinear models\rThe linear models for three and four factor partly nested designs are:\n\\[ y_{ijkl} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\gamma)_{ij} + (\\beta\\gamma)_{jk} + \\epsilon_{ijkl},\\]\n\\[ y_{ijklm} = \\mu + \\alpha_i + \\gamma_j + (\\alpha\\gamma)_{ij} + \\beta_k + \\delta_l + (\\alpha\\delta)_{il} + (\\gamma\\delta)_{jl} + (\\alpha\\gamma\\delta)_{ijl} + \\epsilon_{ijklm}, \\;\\;\\; \\text{(Model 2 additive - 2 between)}\\]\n\\[ y_{ijklm} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\delta_l + (\\gamma\\delta)_{kl} + (\\alpha\\gamma)_{ik} + (\\alpha\\delta)_{il} + (\\alpha\\gamma\\delta)_{ikl} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 2 additive - 1 between)}\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\beta\\) is the effect of the Blocking Factor B and \\(\\epsilon\\) is the random unexplained or residual component.\n\rAssumptions\rAs partly nested designs share elements in common with each of nested, factorial and unreplicated factorial designs, they also share similar assumptions and implications to these other designs. Specifically, hypothesis tests assume that:\n\rthe appropriate residuals are normally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator (see Tables above) be used to explore normality. Scale transformations are often useful.\n\rthe appropriate residuals are equally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\n\rthe appropriate residuals are independent of one another. Critically, experimental units within blocks/subjects should be adequately spaced temporally and spatially to restrict contamination or carryover effects. Non-independence resulting from the hierarchical design should be accounted for.\n\rthat the variance/covariance matrix displays sphericity (strickly, the variance-covariance matrix must display a very specific pattern of sphericity in which both variances and covariances are equal (compound symmetry), however, an F-ratio will still reliably follow an F distribution provided basic sphericity holds). This assumption is likely to be met only if the treatment levels within each block can be randomly ordered. This assumption can be managed by either adjusting the sensitivity of the affected F-ratios or employing linear mixed effects modelling to the design.\n\rthere are no block by within block interactions. Such interactions render non-significant within block effects difficult to interpret unless we assume that there are no block by within block interactions, non-significant within block effects could be due to either an absence of a treatment effect, or as a result of opposing effects within different blocks. As these block by within block interactions are unreplicated, they can neither be formally tested nor is it possible to perform main effects tests to diagnose non-significant within block effects.\n\r\r\r\rSplit-plot design\rData generation\rImagine we has designed an experiment in which we intend to measure a response (\\(y\\)) to one of treatments (three levels; “a1”, “a2” and “a3”). Unfortunately, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability you decide to apply a design (RCB) in which each of the treatments within each of \\(35\\) blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; library(plyr)\r\u0026gt; set.seed(123)\r\u0026gt; nA \u0026lt;- 3\r\u0026gt; nC \u0026lt;- 3\r\u0026gt; nBlock \u0026lt;- 36\r\u0026gt; sigma \u0026lt;- 5\r\u0026gt; sigma.block \u0026lt;- 12\r\u0026gt; n \u0026lt;- nBlock*nC\r\u0026gt; Block \u0026lt;- gl(nBlock, k=1)\r\u0026gt; C \u0026lt;- gl(nC,k=1)\r\u0026gt; \u0026gt; ## Specify the cell means\r\u0026gt; AC.means\u0026lt;-(rbind(c(40,70,80),c(35,50,70),c(35,40,45)))\r\u0026gt; ## Convert these to effects\r\u0026gt; X \u0026lt;- model.matrix(~A*C,data=expand.grid(A=gl(3,k=1),C=gl(3,k=1)))\r\u0026gt; AC \u0026lt;- as.vector(AC.means)\r\u0026gt; AC.effects \u0026lt;- solve(X,AC)\r\u0026gt; \u0026gt; A \u0026lt;- gl(nA,nBlock,n)\r\u0026gt; dt \u0026lt;- expand.grid(C=C,Block=Block)\r\u0026gt; dt \u0026lt;- data.frame(dt,A)\r\u0026gt; \u0026gt; Xmat \u0026lt;- cbind(model.matrix(~-1+Block, data=dt),model.matrix(~A*C, data=dt))\r\u0026gt; block.effects \u0026lt;- rnorm(n = nBlock, mean =0 , sd = sigma.block)\r\u0026gt; all.effects \u0026lt;- c(block.effects, AC.effects)\r\u0026gt; lin.pred \u0026lt;- Xmat %*% all.effects\r\u0026gt; \u0026gt; ## the quadrat observations (within sites) are drawn from\r\u0026gt; ## normal distributions with means according to the site means\r\u0026gt; ## and standard deviations of 5\r\u0026gt; y \u0026lt;- rnorm(n,lin.pred,sigma)\r\u0026gt; data.splt \u0026lt;- data.frame(y=y, A=A,dt)\r\u0026gt; head(data.splt) #print out the first six rows of the data set\ry A C Block A.1\r1 36.04388 1 1 1 1\r2 62.96473 1 2 1 1\r3 71.74448 1 3 1 1\r4 35.33552 1 1 2 1\r5 63.76434 1 2 2 1\r6 76.19828 1 3 2 1\r\u0026gt; \u0026gt; tapply(data.splt$y,data.splt$A,mean)\r1 2 3 65.71431 49.43047 41.36212 \u0026gt; \u0026gt; tapply(data.splt$y,data.splt$C,mean)\r1 2 3 38.41079 53.56792 64.52819 \u0026gt; \u0026gt; replications(y~A*C+Error(Block), data.splt)\rA C A:C 36 36 12 \u0026gt; \u0026gt; library(ggplot2)\r\u0026gt; ggplot(data.splt, aes(y=y, x=C, linetype=A, group=A)) + geom_line(stat=\u0026#39;summary\u0026#39;, fun.y=mean)\r\u0026gt; \u0026gt; ggplot(data.splt, aes(y=y, x=C,color=A)) + geom_point() + facet_wrap(~Block)\r\rExploratory data analysis\rNormality and Homogeneity of variance\n\u0026gt; # check between plot effects\r\u0026gt; boxplot(y~A, ddply(data.splt,~A+Block, summarise,y=mean(y)))\r\u0026gt; \u0026gt; #OR\r\u0026gt; ggplot(ddply(data.splt,~A+Block, summarise,y=mean(y)), aes(y=y, x=A)) + geom_boxplot()\r\u0026gt; \u0026gt; # check within plot effects\r\u0026gt; boxplot(y~A*C, data.splt)\r\u0026gt; \u0026gt; #OR \u0026gt; ggplot(data.splt, aes(y=y, x=C, fill=A)) + geom_boxplot()\rConclusions:\n\rthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\n\rthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the y-axis. Hence it there is no evidence of non-homogeneity.\n\r\rObvious violations could be addressed either by:\n\rtransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\r\rBlock by within-Block interaction\n\u0026gt; library(car)\r\u0026gt; with(data.splt, interaction.plot(C,Block,y))\r\u0026gt; \u0026gt; #OR with ggplot\r\u0026gt; library(ggplot2)\r\u0026gt; ggplot(data.splt, aes(y=y, x=C, group=Block,color=Block)) + geom_line() +\r+ guides(color=guide_legend(ncol=3))\r\u0026gt; \u0026gt; residualPlots(lm(y~Block+A*C, data.splt))\r Test stat Pr(\u0026gt;|Test stat|)\rBlock A C Tukey test 1.4518 0.1466\r\u0026gt; \u0026gt; # the Tukey\u0026#39;s non-additivity test by itself can be obtained via an internal function\r\u0026gt; # within the car package\r\u0026gt; car:::tukeyNonaddTest(lm(y~Block+A*C, data.splt))\rTest Pvalue 1.4517644 0.1465671 \rConclusions:\n\rthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (C). Any trends appear to be reasonably consistent between Blocks.\r\r\r\rExample - split-plot\rIn an attempt to understand the effects on marine animals of short-term exposure to toxic substances, such as might occur following a spill, or a major increase in storm water flows, a it was decided to examine the toxicant in question, Copper, as part of a field experiment in Honk Kong. The experiment consisted of small sources of Cu (small, hemispherical plaster blocks, impregnated with copper), which released the metal into sea water over \\(4\\) or \\(5\\) days. The organism whose response to Cu was being measured was a small, polychaete worm, Hydroides, that attaches to hard surfaces in the sea, and is one of the first species to colonize any surface that is submerged. The biological questions focused on whether the timing of exposure to Cu affects the overall abundance of these worms. The time period of interest was the first or second week after a surface being available.\nThe experimental setup consisted of sheets of black perspex (settlement plates), which provided good surfaces for these worms. Each plate had a plaster block bolted to its centre, and the dissolving block would create a gradient of [Cu] across the plate. Over the two weeks of the experiment, a given plate would have pl ain plaster blocks (Control) or a block containing copper in the first week, followed by a plain block, or a plain block in the first week, followed by a dose of copper in the second week. After two weeks in the water, plates were removed and counted back in the laboratory. Without a clear idea of how sensitive these worms are to copper, an effect of the treatments might show up as an overall difference in the density of worms across a plate, or it could show up as a gradient in abundance across the plate, with a different gradient in different treatments. Therefore, on each plate, the density of worms was recorded at each of four distances from the center of the plate. Let’s have a look at the dataset\n\u0026gt; copper \u0026lt;- read.table(\u0026#39;copper.csv\u0026#39;, header=T, sep=\u0026#39;,\u0026#39;, strip.white=T)\r\u0026gt; head(copper)\rCOPPER PLATE DIST WORMS\r1 control 200 4 11.50\r2 control 200 3 13.00\r3 control 200 2 13.50\r4 control 200 1 12.00\r5 control 39 4 17.75\r6 control 39 3 13.75\rVariables’ description:\nCopper. Categorical listing of the copper treatment (control = no copper applied, week 2 = copper treatment applied in second week and week 1= copper treatment applied in first week) applied to whole plates. Factor A (between plot factor).\nPlate. Substrate provided for polychaete worm colonization on which copper treatment applied. These are the plots (Factor B). Numbers in this column represent numerical labels given to each plate.\nDist. Categorical listing for the four concentric distances from the center of the plate (source of copper treatment) with 1 being the closest and 4 the furthest. Factor C (within plot factor)\nWorms. Density of worms measured. Response variable.\nThe Plates are the “random” groups. Within each Plate, all levels of the Distance factor occur (this is a within group factor). Each Plate can only be of one of the three levels of the Copper treatment. This is therefore a within group (nested) factor. Traditionally, this mixture of nested and randomised block design would be called a partly nested or split-plot design. In Bayesian (multilevel modeling) terms, this is a multi-level model with one hierarchical level the Plates means and another representing the Copper treatment means (based on the Plate means). Exploratory data analysis has indicated that the response variable could be normalised via a forth-root transformation.\nModel fitting\rWe will only explore the matrix parameterisation (random intercepts) of the model, where\n\\[\\text{number of lesions}_i = \\beta \\text{Site}_{j(i)} + \\epsilon_{i},\\]\nwhere \\(\\epsilon_i∼ N(0,\\sigma^2)\\) and we treat Distance as a factor.\n\u0026gt; modelString=\u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau.res)\r+ mu[i] \u0026lt;- inprod(beta[],X[i,]) + inprod(gamma[],Z[i,])\r+ y.err[i] \u0026lt;- y[i] - mu[1]\r+ }\r+ + #Priors and derivatives\r+ for (i in 1:nZ) {\r+ gamma[i] ~ dnorm(0,tau.plate)\r+ }\r+ for (i in 1:nX) {\r+ beta[i] ~ dnorm(0,1.0E-06)\r+ }\r+ + tau.res \u0026lt;- pow(sigma.res,-2)\r+ sigma.res \u0026lt;- z/sqrt(chSq)\r+ z ~ dnorm(0, .0016)I(0,)\r+ chSq ~ dgamma(0.5, 0.5)\r+ + tau.plate \u0026lt;- pow(sigma.plate,-2)\r+ sigma.plate \u0026lt;- z.plate/sqrt(chSq.plate)\r+ z.plate ~ dnorm(0, .0016)I(0,)\r+ chSq.plate ~ dgamma(0.5, 0.5)\r+ + }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;matrixModel.txt\u0026quot;)\r\u0026gt; \u0026gt; \u0026gt; #sort the data set so that the copper treatments are in a more logical order\r\u0026gt; library(dplyr)\r\u0026gt; copper$DIST \u0026lt;- factor(copper$DIST)\r\u0026gt; copper$PLATE \u0026lt;- factor(copper$PLATE)\r\u0026gt; copper.sort \u0026lt;- arrange(copper,COPPER,PLATE,DIST)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~COPPER*DIST, data=copper.sort)\r\u0026gt; Zmat \u0026lt;- model.matrix(~-1+PLATE, data=copper.sort)\r\u0026gt; copper.list \u0026lt;- list(y=copper.sort$WORMS,\r+ X=Xmat, nX=ncol(Xmat),\r+ Z=Zmat, nZ=ncol(Zmat),\r+ n=nrow(copper.sort)\r+ )\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026quot;gamma\u0026quot;,\u0026quot;sigma.res\u0026quot;,\u0026quot;sigma.plate\u0026quot;)\r\u0026gt; burnInSteps = 1000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; library(R2jags)\r\u0026gt; library(coda)\r\u0026gt; \u0026gt; copper.r2jags.b \u0026lt;- jags(data = copper.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;matrixModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 60\rUnobserved stochastic nodes: 31\rTotal graph size: 1971\rInitializing model\r\u0026gt; \u0026gt; print(copper.r2jags.b)\rInference for Bugs model at \u0026quot;matrixModel.txt\u0026quot;, fit using jags,\r2 chains, each with 1500 iterations (first 1000 discarded)\rn.sims = 1000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 10.814 0.685 9.401 10.369 10.795 11.258 12.157 1.001 1000\rbeta[2] -3.544 0.984 -5.440 -4.199 -3.525 -2.869 -1.574 1.002 640\rbeta[3] -10.560 0.966 -12.615 -11.177 -10.559 -9.923 -8.712 1.003 610\rbeta[4] 1.172 0.884 -0.556 0.586 1.199 1.778 2.892 1.002 1000\rbeta[5] 1.582 0.878 -0.167 0.999 1.577 2.158 3.184 1.003 1000\rbeta[6] 2.743 0.857 1.039 2.151 2.719 3.342 4.443 1.003 1000\rbeta[7] -0.073 1.233 -2.504 -0.875 -0.120 0.748 2.508 1.000 1000\rbeta[8] 0.007 1.271 -2.447 -0.792 -0.068 0.868 2.556 1.003 1000\rbeta[9] -0.365 1.257 -2.866 -1.165 -0.397 0.499 1.960 1.000 1000\rbeta[10] 2.184 1.237 -0.254 1.395 2.183 2.954 4.846 1.007 1000\rbeta[11] -0.008 1.204 -2.424 -0.763 -0.013 0.781 2.378 1.008 530\rbeta[12] 4.830 1.235 2.390 4.051 4.840 5.632 7.290 1.018 1000\rgamma[1] 0.182 0.496 -0.719 -0.102 0.117 0.461 1.300 1.023 650\rgamma[2] -0.115 0.515 -1.218 -0.384 -0.074 0.153 0.915 1.020 300\rgamma[3] 0.301 0.541 -0.720 -0.032 0.210 0.593 1.540 1.028 130\rgamma[4] -0.450 0.567 -1.733 -0.791 -0.359 -0.043 0.455 1.032 61\rgamma[5] -0.404 0.520 -1.489 -0.705 -0.328 -0.034 0.455 1.028 130\rgamma[6] 0.867 0.712 -0.169 0.295 0.793 1.306 2.470 1.084 25\rgamma[7] -0.186 0.549 -1.386 -0.497 -0.120 0.106 0.856 1.011 290\rgamma[8] -0.530 0.589 -1.808 -0.936 -0.432 -0.051 0.326 1.059 35\rgamma[9] 0.153 0.523 -0.919 -0.130 0.100 0.444 1.301 1.008 1000\rgamma[10] -0.154 0.512 -1.206 -0.452 -0.101 0.136 0.848 1.026 290\rgamma[11] -0.113 0.517 -1.317 -0.384 -0.078 0.181 0.896 1.004 920\rgamma[12] 0.221 0.546 -0.780 -0.087 0.146 0.541 1.373 1.034 200\rgamma[13] 0.136 0.520 -0.822 -0.170 0.081 0.400 1.345 1.017 1000\rgamma[14] 0.171 0.541 -0.896 -0.123 0.106 0.466 1.345 1.019 470\rgamma[15] -0.085 0.500 -1.090 -0.374 -0.051 0.202 0.886 1.012 1000\rsigma.plate 0.633 0.346 0.050 0.381 0.622 0.842 1.352 1.094 23\rsigma.res 1.385 0.166 1.085 1.274 1.377 1.488 1.750 1.038 44\rdeviance 207.885 8.472 192.227 202.168 207.696 213.399 225.038 1.060 31\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 34.7 and DIC = 242.6\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMCMC diagnostics\rBefore fully exploring the parameters, it is prudent to examine the convergence and mixing diagnostics. Chose either any of the parameterizations (they should yield much the same).\n\u0026gt; library(mcmcplots)\r\u0026gt; denplot(copper.r2jags.b, parms = c(\u0026quot;gamma\u0026quot;,\u0026quot;beta\u0026quot;))\r\u0026gt; traplot(copper.r2jags.b, parms = c(\u0026quot;gamma\u0026quot;,\u0026quot;beta\u0026quot;))\r\u0026gt; \u0026gt; raftery.diag(as.mcmc(copper.r2jags.b))\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r[[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r\u0026gt; \u0026gt; autocorr.diag(as.mcmc(copper.r2jags.b))\rbeta[1] beta[2] beta[3] beta[4] beta[5]\rLag 0 1.000000000 1.00000000 1.000000000 1.000000000 1.000000000\rLag 1 0.021272766 0.04648343 0.007883585 -0.031877909 0.005935939\rLag 5 -0.008158584 -0.04203414 0.003333370 -0.025041071 -0.049596493\rLag 10 0.031505586 -0.03660104 0.063264397 0.005126694 0.061062870\rLag 50 -0.027782043 -0.01419507 -0.063446191 -0.025966769 0.000139520\rbeta[6] beta[7] beta[8] beta[9] beta[10]\rLag 0 1.000000000 1.000000000 1.000000000 1.00000000 1.00000000\rLag 1 0.003141284 0.006332145 -0.016090936 0.02230158 -0.01371002\rLag 5 -0.047609108 -0.015586534 -0.004392271 -0.04095146 0.03636817\rLag 10 -0.021534565 0.002483458 0.022938630 0.03931772 0.09976040\rLag 50 0.018649619 0.039287014 -0.026677246 0.02487322 -0.01257863\rbeta[11] beta[12] deviance gamma[1] gamma[2] gamma[3]\rLag 0 1.000000000 1.00000000 1.00000000 1.00000000 1.00000000 1.00000000\rLag 1 0.003598499 0.04451183 0.42158932 0.09957956 0.03142211 0.07683730\rLag 5 -0.048681325 -0.02569540 0.12353548 0.03983927 -0.00533499 0.02599357\rLag 10 -0.025741832 -0.01822980 0.08655390 -0.02625359 0.05903335 0.05050285\rLag 50 0.008573506 -0.02525275 0.02010397 -0.04670946 -0.04143951 0.01017881\rgamma[4] gamma[5] gamma[6] gamma[7] gamma[8] gamma[9]\rLag 0 1.00000000 1.00000000 1.0000000 1.00000000 1.00000000 1.000000000\rLag 1 0.20659505 0.19599762 0.5113019 0.01034209 0.22908668 0.003942025\rLag 5 0.13726039 0.11488655 0.2890791 -0.07543631 0.15468366 0.039009815\rLag 10 0.08819534 0.06826430 0.1643047 0.03128544 0.03212642 -0.007477517\rLag 50 -0.03923514 0.01121642 -0.1002922 -0.01843480 -0.04706169 -0.012306197\rgamma[10] gamma[11] gamma[12] gamma[13] gamma[14]\rLag 0 1.000000000 1.0000000000 1.000000000 1.00000000 1.000000000\rLag 1 0.010206952 0.0028638893 0.009332531 0.03815594 0.007373479\rLag 5 -0.061360721 0.0008173756 -0.012857899 -0.02174086 0.022461865\rLag 10 -0.013288697 -0.0226321328 0.001324936 0.03479040 0.031318743\rLag 50 0.008887211 -0.0289618811 -0.026443165 0.01353287 0.037485638\rgamma[15] sigma.plate sigma.res\rLag 0 1.000000000 1.0000000 1.00000000\rLag 1 -0.028327792 0.8371048 0.54229156\rLag 5 -0.010034686 0.5053673 0.03764157\rLag 10 -0.010388153 0.3404067 0.01350504\rLag 50 0.002533215 -0.1081944 0.07948304\r\rParameter estimates\r\u0026gt; print(copper.r2jags.b)\rInference for Bugs model at \u0026quot;matrixModel.txt\u0026quot;, fit using jags,\r2 chains, each with 1500 iterations (first 1000 discarded)\rn.sims = 1000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 10.814 0.685 9.401 10.369 10.795 11.258 12.157 1.001 1000\rbeta[2] -3.544 0.984 -5.440 -4.199 -3.525 -2.869 -1.574 1.002 640\rbeta[3] -10.560 0.966 -12.615 -11.177 -10.559 -9.923 -8.712 1.003 610\rbeta[4] 1.172 0.884 -0.556 0.586 1.199 1.778 2.892 1.002 1000\rbeta[5] 1.582 0.878 -0.167 0.999 1.577 2.158 3.184 1.003 1000\rbeta[6] 2.743 0.857 1.039 2.151 2.719 3.342 4.443 1.003 1000\rbeta[7] -0.073 1.233 -2.504 -0.875 -0.120 0.748 2.508 1.000 1000\rbeta[8] 0.007 1.271 -2.447 -0.792 -0.068 0.868 2.556 1.003 1000\rbeta[9] -0.365 1.257 -2.866 -1.165 -0.397 0.499 1.960 1.000 1000\rbeta[10] 2.184 1.237 -0.254 1.395 2.183 2.954 4.846 1.007 1000\rbeta[11] -0.008 1.204 -2.424 -0.763 -0.013 0.781 2.378 1.008 530\rbeta[12] 4.830 1.235 2.390 4.051 4.840 5.632 7.290 1.018 1000\rgamma[1] 0.182 0.496 -0.719 -0.102 0.117 0.461 1.300 1.023 650\rgamma[2] -0.115 0.515 -1.218 -0.384 -0.074 0.153 0.915 1.020 300\rgamma[3] 0.301 0.541 -0.720 -0.032 0.210 0.593 1.540 1.028 130\rgamma[4] -0.450 0.567 -1.733 -0.791 -0.359 -0.043 0.455 1.032 61\rgamma[5] -0.404 0.520 -1.489 -0.705 -0.328 -0.034 0.455 1.028 130\rgamma[6] 0.867 0.712 -0.169 0.295 0.793 1.306 2.470 1.084 25\rgamma[7] -0.186 0.549 -1.386 -0.497 -0.120 0.106 0.856 1.011 290\rgamma[8] -0.530 0.589 -1.808 -0.936 -0.432 -0.051 0.326 1.059 35\rgamma[9] 0.153 0.523 -0.919 -0.130 0.100 0.444 1.301 1.008 1000\rgamma[10] -0.154 0.512 -1.206 -0.452 -0.101 0.136 0.848 1.026 290\rgamma[11] -0.113 0.517 -1.317 -0.384 -0.078 0.181 0.896 1.004 920\rgamma[12] 0.221 0.546 -0.780 -0.087 0.146 0.541 1.373 1.034 200\rgamma[13] 0.136 0.520 -0.822 -0.170 0.081 0.400 1.345 1.017 1000\rgamma[14] 0.171 0.541 -0.896 -0.123 0.106 0.466 1.345 1.019 470\rgamma[15] -0.085 0.500 -1.090 -0.374 -0.051 0.202 0.886 1.012 1000\rsigma.plate 0.633 0.346 0.050 0.381 0.622 0.842 1.352 1.094 23\rsigma.res 1.385 0.166 1.085 1.274 1.377 1.488 1.750 1.038 44\rdeviance 207.885 8.472 192.227 202.168 207.696 213.399 225.038 1.060 31\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 34.7 and DIC = 242.6\rDIC is an estimate of expected predictive error (lower deviance is better).\r\r\rReferences\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1581473594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581473594,"objectID":"8f8997e728c20016329714024b13d58f","permalink":"/jags/partly-nested-anova-jags/block-anova-jags/","publishdate":"2020-02-11T21:13:14-05:00","relpermalink":"/jags/partly-nested-anova-jags/block-anova-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","split-plot","repeated measures","anova"],"title":"Partly Nested Anova - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","anova","STAN","split-plot","repeated measures"],"content":"\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rSplit-plot designs (plots refer to agricultural field plots for which these designs were originally devised) extend unreplicated factorial (randomised complete block and simple repeated measures) designs by incorporating an additional factor whose levels are applied to entire blocks. Similarly, complex repeated measures designs are repeated measures designs in which there are different types of subjects. Consider the example of a randomised complete block. Blocks of four treatments (representing leaf packs subject to different aquatic taxa) were secured in numerous locations throughout a potentially heterogeneous stream. If some of those blocks had been placed in riffles, some in runs and some in pool habitats of the stream, the design becomes a split-plot design incorporating a between block factor (stream region: runs, riffles or pools) and a within block factor (leaf pack exposure type: microbial, macro invertebrate or vertebrate). Furthermore, the design would enable us to investigate whether the roles that different organism scales play on the breakdown of leaf material in stream are consistent across each of the major regions of a stream (interaction between region and exposure type). Alternatively (or in addition), shading could be artificially applied to half of the blocks, thereby introducing a between block effect (whether the block is shaded or not). Extending the repeated measures examples from Tutorial 9.3a, there might have been different populations (such as different species or histories) of rats or sharks. Any single subject (such as an individual shark or rat) can only be of one of the populations types and thus this additional factor represents a between subject effect.\n\rLinear models\rThe linear models for three and four factor partly nested designs are:\n\\[ y_{ijkl} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\gamma)_{ij} + (\\beta\\gamma)_{jk} + \\epsilon_{ijkl},\\]\n\\[ y_{ijklm} = \\mu + \\alpha_i + \\gamma_j + (\\alpha\\gamma)_{ij} + \\beta_k + \\delta_l + (\\alpha\\delta)_{il} + (\\gamma\\delta)_{jl} + (\\alpha\\gamma\\delta)_{ijl} + \\epsilon_{ijklm}, \\;\\;\\; \\text{(Model 2 additive - 2 between)}\\]\n\\[ y_{ijklm} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\delta_l + (\\gamma\\delta)_{kl} + (\\alpha\\gamma)_{ik} + (\\alpha\\delta)_{il} + (\\alpha\\gamma\\delta)_{ikl} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 2 additive - 1 between)}\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\beta\\) is the effect of the Blocking Factor B and \\(\\epsilon\\) is the random unexplained or residual component.\n\rAssumptions\rAs partly nested designs share elements in common with each of nested, factorial and unreplicated factorial designs, they also share similar assumptions and implications to these other designs. Specifically, hypothesis tests assume that:\n\rthe appropriate residuals are normally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator (see Tables above) be used to explore normality. Scale transformations are often useful.\n\rthe appropriate residuals are equally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\n\rthe appropriate residuals are independent of one another. Critically, experimental units within blocks/subjects should be adequately spaced temporally and spatially to restrict contamination or carryover effects. Non-independence resulting from the hierarchical design should be accounted for.\n\rthat the variance/covariance matrix displays sphericity (strickly, the variance-covariance matrix must display a very specific pattern of sphericity in which both variances and covariances are equal (compound symmetry), however, an F-ratio will still reliably follow an F distribution provided basic sphericity holds). This assumption is likely to be met only if the treatment levels within each block can be randomly ordered. This assumption can be managed by either adjusting the sensitivity of the affected F-ratios or employing linear mixed effects modelling to the design.\n\rthere are no block by within block interactions. Such interactions render non-significant within block effects difficult to interpret unless we assume that there are no block by within block interactions, non-significant within block effects could be due to either an absence of a treatment effect, or as a result of opposing effects within different blocks. As these block by within block interactions are unreplicated, they can neither be formally tested nor is it possible to perform main effects tests to diagnose non-significant within block effects.\n\r\r\r\rSplit-plot design\rData generation\rImagine we has designed an experiment in which we intend to measure a response (\\(y\\)) to one of treatments (three levels; “a1”, “a2” and “a3”). Unfortunately, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability you decide to apply a design (RCB) in which each of the treatments within each of \\(35\\) blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; library(plyr)\r\u0026gt; set.seed(123)\r\u0026gt; nA \u0026lt;- 3\r\u0026gt; nC \u0026lt;- 3\r\u0026gt; nBlock \u0026lt;- 36\r\u0026gt; sigma \u0026lt;- 5\r\u0026gt; sigma.block \u0026lt;- 12\r\u0026gt; n \u0026lt;- nBlock*nC\r\u0026gt; Block \u0026lt;- gl(nBlock, k=1)\r\u0026gt; C \u0026lt;- gl(nC,k=1)\r\u0026gt; \u0026gt; ## Specify the cell means\r\u0026gt; AC.means\u0026lt;-(rbind(c(40,70,80),c(35,50,70),c(35,40,45)))\r\u0026gt; ## Convert these to effects\r\u0026gt; X \u0026lt;- model.matrix(~A*C,data=expand.grid(A=gl(3,k=1),C=gl(3,k=1)))\r\u0026gt; AC \u0026lt;- as.vector(AC.means)\r\u0026gt; AC.effects \u0026lt;- solve(X,AC)\r\u0026gt; \u0026gt; A \u0026lt;- gl(nA,nBlock,n)\r\u0026gt; dt \u0026lt;- expand.grid(C=C,Block=Block)\r\u0026gt; dt \u0026lt;- data.frame(dt,A)\r\u0026gt; \u0026gt; Xmat \u0026lt;- cbind(model.matrix(~-1+Block, data=dt),model.matrix(~A*C, data=dt))\r\u0026gt; block.effects \u0026lt;- rnorm(n = nBlock, mean =0 , sd = sigma.block)\r\u0026gt; all.effects \u0026lt;- c(block.effects, AC.effects)\r\u0026gt; lin.pred \u0026lt;- Xmat %*% all.effects\r\u0026gt; \u0026gt; ## the quadrat observations (within sites) are drawn from\r\u0026gt; ## normal distributions with means according to the site means\r\u0026gt; ## and standard deviations of 5\r\u0026gt; y \u0026lt;- rnorm(n,lin.pred,sigma)\r\u0026gt; data.splt \u0026lt;- data.frame(y=y, A=A,dt)\r\u0026gt; head(data.splt) #print out the first six rows of the data set\ry A C Block A.1\r1 36.04388 1 1 1 1\r2 62.96473 1 2 1 1\r3 71.74448 1 3 1 1\r4 35.33552 1 1 2 1\r5 63.76434 1 2 2 1\r6 76.19828 1 3 2 1\r\u0026gt; \u0026gt; tapply(data.splt$y,data.splt$A,mean)\r1 2 3 65.71431 49.43047 41.36212 \u0026gt; \u0026gt; tapply(data.splt$y,data.splt$C,mean)\r1 2 3 38.41079 53.56792 64.52819 \u0026gt; \u0026gt; replications(y~A*C+Error(Block), data.splt)\rA C A:C 36 36 12 \u0026gt; \u0026gt; library(ggplot2)\r\u0026gt; ggplot(data.splt, aes(y=y, x=C, linetype=A, group=A)) + geom_line(stat=\u0026#39;summary\u0026#39;, fun.y=mean)\r\u0026gt; \u0026gt; ggplot(data.splt, aes(y=y, x=C,color=A)) + geom_point() + facet_wrap(~Block)\r\rExploratory data analysis\rNormality and Homogeneity of variance\n\u0026gt; # check between plot effects\r\u0026gt; boxplot(y~A, ddply(data.splt,~A+Block, summarise,y=mean(y)))\r\u0026gt; \u0026gt; #OR\r\u0026gt; ggplot(ddply(data.splt,~A+Block, summarise,y=mean(y)), aes(y=y, x=A)) + geom_boxplot()\r\u0026gt; \u0026gt; # check within plot effects\r\u0026gt; boxplot(y~A*C, data.splt)\r\u0026gt; \u0026gt; #OR \u0026gt; ggplot(data.splt, aes(y=y, x=C, fill=A)) + geom_boxplot()\rConclusions:\n\rthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\n\rthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the y-axis. Hence it there is no evidence of non-homogeneity.\n\r\rObvious violations could be addressed either by:\n\rtransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\r\rBlock by within-Block interaction\n\u0026gt; library(car)\r\u0026gt; with(data.splt, interaction.plot(C,Block,y))\r\u0026gt; \u0026gt; #OR with ggplot\r\u0026gt; library(ggplot2)\r\u0026gt; ggplot(data.splt, aes(y=y, x=C, group=Block,color=Block)) + geom_line() +\r+ guides(color=guide_legend(ncol=3))\r\u0026gt; \u0026gt; residualPlots(lm(y~Block+A*C, data.splt))\r Test stat Pr(\u0026gt;|Test stat|)\rBlock A C Tukey test 1.4518 0.1466\r\u0026gt; \u0026gt; # the Tukey\u0026#39;s non-additivity test by itself can be obtained via an internal function\r\u0026gt; # within the car package\r\u0026gt; car:::tukeyNonaddTest(lm(y~Block+A*C, data.splt))\rTest Pvalue 1.4517644 0.1465671 \rConclusions:\n\rthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (C). Any trends appear to be reasonably consistent between Blocks.\r\r\r\rExample - split-plot\rIn an attempt to understand the effects on marine animals of short-term exposure to toxic substances, such as might occur following a spill, or a major increase in storm water flows, a it was decided to examine the toxicant in question, Copper, as part of a field experiment in Honk Kong. The experiment consisted of small sources of Cu (small, hemispherical plaster blocks, impregnated with copper), which released the metal into sea water over \\(4\\) or \\(5\\) days. The organism whose response to Cu was being measured was a small, polychaete worm, Hydroides, that attaches to hard surfaces in the sea, and is one of the first species to colonize any surface that is submerged. The biological questions focused on whether the timing of exposure to Cu affects the overall abundance of these worms. The time period of interest was the first or second week after a surface being available.\nThe experimental setup consisted of sheets of black perspex (settlement plates), which provided good surfaces for these worms. Each plate had a plaster block bolted to its centre, and the dissolving block would create a gradient of [Cu] across the plate. Over the two weeks of the experiment, a given plate would have pl ain plaster blocks (Control) or a block containing copper in the first week, followed by a plain block, or a plain block in the first week, followed by a dose of copper in the second week. After two weeks in the water, plates were removed and counted back in the laboratory. Without a clear idea of how sensitive these worms are to copper, an effect of the treatments might show up as an overall difference in the density of worms across a plate, or it could show up as a gradient in abundance across the plate, with a different gradient in different treatments. Therefore, on each plate, the density of worms was recorded at each of four distances from the center of the plate. Let’s have a look at the dataset\n\u0026gt; copper \u0026lt;- read.table(\u0026#39;copper.csv\u0026#39;, header=T, sep=\u0026#39;,\u0026#39;, strip.white=T)\r\u0026gt; head(copper)\rCOPPER PLATE DIST WORMS\r1 control 200 4 11.50\r2 control 200 3 13.00\r3 control 200 2 13.50\r4 control 200 1 12.00\r5 control 39 4 17.75\r6 control 39 3 13.75\rVariables’ description:\nCopper. Categorical listing of the copper treatment (control = no copper applied, week 2 = copper treatment applied in second week and week 1= copper treatment applied in first week) applied to whole plates. Factor A (between plot factor).\nPlate. Substrate provided for polychaete worm colonization on which copper treatment applied. These are the plots (Factor B). Numbers in this column represent numerical labels given to each plate.\nDist. Categorical listing for the four concentric distances from the center of the plate (source of copper treatment) with 1 being the closest and 4 the furthest. Factor C (within plot factor)\nWorms. Density of worms measured. Response variable.\nThe Plates are the “random” groups. Within each Plate, all levels of the Distance factor occur (this is a within group factor). Each Plate can only be of one of the three levels of the Copper treatment. This is therefore a within group (nested) factor. Traditionally, this mixture of nested and randomised block design would be called a partly nested or split-plot design. In Bayesian (multilevel modeling) terms, this is a multi-level model with one hierarchical level the Plates means and another representing the Copper treatment means (based on the Plate means). Exploratory data analysis has indicated that the response variable could be normalised via a forth-root transformation.\nModel fitting\rWe will only explore the matrix parameterisation (random intercepts) of the model, where\n\\[\\text{number of lesions}_i = \\beta \\text{Site}_{j(i)} + \\epsilon_{i},\\]\nwhere \\(\\epsilon_i∼ N(0,\\sigma^2)\\) and we treat Distance as a factor.\n\u0026gt; rstanString=\u0026quot;\r+ data{\r+ int n;\r+ int nZ;\r+ int nX;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ matrix [n,nZ] Z;\r+ vector [nX] a0;\r+ matrix [nX,nX] A0;\r+ }\r+ + parameters{\r+ vector [nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ vector [nZ] gamma;\r+ real\u0026lt;lower=0\u0026gt; sigma_Z;\r+ }\r+ transformed parameters {\r+ vector [n] mu;\r+ + mu = X*beta + Z*gamma; + } + model{\r+ // Priors\r+ beta ~ multi_normal(a0,A0);\r+ gamma ~ normal( 0 , sigma_Z );\r+ sigma_Z ~ cauchy(0,25);\r+ sigma ~ cauchy(0,25);\r+ + y ~ normal( mu , sigma );\r+ }\r+ generated quantities {\r+ vector [n] y_err;\r+ real\u0026lt;lower=0\u0026gt; sd_Resid;\r+ + y_err = y - mu;\r+ sd_Resid = sd(y_err);\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstanString, con = \u0026quot;matrixModel.stan\u0026quot;)\r\u0026gt; \u0026gt; \u0026gt; #sort the data set so that the copper treatments are in a more logical order\r\u0026gt; library(dplyr)\r\u0026gt; copper$DIST \u0026lt;- factor(copper$DIST)\r\u0026gt; copper$PLATE \u0026lt;- factor(copper$PLATE)\r\u0026gt; copper.sort \u0026lt;- arrange(copper,COPPER,PLATE,DIST)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~COPPER*DIST, data=copper.sort)\r\u0026gt; Zmat \u0026lt;- model.matrix(~-1+PLATE, data=copper.sort)\r\u0026gt; copper.list \u0026lt;- list(y=copper.sort$WORMS,\r+ X=Xmat, nX=ncol(Xmat),\r+ Z=Zmat, nZ=ncol(Zmat),\r+ n=nrow(copper.sort),\r+ a0=rep(0,ncol(Xmat)), A0=diag(100000,ncol(Xmat))\r+ )\r\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026quot;gamma\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma_Z\u0026quot;)\r\u0026gt; burnInSteps = 500\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; library(rstan)\r\u0026gt; library(coda)\r\u0026gt; \u0026gt; copper.rstan.a \u0026lt;- stan(data = copper.list, file = \u0026quot;matrixModel.stan\u0026quot;, + chains = nChains, pars = params, iter = nIter, + warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;matrixModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 2500 [ 0%] (Warmup)\rChain 1: Iteration: 250 / 2500 [ 10%] (Warmup)\rChain 1: Iteration: 500 / 2500 [ 20%] (Warmup)\rChain 1: Iteration: 501 / 2500 [ 20%] (Sampling)\rChain 1: Iteration: 750 / 2500 [ 30%] (Sampling)\rChain 1: Iteration: 1000 / 2500 [ 40%] (Sampling)\rChain 1: Iteration: 1250 / 2500 [ 50%] (Sampling)\rChain 1: Iteration: 1500 / 2500 [ 60%] (Sampling)\rChain 1: Iteration: 1750 / 2500 [ 70%] (Sampling)\rChain 1: Iteration: 2000 / 2500 [ 80%] (Sampling)\rChain 1: Iteration: 2250 / 2500 [ 90%] (Sampling)\rChain 1: Iteration: 2500 / 2500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.359 seconds (Warm-up)\rChain 1: 1.351 seconds (Sampling)\rChain 1: 1.71 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;matrixModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 2500 [ 0%] (Warmup)\rChain 2: Iteration: 250 / 2500 [ 10%] (Warmup)\rChain 2: Iteration: 500 / 2500 [ 20%] (Warmup)\rChain 2: Iteration: 501 / 2500 [ 20%] (Sampling)\rChain 2: Iteration: 750 / 2500 [ 30%] (Sampling)\rChain 2: Iteration: 1000 / 2500 [ 40%] (Sampling)\rChain 2: Iteration: 1250 / 2500 [ 50%] (Sampling)\rChain 2: Iteration: 1500 / 2500 [ 60%] (Sampling)\rChain 2: Iteration: 1750 / 2500 [ 70%] (Sampling)\rChain 2: Iteration: 2000 / 2500 [ 80%] (Sampling)\rChain 2: Iteration: 2250 / 2500 [ 90%] (Sampling)\rChain 2: Iteration: 2500 / 2500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.289 seconds (Warm-up)\rChain 2: 1.102 seconds (Sampling)\rChain 2: 1.391 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(copper.rstan.a, par = c(\u0026quot;beta\u0026quot;,\u0026quot;gamma\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma_Z\u0026quot;))\rInference for Stan model: matrixModel.\r2 chains, each with iter=2500; warmup=500; thin=1; post-warmup draws per chain=2000, total post-warmup draws=4000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 10.87 0.02 0.70 9.48 10.41 10.88 11.34 12.24 1426 1\rbeta[2] -3.59 0.02 0.97 -5.39 -4.22 -3.62 -2.94 -1.63 1576 1\rbeta[3] -10.64 0.03 1.00 -12.56 -11.30 -10.63 -10.00 -8.63 1461 1\rbeta[4] 1.13 0.02 0.89 -0.58 0.52 1.14 1.73 2.86 1590 1\rbeta[5] 1.53 0.02 0.88 -0.22 0.97 1.51 2.10 3.24 1694 1\rbeta[6] 2.68 0.02 0.88 0.97 2.08 2.68 3.28 4.44 1773 1\rbeta[7] -0.04 0.03 1.24 -2.58 -0.86 -0.02 0.79 2.35 1612 1\rbeta[8] 0.09 0.03 1.27 -2.37 -0.77 0.10 0.90 2.58 1500 1\rbeta[9] -0.29 0.03 1.24 -2.78 -1.10 -0.31 0.53 2.17 1916 1\rbeta[10] 2.25 0.03 1.26 -0.31 1.44 2.24 3.09 4.73 1689 1\rbeta[11] 0.04 0.03 1.22 -2.37 -0.76 0.03 0.89 2.49 1913 1\rbeta[12] 4.93 0.03 1.26 2.47 4.06 4.96 5.77 7.39 1862 1\rgamma[1] 0.14 0.01 0.50 -0.82 -0.13 0.09 0.41 1.25 3366 1\rgamma[2] -0.12 0.01 0.48 -1.19 -0.38 -0.08 0.16 0.80 3072 1\rgamma[3] 0.27 0.01 0.50 -0.61 -0.04 0.20 0.56 1.36 1982 1\rgamma[4] -0.42 0.02 0.56 -1.70 -0.75 -0.32 -0.02 0.49 1128 1\rgamma[5] -0.35 0.01 0.54 -1.57 -0.68 -0.27 0.01 0.57 1298 1\rgamma[6] 0.77 0.03 0.66 -0.17 0.23 0.68 1.21 2.27 527 1\rgamma[7] -0.14 0.01 0.49 -1.21 -0.41 -0.10 0.13 0.78 2791 1\rgamma[8] -0.48 0.02 0.57 -1.78 -0.83 -0.38 -0.06 0.38 994 1\rgamma[9] 0.14 0.01 0.51 -0.85 -0.13 0.09 0.42 1.27 3373 1\rgamma[10] -0.12 0.01 0.49 -1.22 -0.38 -0.07 0.15 0.87 2386 1\rgamma[11] -0.13 0.01 0.50 -1.25 -0.38 -0.07 0.15 0.84 3156 1\rgamma[12] 0.23 0.01 0.51 -0.72 -0.08 0.15 0.51 1.39 2410 1\rgamma[13] 0.12 0.01 0.48 -0.86 -0.15 0.08 0.37 1.19 2860 1\rgamma[14] 0.10 0.01 0.47 -0.81 -0.16 0.06 0.36 1.12 3695 1\rgamma[15] -0.07 0.01 0.48 -1.07 -0.33 -0.05 0.18 0.92 2868 1\rsigma 1.40 0.00 0.16 1.12 1.28 1.39 1.50 1.74 1976 1\rsigma_Z 0.59 0.02 0.33 0.07 0.35 0.57 0.78 1.32 293 1\rSamples were drawn using NUTS(diag_e) at Thu Feb 20 12:32:28 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\rMCMC diagnostics\rBefore fully exploring the parameters, it is prudent to examine the convergence and mixing diagnostics. Chose either any of the parameterizations (they should yield much the same).\n\u0026gt; library(mcmcplots)\r\u0026gt; mcmc\u0026lt;-As.mcmc.list(copper.rstan.a)\r\u0026gt; denplot(mcmc, parms = c(\u0026quot;gamma\u0026quot;,\u0026quot;beta\u0026quot;))\r\u0026gt; traplot(mcmc, parms = c(\u0026quot;gamma\u0026quot;,\u0026quot;beta\u0026quot;))\r\u0026gt; \u0026gt; raftery.diag(mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r[[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r\u0026gt; \u0026gt; autocorr.diag(mcmc)\rbeta[1] beta[2] beta[3] beta[4] beta[5]\rLag 0 1.000000000 1.000000000 1.000000000 1.000000000 1.000000000\rLag 1 0.374720353 0.341183169 0.339353246 0.252337507 0.223210571\rLag 5 0.037069892 0.042278779 0.047564125 0.047007030 0.044328516\rLag 10 -0.018950104 -0.021881833 -0.015112174 0.011465780 -0.008203347\rLag 50 -0.005272957 0.005509308 0.001729163 -0.003792619 0.015467083\rbeta[6] beta[7] beta[8] beta[9] beta[10]\rLag 0 1.000000000 1.000000000 1.000000000 1.0000000000 1.00000000\rLag 1 0.248289032 0.238576715 0.236228981 0.1646087721 0.19406340\rLag 5 -0.001293327 0.049527024 0.037748575 0.0457484755 0.05185113\rLag 10 -0.004761031 0.016421650 0.006103246 0.0152931698 -0.01273015\rLag 50 0.009409452 -0.004834221 0.014643862 -0.0005634728 0.01928668\rbeta[11] beta[12] gamma[1] gamma[2] gamma[3]\rLag 0 1.000000000 1.00000000 1.000000000 1.000000000 1.00000000\rLag 1 0.213823117 0.19537107 -0.034803110 -0.029616893 0.07949767\rLag 5 0.003448645 0.03142149 -0.003593144 0.003964975 0.02093772\rLag 10 -0.010336339 -0.01411371 0.007406842 0.015861539 0.01734448\rLag 50 0.013120369 0.01958243 0.012279525 -0.010622170 0.02076734\rgamma[4] gamma[5] gamma[6] gamma[7] gamma[8] gamma[9]\rLag 0 1.0000000000 1.00000000 1.000000000 1.00000000 1.00000000 1.000000000\rLag 1 0.0777348449 0.04146761 0.253395982 -0.02762790 0.15664570 -0.122703697\rLag 5 0.0918211517 0.05188041 0.186109087 0.01799780 0.11365334 0.014013010\rLag 10 0.0205419176 0.01488015 0.111254462 0.01787382 0.07047541 -0.036581117\rLag 50 -0.0002484341 0.03083189 0.008178744 -0.01353451 0.01312759 0.009694576\rgamma[10] gamma[11] gamma[12] gamma[13] gamma[14]\rLag 0 1.00000000 1.000000000 1.0000000000 1.000000000 1.000000000\rLag 1 0.01442355 0.020369909 0.0743534500 0.016227217 -0.118676601\rLag 5 0.03061277 0.019660434 0.0165038770 -0.001870110 0.031972820\rLag 10 0.01772291 0.004535563 0.0007313124 0.009756988 0.006852258\rLag 50 0.03072942 -0.007224169 0.0055715797 0.008139908 -0.013028616\rgamma[15] sigma sigma_Z lp__\rLag 0 1.000000000 1.00000000 1.00000000 1.0000000\rLag 1 0.034449386 0.04127144 0.64455495 0.8349340\rLag 5 0.025826600 0.03738441 0.34261919 0.5098635\rLag 10 0.006380207 0.03486901 0.18239312 0.3103617\rLag 50 -0.008016675 0.01565392 0.04153586 0.1096875\r\rParameter estimates\r\u0026gt; print(copper.rstan.a)\rInference for Stan model: matrixModel.\r2 chains, each with iter=2500; warmup=500; thin=1; post-warmup draws per chain=2000, total post-warmup draws=4000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 10.87 0.02 0.70 9.48 10.41 10.88 11.34 12.24 1426 1\rbeta[2] -3.59 0.02 0.97 -5.39 -4.22 -3.62 -2.94 -1.63 1576 1\rbeta[3] -10.64 0.03 1.00 -12.56 -11.30 -10.63 -10.00 -8.63 1461 1\rbeta[4] 1.13 0.02 0.89 -0.58 0.52 1.14 1.73 2.86 1590 1\rbeta[5] 1.53 0.02 0.88 -0.22 0.97 1.51 2.10 3.24 1694 1\rbeta[6] 2.68 0.02 0.88 0.97 2.08 2.68 3.28 4.44 1773 1\rbeta[7] -0.04 0.03 1.24 -2.58 -0.86 -0.02 0.79 2.35 1612 1\rbeta[8] 0.09 0.03 1.27 -2.37 -0.77 0.10 0.90 2.58 1500 1\rbeta[9] -0.29 0.03 1.24 -2.78 -1.10 -0.31 0.53 2.17 1916 1\rbeta[10] 2.25 0.03 1.26 -0.31 1.44 2.24 3.09 4.73 1689 1\rbeta[11] 0.04 0.03 1.22 -2.37 -0.76 0.03 0.89 2.49 1913 1\rbeta[12] 4.93 0.03 1.26 2.47 4.06 4.96 5.77 7.39 1862 1\rgamma[1] 0.14 0.01 0.50 -0.82 -0.13 0.09 0.41 1.25 3366 1\rgamma[2] -0.12 0.01 0.48 -1.19 -0.38 -0.08 0.16 0.80 3072 1\rgamma[3] 0.27 0.01 0.50 -0.61 -0.04 0.20 0.56 1.36 1982 1\rgamma[4] -0.42 0.02 0.56 -1.70 -0.75 -0.32 -0.02 0.49 1128 1\rgamma[5] -0.35 0.01 0.54 -1.57 -0.68 -0.27 0.01 0.57 1298 1\rgamma[6] 0.77 0.03 0.66 -0.17 0.23 0.68 1.21 2.27 527 1\rgamma[7] -0.14 0.01 0.49 -1.21 -0.41 -0.10 0.13 0.78 2791 1\rgamma[8] -0.48 0.02 0.57 -1.78 -0.83 -0.38 -0.06 0.38 994 1\rgamma[9] 0.14 0.01 0.51 -0.85 -0.13 0.09 0.42 1.27 3373 1\rgamma[10] -0.12 0.01 0.49 -1.22 -0.38 -0.07 0.15 0.87 2386 1\rgamma[11] -0.13 0.01 0.50 -1.25 -0.38 -0.07 0.15 0.84 3156 1\rgamma[12] 0.23 0.01 0.51 -0.72 -0.08 0.15 0.51 1.39 2410 1\rgamma[13] 0.12 0.01 0.48 -0.86 -0.15 0.08 0.37 1.19 2860 1\rgamma[14] 0.10 0.01 0.47 -0.81 -0.16 0.06 0.36 1.12 3695 1\rgamma[15] -0.07 0.01 0.48 -1.07 -0.33 -0.05 0.18 0.92 2868 1\rsigma 1.40 0.00 0.16 1.12 1.28 1.39 1.50 1.74 1976 1\rsigma_Z 0.59 0.02 0.33 0.07 0.35 0.57 0.78 1.32 293 1\rlp__ -45.65 0.81 8.87 -59.73 -51.16 -46.96 -41.97 -21.53 119 1\rSamples were drawn using NUTS(diag_e) at Thu Feb 20 12:32:28 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\r\rReferences\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\r\r\r","date":1581473594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581473594,"objectID":"63b9bb0856a5816130149fbc27906c24","permalink":"/stan/partly-nested-anova-stan/partly-nested-anova-stan/","publishdate":"2020-02-11T21:13:14-05:00","relpermalink":"/stan/partly-nested-anova-stan/partly-nested-anova-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","split-plot","repeated measures","anova"],"title":"Partly Nested Anova - STAN","type":"STAN"},{"authors":[],"categories":null,"content":"","date":1581411600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581411600,"objectID":"653a898aad846976703184dfd5ad1869","permalink":"/talk/heartcourse2020/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/talk/heartcourse2020/","section":"talk","summary":"One day short course","tags":["Economic Evaluations","Clinical Trials","HEART"],"title":"Understanding Health Economics in Clinical Trials","type":"talk"},{"authors":["Andrea Gabrio"],"categories":["R","anova","JAGS","randomised complete block"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rIn the previous tutorial (nested ANOVA), we introduced the concept of employing sub-replicates that are nested within the main treatment levels as a means of absorbing some of the unexplained variability that would otherwise arise from designs in which sampling units are selected from amongst highly heterogeneous conditions. Such (nested) designs are useful in circumstances where the levels of the main treatment (such as burnt and un-burnt sites) occur at a much larger temporal or spatial scale than the experimental/sampling units (e.g. vegetation monitoring quadrats). For circumstances in which the main treatments can be applied (or naturally occur) at the same scale as the sampling units (such as whether a stream rock is enclosed by a fish proof fence or not), an alternative design is available. In this design (randomised complete block design), each of the levels of the main treatment factor are grouped (blocked) together (in space and/or time) and therefore, whilst the conditions between the groups (referred to as “blocks”) might vary substantially, the conditions under which each of the levels of the treatment are tested within any given block are far more homogeneous.\nIf any differences between blocks (due to the heterogeneity) can account for some of the total variability between the sampling units (thereby reducing the amount of variability that the main treatment(s) failed to explain), then the main test of treatment effects will be more powerful/sensitive. As an simple example of a randomised complete block (RCB) design, consider an investigation into the roles of different organism scales (microbial, macro invertebrate and vertebrate) on the breakdown of leaf debris packs within streams. An experiment could consist of four treatment levels - leaf packs protected by fish-proof mesh, leaf packs protected by fine macro invertebrate exclusion mesh, leaf packs protected by dissolving antibacterial tablets, and leaf packs relatively unprotected as controls. As an acknowledgement that there are many other unmeasured factors that could influence leaf pack breakdown (such as flow velocity, light levels, etc) and that these are likely to vary substantially throughout a stream, the treatments are to be arranged into groups or “blocks” (each containing a single control, microbial, macro invertebrate and fish protected leaf pack). Blocks of treatment sets are then secured in locations haphazardly selected throughout a particular reach of stream. Importantly, the arrangement of treatments in each block must be randomized to prevent the introduction of some systematic bias - such as light angle, current direction etc.\nBlocking does however come at a cost. The blocks absorb both unexplained variability as well as degrees of freedom from the residuals. Consequently, if the amount of the total unexplained variation that is absorbed by the blocks is not sufficiently large enough to offset the reduction in degrees of freedom (which may result from either less than expected heterogeneity, or due to the scale at which the blocks are established being inappropriate to explain much of the variation), for a given number of sampling units (leaf packs), the tests of main treatment effects will suffer power reductions. Treatments can also be applied sequentially or repeatedly at the scale of the entire block, such that at any single time, only a single treatment level is being applied (see the lower two sub-figures above). Such designs are called repeated measures. A repeated measures ANOVA is to an single factor ANOVA as a paired t-test is to a independent samples t-test. One example of a repeated measures analysis might be an investigation into the effects of a five different diet drugs (four doses and a placebo) on the food intake of lab rats. Each of the rats (“subjects”) is subject to each of the four drugs (within subject effects) which are administered in a random order. In another example, temporal recovery responses of sharks to bi-catch entanglement stresses might be simulated by analyzing blood samples collected from captive sharks (subjects) every half hour for three hours following a stress inducing restraint. This repeated measures design allows the anticipated variability in stress tolerances between individual sharks to be accounted for in the analysis (so as to permit more powerful test of the main treatments). Furthermore, by performing repeated measures on the same subjects, repeated measures designs reduce the number of subjects required for the investigation. Essentially, this is a randomised complete block design except that the within subject (block) effect (e.g. time since stress exposure) cannot be randomised.\nTo suppress contamination effects resulting from the proximity of treatment sampling units within a block, units should be adequately spaced in time and space. For example, the leaf packs should not be so close to one another that the control packs are effected by the antibacterial tablets and there should be sufficient recovery time between subsequent drug administrations. In addition, the order or arrangement of treatments within the blocks must be randomized so as to prevent both confounding as well as computational complications. Whilst this is relatively straight forward for the classic randomized complete block design (such as the leaf packs in streams), it is logically not possible for repeated measures designs. Blocking factors are typically random factors that represent all the possible blocks that could be selected. As such, no individual block can truly be replicated. Randomised complete block and repeated measures designs can therefore also be thought of as un-replicated factorial designs in which there are two or more factors but that the interactions between the blocks and all the within block factors are not replicated.\n\rLinear models\rThe linear models for two and three factor nested design are:\n\\[ y_{ij} = \\mu + \\beta_i + \\alpha_j + \\epsilon_{ij},\\]\n\\[ y_{ijk} = \\mu + \\beta_i + \\alpha_j + \\gamma_k + (\\beta\\alpha)_{ij} + (\\beta\\gamma)_{ik} + (\\alpha\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 1)}\\]\n\\[ y_{ijk} = \\mu + \\beta_i + \\alpha_j + \\gamma_k + (\\alpha\\gamma)_{jk} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 2)},\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\beta\\) is the effect of the Blocking Factor B (\\(\\sum \\beta=0\\)), \\(\\alpha\\) and \\(\\gamma\\) are the effects of withing block Factor A and Factor C, respectively, and \\(\\epsilon \\sim N(0,\\sigma^2)\\) is the random unexplained or residual component.\nTests for the effects of blocks as well as effects within blocks assume that there are no interactions between blocks and the within block effects. That is, it is assumed that any effects are of similar nature within each of the blocks. Whilst this assumption may well hold for experiments that are able to consciously set the scale over which the blocking units are arranged, when designs utilize arbitrary or naturally occurring blocking units, the magnitude and even polarity of the main effects are likely to vary substantially between the blocks. The preferred (non-additive or “Model 1”) approach to un-replicated factorial analysis of some bio-statisticians is to include the block by within subject effect interactions (e.g. \\(\\beta\\alpha\\)). Whilst these interaction effects cannot be formally tested, they can be used as the denominators in F-ratio calculations of their respective main effects tests. Proponents argue that since these blocking interactions cannot be formally tested, there is no sound inferential basis for using these error terms separately. Alternatively, models can be fitted additively (“Model 2”) whereby all the block by within subject effect interactions are pooled into a single residual term (\\(\\epsilon\\)). Although the latter approach is simpler, each of the within subject effects tests do assume that there are no interactions involving the blocks and that perhaps even more restrictively, that sphericity holds across the entire design.\n\rAssumptions\rAs with other ANOVA designs, the reliability of hypothesis tests is dependent on the residuals being:\n\rnormally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator should be used to explore normality. Scale transformations are often useful.\n\requally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\n\rindependent of one another. Although the observations within a block may not strictly be independent, provided the treatments are applied or ordered randomly within each block or subject, within block proximity effects on the residuals should be random across all blocks and thus the residuals should still be independent of one another. Nevertheless, it is important that experimental units within blocks are adequately spaced in space and time so as to suppress contamination or carryover effects.\n\r\r\r\rSimple RCB\rData generation\rImagine we has designed an experiment in which we intend to measure a response (y) to one of treatments (three levels; “a1”, “a2” and “a3”). Unfortunately, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability you decide to apply a design (RCB) in which each of the treatments within each of 35 blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; library(plyr)\r\u0026gt; set.seed(123)\r\u0026gt; nTreat \u0026lt;- 3\r\u0026gt; nBlock \u0026lt;- 35\r\u0026gt; sigma \u0026lt;- 5\r\u0026gt; sigma.block \u0026lt;- 12\r\u0026gt; n \u0026lt;- nBlock*nTreat\r\u0026gt; Block \u0026lt;- gl(nBlock, k=1)\r\u0026gt; A \u0026lt;- gl(nTreat,k=1)\r\u0026gt; dt \u0026lt;- expand.grid(A=A,Block=Block)\r\u0026gt; #Xmat \u0026lt;- model.matrix(~Block + A + Block:A, data=dt)\r\u0026gt; Xmat \u0026lt;- model.matrix(~-1+Block + A, data=dt)\r\u0026gt; block.effects \u0026lt;- rnorm(n = nBlock, mean = 40, sd = sigma.block)\r\u0026gt; A.effects \u0026lt;- c(30,40)\r\u0026gt; all.effects \u0026lt;- c(block.effects,A.effects)\r\u0026gt; lin.pred \u0026lt;- Xmat %*% all.effects\r\u0026gt; \u0026gt; # OR\r\u0026gt; Xmat \u0026lt;- cbind(model.matrix(~-1+Block,data=dt),model.matrix(~-1+A,data=dt))\r\u0026gt; ## Sum to zero block effects\r\u0026gt; block.effects \u0026lt;- rnorm(n = nBlock, mean = 0, sd = sigma.block)\r\u0026gt; A.effects \u0026lt;- c(40,70,80)\r\u0026gt; all.effects \u0026lt;- c(block.effects,A.effects)\r\u0026gt; lin.pred \u0026lt;- Xmat %*% all.effects\r\u0026gt; \u0026gt; \u0026gt; \u0026gt; ## the quadrat observations (within sites) are drawn from\r\u0026gt; ## normal distributions with means according to the site means\r\u0026gt; ## and standard deviations of 5\r\u0026gt; y \u0026lt;- rnorm(n,lin.pred,sigma)\r\u0026gt; data.rcb \u0026lt;- data.frame(y=y, expand.grid(A=A, Block=Block))\r\u0026gt; head(data.rcb) #print out the first six rows of the data set\ry A Block\r1 45.80853 1 1\r2 66.71784 2 1\r3 93.29238 3 1\r4 43.10101 1 2\r5 73.20697 2 2\r6 91.77487 3 2\r\rExploratory data analysis\rNormality and Homogeneity of variance\n\u0026gt; boxplot(y~A, data.rcb)\rConclusions:\n\rthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\n\rthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. . More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\n\r\rObvious violations could be addressed either by:\n\rtransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\r\rBlock by within-Block interaction\n\u0026gt; library(car)\r\u0026gt; with(data.rcb, interaction.plot(A,Block,y))\r\u0026gt; \u0026gt; #OR with ggplot\r\u0026gt; library(ggplot2)\r\u0026gt; ggplot(data.rcb, aes(y=y, x=A, group=Block,color=Block)) + geom_line() +\r+ guides(color=guide_legend(ncol=3))\r\u0026gt; \u0026gt; residualPlots(lm(y~Block+A, data.rcb))\r Test stat Pr(\u0026gt;|Test stat|)\rBlock A Tukey test -1.4163 0.1567\r\u0026gt; \u0026gt; # the Tukey\u0026#39;s non-additivity test by itself can be obtained via an internal function\r\u0026gt; # within the car package\r\u0026gt; car:::tukeyNonaddTest(lm(y~Block+A, data.rcb))\rTest Pvalue -1.4163343 0.1566776 \u0026gt; \u0026gt; # alternatively, there is also a Tukey\u0026#39;s non-additivity test within the\r\u0026gt; # asbio package\r\u0026gt; library(asbio)\r\u0026gt; with(data.rcb,tukey.add.test(y,A,Block))\rTukey\u0026#39;s one df test for additivity F = 2.0060029 Denom df = 67 p-value = 0.1613102\rConclusions:\n\rthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (A). Any trends appear to be reasonably consistent between Blocks.\r\r\rModel fitting\rFull parameterisation\n\\[ y_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\beta_0 + \\beta_i + \\gamma_{j(i)}, \\]\nwhere \\(\\gamma_{ij)} \\sim N(0, \\sigma^2_B)\\), \\(\\beta_0, \\beta_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\beta_0\\)) and two treatment effects (\\(\\beta_i\\), where \\(i\\) is \\(1,2\\)).\nMatrix parameterisation\n\\[ y_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\boldsymbol \\beta \\boldsymbol X + \\gamma_{j(i)}, \\]\nwhere \\(\\gamma_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\boldsymbol \\beta \\sim MVN(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\alpha_0\\)) and two treatment effects (\\(\\beta_i\\), where \\(i\\) is \\(1,2\\)). The matrix parameterisation is a compressed notation, In this parameterisation, there are three alpha parameters (one representing the mean of treatment a1, and the other two representing the treatment effects (differences between a2 and a1 and a3 and a1). In generating priors for each of these three alpha parameters, we could loop through each and define a non-informative normal prior to each (as in the Full parameterisation version). However, it turns out that it is more efficient (in terms of mixing and thus the number of necessary iterations) to define the priors from a multivariate normal distribution. This has as many means as there are parameters to estimate (\\(3\\)) and a \\(3\\times3\\) matrix of zeros and \\(100\\) in the diagonals.\n\\[ \\boldsymbol \\mu =\r\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\;\\;\\; \\sigma^2 \\sim \\begin{bmatrix}\r1000000 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 1000000 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 1000000\r\\end{bmatrix}. \\]\nHierarchical parameterisation\n\\[ y_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}= \\beta_0 + \\beta_i + \\gamma_{j(i)}, \\]\nwhere \\(\\gamma_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\beta_0, \\beta_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\).\nRather than assume a specific variance-covariance structure, just like lme we can incorporate an appropriate structure to account for different dependency/correlation structures in our data. In RCB designs, it is prudent to capture the residuals to allow checks that there are no outstanding dependency issues following model fitting.\n\rFull effect parameterisation\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- beta0 + beta[A[i]] + gamma[Block[i]]\r+ res[i] \u0026lt;- y[i]-mu[i]\r+ }\r+ + #Priors\r+ beta0 ~ dnorm(0, 1.0E-6)\r+ beta[1] \u0026lt;- 0\r+ for (i in 2:nA) {\r+ beta[i] ~ dnorm(0, 1.0E-6) #prior\r+ }\r+ for (i in 1:nBlock) {\r+ gamma[i] ~ dnorm(0, tau.B) #prior\r+ }\r+ tau \u0026lt;- pow(sigma,-2)\r+ sigma \u0026lt;- z/sqrt(chSq) + z ~ dnorm(0, 0.0016)I(0,) #1/25^2 = 0.0016\r+ chSq ~ dgamma(0.5, 0.5)\r+ + tau.B \u0026lt;- pow(sigma.B,-2)\r+ sigma.B \u0026lt;- z/sqrt(chSq.B) + z.B ~ dnorm(0, 0.0016)I(0,) #1/25^2 = 0.0016\r+ chSq.B ~ dgamma(0.5, 0.5)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;fullModel.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; data.rcb.list \u0026lt;- with(data.rcb,\r+ list(y=y,\r+ Block=as.numeric(Block),\r+ A=as.numeric(A),\r+ n=nrow(data.rcb),\r+ nBlock=length(levels(Block)),\r+ nA = length(levels(A))\r+ )\r+ )\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta0\u0026quot;,\u0026quot;beta\u0026quot;,\u0026#39;gamma\u0026#39;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma.B\u0026quot;,\u0026quot;res\u0026quot;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\u0026gt; library(R2jags)\rNow run the JAGS code via the R2jags interface.\n\u0026gt; data.rcb.r2jags.f \u0026lt;- jags(data = data.rcb.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;fullModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 105\rUnobserved stochastic nodes: 42\rTotal graph size: 582\rInitializing model\r\u0026gt; \u0026gt; print(data.rcb.r2jags.f)\rInference for Bugs model at \u0026quot;fullModel.txt\u0026quot;, fit using jags,\r2 chains, each with 4500 iterations (first 3000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000 1\rbeta[2] 27.923 1.236 25.587 27.086 27.918 28.736 30.363 1.001 3000\rbeta[3] 40.263 1.229 37.821 39.463 40.271 41.070 42.706 1.001 3000\rbeta0 41.834 2.154 37.519 40.406 41.833 43.338 46.002 1.001 3000\rgamma[1] 3.731 3.394 -2.865 1.418 3.748 6.043 10.452 1.001 3000\rgamma[2] 4.534 3.439 -2.033 2.182 4.508 6.865 11.317 1.003 690\rgamma[3] -3.951 3.464 -10.690 -6.324 -3.884 -1.600 2.829 1.001 3000\rgamma[4] -4.129 3.454 -10.758 -6.477 -4.200 -1.784 2.630 1.003 780\rgamma[5] -5.314 3.480 -12.143 -7.633 -5.325 -2.947 1.594 1.001 3000\rgamma[6] -6.050 3.377 -12.486 -8.331 -6.071 -3.700 0.524 1.001 3000\rgamma[7] -0.709 3.373 -7.083 -3.017 -0.728 1.585 6.032 1.001 3000\rgamma[8] -15.033 3.446 -21.689 -17.322 -15.065 -12.741 -7.939 1.001 3000\rgamma[9] 27.856 3.444 20.996 25.498 27.927 30.226 34.525 1.001 3000\rgamma[10] 12.830 3.591 5.798 10.453 12.809 15.249 20.065 1.001 3000\rgamma[11] -14.936 3.427 -21.825 -17.228 -14.945 -12.635 -8.165 1.001 3000\rgamma[12] -7.878 3.427 -14.571 -10.161 -7.929 -5.551 -1.275 1.001 3000\rgamma[13] -10.865 3.430 -17.569 -13.203 -10.852 -8.555 -4.076 1.001 2100\rgamma[14] 9.153 3.466 2.557 6.679 9.161 11.473 15.941 1.002 1100\rgamma[15] -3.897 3.495 -10.811 -6.227 -3.882 -1.565 2.866 1.001 3000\rgamma[16] 1.321 3.437 -5.486 -0.927 1.328 3.539 8.066 1.001 3000\rgamma[17] -4.137 3.445 -11.102 -6.451 -4.046 -1.802 2.384 1.001 2300\rgamma[18] -4.257 3.449 -10.970 -6.607 -4.280 -1.978 2.695 1.001 2900\rgamma[19] 16.435 3.468 9.749 14.031 16.449 18.830 23.059 1.002 1600\rgamma[20] -5.108 3.439 -11.784 -7.392 -5.100 -2.851 1.708 1.001 3000\rgamma[21] 18.935 3.517 12.023 16.632 18.857 21.210 25.944 1.001 3000\rgamma[22] -20.654 3.459 -27.290 -23.041 -20.593 -18.341 -14.063 1.001 3000\rgamma[23] 7.325 3.570 0.265 4.959 7.346 9.652 14.226 1.001 2500\rgamma[24] -1.300 3.500 -8.248 -3.680 -1.256 1.043 5.590 1.001 3000\rgamma[25] -6.114 3.419 -12.672 -8.442 -6.078 -3.725 0.554 1.001 2500\rgamma[26] 1.038 3.451 -5.502 -1.351 1.018 3.415 7.820 1.001 3000\rgamma[27] -4.346 3.400 -10.942 -6.604 -4.351 -1.980 2.254 1.001 3000\rgamma[28] -4.721 3.368 -11.281 -6.939 -4.724 -2.572 2.057 1.001 3000\rgamma[29] -12.328 3.513 -19.166 -14.660 -12.295 -10.096 -5.361 1.001 3000\rgamma[30] -12.858 3.534 -19.927 -15.216 -12.782 -10.455 -5.999 1.001 3000\rgamma[31] 0.272 3.457 -6.677 -2.020 0.279 2.562 7.061 1.002 1700\rgamma[32] 8.682 3.389 1.905 6.358 8.769 10.986 15.304 1.002 1100\rgamma[33] 0.315 3.433 -6.393 -2.013 0.292 2.624 7.101 1.001 3000\rgamma[34] 9.586 3.491 2.775 7.232 9.621 11.954 16.438 1.002 1600\rgamma[35] 23.906 3.429 17.230 21.620 23.976 26.208 30.709 1.004 660\rres[1] 0.243 2.906 -5.276 -1.686 0.233 2.121 5.940 1.001 3000\rres[2] -6.771 2.869 -12.345 -8.647 -6.778 -4.904 -1.088 1.001 3000\rres[3] 7.465 2.940 1.587 5.472 7.482 9.382 13.252 1.001 3000\rres[4] -3.267 2.927 -8.857 -5.249 -3.282 -1.245 2.325 1.003 710\rres[5] -1.085 2.946 -6.792 -3.130 -1.110 0.894 4.685 1.003 580\rres[6] 5.144 2.913 -0.568 3.244 5.128 7.112 11.094 1.003 600\rres[7] -0.049 2.992 -5.806 -2.023 -0.081 1.917 5.915 1.002 1400\rres[8] -2.652 3.020 -8.617 -4.629 -2.690 -0.658 3.254 1.001 2100\rres[9] 2.018 2.992 -3.965 -0.008 1.989 4.022 7.816 1.001 1900\rres[10] -2.071 2.894 -7.884 -4.016 -2.075 -0.143 3.626 1.003 790\rres[11] 0.729 2.960 -5.122 -1.274 0.754 2.666 6.590 1.003 660\rres[12] 0.287 2.974 -5.811 -1.700 0.396 2.232 5.977 1.003 700\rres[13] -2.939 2.956 -8.894 -4.938 -2.989 -0.959 2.831 1.001 3000\rres[14] 4.213 2.943 -1.578 2.254 4.220 6.162 10.013 1.002 3000\rres[15] -2.451 2.949 -8.246 -4.449 -2.478 -0.503 3.535 1.001 3000\rres[16] -2.462 2.911 -8.169 -4.429 -2.422 -0.560 3.167 1.001 3000\rres[17] 3.440 2.912 -2.164 1.559 3.394 5.390 9.051 1.001 3000\rres[18] -2.208 2.908 -7.897 -4.117 -2.183 -0.279 3.505 1.001 3000\rres[19] -5.249 2.902 -10.958 -7.149 -5.263 -3.289 0.456 1.001 2800\rres[20] 4.201 2.886 -1.484 2.238 4.217 6.100 9.930 1.001 3000\rres[21] 1.085 2.889 -4.579 -0.852 1.104 2.988 6.981 1.001 3000\rres[22] 0.756 2.958 -5.385 -1.108 0.712 2.747 6.541 1.001 3000\rres[23] 1.284 2.979 -4.882 -0.642 1.295 3.319 6.941 1.001 3000\rres[24] -5.388 2.941 -11.391 -7.320 -5.410 -3.425 0.336 1.001 3000\rres[25] 3.141 2.979 -2.786 1.153 3.135 5.090 9.051 1.001 3000\rres[26] -4.587 2.978 -10.372 -6.597 -4.605 -2.589 1.268 1.001 3000\rres[27] 7.011 2.963 1.359 4.983 6.994 8.965 12.803 1.001 3000\rres[28] 7.495 3.018 1.549 5.443 7.508 9.538 13.331 1.002 1600\rres[29] 0.730 3.013 -5.133 -1.311 0.723 2.759 6.603 1.001 2200\rres[30] -5.563 3.054 -11.597 -7.683 -5.577 -3.452 0.255 1.001 2100\rres[31] -3.927 2.962 -9.656 -5.903 -3.900 -1.965 1.925 1.001 3000\rres[32] 2.986 2.928 -2.794 1.134 2.985 4.946 8.735 1.001 3000\rres[33] -1.871 2.951 -7.734 -3.824 -1.864 0.076 4.020 1.001 3000\rres[34] -0.528 2.951 -6.322 -2.505 -0.516 1.395 5.205 1.001 3000\rres[35] -1.472 2.949 -7.285 -3.350 -1.439 0.455 4.220 1.001 3000\rres[36] 0.722 2.938 -4.922 -1.223 0.716 2.663 6.487 1.001 3000\rres[37] -0.493 2.928 -6.329 -2.503 -0.515 1.546 5.111 1.002 940\rres[38] -2.832 2.938 -8.610 -4.822 -2.827 -0.812 2.778 1.002 1300\rres[39] 1.268 2.931 -4.339 -0.752 1.281 3.287 6.896 1.002 1200\rres[40] 2.968 2.956 -2.952 0.972 2.987 4.925 8.768 1.003 540\rres[41] -2.427 2.942 -8.293 -4.383 -2.363 -0.534 3.629 1.003 660\rres[42] 1.150 2.922 -4.453 -0.813 1.176 3.054 7.197 1.003 620\rres[43] -7.026 2.953 -12.859 -8.930 -7.097 -5.036 -1.170 1.001 3000\rres[44] 2.862 2.915 -2.600 0.922 2.711 4.854 8.438 1.001 3000\rres[45] 3.397 2.955 -2.273 1.404 3.364 5.361 9.127 1.001 3000\rres[46] 1.390 2.972 -4.597 -0.607 1.321 3.366 7.167 1.001 2000\rres[47] 2.490 2.991 -3.362 0.502 2.495 4.531 8.375 1.001 3000\rres[48] -3.582 2.963 -9.351 -5.573 -3.608 -1.606 2.286 1.001 2700\rres[49] -2.288 2.916 -7.729 -4.298 -2.366 -0.291 3.588 1.003 1000\rres[50] -1.083 2.906 -6.569 -3.067 -1.105 0.863 4.716 1.002 1300\rres[51] 2.286 2.912 -3.216 0.331 2.244 4.222 8.075 1.002 1200\rres[52] -2.829 2.903 -8.540 -4.796 -2.804 -0.891 2.706 1.001 3000\rres[53] 1.533 2.928 -4.178 -0.380 1.544 3.452 7.214 1.001 2700\rres[54] 0.366 2.907 -5.326 -1.576 0.391 2.274 6.016 1.001 3000\rres[55] 7.373 2.957 1.529 5.353 7.414 9.358 13.176 1.002 1300\rres[56] -3.029 2.984 -9.041 -4.985 -2.963 -1.047 2.888 1.002 1000\rres[57] -0.932 2.961 -6.889 -2.936 -0.824 0.995 4.741 1.002 1100\rres[58] 0.955 2.941 -4.752 -1.089 0.981 2.907 6.650 1.001 3000\rres[59] -2.168 2.938 -8.052 -4.106 -2.164 -0.262 3.668 1.001 3000\rres[60] -0.054 2.957 -5.874 -2.011 0.024 1.879 5.755 1.001 3000\rres[61] 4.652 3.013 -1.277 2.681 4.595 6.649 10.447 1.001 3000\rres[62] 1.763 3.015 -4.198 -0.283 1.800 3.811 7.690 1.001 3000\rres[63] -2.627 2.975 -8.515 -4.603 -2.640 -0.650 3.124 1.001 3000\rres[64] -1.878 3.019 -7.923 -3.808 -1.879 0.207 3.832 1.001 3000\rres[65] -7.955 2.986 -14.124 -9.906 -7.908 -5.914 -2.185 1.001 3000\rres[66] 5.629 3.022 -0.370 3.651 5.702 7.692 11.441 1.001 3000\rres[67] -9.447 2.997 -15.297 -11.427 -9.472 -7.515 -3.360 1.002 1100\rres[68] 3.633 3.011 -2.241 1.639 3.558 5.638 9.647 1.002 1400\rres[69] 7.139 3.005 1.189 5.081 7.118 9.082 13.136 1.002 1300\rres[70] -6.267 2.959 -11.956 -8.323 -6.249 -4.253 -0.493 1.001 3000\rres[71] 6.538 2.978 0.563 4.494 6.525 8.512 12.391 1.001 3000\rres[72] -0.621 2.967 -6.432 -2.609 -0.683 1.364 5.125 1.001 3000\rres[73] -0.989 2.943 -6.875 -2.968 -0.975 0.949 4.951 1.001 3000\rres[74] 1.375 2.946 -4.265 -0.614 1.336 3.269 7.187 1.001 2400\rres[75] -1.399 2.934 -7.135 -3.371 -1.401 0.618 4.478 1.001 2700\rres[76] -0.971 2.970 -6.912 -2.914 -1.004 1.027 4.841 1.001 3000\rres[77] -3.549 2.958 -9.315 -5.540 -3.567 -1.572 2.143 1.001 3000\rres[78] 4.860 2.940 -0.988 2.887 4.805 6.835 10.606 1.001 3000\rres[79] 6.984 2.964 1.461 4.967 6.920 9.004 12.824 1.001 3000\rres[80] -7.875 3.011 -13.687 -9.926 -7.936 -5.815 -1.954 1.001 3000\rres[81] 0.160 2.947 -5.484 -1.824 0.127 2.155 6.047 1.001 3000\rres[82] 2.734 2.915 -3.183 0.820 2.794 4.729 8.263 1.001 3000\rres[83] 2.626 2.932 -3.161 0.748 2.624 4.515 8.266 1.001 3000\rres[84] -6.416 2.954 -12.297 -8.315 -6.382 -4.439 -0.826 1.001 3000\rres[85] -2.326 3.020 -8.242 -4.293 -2.308 -0.307 3.564 1.001 3000\rres[86] -1.054 3.030 -7.047 -3.116 -1.048 0.971 4.956 1.001 3000\rres[87] 0.823 3.034 -5.036 -1.236 0.832 2.846 6.820 1.001 3000\rres[88] -3.700 3.021 -9.662 -5.732 -3.744 -1.656 2.141 1.001 3000\rres[89] 5.124 3.000 -0.700 3.125 5.166 7.053 10.967 1.001 3000\rres[90] -3.973 3.026 -9.965 -5.967 -3.979 -1.965 2.004 1.001 3000\rres[91] 6.800 2.936 1.114 4.888 6.726 8.792 12.683 1.001 2100\rres[92] -1.633 2.932 -7.382 -3.615 -1.645 0.303 4.342 1.002 1500\rres[93] -5.027 2.937 -10.801 -6.975 -5.090 -3.114 0.895 1.002 1600\rres[94] 11.068 2.894 5.415 9.144 11.037 12.950 16.844 1.017 920\rres[95] -5.145 2.871 -10.758 -7.025 -5.209 -3.257 0.653 1.002 920\rres[96] -3.909 2.889 -9.670 -5.821 -3.975 -2.083 2.104 1.002 1000\rres[97] 1.670 2.927 -4.070 -0.286 1.655 3.647 7.528 1.001 2600\rres[98] -1.855 2.902 -7.533 -3.814 -1.846 0.085 3.932 1.001 3000\rres[99] 0.809 2.905 -4.984 -1.113 0.807 2.832 6.509 1.001 3000\rres[100] 1.492 2.952 -4.294 -0.488 1.451 3.471 7.192 1.001 2000\rres[101] 0.647 2.987 -5.287 -1.341 0.649 2.697 6.465 1.002 1500\rres[102] -0.289 2.974 -6.080 -2.262 -0.296 1.781 5.395 1.002 1600\rres[103] -1.309 2.976 -7.063 -3.273 -1.394 0.728 4.790 1.004 440\rres[104] 11.580 2.959 5.725 9.638 11.567 13.515 17.434 1.003 770\rres[105] -5.108 2.939 -10.788 -7.110 -5.108 -3.165 0.707 1.006 490\rsigma 5.090 0.453 4.294 4.775 5.059 5.360 6.091 1.002 980\rsigma.B 11.494 1.491 8.926 10.487 11.365 12.348 14.912 1.002 920\rdeviance 637.702 11.556 618.449 629.190 636.668 645.140 663.417 1.001 3000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 66.8 and DIC = 704.5\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMatrix parameterisation\r\u0026gt; modelString2=\u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- inprod(beta[],X[i,]) + gamma[Block[i]]\r+ res[i] \u0026lt;- y[i]-mu[i]\r+ } + + #Priors\r+ beta ~ dmnorm(a0,A0)\r+ for (i in 1:nBlock) {\r+ gamma[i] ~ dnorm(0, tau.B) #prior\r+ }\r+ tau \u0026lt;- pow(sigma,-2)\r+ sigma \u0026lt;- z/sqrt(chSq) + z ~ dnorm(0, 0.0016)I(0,) #1/25^2 = 0.0016\r+ chSq ~ dgamma(0.5, 0.5)\r+ + tau.B \u0026lt;- pow(sigma.B,-2)\r+ sigma.B \u0026lt;- z/sqrt(chSq.B) + z.B ~ dnorm(0, 0.0016)I(0,) #1/25^2 = 0.0016\r+ chSq.B ~ dgamma(0.5, 0.5)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString2, con = \u0026quot;matrixModel.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; A.Xmat \u0026lt;- model.matrix(~A,data.rcb)\r\u0026gt; data.rcb.list \u0026lt;- with(data.rcb,\r+ list(y=y,\r+ Block=as.numeric(Block),\r+ X=A.Xmat,\r+ n=nrow(data.rcb),\r+ nBlock=length(levels(Block)),\r+ a0=rep(0,3), A0=diag(3)\r+ )\r+ )\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026#39;gamma\u0026#39;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma.B\u0026quot;,\u0026quot;res\u0026quot;)\r\u0026gt; adaptSteps = 1000\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\rNow run the JAGS code via the R2jags interface.\n\u0026gt; data.rcb.r2jags.m \u0026lt;- jags(data = data.rcb.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;matrixModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 105\rUnobserved stochastic nodes: 40\rTotal graph size: 910\rInitializing model\r\u0026gt; \u0026gt; print(data.rcb.r2jags.m)\rInference for Bugs model at \u0026quot;matrixModel.txt\u0026quot;, fit using jags,\r2 chains, each with 4500 iterations (first 3000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 0.550 1.010 -1.396 -0.163 0.543 1.212 2.618 1.001 3000\rbeta[2] 0.624 0.969 -1.267 -0.021 0.609 1.267 2.494 1.002 830\rbeta[3] 1.556 1.005 -0.458 0.886 1.564 2.248 3.495 1.001 3000\rgamma[1] 64.957 12.115 41.593 56.722 64.856 72.937 89.019 1.001 3000\rgamma[2] 65.491 12.061 40.343 57.573 65.450 73.443 89.230 1.001 3000\rgamma[3] 57.107 11.937 34.004 49.204 56.969 65.191 79.840 1.001 3000\rgamma[4] 56.660 11.705 32.676 48.922 56.796 64.418 79.244 1.002 980\rgamma[5] 55.496 12.269 30.868 47.463 55.494 63.576 79.632 1.001 3000\rgamma[6] 54.801 11.877 31.954 46.951 54.385 62.674 78.056 1.002 3000\rgamma[7] 60.234 11.740 36.788 52.823 60.472 67.916 83.053 1.001 3000\rgamma[8] 45.171 11.789 21.628 37.438 45.279 53.104 69.031 1.002 3000\rgamma[9] 89.740 11.870 66.877 81.784 89.985 97.893 112.876 1.001 3000\rgamma[10] 74.392 11.959 51.486 65.937 74.289 82.703 98.386 1.002 1500\rgamma[11] 45.824 11.994 22.631 37.864 45.861 53.737 69.411 1.002 1100\rgamma[12] 52.874 11.847 29.777 44.934 53.044 61.009 75.635 1.002 1800\rgamma[13] 49.828 12.010 26.670 41.934 49.809 57.903 73.611 1.001 3000\rgamma[14] 70.252 11.879 46.842 62.471 70.259 78.152 93.618 1.001 3000\rgamma[15] 56.956 11.790 34.318 48.968 56.782 65.102 79.771 1.001 3000\rgamma[16] 62.229 12.088 39.561 53.779 62.120 70.801 85.754 1.002 1500\rgamma[17] 56.154 11.923 32.379 48.260 56.301 64.248 79.156 1.004 840\rgamma[18] 56.302 11.789 32.839 48.585 56.477 64.399 79.221 1.001 3000\rgamma[19] 78.011 12.127 53.246 69.932 78.041 86.393 100.917 1.001 3000\rgamma[20] 55.445 11.822 32.414 47.480 55.477 63.566 78.456 1.002 3000\rgamma[21] 79.975 11.935 56.573 72.184 80.069 87.728 103.091 1.001 3000\rgamma[22] 39.667 11.800 16.288 31.836 39.723 47.341 62.882 1.001 3000\rgamma[23] 68.605 11.860 45.831 60.540 68.721 76.702 91.376 1.001 3000\rgamma[24] 59.858 12.057 36.038 51.799 59.900 67.940 83.312 1.001 2200\rgamma[25] 54.974 11.970 31.161 47.135 55.153 62.882 78.327 1.002 1400\rgamma[26] 62.397 11.915 38.745 54.113 62.260 70.437 86.239 1.002 3000\rgamma[27] 56.526 11.968 32.943 48.610 56.627 64.491 80.067 1.003 1800\rgamma[28] 56.062 12.002 33.315 48.254 56.158 64.016 79.264 1.001 3000\rgamma[29] 47.976 11.787 24.940 39.984 47.840 55.752 71.140 1.001 3000\rgamma[30] 47.866 11.894 24.408 40.161 47.877 55.955 70.787 1.001 3000\rgamma[31] 61.528 12.021 37.815 53.617 61.620 69.800 84.596 1.001 3000\rgamma[32] 70.047 11.805 46.872 62.230 70.224 78.030 93.180 1.001 3000\rgamma[33] 61.830 11.934 38.654 53.718 61.828 69.563 85.450 1.002 1800\rgamma[34] 70.909 12.075 47.408 62.788 70.805 78.794 95.338 1.002 3000\rgamma[35] 85.532 12.138 61.716 77.422 85.479 93.434 109.336 1.001 3000\rres[1] -19.698 12.037 -43.684 -27.635 -19.684 -11.694 3.838 1.001 3000\rres[2] 0.587 12.062 -23.374 -7.403 0.579 8.638 24.005 1.001 2500\rres[3] 26.230 12.050 2.155 18.315 26.057 34.356 50.024 1.001 3000\rres[4] -22.940 11.997 -46.569 -30.787 -23.060 -15.111 2.226 1.001 3000\rres[5] 6.542 11.989 -16.855 -1.311 6.542 14.381 31.699 1.001 3000\rres[6] 24.179 11.977 0.862 16.176 24.040 31.902 49.149 1.001 3000\rres[7] -19.824 11.904 -42.484 -27.853 -19.648 -11.889 3.474 1.001 3000\rres[8] 4.872 11.923 -17.801 -3.160 4.933 12.798 28.368 1.001 3000\rres[9] 20.951 11.893 -1.904 12.827 21.045 28.785 44.008 1.001 3000\rres[10] -21.576 11.660 -44.079 -29.235 -21.708 -13.770 2.548 1.002 920\rres[11] 8.523 11.658 -14.011 0.830 8.435 16.332 32.472 1.002 1100\rres[12] 19.489 11.657 -2.829 11.774 19.266 27.343 43.385 1.002 910\rres[13] -22.465 12.167 -46.486 -30.516 -22.434 -14.565 2.114 1.001 3000\rres[14] 11.986 12.183 -11.847 3.842 12.016 20.032 36.714 1.001 3000\rres[15] 16.730 12.218 -7.281 8.522 16.737 24.706 41.406 1.001 3000\rres[16] -22.029 11.845 -45.221 -29.925 -21.621 -14.244 0.444 1.001 3000\rres[17] 11.172 11.874 -12.338 3.326 11.433 18.964 33.977 1.001 3000\rres[18] 16.933 11.886 -6.385 8.941 17.228 24.796 39.651 1.001 3000\rres[19] -24.908 11.723 -47.784 -32.552 -25.170 -17.491 -1.875 1.001 3000\rres[20] 11.841 11.745 -11.265 4.268 11.662 19.369 35.065 1.001 3000\rres[21] 20.133 11.744 -2.743 12.424 19.956 27.740 43.426 1.001 3000\rres[22] -18.164 11.729 -41.484 -26.023 -18.365 -10.358 5.425 1.001 3000\rres[23] 9.664 11.740 -13.656 1.865 9.491 17.451 33.025 1.001 3000\rres[24] 14.399 11.768 -8.920 6.550 14.224 22.160 38.381 1.001 3000\rres[25] -17.459 11.814 -40.240 -25.508 -17.731 -9.597 5.503 1.001 3000\rres[26] 2.112 11.834 -20.695 -5.859 1.791 9.903 25.291 1.001 3000\rres[27] 25.119 11.869 2.237 17.073 24.856 33.014 48.190 1.001 3000\rres[28] -12.783 11.927 -36.486 -21.081 -12.685 -4.484 10.101 1.002 1600\rres[29] 7.751 11.951 -16.443 -0.606 7.820 16.063 30.397 1.001 2100\rres[30] 12.866 11.971 -10.922 4.637 12.799 21.163 36.114 1.002 1600\rres[31] -23.404 11.959 -47.054 -31.381 -23.430 -15.471 -0.221 1.002 1200\rres[32] 10.809 11.960 -12.765 2.757 10.883 18.657 33.990 1.002 1400\rres[33] 17.359 11.972 -6.134 9.343 17.364 25.293 40.617 1.002 1100\rres[34] -19.997 11.800 -42.736 -28.106 -20.202 -12.124 3.343 1.002 2500\rres[35] 6.359 11.812 -16.232 -1.893 6.081 14.115 29.804 1.002 1900\rres[36] 19.960 11.807 -2.519 11.753 19.730 27.900 43.458 1.002 2600\rres[37] -19.902 11.980 -43.600 -27.871 -19.843 -12.058 3.587 1.001 3000\rres[38] 5.059 12.005 -18.429 -3.025 5.164 12.756 28.837 1.001 3000\rres[39] 20.566 11.996 -3.070 12.608 20.634 28.366 44.009 1.001 3000\rres[40] -16.847 11.831 -39.884 -24.766 -17.012 -9.239 6.596 1.001 3000\rres[41] 5.057 11.855 -18.074 -2.883 4.927 12.687 28.394 1.001 3000\rres[42] 20.042 11.830 -3.241 12.206 20.003 27.718 43.766 1.001 3000\rres[43] -26.596 11.746 -49.680 -34.580 -26.349 -18.627 -3.862 1.001 3000\rres[44] 10.592 11.735 -12.047 2.531 10.659 18.547 33.288 1.001 3000\rres[45] 22.535 11.759 -0.427 14.411 22.577 30.481 45.274 1.001 3000\rres[46] -18.234 12.031 -41.669 -26.696 -18.076 -9.978 4.443 1.002 1200\rres[47] 10.165 12.061 -13.282 1.662 10.365 18.474 33.054 1.002 1500\rres[48] 15.501 12.060 -7.938 7.051 15.690 23.578 38.325 1.002 1200\rres[49] -21.296 11.891 -44.247 -29.499 -21.509 -13.441 2.004 1.002 1000\rres[50] 7.208 11.902 -15.618 -0.939 6.981 15.152 30.441 1.002 860\rres[51] 21.986 11.895 -0.592 13.867 21.799 29.853 45.640 1.002 1000\rres[52] -22.104 11.729 -44.941 -29.987 -22.312 -14.241 1.133 1.001 3000\rres[53] 9.556 11.751 -13.024 1.751 9.257 17.457 33.011 1.001 3000\rres[54] 19.797 11.734 -2.966 11.838 19.629 27.542 43.295 1.001 3000\rres[55] -12.918 12.097 -35.949 -21.363 -12.982 -4.866 11.578 1.001 3000\rres[56] 3.979 12.106 -19.166 -4.374 3.889 12.063 28.610 1.001 3000\rres[57] 17.484 12.122 -5.688 8.981 17.300 25.557 42.114 1.001 3000\rres[58] -18.315 11.797 -41.135 -26.620 -18.209 -10.520 4.471 1.001 3000\rres[59] 5.862 11.815 -16.900 -2.341 6.146 13.803 29.023 1.001 3000\rres[60] 19.383 11.783 -3.464 11.144 19.447 27.072 42.559 1.001 3000\rres[61] -15.105 11.900 -38.125 -22.888 -15.284 -7.276 8.233 1.001 3000\rres[62] 9.306 11.923 -13.585 1.507 9.128 17.140 32.775 1.001 3000\rres[63] 16.323 11.956 -6.322 8.357 16.116 24.316 39.779 1.001 3000\rres[64] -20.915 11.793 -44.085 -28.598 -21.048 -13.147 2.444 1.001 3000\rres[65] 0.307 11.820 -22.750 -7.598 0.334 7.975 23.864 1.001 3000\rres[66] 25.298 11.813 2.156 17.551 25.247 33.151 48.893 1.001 3000\rres[67] -29.443 11.830 -52.213 -37.430 -29.692 -21.434 -6.575 1.001 3000\rres[68] 10.936 11.848 -11.706 2.810 10.811 18.885 33.881 1.001 3000\rres[69] 25.850 11.873 2.835 17.722 25.645 33.821 48.845 1.001 3000\rres[70] -26.142 11.991 -49.697 -34.072 -26.243 -18.049 -2.711 1.001 2300\rres[71] 13.963 11.984 -9.101 6.160 13.990 22.029 37.049 1.001 3000\rres[72] 18.211 12.015 -4.983 10.314 18.104 26.258 41.685 1.001 2300\rres[73] -20.794 11.932 -43.963 -28.725 -20.938 -12.958 3.106 1.002 1600\rres[74] 8.870 11.937 -14.459 0.895 8.787 16.905 32.822 1.001 2000\rres[75] 17.504 11.929 -5.274 9.402 17.317 25.457 41.765 1.002 1600\rres[76] -21.046 11.851 -44.910 -28.922 -21.131 -12.843 2.003 1.002 3000\rres[77] 3.675 11.857 -19.985 -4.059 3.613 11.706 27.196 1.003 3000\rres[78] 23.492 11.881 -0.028 15.552 23.399 31.576 46.776 1.003 3000\rres[79] -12.603 11.937 -35.961 -20.361 -12.719 -4.579 10.948 1.002 2900\rres[80] -0.163 11.955 -23.303 -8.120 -0.343 7.789 23.405 1.002 2200\rres[81] 19.279 11.963 -4.001 11.319 19.160 27.293 42.755 1.002 3000\rres[82] -16.766 11.961 -39.955 -24.721 -16.887 -9.030 6.016 1.001 3000\rres[83] 10.426 11.958 -13.000 2.504 10.510 18.221 32.859 1.001 3000\rres[84] 12.792 11.939 -10.192 4.873 12.720 20.537 35.661 1.001 3000\rres[85] -21.347 11.725 -44.125 -28.923 -21.164 -13.609 1.848 1.001 3000\rres[86] 7.224 11.690 -15.758 -0.362 7.382 15.079 30.544 1.001 3000\rres[87] 20.510 11.739 -2.412 12.708 20.647 28.327 43.283 1.001 3000\rres[88] -23.140 11.838 -46.005 -31.274 -23.099 -15.502 -0.066 1.001 3000\rres[89] 12.983 11.858 -9.972 4.729 13.009 20.680 36.860 1.001 3000\rres[90] 15.293 11.892 -7.122 6.928 15.405 23.032 38.696 1.001 3000\rres[91] -13.173 11.935 -36.126 -21.376 -13.267 -5.416 10.293 1.001 3000\rres[92] 5.694 11.935 -17.082 -2.485 5.640 13.450 29.247 1.001 3000\rres[93] 13.708 11.936 -9.020 5.488 13.624 21.589 37.402 1.001 3000\rres[94] -9.013 11.766 -31.685 -16.863 -9.233 -1.091 14.418 1.001 3000\rres[95] 2.073 11.780 -20.569 -5.756 1.871 9.830 25.639 1.001 3000\rres[96] 14.717 11.782 -8.181 6.737 14.422 22.602 37.767 1.001 3000\rres[97] -18.561 11.906 -41.914 -26.360 -18.779 -10.492 4.073 1.002 1300\rres[98] 5.213 11.908 -18.272 -2.559 5.068 13.190 28.414 1.002 1100\rres[99] 19.285 11.939 -4.469 11.483 19.203 27.469 42.441 1.002 1300\rres[100] -18.547 12.018 -42.274 -26.484 -18.430 -10.631 5.083 1.001 3000\rres[101] 7.907 12.020 -16.041 -0.084 8.072 15.733 31.471 1.001 3000\rres[102] 18.379 12.034 -5.679 10.333 18.514 26.429 41.930 1.001 3000\rres[103] -21.652 12.104 -45.310 -29.603 -21.605 -13.692 1.709 1.001 2500\rres[104] 18.537 12.095 -5.105 10.797 18.645 26.404 42.117 1.001 1900\rres[105] 13.256 12.137 -10.949 5.489 13.253 21.341 36.897 1.001 2600\rsigma 20.838 1.809 17.597 19.556 20.736 21.936 24.751 1.002 1000\rsigma.B 63.500 7.812 50.201 58.005 62.978 68.138 80.806 1.001 3000\rdeviance 934.350 11.459 914.767 926.442 933.465 941.520 959.367 1.004 460\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 65.5 and DIC = 999.9\rDIC is an estimate of expected predictive error (lower deviance is better).\rFor a simple model with only two hierarchical levels, the model is the same as above. If you want to include finite-population standard deviations in the model you can use the following code.\n\u0026gt; modelString3=\u0026quot;\r+ model {\r+ #Likelihood (esimating site means (gamma.site)\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- gamma[Block[i]] + inprod(beta[], X[i,]) + y.err[i]\u0026lt;- mu[i]-y[i]\r+ }\r+ for (i in 1:nBlock) {\r+ gamma[i] ~ dnorm(0, tau.block)\r+ }\r+ #Priors\r+ for (i in 1:nX) {\r+ beta[i] ~ dnorm(0, 1.0E-6) #prior\r+ }\r+ sigma ~ dunif(0, 100)\r+ tau \u0026lt;- 1 / (sigma * sigma)\r+ sigma.block ~ dunif(0, 100)\r+ tau.block \u0026lt;- 1 / (sigma.block * sigma.block)\r+ + sd.y \u0026lt;- sd(y.err)\r+ sd.block \u0026lt;- sd(gamma)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString3, con = \u0026quot;SDModel.txt\u0026quot;)\r\u0026gt; \u0026gt; #data list\r\u0026gt; A.Xmat \u0026lt;- model.matrix(~A,ddply(data.rcb,~Block,catcolwise(unique)))\r\u0026gt; data.rcb.list \u0026lt;- with(data.rcb,\r+ list(y=y,\r+ Block=Block,\r+ X= A.Xmat,\r+ n=nrow(data.rcb),\r+ nBlock=length(levels(Block)),\r+ nX = ncol(A.Xmat)\r+ )\r+ )\r\u0026gt; \u0026gt; #parameters and chain details\r\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sd.y\u0026quot;,\u0026#39;sd.block\u0026#39;,\u0026#39;sigma.block\u0026#39;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; data.rcb.r2jagsSD \u0026lt;- jags(data = data.rcb.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;SDModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 105\rUnobserved stochastic nodes: 40\rTotal graph size: 899\rInitializing model\r\u0026gt; \u0026gt; print(data.rcb.r2jagsSD)\rInference for Bugs model at \u0026quot;SDModel.txt\u0026quot;, fit using jags,\r2 chains, each with 4500 iterations (first 3000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 41.715 2.196 37.449 40.231 41.710 43.183 45.995 1.001 3000\rbeta[2] 27.928 1.209 25.537 27.146 27.918 28.713 30.317 1.002 980\rbeta[3] 40.272 1.210 37.832 39.461 40.267 41.096 42.658 1.001 2300\rsd.block 11.358 0.519 10.353 11.029 11.345 11.706 12.370 1.001 3000\rsd.y 5.014 0.260 4.592 4.827 4.986 5.172 5.609 1.002 1300\rsigma 5.074 0.443 4.322 4.752 5.045 5.350 6.036 1.001 3000\rsigma.block 11.692 1.546 9.114 10.589 11.586 12.612 15.118 1.002 3000\rdeviance 637.262 10.949 618.392 629.413 636.321 644.252 660.930 1.001 2200\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 59.9 and DIC = 697.2\rDIC is an estimate of expected predictive error (lower deviance is better).\rCalculate \\(R^2\\) from the posterior of the model.\n\u0026gt; data.rcb.mcmc.listSD \u0026lt;- as.mcmc(data.rcb.r2jagsSD)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~A, data.rcb)\r\u0026gt; coefs \u0026lt;- data.rcb.r2jagsSD$BUGSoutput$sims.list[[\u0026#39;beta\u0026#39;]]\r\u0026gt; fitted \u0026lt;- coefs %*% t(Xmat)\r\u0026gt; X.var \u0026lt;- aaply(fitted,1,function(x){var(x)})\r\u0026gt; Z.var \u0026lt;- data.rcb.r2jagsSD$BUGSoutput$sims.list[[\u0026#39;sd.block\u0026#39;]]^2\r\u0026gt; R.var \u0026lt;- data.rcb.r2jagsSD$BUGSoutput$sims.list[[\u0026#39;sd.y\u0026#39;]]^2\r\u0026gt; R2.marginal \u0026lt;- (X.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.marginal \u0026lt;- data.frame(Mean=mean(R2.marginal), Median=median(R2.marginal), HPDinterval(as.mcmc(R2.marginal)))\r\u0026gt; R2.conditional \u0026lt;- (X.var+Z.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.conditional \u0026lt;- data.frame(Mean=mean(R2.conditional),\r+ Median=median(R2.conditional), HPDinterval(as.mcmc(R2.conditional)))\r\u0026gt; R2.block \u0026lt;- (Z.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.block \u0026lt;- data.frame(Mean=mean(R2.block), Median=median(R2.block), HPDinterval(as.mcmc(R2.block)))\r\u0026gt; R2.res\u0026lt;-(R.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.res \u0026lt;- data.frame(Mean=mean(R2.res), Median=median(R2.res), HPDinterval(as.mcmc(R2.res)))\r\u0026gt; \u0026gt; rbind(R2.block=R2.block, R2.marginal=R2.marginal, R2.res=R2.res, R2.conditional=R2.conditional)\rMean Median lower upper\rR2.block 0.2927774 0.29248056 0.24902731 0.33605200\rR2.marginal 0.6500204 0.65101312 0.60509352 0.68965593\rR2.res 0.0572022 0.05628758 0.04596228 0.07055798\rR2.conditional 0.9427978 0.94371242 0.92944202 0.95403772\r\rPlanned comparisonsand pairwise tests\rSince there are no restrictions on the type and number of comparisons derived from the posteriors, Bayesian analyses provide a natural framework for exploring additional contrasts and comparisons. For example, to compare all possible levels:\n\u0026gt; coefs \u0026lt;- data.rcb.r2jags.m$BUGSoutput$sims.list[[c(\u0026#39;beta\u0026#39;)]]\r\u0026gt; head(coefs)\r[,1] [,2] [,3]\r[1,] -1.0697767 -0.46647636 0.4808020\r[2,] 0.6186153 1.46210386 2.3592529\r[3,] -1.5100302 0.09180824 1.1835869\r[4,] -0.3127107 0.66392714 -0.5681012\r[5,] 1.5552936 1.06785499 2.6443403\r[6,] 0.7282182 0.59829747 2.8548669\r\u0026gt; \u0026gt; newdata \u0026lt;- data.frame(A=levels(data.rcb$A))\r\u0026gt; # A Tukeys contrast matrix\r\u0026gt; library(multcomp)\r\u0026gt; tuk.mat \u0026lt;- contrMat(n=table(newdata$A), type=\u0026quot;Tukey\u0026quot;)\r\u0026gt; Xmat \u0026lt;- model.matrix(~A, data=newdata)\r\u0026gt; pairwise.mat \u0026lt;- tuk.mat %*% Xmat\r\u0026gt; pairwise.mat\r(Intercept) A2 A3\r2 - 1 0 1 0\r3 - 1 0 0 1\r3 - 2 0 -1 1\r\u0026gt; \u0026gt; comps \u0026lt;- coefs %*% t(pairwise.mat)\r\u0026gt; \u0026gt; MCMCsum \u0026lt;- function(x) {\r+ data.frame(Median=median(x, na.rm=TRUE), t(quantile(x,na.rm=TRUE)),\r+ HPDinterval(as.mcmc(x)),HPDinterval(as.mcmc(x),p=0.5))\r+ }\r\u0026gt; \u0026gt; (comps \u0026lt;-plyr:::adply(comps,2,MCMCsum))\rX1 Median X0. X25. X50. X75. X100. lower\r1 2 - 1 0.6093838 -2.556240 -0.020575421 0.6093838 1.267051 4.786166 -1.2766747\r2 3 - 1 1.5638199 -1.833977 0.886430287 1.5638199 2.248195 4.835948 -0.4024791\r3 3 - 2 0.9310770 -4.672228 -0.003765539 0.9310770 1.864871 5.592247 -1.5970184\rupper lower.1 upper.1\r1 2.479999 0.03512204 1.316762\r2 3.539364 0.92897200 2.273364\r3 3.728297 -0.03345687 1.823124\r\r\rRCB (repeated measures) - continuous within\rData generation\rImagine now that we has designed an experiment to investigate the effects of a continuous predictor (\\(x\\), for example time) on a response (\\(y\\)). Again, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability, we again decide to apply a design (RCB) in which each of the levels of \\(X\\) (such as time) treatments within each of \\(35\\) blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(123)\r\u0026gt; slope \u0026lt;- 30\r\u0026gt; intercept \u0026lt;- 200\r\u0026gt; nBlock \u0026lt;- 35\r\u0026gt; nTime \u0026lt;- 10\r\u0026gt; sigma \u0026lt;- 50\r\u0026gt; sigma.block \u0026lt;- 30\r\u0026gt; n \u0026lt;- nBlock*nTime\r\u0026gt; Block \u0026lt;- gl(nBlock, k=1)\r\u0026gt; Time \u0026lt;- 1:10\r\u0026gt; rho \u0026lt;- 0.8\r\u0026gt; dt \u0026lt;- expand.grid(Time=Time,Block=Block)\r\u0026gt; Xmat \u0026lt;- model.matrix(~-1+Block + Time, data=dt)\r\u0026gt; block.effects \u0026lt;- rnorm(n = nBlock, mean = intercept, sd = sigma.block)\r\u0026gt; #A.effects \u0026lt;- c(30,40)\r\u0026gt; all.effects \u0026lt;- c(block.effects,slope)\r\u0026gt; lin.pred \u0026lt;- Xmat %*% all.effects\r\u0026gt; \u0026gt; # OR\r\u0026gt; Xmat \u0026lt;- cbind(model.matrix(~-1+Block,data=dt),model.matrix(~Time,data=dt))\r\u0026gt; ## Sum to zero block effects\r\u0026gt; ##block.effects \u0026lt;- rnorm(n = nBlock, mean = 0, sd = sigma.block)\r\u0026gt; ###A.effects \u0026lt;- c(40,70,80)\r\u0026gt; ##all.effects \u0026lt;- c(block.effects,intercept,slope)\r\u0026gt; ##lin.pred \u0026lt;- Xmat %*% all.effects\r\u0026gt; \u0026gt; ## the quadrat observations (within sites) are drawn from\r\u0026gt; ## normal distributions with means according to the site means\r\u0026gt; ## and standard deviations of 5\r\u0026gt; eps \u0026lt;- NULL\r\u0026gt; eps[1] \u0026lt;- 0\r\u0026gt; for (j in 2:n) {\r+ eps[j] \u0026lt;- rho*eps[j-1] #residuals\r+ }\r\u0026gt; y \u0026lt;- rnorm(n,lin.pred,sigma)+eps\r\u0026gt; \u0026gt; #OR\r\u0026gt; eps \u0026lt;- NULL\r\u0026gt; # first value cant be autocorrelated\r\u0026gt; eps[1] \u0026lt;- rnorm(1,0,sigma)\r\u0026gt; for (j in 2:n) {\r+ eps[j] \u0026lt;- rho*eps[j-1] + rnorm(1, mean = 0, sd = sigma) #residuals\r+ }\r\u0026gt; y \u0026lt;- lin.pred + eps\r\u0026gt; data.rm \u0026lt;- data.frame(y=y, dt)\r\u0026gt; head(data.rm) #print out the first six rows of the data set\ry Time Block\r1 282.1142 1 1\r2 321.1404 2 1\r3 278.7700 3 1\r4 285.8709 4 1\r5 336.6390 5 1\r6 333.5961 6 1\r\u0026gt; \u0026gt; ggplot(data.rm, aes(y=y, x=Time)) + geom_smooth(method=\u0026#39;lm\u0026#39;) + geom_point() + facet_wrap(~Block)\r\rExploratory data analysis\rNormality and Homogeneity of variance\n\u0026gt; boxplot(y~Time, data.rm)\r\u0026gt; \u0026gt; ggplot(data.rm, aes(y=y, x=factor(Time))) + geom_boxplot()\rConclusions:\n\rthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\n\rthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\n\r\rObvious violations could be addressed either by:\n\rtransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\r\rBlock by within-Block interaction\n\u0026gt; with(data.rm, interaction.plot(Time,Block,y))\r\u0026gt; \u0026gt; ggplot(data.rm, aes(y=y, x=Time, color=Block, group=Block)) + geom_line() +\r+ guides(color=guide_legend(ncol=3))\r\u0026gt; \u0026gt; residualPlots(lm(y~Block+Time, data.rm))\r Test stat Pr(\u0026gt;|Test stat|)\rBlock Time -0.7274 0.4675\rTukey test -0.9809 0.3267\r\u0026gt; \u0026gt; # the Tukey\u0026#39;s non-additivity test by itself can be obtained via an internal function\r\u0026gt; # within the car package\r\u0026gt; car:::tukeyNonaddTest(lm(y~Block+Time, data.rm))\rTest Pvalue -0.9808606 0.3266615 \u0026gt; \u0026gt; # alternatively, there is also a Tukey\u0026#39;s non-additivity test within the\r\u0026gt; # asbio package\r\u0026gt; with(data.rm,tukey.add.test(y,Time,Block))\rTukey\u0026#39;s one df test for additivity F = 0.3997341 Denom df = 305 p-value = 0.5277003\rConclusions:\n\rthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (Time). Any trends appear to be reasonably consistent between Blocks.\r\rSphericity\nSince the levels of Time cannot be randomly assigned, it is likely that sphericity is not met. We can explore whether there is an auto-correlation patterns in the residuals. Note, as there was only ten time periods, it does not make logical sense to explore lags above \\(10\\).\n\u0026gt; library(nlme)\r\u0026gt; data.rm.lme \u0026lt;- lme(y~Time, random=~1|Block, data=data.rm)\r\u0026gt; acf(resid(data.rm.lme), lag=10)\rConclusions:\nThe autocorrelation factor (ACF) at a range of lags up to \\(10\\), indicate that there is a cyclical pattern of residual auto-correlation. We really should explore incorporating some form of correlation structure into our model.\n\rModel fitting\r\rFull effect parameterisation\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- beta0 + beta*Time[i] + gamma[Block[i]]\r+ res[i] \u0026lt;- y[i]-mu[i]\r+ }\r+ + #Priors\r+ beta0 ~ dnorm(0, 1.0E-6)\r+ beta ~ dnorm(0, 1.0E-6) #prior\r+ + for (i in 1:nBlock) {\r+ gamma[i] ~ dnorm(0, tau.B) #prior\r+ }\r+ tau \u0026lt;- pow(sigma,-2)\r+ sigma \u0026lt;- z/sqrt(chSq) + z ~ dnorm(0, 0.0016)I(0,) #1/25^2 = 0.0016\r+ chSq ~ dgamma(0.5, 0.5)\r+ + tau.B \u0026lt;- pow(sigma.B,-2)\r+ sigma.B \u0026lt;- z/sqrt(chSq.B) + z.B ~ dnorm(0, 0.0016)I(0,) #1/25^2 = 0.0016\r+ chSq.B ~ dgamma(0.5, 0.5)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;fullModel2.txt\u0026quot;)\r\u0026gt; \u0026gt; data.rm.list \u0026lt;- with(data.rm,\r+ list(y=y,\r+ Block=as.numeric(Block),\r+ Time=Time,\r+ n=nrow(data.rm),\r+ nBlock=length(levels(Block))\r+ )\r+ )\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026quot;beta0\u0026quot;,\u0026quot;beta\u0026quot;,\u0026#39;gamma\u0026#39;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma.B\u0026quot;,\u0026quot;res\u0026quot;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; data.rm.r2jags.f \u0026lt;- jags(data = data.rm.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;fullModel2.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 350\rUnobserved stochastic nodes: 41\rTotal graph size: 1815\rInitializing model\r\u0026gt; \u0026gt; print(data.rm.r2jags.f)\rInference for Bugs model at \u0026quot;fullModel2.txt\u0026quot;, fit using jags,\r2 chains, each with 4500 iterations (first 3000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat\rbeta 30.689 1.047 28.609 30.017 30.687 31.401 32.705 1.001\rbeta0 189.009 12.648 164.589 180.318 189.054 197.523 213.976 1.001\rgamma[1] -35.015 20.219 -74.991 -47.706 -35.021 -21.382 3.826 1.002\rgamma[2] -52.026 20.114 -91.663 -65.012 -52.008 -38.575 -12.685 1.001\rgamma[3] 20.417 19.878 -19.313 7.462 20.587 33.579 60.228 1.001\rgamma[4] 0.671 20.295 -38.799 -12.839 0.882 14.490 39.807 1.001\rgamma[5] 67.812 19.967 29.683 54.189 67.514 81.150 109.392 1.001\rgamma[6] 36.338 19.760 -2.575 22.780 36.203 49.381 76.772 1.001\rgamma[7] 24.072 20.155 -14.701 10.740 24.009 36.728 63.397 1.001\rgamma[8] -31.199 20.016 -70.564 -44.687 -31.149 -17.691 7.011 1.001\rgamma[9] 73.971 20.034 35.053 60.309 73.846 87.726 113.132 1.003\rgamma[10] 58.034 19.900 19.397 44.730 58.085 71.380 97.283 1.001\rgamma[11] 141.644 20.387 101.956 127.950 141.240 154.897 181.751 1.001\rgamma[12] 5.655 20.094 -32.833 -7.787 5.349 19.065 47.017 1.001\rgamma[13] -44.187 20.168 -84.778 -57.576 -44.641 -30.571 -5.599 1.001\rgamma[14] -23.866 19.908 -63.673 -37.311 -23.653 -10.578 14.435 1.001\rgamma[15] 30.407 20.239 -8.109 16.928 30.379 43.830 70.587 1.001\rgamma[16] 103.433 20.123 64.608 90.052 103.087 116.736 143.495 1.002\rgamma[17] 91.556 20.060 53.115 77.561 91.725 104.473 131.814 1.002\rgamma[18] -63.563 20.127 -102.913 -77.195 -63.210 -50.190 -24.916 1.002\rgamma[19] 16.404 19.820 -21.892 2.880 16.232 29.420 55.497 1.001\rgamma[20] -26.858 19.837 -66.283 -39.890 -26.676 -13.651 12.760 1.002\rgamma[21] -104.771 19.743 -143.174 -117.701 -105.347 -92.448 -64.620 1.001\rgamma[22] -14.307 19.903 -54.617 -27.704 -14.041 -0.901 23.918 1.001\rgamma[23] -81.493 19.863 -121.932 -94.860 -81.367 -67.618 -43.350 1.001\rgamma[24] -86.520 20.067 -125.826 -100.133 -86.297 -73.003 -47.481 1.001\rgamma[25] -47.166 20.417 -86.549 -61.568 -47.155 -33.479 -6.020 1.001\rgamma[26] -92.375 19.497 -130.380 -105.540 -92.310 -79.043 -54.017 1.002\rgamma[27] 20.875 20.031 -18.328 6.625 20.873 34.275 60.661 1.002\rgamma[28] 74.464 19.909 36.179 61.091 74.369 87.433 114.480 1.001\rgamma[29] -0.792 19.771 -39.202 -14.402 -0.589 12.677 36.999 1.001\rgamma[30] -75.855 20.350 -116.077 -89.286 -76.179 -62.465 -35.911 1.001\rgamma[31] -58.457 20.104 -97.479 -72.394 -58.390 -44.695 -19.492 1.001\rgamma[32] -53.274 20.080 -91.482 -66.984 -53.201 -40.309 -13.057 1.001\rgamma[33] 15.405 20.131 -22.770 1.695 15.048 28.831 55.869 1.001\rgamma[34] 59.338 20.334 19.750 45.810 59.517 72.790 99.194 1.001\rgamma[35] 56.933 20.301 15.197 43.906 57.388 70.588 94.583 1.001\rres[1] 97.432 17.921 62.629 85.359 97.337 109.111 134.580 1.002\rres[2] 105.769 17.698 71.182 93.805 105.826 117.408 142.144 1.002\rres[3] 32.710 17.534 -1.148 20.909 32.522 44.178 68.460 1.002\rres[4] 9.121 17.431 -23.865 -2.637 8.871 20.532 44.239 1.002\rres[5] 29.201 17.391 -4.021 17.641 28.835 40.540 64.071 1.002\rres[6] -4.531 17.414 -38.094 -16.092 -4.863 6.832 30.205 1.002\rres[7] -107.185 17.500 -140.651 -118.950 -107.516 -95.741 -72.125 1.002\rres[8] -37.350 17.647 -71.015 -49.217 -37.556 -25.708 -1.746 1.002\rres[9] -67.307 17.855 -101.635 -79.246 -67.326 -55.699 -31.035 1.002\rres[10] -78.614 18.121 -113.985 -90.735 -78.654 -66.764 -42.063 1.002\rres[11] 49.884 18.193 13.728 37.834 50.102 61.789 84.845 1.001\rres[12] 11.595 17.930 -23.183 -0.446 11.692 23.269 46.321 1.001\rres[13] 61.820 17.726 27.566 49.860 61.924 73.375 96.299 1.001\rres[14] -3.458 17.582 -37.401 -15.192 -3.465 8.046 30.969 1.001\rres[15] -10.511 17.499 -44.587 -21.767 -10.459 1.089 24.112 1.001\rres[16] -2.243 17.479 -36.572 -13.604 -2.049 9.337 32.255 1.001\rres[17] -50.520 17.522 -85.179 -61.867 -50.373 -38.782 -16.278 1.001\rres[18] -62.585 17.627 -97.953 -73.969 -62.501 -50.564 -28.305 1.001\rres[19] -42.079 17.792 -77.762 -53.482 -42.166 -30.142 -7.621 1.001\rres[20] 9.165 18.018 -26.988 -2.455 9.147 21.271 43.778 1.001\rres[21] -77.926 17.501 -113.047 -89.469 -78.006 -66.052 -43.688 1.002\rres[22] -73.189 17.257 -108.057 -84.536 -73.411 -61.442 -39.850 1.002\rres[23] -14.228 17.075 -48.379 -25.553 -14.413 -2.773 18.665 1.002\rres[24] -31.958 16.955 -66.594 -43.263 -32.141 -20.655 0.554 1.002\rres[25] -7.975 16.900 -42.439 -19.028 -8.192 3.408 24.762 1.002\rres[26] 24.320 16.909 -9.262 13.308 24.202 35.576 57.313 1.002\rres[27] 38.799 16.983 5.216 27.563 38.571 50.247 72.421 1.002\rres[28] 69.516 17.120 35.582 58.330 69.291 80.872 103.259 1.002\rres[29] 55.153 17.321 20.895 43.887 54.848 66.615 89.335 1.002\rres[30] 28.976 17.581 -5.726 17.342 28.565 40.577 63.635 1.002\rres[31] -121.587 17.872 -156.152 -133.567 -121.526 -109.634 -87.382 1.001\rres[32] -100.256 17.618 -133.884 -111.966 -100.063 -88.659 -66.409 1.001\rres[33] -57.168 17.423 -90.542 -68.902 -57.110 -45.627 -22.964 1.001\rres[34] -17.580 17.290 -51.232 -29.462 -17.603 -6.075 16.449 1.001\rres[35] -40.581 17.219 -74.430 -52.498 -40.599 -29.045 -6.738 1.001\rres[36] 57.619 17.212 23.794 45.812 57.457 69.114 91.210 1.001\rres[37] 61.388 17.269 27.008 49.782 61.277 72.884 95.367 1.004\rres[38] 56.259 17.389 21.473 44.753 56.294 67.702 90.416 1.001\rres[39] 109.316 17.570 73.992 97.577 109.390 120.643 143.528 1.001\rres[40] 52.088 17.811 16.206 39.944 52.135 63.655 86.709 1.001\rres[41] -38.914 17.643 -73.656 -50.658 -38.634 -26.860 -5.046 1.001\rres[42] 77.326 17.420 42.616 65.720 77.575 89.215 111.120 1.001\rres[43] 50.865 17.258 16.915 39.497 51.222 62.617 84.564 1.001\rres[44] 110.679 17.159 76.469 99.239 111.186 122.203 144.204 1.001\rres[45] 4.790 17.122 -29.711 -6.616 5.343 16.301 38.330 1.001\rres[46] -17.661 17.150 -52.491 -29.227 -17.113 -5.957 16.017 1.001\rres[47] -7.311 17.242 -42.494 -18.933 -6.757 4.462 26.166 1.001\rres[48] -3.089 17.396 -38.865 -14.616 -2.473 8.846 31.113 1.001\rres[49] -65.133 17.611 -101.697 -76.777 -64.519 -53.174 -30.398 1.001\rres[50] -63.661 17.886 -100.707 -75.693 -63.048 -51.501 -28.264 1.001\rres[51] -31.518 17.144 -65.465 -43.209 -31.355 -19.712 1.672 1.001\rres[52] 14.815 16.893 -18.897 3.470 14.776 26.305 47.890 1.001\rres[53] 70.347 16.704 36.921 58.939 70.092 81.766 102.877 1.001\rres[54] -50.853 16.579 -83.834 -62.055 -51.073 -39.619 -18.942 1.001\rres[55] 25.083 16.520 -8.057 14.027 24.935 36.357 56.754 1.001\rres[56] -38.143 16.527 -71.222 -49.423 -38.357 -26.836 -6.450 1.001\rres[57] -4.070 16.600 -36.763 -15.317 -4.236 7.197 27.724 1.001\rres[58] 33.306 16.738 0.513 22.191 33.191 44.522 65.598 1.001\rres[59] 20.080 16.940 -12.937 8.828 19.836 31.550 52.764 1.001\rres[60] -12.900 17.204 -46.161 -24.276 -13.088 -1.463 20.472 1.001\rres[61] -17.368 17.885 -51.348 -29.241 -17.425 -5.291 18.072 1.001\rres[62] 7.369 17.652 -26.193 -4.253 7.234 19.236 42.448 1.001\rres[63] 49.245 17.479 15.639 37.647 49.046 60.880 83.698 1.001\rres[64] -64.174 17.368 -97.742 -75.711 -64.357 -52.534 -30.120 1.001\rres[65] -134.249 17.320 -167.266 -146.107 -134.437 -122.586 -100.604 1.001\rres[66] -37.107 17.334 -70.680 -48.991 -37.270 -25.543 -3.298 1.001\rres[67] 21.279 17.412 -12.492 9.532 21.267 33.096 55.105 1.001\rres[68] 37.284 17.552 3.245 25.564 37.265 49.184 71.420 1.001\rres[69] 63.944 17.753 29.095 52.068 63.864 75.938 98.781 1.001\rres[70] 95.234 18.013 60.203 83.123 95.270 107.455 130.632 1.001\rres[71] -48.396 17.620 -83.102 -60.471 -48.230 -36.426 -13.894 1.001\rres[72] 16.818 17.403 -17.131 4.932 16.936 28.621 50.566 1.001\rres[73] -10.912 17.247 -44.871 -22.609 -10.795 0.606 22.307 1.001\rres[74] 2.547 17.154 -31.372 -8.982 2.664 14.012 35.509 1.001\rres[75] -13.113 17.125 -47.212 -24.603 -12.951 -1.578 20.054 1.001\rres[76] 32.577 17.159 -1.807 20.989 32.596 44.219 65.805 1.001\rres[77] 7.970 17.257 -27.008 -3.791 7.911 19.682 41.186 1.001\rres[78] 31.495 17.418 -3.602 19.914 31.400 43.395 65.506 1.001\rres[79] 4.718 17.639 -30.550 -6.860 4.806 16.702 39.109 1.001\rres[80] -51.946 17.919 -87.119 -63.726 -51.922 -39.897 -17.025 1.001\rres[81] -63.210 17.647 -97.791 -75.690 -63.212 -51.190 -29.440 1.002\rres[82] -31.067 17.390 -65.386 -43.236 -31.079 -19.273 2.374 1.002\rres[83] 43.678 17.193 9.558 31.772 43.540 55.495 76.981 1.002\rres[84] 20.380 17.058 -13.497 8.805 20.360 32.176 53.934 1.002\rres[85] 54.597 16.986 21.096 43.276 54.749 66.249 87.971 1.002\rres[86] 124.353 16.979 90.938 113.063 124.583 135.902 157.927 1.002\rres[87] 67.176 17.037 33.903 55.809 67.439 78.691 101.104 1.002\rres[88] -30.778 17.158 -64.046 -42.370 -30.407 -19.292 2.913 1.002\rres[89] -55.098 17.342 -88.751 -66.851 -54.716 -43.392 -21.208 1.002\rres[90] -73.427 17.586 -107.590 -85.203 -73.116 -61.615 -38.693 1.002\rres[91] -39.879 17.759 -75.389 -51.772 -39.985 -28.068 -4.896 1.001\rres[92] 40.803 17.486 5.829 29.249 40.623 52.320 75.462 1.001\rres[93] 3.288 17.272 -30.880 -8.346 3.017 14.813 37.091 1.001\rres[94] 8.096 17.120 -26.071 -3.487 7.840 19.496 41.421 1.001\rres[95] -18.230 17.031 -51.683 -29.609 -18.372 -6.678 14.819 1.001\rres[96] -27.022 17.006 -60.187 -38.426 -27.134 -15.863 6.296 1.001\rres[97] -19.513 17.045 -52.825 -30.880 -19.632 -8.423 13.668 1.001\rres[98] 37.064 17.149 3.831 25.570 37.124 48.265 70.659 1.001\rres[99] 21.843 17.315 -12.203 10.180 21.878 33.093 55.675 1.001\rres[100] 39.105 17.542 5.025 27.487 38.948 50.788 73.056 1.001\rres[101] 29.449 17.991 -5.205 17.155 29.781 41.807 63.442 1.001\rres[102] 49.685 17.766 15.218 37.673 50.072 61.814 82.819 1.001\rres[103] -8.723 17.601 -43.029 -20.739 -8.303 3.244 24.590 1.001\rres[104] 54.477 17.498 20.833 42.653 54.759 66.407 87.773 1.001\rres[105] 4.508 17.456 -29.390 -7.159 4.817 16.421 37.676 1.001\rres[106] -21.847 17.477 -55.720 -33.395 -21.535 -9.860 11.772 1.001\rres[107] 32.423 17.561 -2.084 20.791 32.811 44.448 66.578 1.001\rres[108] 70.203 17.706 35.619 58.406 70.311 82.156 104.571 1.001\rres[109] -18.915 17.912 -54.616 -30.740 -18.911 -6.868 15.481 1.001\rres[110] -79.501 18.176 -115.834 -91.501 -79.578 -67.080 -44.590 1.001\rres[111] -35.407 17.785 -70.692 -46.976 -35.584 -23.160 -0.930 1.001\rres[112] -16.834 17.533 -51.394 -28.356 -17.011 -4.647 17.012 1.001\rres[113] -2.964 17.342 -37.311 -14.283 -3.253 9.068 30.829 1.001\rres[114] 17.958 17.211 -16.244 6.888 17.697 29.740 51.566 1.001\rres[115] 43.960 17.144 9.743 32.909 43.779 55.749 77.773 1.002\rres[116] 6.922 17.141 -27.302 -4.089 6.746 18.522 41.245 1.002\rres[117] -42.437 17.202 -76.634 -53.389 -42.592 -31.097 -7.471 1.002\rres[118] 18.962 17.325 -15.305 7.723 18.882 30.432 53.655 1.003\rres[119] 54.157 17.511 19.505 42.669 54.007 65.841 88.924 1.003\rres[120] -30.836 17.757 -66.091 -42.472 -31.031 -18.943 4.597 1.003\rres[121] 29.694 17.913 -5.583 17.603 29.789 41.163 65.302 1.001\rres[122] -8.428 17.668 -42.894 -20.446 -8.260 2.790 26.838 1.001\rres[123] -97.805 17.483 -132.040 -109.658 -97.722 -86.688 -63.003 1.001\rres[124] -58.400 17.360 -92.551 -70.268 -58.013 -47.184 -24.118 1.001\rres[125] -38.480 17.298 -72.858 -50.159 -38.072 -27.272 -4.162 1.001\rres[126] -23.590 17.301 -58.378 -35.298 -23.103 -12.266 10.906 1.001\rres[127] 3.860 17.366 -31.068 -7.655 4.299 15.220 38.206 1.001\rres[128] 58.998 17.494 23.758 47.395 59.472 70.391 93.062 1.001\rres[129] 69.127 17.683 33.354 57.321 69.673 80.505 103.147 1.001\rres[130] 35.991 17.931 -0.747 24.279 36.423 47.825 70.397 1.001\rres[131] -18.707 17.753 -53.599 -30.549 -18.607 -6.714 16.819 1.003\rres[132] -14.747 17.516 -48.954 -26.537 -14.555 -2.845 20.061 1.003\rres[133] 10.374 17.340 -23.749 -1.370 10.597 22.133 44.664 1.003\rres[134] -37.152 17.225 -70.611 -48.839 -37.026 -25.712 -3.136 1.002\rres[135] -32.541 17.174 -65.860 -44.108 -32.536 -21.077 1.265 1.002\rres[136] 28.588 17.186 -4.848 17.066 28.625 39.992 62.044 1.002\rres[137] 23.576 17.262 -10.078 11.987 23.698 35.092 57.117 1.002\rres[138] -10.078 17.401 -43.670 -21.709 -9.920 1.610 23.933 1.001\rres[139] -16.016 17.601 -49.961 -27.799 -15.770 -4.358 18.774 1.001\rres[140] 48.626 17.861 14.303 36.410 48.905 60.610 83.764 1.001\rres[141] 4.593 17.780 -29.625 -7.191 4.424 16.505 38.788 1.001\rres[142] 57.463 17.544 23.749 45.968 57.368 69.300 91.358 1.001\rres[143] 44.743 17.368 11.543 33.225 44.780 56.414 78.495 1.001\rres[144] 47.987 17.253 14.659 36.564 48.140 59.625 81.776 1.001\rres[145] 2.009 17.202 -31.053 -9.596 2.094 13.679 35.963 1.001\rres[146] 23.279 17.214 -9.893 11.605 23.253 34.820 57.175 1.001\rres[147] -15.427 17.290 -49.433 -27.118 -15.449 -4.007 19.070 1.001\rres[148] -92.242 17.428 -126.419 -104.123 -92.223 -80.822 -57.405 1.001\rres[149] -76.403 17.628 -110.319 -88.293 -76.440 -64.957 -41.280 1.001\rres[150] 27.022 17.887 -7.788 14.991 26.934 38.901 62.553 1.001\rres[151] 56.524 17.435 23.497 44.650 56.376 68.008 91.937 1.002\rres[152] 94.888 17.193 62.473 83.265 94.748 106.107 129.551 1.001\rres[153] 85.122 17.012 53.069 73.707 84.983 96.084 119.032 1.001\rres[154] 28.800 16.893 -3.079 17.754 28.668 39.844 62.380 1.001\rres[155] 3.921 16.839 -28.102 -7.248 3.904 15.053 37.301 1.001\rres[156] -19.671 16.850 -51.711 -30.814 -19.649 -8.511 13.270 1.001\rres[157] -48.454 16.926 -81.115 -59.563 -48.405 -37.301 -15.678 1.001\rres[158] -12.976 17.067 -46.088 -24.257 -12.925 -1.706 20.012 1.001\rres[159] -79.807 17.269 -113.216 -91.035 -79.682 -68.569 -46.865 1.001\rres[160] -30.224 17.532 -64.395 -41.789 -29.847 -18.816 2.893 1.001\rres[161] -10.710 17.984 -46.042 -22.979 -10.276 1.572 24.298 1.002\rres[162] -82.452 17.740 -118.119 -94.601 -82.059 -70.317 -47.916 1.002\rres[163] -48.077 17.555 -83.627 -59.974 -47.795 -35.994 -13.951 1.002\rres[164] 68.821 17.431 33.752 56.976 68.995 80.691 102.233 1.005\rres[165] 12.830 17.369 -22.123 1.019 12.897 24.719 45.628 1.002\rres[166] 38.006 17.370 3.280 26.085 37.983 49.937 71.003 1.002\rres[167] -23.347 17.435 -58.240 -35.377 -23.237 -11.414 9.865 1.002\rres[168] 22.078 17.561 -13.256 9.887 22.185 34.157 55.218 1.001\rres[169] 15.237 17.749 -20.525 3.105 15.325 27.296 48.552 1.001\rres[170] 79.730 17.996 43.542 67.295 79.674 91.820 113.635 1.002\rres[171] 63.717 17.877 28.767 51.218 63.625 75.992 98.346 1.002\rres[172] 50.693 17.611 16.356 38.398 50.624 62.696 84.475 1.001\rres[173] 16.355 17.405 -17.386 4.388 16.412 28.087 49.899 1.001\rres[174] 5.229 17.259 -28.246 -6.690 5.281 16.848 38.937 1.001\rres[175] -25.424 17.176 -58.537 -37.419 -25.426 -13.955 7.970 1.001\rres[176] -60.299 17.157 -93.240 -72.218 -60.299 -48.754 -26.651 1.001\rres[177] -17.707 17.202 -50.497 -29.605 -17.777 -6.222 15.477 1.001\rres[178] -67.087 17.310 -99.859 -79.145 -67.067 -55.435 -33.398 1.001\rres[179] 21.851 17.480 -10.981 9.770 21.871 33.713 56.070 1.001\rres[180] -40.647 17.711 -73.957 -52.771 -40.513 -28.613 -5.841 1.001\rres[181] -19.458 17.542 -54.333 -31.085 -19.422 -7.722 15.640 1.001\rres[182] 13.382 17.297 -20.657 2.031 13.446 24.888 47.985 1.001\rres[183] 42.203 17.113 8.488 30.978 42.207 53.606 75.709 1.001\rres[184] 20.699 16.991 -12.648 9.734 20.572 32.070 53.897 1.001\rres[185] 22.419 16.933 -10.796 11.505 22.206 33.885 55.335 1.001\rres[186] 67.746 16.941 34.410 57.004 67.445 79.392 100.423 1.001\rres[187] -17.017 17.012 -50.239 -27.920 -17.298 -5.427 15.763 1.001\rres[188] -51.228 17.148 -84.441 -62.328 -51.548 -39.688 -18.069 1.001\rres[189] -23.628 17.345 -57.805 -34.767 -23.760 -11.997 10.684 1.001\rres[190] -39.945 17.603 -74.634 -51.215 -40.154 -28.208 -5.214 1.001\rres[191] 52.530 17.512 18.626 41.139 52.206 64.088 87.150 1.006\rres[192] 79.593 17.246 46.248 68.448 79.388 91.100 113.049 1.017\rres[193] 71.051 17.040 37.868 59.980 70.793 82.571 104.153 1.021\rres[194] -14.917 16.897 -48.034 -25.902 -15.296 -3.462 18.041 1.005\rres[195] -7.135 16.817 -39.943 -18.047 -7.544 4.215 25.897 1.005\rres[196] -18.174 16.803 -51.133 -29.182 -18.623 -6.895 14.827 1.005\rres[197] -16.439 16.854 -48.969 -27.463 -16.762 -5.147 16.557 1.004\rres[198] -69.150 16.970 -102.173 -80.328 -69.361 -57.572 -35.980 1.004\rres[199] -27.445 17.149 -60.854 -38.683 -27.690 -15.800 6.299 1.003\rres[200] -71.100 17.389 -104.684 -82.727 -71.394 -59.189 -37.691 1.003\rres[201] 1.426 17.309 -34.407 -9.571 1.332 12.962 35.011 1.001\rres[202] 36.131 17.046 0.771 25.266 36.227 47.347 69.464 1.001\rres[203] 5.510 16.844 -28.967 -5.374 5.685 16.535 38.753 1.001\rres[204] 49.200 16.705 15.453 38.505 49.435 60.187 82.079 1.001\rres[205] -10.960 16.631 -44.546 -21.713 -10.492 -0.147 21.703 1.001\rres[206] -133.889 16.623 -167.539 -144.693 -133.301 -123.036 -101.431 1.001\rres[207] -68.634 16.681 -102.616 -79.683 -67.975 -57.769 -36.182 1.001\rres[208] 2.212 16.804 -31.897 -8.838 2.830 13.283 34.752 1.001\rres[209] 2.431 16.991 -31.627 -8.767 3.000 13.550 35.269 1.001\rres[210] 41.968 17.240 6.811 30.539 42.440 53.467 75.061 1.001\rres[211] -67.622 17.508 -102.075 -79.236 -67.541 -56.136 -32.937 1.001\rres[212] -57.530 17.266 -91.092 -68.962 -57.500 -46.025 -22.873 1.001\rres[213] -140.313 17.084 -173.718 -151.700 -140.155 -128.926 -105.859 1.002\rres[214] -50.542 16.964 -83.840 -61.841 -50.386 -39.139 -16.241 1.002\rres[215] 55.074 16.909 22.122 43.726 55.352 66.374 89.195 1.002\rres[216] 100.133 16.919 67.118 88.860 100.592 111.422 133.899 1.002\rres[217] 80.975 16.993 48.141 69.704 81.338 92.108 115.080 1.002\rres[218] 65.212 17.132 32.285 53.430 65.470 76.536 100.152 1.001\rres[219] -21.674 17.332 -55.118 -33.640 -21.456 -10.022 13.856 1.002\rres[220] 24.003 17.593 -9.530 11.811 24.101 35.907 59.852 1.002\rres[221] 60.185 17.783 25.820 48.457 59.813 72.436 96.171 1.001\rres[222] 26.825 17.539 -6.907 15.267 26.510 38.780 62.320 1.001\rres[223] -37.765 17.355 -71.270 -49.250 -38.173 -25.946 -2.142 1.001\rres[224] -33.962 17.232 -67.496 -45.390 -34.499 -22.046 1.130 1.001\rres[225] -58.522 17.173 -91.829 -70.044 -59.083 -46.732 -23.657 1.001\rres[226] -55.706 17.177 -88.390 -67.397 -56.263 -44.042 -21.182 1.001\rres[227] -94.620 17.245 -127.457 -106.411 -95.119 -82.969 -60.749 1.001\rres[228] 19.371 17.375 -13.948 7.607 19.044 31.010 53.459 1.001\rres[229] 25.246 17.568 -8.529 13.626 24.828 37.075 59.410 1.001\rres[230] 84.355 17.820 50.467 72.405 83.982 96.570 118.968 1.001\rres[231] -31.497 18.030 -66.696 -43.447 -31.785 -19.738 3.853 1.001\rres[232] -33.555 17.750 -68.255 -45.064 -33.695 -22.067 1.221 1.001\rres[233] -46.454 17.529 -80.525 -58.085 -46.585 -35.121 -11.905 1.001\rres[234] -84.283 17.368 -118.128 -95.729 -84.398 -73.094 -50.480 1.001\rres[235] 23.792 17.270 -9.429 12.004 23.713 34.911 57.625 1.001\rres[236] -37.979 17.234 -71.525 -49.693 -38.039 -26.826 -3.726 1.001\rres[237] -0.851 17.262 -34.217 -12.666 -0.823 10.179 33.380 1.001\rres[238] 55.116 17.354 21.132 43.276 55.278 66.313 89.932 1.001\rres[239] 66.340 17.507 31.871 54.397 66.435 77.816 102.154 1.001\rres[240] 22.509 17.721 -12.331 10.444 22.467 34.188 58.690 1.001\rres[241] 48.779 17.985 13.144 36.603 49.101 61.358 82.744 1.001\rres[242] 54.607 17.728 19.723 42.634 54.853 67.014 88.097 1.001\rres[243] -2.573 17.531 -36.920 -14.310 -2.250 9.522 31.153 1.001\rres[244] -64.682 17.394 -98.704 -76.398 -64.353 -52.693 -31.136 1.001\rres[245] 59.232 17.320 25.143 47.518 59.597 70.983 92.873 1.001\rres[246] 19.963 17.309 -14.316 8.145 20.310 31.886 53.371 1.001\rres[247] -70.443 17.361 -105.373 -82.320 -70.049 -58.397 -36.717 1.001\rres[248] -23.463 17.476 -58.679 -35.360 -23.248 -11.587 10.598 1.001\rres[249] 2.831 17.652 -32.482 -8.951 3.087 14.786 37.287 1.001\rres[250] -59.475 17.888 -95.014 -71.176 -59.315 -47.543 -24.802 1.001\rres[251] -118.664 17.326 -151.645 -130.208 -118.894 -107.309 -83.446 1.002\rres[252] -91.020 17.066 -123.512 -102.427 -91.199 -79.836 -57.008 1.001\rres[253] -6.258 16.868 -38.555 -17.830 -6.464 4.709 27.751 1.001\rres[254] 36.251 16.734 4.248 24.639 35.981 47.226 70.217 1.001\rres[255] 13.667 16.664 -18.023 2.118 13.400 24.523 47.428 1.001\rres[256] -21.601 16.659 -53.042 -33.062 -21.878 -10.646 12.267 1.001\rres[257] 5.310 16.721 -26.340 -6.021 5.033 16.411 39.015 1.001\rres[258] 21.015 16.847 -10.852 9.662 20.790 32.193 55.145 1.001\rres[259] 57.059 17.037 24.518 45.611 56.831 68.241 92.442 1.002\rres[260] 34.481 17.288 1.254 23.094 34.411 45.908 70.086 1.001\rres[261] 50.413 17.715 15.780 38.279 50.907 62.488 84.294 1.002\rres[262] 1.013 17.462 -32.559 -11.003 1.463 13.068 33.840 1.002\rres[263] -13.632 17.268 -46.927 -25.466 -13.251 -1.826 18.917 1.002\rres[264] 28.083 17.137 -5.178 16.694 28.340 39.623 60.566 1.003\rres[265] 73.774 17.069 40.522 62.477 73.840 85.174 106.109 1.003\rres[266] -36.234 17.065 -69.304 -47.505 -35.963 -24.747 -3.793 1.003\rres[267] -22.093 17.125 -55.763 -33.330 -21.828 -10.616 10.070 1.003\rres[268] 14.160 17.249 -19.827 3.041 14.507 25.776 46.182 1.003\rres[269] -59.954 17.435 -93.778 -71.235 -59.514 -48.101 -27.315 1.003\rres[270] -22.812 17.681 -57.121 -34.293 -22.324 -10.851 10.918 1.003\rres[271] -125.907 17.458 -160.933 -137.188 -125.802 -114.461 -92.053 1.001\rres[272] -62.314 17.229 -97.496 -73.416 -62.473 -50.835 -29.085 1.001\rres[273] -35.666 17.061 -70.145 -46.545 -35.800 -24.427 -3.232 1.001\rres[274] -2.957 16.956 -37.392 -13.883 -2.813 8.075 29.213 1.001\rres[275] -9.344 16.916 -43.467 -20.207 -9.343 1.681 22.885 1.001\rres[276] 22.554 16.940 -11.628 11.868 22.593 33.645 54.629 1.001\rres[277] 73.779 17.029 39.473 63.039 73.780 84.850 106.184 1.001\rres[278] 143.908 17.181 109.484 132.820 143.915 155.171 176.723 1.001\rres[279] 100.141 17.395 65.573 88.965 100.141 111.576 133.506 1.001\rres[280] -46.044 17.669 -80.667 -57.400 -45.955 -34.590 -12.407 1.001\rres[281] -5.699 17.595 -39.468 -17.381 -5.770 5.983 28.301 1.001\rres[282] 0.419 17.343 -33.174 -10.923 0.288 11.974 34.262 1.001\rres[283] -12.869 17.152 -46.211 -24.317 -13.049 -1.401 20.398 1.001\rres[284] 12.545 17.023 -20.331 1.056 12.458 24.113 45.686 1.001\rres[285] 54.857 16.958 22.320 43.310 54.741 66.049 88.406 1.001\rres[286] 12.136 16.958 -20.207 0.644 11.947 23.530 45.348 1.001\rres[287] -10.984 17.022 -43.192 -22.616 -10.977 0.294 22.275 1.001\rres[288] 4.979 17.150 -27.724 -6.762 5.048 16.454 38.007 1.001\rres[289] -29.791 17.340 -62.952 -41.651 -29.779 -18.299 3.958 1.001\rres[290] -25.671 17.591 -59.797 -37.708 -25.457 -14.066 8.558 1.001\rres[291] 28.550 17.963 -7.697 16.728 28.845 40.812 62.212 1.001\rres[292] -9.089 17.712 -45.024 -20.553 -8.933 2.924 24.333 1.001\rres[293] -49.732 17.521 -85.124 -61.052 -49.636 -38.060 -16.508 1.001\rres[294] -58.677 17.390 -93.259 -69.572 -58.567 -47.134 -25.799 1.001\rres[295] -57.955 17.322 -92.707 -68.933 -57.842 -46.344 -25.049 1.001\rres[296] -3.734 17.317 -38.668 -14.858 -3.653 7.873 29.033 1.001\rres[297] 69.494 17.375 34.053 58.293 69.643 81.139 102.719 1.001\rres[298] 42.465 17.496 6.356 31.206 42.578 54.056 75.774 1.001\rres[299] 7.231 17.678 -29.387 -4.288 7.395 19.167 40.819 1.001\rres[300] -23.337 17.919 -60.558 -34.994 -23.228 -11.302 10.882 1.001\rres[301] -51.903 17.982 -88.248 -63.876 -52.309 -39.714 -17.303 1.001\rres[302] -37.852 17.747 -73.574 -49.599 -38.337 -25.637 -3.466 1.001\rres[303] 9.383 17.571 -25.880 -2.168 9.060 21.337 43.272 1.001\rres[304] 6.786 17.457 -27.982 -4.692 6.466 18.794 40.534 1.001\rres[305] -83.287 17.404 -117.576 -94.688 -83.511 -71.384 -50.132 1.001\rres[306] -56.131 17.415 -90.605 -67.604 -56.382 -44.092 -22.768 1.001\rres[307] 29.387 17.488 -5.255 17.630 29.168 41.485 62.770 1.001\rres[308] 97.884 17.624 63.110 86.054 97.739 110.020 131.499 1.001\rres[309] 53.516 17.820 18.505 41.582 53.468 65.807 87.676 1.001\rres[310] -20.057 18.074 -55.492 -32.169 -20.032 -7.574 14.442 1.001\rres[311] 101.300 17.549 67.220 89.168 101.137 113.302 135.233 1.001\rres[312] 83.175 17.312 49.506 71.360 82.808 95.098 116.774 1.001\rres[313] 71.785 17.135 37.980 60.064 71.677 83.600 105.227 1.001\rres[314] 88.437 17.022 55.339 76.901 88.506 100.014 122.116 1.001\rres[315] -0.110 16.972 -33.336 -11.341 0.071 11.200 33.192 1.001\rres[316] -26.794 16.987 -60.411 -38.159 -26.771 -15.599 6.418 1.001\rres[317] -88.891 17.067 -122.585 -100.389 -88.824 -77.593 -55.404 1.001\rres[318] -96.339 17.209 -130.866 -107.673 -96.328 -84.910 -62.716 1.001\rres[319] -61.837 17.414 -96.679 -73.288 -61.840 -50.019 -28.051 1.001\rres[320] -108.552 17.679 -143.474 -120.010 -108.694 -96.508 -74.344 1.001\rres[321] -74.410 17.625 -109.575 -85.816 -74.485 -62.663 -39.753 1.001\rres[322] -41.400 17.380 -75.871 -52.760 -41.519 -29.797 -7.178 1.001\rres[323] -74.807 17.196 -108.901 -86.261 -74.760 -63.308 -41.163 1.001\rres[324] -45.144 17.074 -79.209 -56.502 -45.026 -33.729 -12.103 1.001\rres[325] 4.537 17.016 -29.468 -6.710 4.542 15.856 37.618 1.001\rres[326] 59.794 17.022 26.116 48.375 59.758 71.282 93.240 1.001\rres[327] 40.165 17.092 6.814 28.767 40.355 51.655 73.479 1.001\rres[328] 30.285 17.226 -3.464 18.692 30.679 41.872 63.673 1.001\rres[329] 22.589 17.422 -10.860 10.756 23.053 34.110 56.454 1.001\rres[330] 92.703 17.678 58.616 80.766 93.284 104.245 126.965 1.001\rres[331] 95.283 17.977 59.650 83.167 95.192 107.412 130.221 1.001\rres[332] 112.719 17.720 77.424 100.669 112.724 124.693 147.222 1.001\rres[333] 70.443 17.522 35.436 58.422 70.438 82.385 104.268 1.001\rres[334] 69.514 17.384 34.563 57.587 69.353 81.410 102.556 1.001\rres[335] 70.134 17.309 35.643 58.219 69.855 81.817 103.521 1.001\rres[336] -1.755 17.297 -35.726 -13.635 -1.976 10.018 31.941 1.001\rres[337] -93.736 17.349 -127.604 -105.548 -93.768 -81.835 -60.000 1.001\rres[338] -48.951 17.463 -82.779 -60.869 -49.029 -36.839 -14.994 1.001\rres[339] -121.819 17.638 -156.071 -133.910 -122.015 -109.591 -87.286 1.001\rres[340] -103.701 17.874 -138.279 -115.672 -103.713 -91.305 -68.978 1.001\rres[341] -69.317 18.059 -103.485 -81.639 -69.415 -57.406 -33.457 1.001\rres[342] -32.346 17.788 -65.785 -44.511 -32.362 -20.467 2.839 1.001\rres[343] -21.629 17.575 -54.850 -33.597 -21.808 -9.882 13.449 1.001\rres[344] -59.307 17.422 -92.307 -71.080 -59.415 -47.483 -24.425 1.001\rres[345] -3.627 17.331 -36.621 -15.370 -3.984 8.092 31.060 1.001\rres[346] 78.394 17.304 46.202 66.537 78.021 90.261 112.620 1.001\rres[347] 100.999 17.339 68.667 89.254 100.770 113.032 135.290 1.001\rres[348] -22.296 17.438 -55.281 -34.095 -22.507 -10.328 12.264 1.001\rres[349] 46.092 17.598 12.847 34.241 46.001 58.157 80.422 1.001\rres[350] 27.883 17.819 -5.915 15.648 27.735 40.121 63.077 1.001\rsigma 55.917 2.244 51.705 54.351 55.829 57.468 60.369 1.003\rsigma.B 64.474 8.406 50.251 58.466 63.695 69.675 83.144 1.006\rdeviance 3809.753 9.145 3794.047 3803.114 3809.077 3815.608 3829.461 1.003\rn.eff\rbeta 3000\rbeta0 3000\rgamma[1] 1800\rgamma[2] 3000\rgamma[3] 2100\rgamma[4] 3000\rgamma[5] 3000\rgamma[6] 3000\rgamma[7] 3000\rgamma[8] 3000\rgamma[9] 2800\rgamma[10] 3000\rgamma[11] 2100\rgamma[12] 3000\rgamma[13] 3000\rgamma[14] 3000\rgamma[15] 2500\rgamma[16] 1700\rgamma[17] 1700\rgamma[18] 1800\rgamma[19] 3000\rgamma[20] 1500\rgamma[21] 3000\rgamma[22] 3000\rgamma[23] 3000\rgamma[24] 3000\rgamma[25] 3000\rgamma[26] 1700\rgamma[27] 1500\rgamma[28] 3000\rgamma[29] 2300\rgamma[30] 3000\rgamma[31] 3000\rgamma[32] 3000\rgamma[33] 2500\rgamma[34] 3000\rgamma[35] 3000\rres[1] 1100\rres[2] 1000\rres[3] 990\rres[4] 940\rres[5] 910\rres[6] 880\rres[7] 860\rres[8] 850\rres[9] 840\rres[10] 840\rres[11] 3000\rres[12] 3000\rres[13] 3000\rres[14] 3000\rres[15] 3000\rres[16] 3000\rres[17] 3000\rres[18] 3000\rres[19] 3000\rres[20] 3000\rres[21] 1200\rres[22] 1200\rres[23] 1100\rres[24] 1000\rres[25] 1000\rres[26] 970\rres[27] 950\rres[28] 1200\rres[29] 920\rres[30] 920\rres[31] 3000\rres[32] 3000\rres[33] 3000\rres[34] 3000\rres[35] 3000\rres[36] 3000\rres[37] 3000\rres[38] 3000\rres[39] 3000\rres[40] 3000\rres[41] 3000\rres[42] 3000\rres[43] 3000\rres[44] 3000\rres[45] 3000\rres[46] 3000\rres[47] 3000\rres[48] 3000\rres[49] 3000\rres[50] 3000\rres[51] 3000\rres[52] 3000\rres[53] 3000\rres[54] 3000\rres[55] 3000\rres[56] 3000\rres[57] 3000\rres[58] 3000\rres[59] 3000\rres[60] 3000\rres[61] 3000\rres[62] 3000\rres[63] 3000\rres[64] 3000\rres[65] 3000\rres[66] 3000\rres[67] 3000\rres[68] 3000\rres[69] 3000\rres[70] 3000\rres[71] 3000\rres[72] 3000\rres[73] 3000\rres[74] 3000\rres[75] 3000\rres[76] 3000\rres[77] 3000\rres[78] 3000\rres[79] 3000\rres[80] 3000\rres[81] 1100\rres[82] 1100\rres[83] 1000\rres[84] 970\rres[85] 930\rres[86] 930\rres[87] 1100\rres[88] 860\rres[89] 850\rres[90] 850\rres[91] 3000\rres[92] 3000\rres[93] 3000\rres[94] 3000\rres[95] 3000\rres[96] 3000\rres[97] 3000\rres[98] 3000\rres[99] 3000\rres[100] 3000\rres[101] 2700\rres[102] 2700\rres[103] 2800\rres[104] 3000\rres[105] 3000\rres[106] 3000\rres[107] 3000\rres[108] 3000\rres[109] 3000\rres[110] 3000\rres[111] 3000\rres[112] 3000\rres[113] 3000\rres[114] 3000\rres[115] 3000\rres[116] 3000\rres[117] 3000\rres[118] 3000\rres[119] 3000\rres[120] 3000\rres[121] 3000\rres[122] 3000\rres[123] 3000\rres[124] 3000\rres[125] 3000\rres[126] 3000\rres[127] 3000\rres[128] 3000\rres[129] 3000\rres[130] 3000\rres[131] 3000\rres[132] 3000\rres[133] 3000\rres[134] 3000\rres[135] 3000\rres[136] 3000\rres[137] 3000\rres[138] 3000\rres[139] 3000\rres[140] 3000\rres[141] 2700\rres[142] 2700\rres[143] 2800\rres[144] 3000\rres[145] 3000\rres[146] 3000\rres[147] 3000\rres[148] 3000\rres[149] 3000\rres[150] 3000\rres[151] 1700\rres[152] 2200\rres[153] 2300\rres[154] 1900\rres[155] 1900\rres[156] 2000\rres[157] 2200\rres[158] 2300\rres[159] 2500\rres[160] 2700\rres[161] 1500\rres[162] 1500\rres[163] 1500\rres[164] 1000\rres[165] 1600\rres[166] 1700\rres[167] 1800\rres[168] 1900\rres[169] 2000\rres[170] 1600\rres[171] 3000\rres[172] 1900\rres[173] 1900\rres[174] 2000\rres[175] 2000\rres[176] 2100\rres[177] 2300\rres[178] 2400\rres[179] 2600\rres[180] 2800\rres[181] 3000\rres[182] 3000\rres[183] 3000\rres[184] 3000\rres[185] 3000\rres[186] 3000\rres[187] 3000\rres[188] 3000\rres[189] 3000\rres[190] 3000\rres[191] 1500\rres[192] 630\rres[193] 570\rres[194] 1600\rres[195] 1700\rres[196] 1800\rres[197] 1900\rres[198] 2000\rres[199] 2100\rres[200] 2300\rres[201] 3000\rres[202] 3000\rres[203] 3000\rres[204] 3000\rres[205] 3000\rres[206] 3000\rres[207] 3000\rres[208] 3000\rres[209] 3000\rres[210] 3000\rres[211] 2000\rres[212] 1900\rres[213] 1700\rres[214] 1600\rres[215] 1600\rres[216] 1700\rres[217] 1700\rres[218] 1900\rres[219] 1400\rres[220] 1400\rres[221] 3000\rres[222] 3000\rres[223] 3000\rres[224] 3000\rres[225] 3000\rres[226] 3000\rres[227] 3000\rres[228] 3000\rres[229] 3000\rres[230] 3000\rres[231] 3000\rres[232] 3000\rres[233] 3000\rres[234] 3000\rres[235] 3000\rres[236] 3000\rres[237] 3000\rres[238] 3000\rres[239] 3000\rres[240] 3000\rres[241] 3000\rres[242] 3000\rres[243] 3000\rres[244] 3000\rres[245] 3000\rres[246] 3000\rres[247] 3000\rres[248] 3000\rres[249] 3000\rres[250] 3000\rres[251] 1800\rres[252] 1800\rres[253] 1900\rres[254] 1900\rres[255] 2000\rres[256] 2100\rres[257] 2200\rres[258] 2400\rres[259] 3000\rres[260] 2800\rres[261] 930\rres[262] 880\rres[263] 830\rres[264] 790\rres[265] 750\rres[266] 740\rres[267] 720\rres[268] 710\rres[269] 710\rres[270] 710\rres[271] 3000\rres[272] 3000\rres[273] 3000\rres[274] 3000\rres[275] 3000\rres[276] 3000\rres[277] 3000\rres[278] 3000\rres[279] 3000\rres[280] 3000\rres[281] 2600\rres[282] 2600\rres[283] 2700\rres[284] 2800\rres[285] 3000\rres[286] 3000\rres[287] 3000\rres[288] 3000\rres[289] 3000\rres[290] 3000\rres[291] 3000\rres[292] 3000\rres[293] 3000\rres[294] 3000\rres[295] 3000\rres[296] 3000\rres[297] 3000\rres[298] 3000\rres[299] 3000\rres[300] 3000\rres[301] 3000\rres[302] 3000\rres[303] 3000\rres[304] 3000\rres[305] 3000\rres[306] 3000\rres[307] 3000\rres[308] 3000\rres[309] 3000\rres[310] 3000\rres[311] 3000\rres[312] 3000\rres[313] 3000\rres[314] 3000\rres[315] 3000\rres[316] 3000\rres[317] 3000\rres[318] 3000\rres[319] 3000\rres[320] 3000\rres[321] 2700\rres[322] 2800\rres[323] 2900\rres[324] 3000\rres[325] 3000\rres[326] 3000\rres[327] 3000\rres[328] 3000\rres[329] 3000\rres[330] 3000\rres[331] 3000\rres[332] 3000\rres[333] 3000\rres[334] 3000\rres[335] 3000\rres[336] 3000\rres[337] 3000\rres[338] 3000\rres[339] 3000\rres[340] 3000\rres[341] 3000\rres[342] 3000\rres[343] 3000\rres[344] 3000\rres[345] 3000\rres[346] 3000\rres[347] 3000\rres[348] 3000\rres[349] 3000\rres[350] 3000\rsigma 700\rsigma.B 1000\rdeviance 720\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 41.8 and DIC = 3851.5\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMatrix parameterisation\r\u0026gt; modelString2=\u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- inprod(beta[],X[i,]) + gamma[Block[i]]\r+ res[i] \u0026lt;- y[i]-mu[i]\r+ } + + #Priors\r+ beta ~ dmnorm(a0,A0)\r+ for (i in 1:nBlock) {\r+ gamma[i] ~ dnorm(0, tau.B) #prior\r+ }\r+ tau \u0026lt;- pow(sigma,-2)\r+ sigma \u0026lt;- z/sqrt(chSq) + z ~ dnorm(0, 0.0016)I(0,) #1/25^2 = 0.0016\r+ chSq ~ dgamma(0.5, 0.5)\r+ + tau.B \u0026lt;- pow(sigma.B,-2)\r+ sigma.B \u0026lt;- z/sqrt(chSq.B) + z.B ~ dnorm(0, 0.0016)I(0,) #1/25^2 = 0.0016\r+ chSq.B ~ dgamma(0.5, 0.5)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString2, con = \u0026quot;matrixModel2.txt\u0026quot;)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~Time,data.rm)\r\u0026gt; data.rm.list \u0026lt;- with(data.rm,\r+ list(y=y,\r+ Block=as.numeric(Block),\r+ X=Xmat,\r+ n=nrow(data.rm),\r+ nBlock=length(levels(Block)),\r+ a0=rep(0,ncol(Xmat)), A0=diag(ncol(Xmat))\r+ )\r+ )\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026#39;gamma\u0026#39;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma.B\u0026quot;,\u0026quot;res\u0026quot;)\r\u0026gt; adaptSteps = 1000\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; data.rm.r2jags.m \u0026lt;- jags(data = data.rm.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;matrixModel2.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 350\rUnobserved stochastic nodes: 40\rTotal graph size: 2521\rInitializing model\r\u0026gt; \u0026gt; print(data.rm.r2jags.m)\rInference for Bugs model at \u0026quot;matrixModel2.txt\u0026quot;, fit using jags,\r2 chains, each with 4500 iterations (first 3000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat\rbeta[1] 0.118 0.993 -1.825 -0.560 0.127 0.772 2.136 1.002\rbeta[2] 9.067 1.041 6.989 8.384 9.085 9.754 11.096 1.009\rgamma[1] 268.496 28.003 214.165 249.963 268.515 287.365 322.199 1.002\rgamma[2] 249.357 27.207 196.523 230.541 249.374 268.093 303.009 1.001\rgamma[3] 326.337 28.472 271.718 306.769 325.694 345.668 383.423 1.002\rgamma[4] 305.015 28.782 249.411 285.199 305.040 324.243 361.356 1.001\rgamma[5] 377.709 27.831 324.335 358.940 377.037 396.616 432.150 1.001\rgamma[6] 343.430 27.578 288.864 325.843 342.939 361.444 396.819 1.001\rgamma[7] 332.166 27.783 278.248 313.377 331.664 351.388 385.777 1.002\rgamma[8] 271.616 27.163 217.184 253.588 271.956 289.628 322.963 1.001\rgamma[9] 384.589 27.386 332.007 366.136 384.502 402.093 440.209 1.001\rgamma[10] 367.756 27.587 314.137 349.123 368.026 386.501 421.707 1.001\rgamma[11] 457.715 27.438 405.652 439.504 457.268 476.098 513.423 1.001\rgamma[12] 312.867 27.216 262.216 293.798 313.154 331.284 366.377 1.001\rgamma[13] 258.804 27.818 201.324 240.475 259.644 276.264 313.330 1.002\rgamma[14] 279.449 28.042 224.175 260.014 279.067 298.411 335.404 1.001\rgamma[15] 337.363 28.099 284.866 317.715 336.998 356.109 394.348 1.001\rgamma[16] 416.383 27.354 363.925 397.826 416.595 434.011 473.215 1.002\rgamma[17] 403.080 27.501 349.003 384.921 403.079 421.627 456.974 1.002\rgamma[18] 237.418 27.946 183.713 218.161 237.435 256.648 290.420 1.001\rgamma[19] 323.886 27.728 270.777 305.233 323.114 342.641 377.533 1.001\rgamma[20] 275.462 28.013 220.684 255.987 275.838 294.317 330.957 1.001\rgamma[21] 194.059 27.398 141.539 175.756 194.167 211.807 248.181 1.002\rgamma[22] 289.821 27.693 236.092 271.210 289.707 307.332 344.403 1.001\rgamma[23] 217.663 28.050 164.441 197.887 217.932 235.812 274.184 1.002\rgamma[24] 212.748 27.923 159.816 193.937 211.567 231.708 268.892 1.001\rgamma[25] 255.843 26.839 204.539 237.181 255.916 274.542 309.265 1.003\rgamma[26] 206.396 28.055 152.547 187.160 206.227 225.611 261.170 1.001\rgamma[27] 327.559 27.486 273.350 309.633 327.509 345.364 380.375 1.002\rgamma[28] 384.714 27.705 330.300 365.884 384.552 403.155 437.847 1.003\rgamma[29] 304.783 28.077 251.111 284.961 304.255 323.437 360.045 1.001\rgamma[30] 225.251 28.017 168.742 206.532 226.000 243.944 279.897 1.001\rgamma[31] 242.036 28.513 184.632 223.534 242.114 260.552 297.438 1.001\rgamma[32] 250.059 27.603 197.965 231.432 249.373 268.506 305.275 1.003\rgamma[33] 321.386 27.863 266.714 302.647 321.462 340.165 377.552 1.003\rgamma[34] 368.704 27.907 314.047 350.229 368.434 387.452 423.293 1.001\rgamma[35] 365.968 27.738 311.338 347.444 365.930 385.018 420.415 1.005\rres[1] 4.433 27.784 -48.877 -14.717 4.562 22.705 58.869 1.002\rres[2] 34.393 27.628 -18.119 15.474 34.478 52.583 88.788 1.002\rres[3] -17.044 27.510 -69.264 -35.707 -16.883 0.972 37.088 1.002\rres[4] -19.010 27.431 -70.907 -37.537 -18.756 -0.634 35.109 1.002\rres[5] 22.692 27.391 -29.376 4.107 22.942 41.160 77.308 1.001\rres[6] 10.582 27.391 -41.038 -7.968 10.837 28.907 65.778 1.001\rres[7] -70.449 27.431 -121.778 -88.991 -69.960 -52.214 -14.834 1.001\rres[8] 21.008 27.509 -30.615 2.408 21.241 39.197 76.881 1.001\rres[9] 12.673 27.627 -39.023 -6.003 12.972 31.145 68.777 1.001\rres[10] 22.988 27.783 -29.250 4.168 22.937 41.683 79.296 1.001\rres[11] -40.986 26.942 -94.515 -59.717 -41.262 -22.608 11.356 1.001\rres[12] -57.652 26.765 -110.392 -76.201 -57.806 -39.438 -5.452 1.001\rres[13] 14.195 26.627 -37.606 -4.203 14.145 32.218 66.504 1.001\rres[14] -29.460 26.529 -81.001 -47.601 -29.683 -11.482 22.755 1.001\rres[15] -14.892 26.472 -66.711 -32.886 -15.051 3.149 37.784 1.001\rres[16] 15.000 26.456 -36.965 -3.255 14.703 33.011 68.048 1.001\rres[17] -11.656 26.481 -63.530 -29.872 -12.202 6.352 41.041 1.001\rres[18] -2.098 26.546 -53.836 -20.736 -2.884 15.886 51.209 1.001\rres[19] 40.030 26.652 -11.158 21.366 39.102 58.171 93.729 1.001\rres[20] 112.896 26.799 61.236 94.306 112.162 131.227 167.261 1.001\rres[21] -173.333 28.266 -230.343 -192.502 -172.496 -153.934 -119.008 1.002\rres[22] -146.973 28.095 -203.369 -166.084 -146.394 -127.937 -93.044 1.002\rres[23] -66.390 27.961 -122.215 -85.409 -65.713 -47.492 -12.298 1.002\rres[24] -62.498 27.865 -117.833 -81.428 -61.625 -43.630 -7.786 1.001\rres[25] -16.892 27.808 -71.818 -35.704 -16.206 2.104 37.863 1.001\rres[26] 37.026 27.790 -17.773 18.140 37.505 55.996 91.643 1.001\rres[27] 73.127 27.811 18.616 54.532 73.671 91.725 127.823 1.001\rres[28] 125.466 27.871 71.518 106.855 125.737 144.036 180.266 1.001\rres[29] 132.725 27.969 78.584 113.924 133.004 151.381 187.753 1.001\rres[30] 128.171 28.106 73.520 109.247 128.243 147.168 184.155 1.001\rres[31] -215.417 28.519 -271.907 -234.515 -215.477 -196.031 -160.014 1.001\rres[32] -172.464 28.331 -228.398 -191.559 -172.626 -153.223 -117.470 1.001\rres[33] -107.754 28.180 -163.519 -126.542 -107.988 -88.710 -53.225 1.001\rres[34] -46.543 28.067 -102.122 -65.407 -46.791 -27.844 8.278 1.001\rres[35] -47.922 27.992 -103.460 -66.793 -48.157 -29.112 7.029 1.001\rres[36] 71.901 27.955 16.630 52.816 71.633 90.451 126.471 1.001\rres[37] 97.292 27.958 42.344 78.098 97.282 115.785 151.917 1.001\rres[38] 113.786 27.999 59.110 94.749 113.857 132.397 169.334 1.001\rres[39] 188.465 28.078 134.278 169.268 188.455 207.026 244.767 1.001\rres[40] 152.859 28.196 98.827 133.783 152.725 171.438 209.588 1.001\rres[41] -138.298 27.589 -192.214 -157.018 -137.668 -119.884 -85.511 1.001\rres[42] -0.435 27.414 -54.092 -19.066 0.070 17.947 51.755 1.001\rres[43] -5.275 27.278 -58.296 -23.835 -4.661 12.955 46.693 1.001\rres[44] 76.163 27.182 23.390 57.931 76.757 94.384 128.160 1.001\rres[45] -8.105 27.124 -60.470 -26.335 -7.526 10.317 43.968 1.001\rres[46] -8.933 27.107 -61.592 -27.005 -8.421 9.419 43.130 1.001\rres[47] 23.039 27.130 -29.532 5.000 23.698 41.485 75.548 1.001\rres[48] 48.884 27.192 -3.759 30.945 49.511 67.086 101.297 1.001\rres[49] 8.462 27.295 -44.134 -9.630 8.812 26.700 61.215 1.001\rres[50] 31.557 27.436 -20.973 13.381 31.746 50.020 84.424 1.001\rres[51] -128.097 27.415 -180.770 -146.327 -127.910 -110.576 -73.568 1.001\rres[52] -60.141 27.254 -112.252 -78.239 -59.732 -42.653 -5.561 1.001\rres[53] 17.013 27.132 -35.210 -1.266 17.578 34.265 71.525 1.001\rres[54] -82.564 27.049 -134.784 -100.658 -82.293 -65.282 -27.984 1.002\rres[55] 14.994 27.006 -36.578 -3.173 15.282 32.302 69.851 1.002\rres[56] -26.609 27.004 -78.326 -44.704 -26.580 -9.314 28.128 1.002\rres[57] 29.085 27.041 -22.987 10.998 29.221 46.243 83.856 1.002\rres[58] 88.084 27.118 35.657 69.858 88.230 105.263 142.463 1.004\rres[59] 96.481 27.235 43.718 78.058 96.354 113.557 150.554 1.004\rres[60] 85.123 27.391 32.058 66.554 85.013 102.661 139.651 1.005\rres[61] -114.949 27.627 -168.618 -134.296 -114.394 -96.450 -60.949 1.001\rres[62] -68.590 27.512 -121.492 -87.723 -68.056 -50.063 -14.649 1.001\rres[63] -5.091 27.435 -57.658 -24.187 -4.767 13.171 48.423 1.001\rres[64] -96.888 27.399 -149.185 -115.932 -96.721 -78.893 -43.557 1.001\rres[65] -145.340 27.401 -197.553 -164.388 -145.424 -127.432 -91.411 1.001\rres[66] -26.577 27.444 -78.399 -45.694 -26.594 -8.455 27.555 1.001\rres[67] 53.432 27.525 1.678 34.253 53.095 71.854 107.546 1.001\rres[68] 91.059 27.646 39.251 71.830 90.780 109.558 145.538 1.003\rres[69] 139.341 27.805 87.022 120.095 138.966 157.961 193.921 1.001\rres[70] 192.254 28.001 139.613 173.011 191.889 211.174 247.685 1.001\rres[71] -140.697 26.945 -192.131 -158.462 -140.921 -122.682 -86.498 1.001\rres[72] -53.861 26.803 -104.692 -71.382 -53.867 -36.045 -0.026 1.001\rres[73] -59.969 26.701 -110.888 -77.531 -60.116 -42.419 -5.779 1.001\rres[74] -24.888 26.639 -75.800 -42.339 -25.220 -7.759 28.792 1.001\rres[75] -18.925 26.617 -70.569 -36.419 -19.228 -1.820 34.501 1.001\rres[76] 48.388 26.637 -3.281 30.722 48.168 65.796 102.013 1.001\rres[77] 45.403 26.697 -6.795 27.656 45.293 62.830 99.324 1.001\rres[78] 90.550 26.797 38.062 72.851 90.464 108.076 144.636 1.001\rres[79] 85.396 26.937 32.309 67.374 84.992 103.181 139.994 1.001\rres[80] 50.353 27.116 -2.874 32.360 50.071 68.275 105.635 1.001\rres[81] -163.315 27.183 -218.512 -180.651 -163.268 -145.149 -111.213 1.001\rres[82] -109.549 27.050 -164.522 -126.836 -109.407 -91.446 -57.185 1.001\rres[83] -13.182 26.957 -68.058 -30.309 -13.185 4.672 38.759 1.001\rres[84] -14.857 26.904 -69.375 -32.207 -15.130 2.965 37.149 1.001\rres[85] 40.982 26.891 -12.987 23.736 40.580 58.715 93.178 1.001\rres[86] 132.360 26.918 78.198 115.020 131.926 150.358 184.466 1.001\rres[87] 96.805 26.986 42.362 79.456 96.215 114.807 149.347 1.001\rres[88] 20.474 27.093 -33.942 3.067 19.924 38.470 72.341 1.001\rres[89] 17.776 27.240 -36.170 0.154 17.205 35.757 69.869 1.001\rres[90] 21.069 27.426 -33.512 3.375 20.566 38.921 73.507 1.001\rres[91] -139.087 27.401 -192.251 -157.857 -139.429 -120.731 -85.751 1.001\rres[92] -36.783 27.249 -89.549 -55.363 -37.012 -18.606 16.371 1.001\rres[93] -52.676 27.136 -105.357 -71.142 -52.862 -34.505 0.149 1.001\rres[94] -26.246 27.062 -78.812 -44.487 -26.694 -8.200 27.216 1.001\rres[95] -30.949 27.028 -83.569 -49.064 -31.432 -12.883 22.822 1.001\rres[96] -18.119 27.035 -70.366 -36.181 -18.630 -0.071 35.799 1.001\rres[97] 11.013 27.081 -42.062 -7.308 10.371 29.090 64.960 1.001\rres[98] 89.212 27.167 36.723 70.825 88.487 107.474 143.357 1.001\rres[99] 95.614 27.293 43.369 77.242 94.962 114.096 150.199 1.001\rres[100] 134.498 27.457 80.966 116.052 134.051 153.002 189.417 1.001\rres[101] -76.108 27.204 -130.082 -94.332 -75.847 -57.878 -24.353 1.001\rres[102] -34.250 27.032 -87.681 -52.048 -34.152 -16.151 17.341 1.001\rres[103] -71.035 26.900 -124.029 -88.589 -71.012 -53.026 -19.557 1.001\rres[104] 13.787 26.807 -38.887 -3.638 13.685 31.994 65.003 1.001\rres[105] -14.560 26.754 -67.146 -32.253 -14.734 3.512 36.383 1.001\rres[106] -19.293 26.742 -72.025 -36.855 -19.308 -1.263 32.398 1.001\rres[107] 56.600 26.770 3.888 38.981 56.413 74.590 108.690 1.001\rres[108] 116.002 26.839 62.639 98.370 115.911 133.987 168.211 1.001\rres[109] 48.507 26.948 -4.467 30.803 48.406 66.425 100.281 1.001\rres[110] 9.543 27.096 -43.482 -8.442 9.395 27.710 61.257 1.001\rres[111] -132.106 27.068 -185.510 -150.360 -132.361 -113.290 -81.768 1.002\rres[112] -91.911 26.941 -144.896 -110.112 -92.368 -73.497 -41.675 1.002\rres[113] -56.418 26.853 -110.121 -74.546 -56.821 -38.144 -6.066 1.002\rres[114] -13.873 26.805 -66.754 -32.098 -14.064 4.250 36.754 1.002\rres[115] 33.751 26.798 -19.527 15.438 33.439 51.710 84.504 1.002\rres[116] 18.335 26.831 -34.945 -0.042 17.997 36.508 69.231 1.003\rres[117] -9.402 26.904 -62.947 -27.827 -9.801 8.739 41.996 1.003\rres[118] 73.620 27.018 20.102 55.043 73.349 91.735 125.589 1.003\rres[119] 130.437 27.170 77.165 111.769 130.243 148.448 183.288 1.003\rres[120] 67.067 27.362 14.395 48.375 66.985 85.315 120.963 1.004\rres[121] -62.784 27.602 -117.124 -80.547 -63.554 -44.541 -5.745 1.002\rres[122] -79.284 27.432 -132.901 -96.959 -80.152 -61.213 -23.216 1.002\rres[123] -147.038 27.300 -200.594 -164.755 -147.987 -129.337 -91.122 1.002\rres[124] -86.011 27.208 -139.363 -103.583 -87.079 -68.166 -30.561 1.001\rres[125] -44.469 27.155 -96.901 -61.934 -45.519 -26.882 10.835 1.001\rres[126] -7.957 27.143 -60.233 -25.501 -9.007 9.627 47.109 1.001\rres[127] 41.116 27.170 -10.769 23.449 40.140 58.470 96.951 1.001\rres[128] 117.876 27.237 66.278 99.824 117.103 135.377 173.458 1.003\rres[129] 149.628 27.343 98.060 131.402 148.903 167.283 205.474 1.002\rres[130] 138.114 27.489 85.817 119.560 137.512 155.791 194.856 1.002\rres[131] -111.510 27.838 -167.229 -130.194 -111.277 -92.014 -56.583 1.002\rres[132] -85.928 27.671 -141.664 -104.693 -85.568 -66.741 -30.867 1.002\rres[133] -39.184 27.543 -95.068 -57.826 -39.100 -20.158 15.452 1.002\rres[134] -65.087 27.454 -120.528 -83.757 -65.031 -46.398 -10.806 1.002\rres[135] -38.854 27.404 -94.235 -57.313 -38.905 -20.233 15.932 1.002\rres[136] 43.897 27.394 -11.085 25.502 43.662 62.725 99.335 1.002\rres[137] 60.508 27.423 5.661 42.259 60.002 79.149 116.033 1.002\rres[138] 48.476 27.491 -6.511 30.204 47.961 67.171 103.838 1.002\rres[139] 64.161 27.599 9.521 45.658 63.389 82.927 119.918 1.002\rres[140] 150.425 27.745 95.799 131.748 149.642 169.252 206.380 1.003\rres[141] -91.850 27.881 -148.268 -110.602 -91.492 -72.233 -39.029 1.001\rres[142] -17.357 27.708 -73.303 -36.036 -17.162 1.868 35.260 1.001\rres[143] -8.456 27.572 -63.972 -27.298 -8.090 10.649 43.948 1.001\rres[144] 16.411 27.476 -38.675 -2.242 16.566 35.463 68.828 1.001\rres[145] -7.945 27.418 -62.620 -26.453 -7.775 11.028 44.009 1.001\rres[146] 34.948 27.400 -19.537 16.194 35.119 53.767 87.148 1.001\rres[147] 17.864 27.422 -36.519 -0.769 17.959 36.562 70.397 1.001\rres[148] -37.328 27.483 -92.124 -55.967 -37.389 -18.410 15.393 1.001\rres[149] 0.133 27.583 -54.930 -18.289 0.029 18.855 53.369 1.001\rres[150] 125.181 27.722 69.744 106.772 124.931 144.185 179.294 1.001\rres[151] -45.913 27.112 -101.600 -63.384 -46.504 -27.406 6.233 1.002\rres[152] 14.074 26.949 -41.422 -3.190 13.364 32.310 66.645 1.001\rres[153] 25.930 26.825 -28.692 8.617 25.258 43.842 78.335 1.001\rres[154] -8.769 26.741 -62.803 -25.915 -9.134 8.810 43.512 1.001\rres[155] -12.026 26.698 -66.536 -29.146 -12.358 5.538 40.695 1.001\rres[156] -13.996 26.695 -68.442 -31.190 -14.374 3.511 39.057 1.001\rres[157] -21.157 26.732 -75.444 -38.415 -21.645 -3.442 32.176 1.001\rres[158] 35.944 26.810 -17.938 18.462 35.409 53.573 89.820 1.001\rres[159] -9.265 26.928 -63.044 -26.958 -9.879 8.560 44.932 1.001\rres[160] 61.941 27.086 8.272 44.148 61.146 79.841 116.112 1.001\rres[161] -111.720 27.299 -164.219 -130.281 -111.941 -93.352 -57.961 1.002\rres[162] -161.840 27.142 -214.803 -180.270 -162.077 -143.321 -108.335 1.002\rres[163] -105.843 27.025 -158.096 -124.374 -106.136 -87.584 -52.496 1.002\rres[164] 32.678 26.947 -18.932 14.279 32.115 50.832 85.368 1.002\rres[165] -1.691 26.909 -53.352 -20.188 -2.310 16.517 51.266 1.002\rres[166] 45.107 26.912 -6.211 26.808 44.518 63.279 97.999 1.002\rres[167] 5.376 26.954 -46.126 -12.677 4.882 23.392 58.513 1.002\rres[168] 72.424 27.037 20.741 54.308 72.204 90.456 125.956 1.002\rres[169] 87.205 27.159 35.982 68.965 87.205 105.288 141.275 1.002\rres[170] 173.321 27.321 121.880 155.039 173.059 191.695 228.030 1.003\rres[171] -26.750 27.741 -79.583 -45.635 -26.886 -7.432 26.470 1.001\rres[172] -18.152 27.612 -70.442 -37.010 -18.270 0.993 35.150 1.001\rres[173] -30.868 27.522 -82.411 -49.842 -30.897 -11.752 22.359 1.001\rres[174] -20.371 27.471 -72.112 -39.216 -20.399 -1.090 32.975 1.001\rres[175] -29.402 27.460 -81.284 -48.575 -29.553 -9.855 23.536 1.001\rres[176] -42.654 27.488 -94.488 -61.958 -42.932 -23.151 10.118 1.001\rres[177] 21.559 27.555 -30.491 2.158 21.213 40.881 74.590 1.001\rres[178] -6.198 27.662 -58.988 -25.815 -6.244 13.036 47.593 1.001\rres[179] 104.363 27.807 51.627 84.332 104.497 123.625 158.553 1.001\rres[180] 63.487 27.989 10.897 43.415 63.657 82.927 118.256 1.002\rres[181] -116.426 27.477 -170.058 -135.254 -115.973 -98.040 -63.859 1.001\rres[182] -61.964 27.315 -115.028 -80.596 -61.367 -43.741 -8.748 1.001\rres[183] -11.520 27.191 -65.196 -29.915 -10.904 6.736 41.954 1.001\rres[184] -11.403 27.107 -64.157 -29.928 -10.938 7.033 42.438 1.001\rres[185] 11.940 27.063 -40.738 -6.618 12.599 30.143 65.828 1.001\rres[186] 78.890 27.059 26.180 60.481 79.578 97.004 132.471 1.001\rres[187] 15.749 27.094 -36.961 -2.786 16.169 33.923 68.912 1.001\rres[188] 3.160 27.170 -49.311 -15.573 3.325 21.357 56.992 1.001\rres[189] 52.382 27.285 -0.331 33.687 52.699 70.616 106.503 1.001\rres[190] 57.688 27.439 4.937 38.848 57.982 75.882 112.335 1.001\rres[191] -39.277 27.823 -93.946 -58.037 -39.479 -19.984 14.965 1.001\rres[192] 9.409 27.662 -44.962 -9.063 9.312 28.464 63.569 1.001\rres[193] 22.489 27.540 -32.351 3.890 22.479 41.387 76.748 1.001\rres[194] -41.856 27.457 -96.305 -60.387 -41.784 -23.180 12.866 1.001\rres[195] -12.452 27.413 -66.510 -31.155 -12.405 6.113 42.359 1.001\rres[196] -1.869 27.408 -55.985 -20.561 -1.913 16.685 53.271 1.001\rres[197] 21.489 27.443 -32.127 2.711 21.389 39.871 76.814 1.001\rres[198] -9.600 27.518 -62.931 -28.632 -9.617 8.338 45.590 1.002\rres[199] 53.727 27.631 -0.059 34.427 53.676 71.736 108.974 1.002\rres[200] 31.695 27.783 -22.231 12.143 31.743 49.979 87.523 1.002\rres[201] -86.890 27.181 -140.177 -104.770 -87.118 -68.603 -35.106 1.001\rres[202] -30.563 27.031 -83.642 -48.350 -31.011 -12.345 20.910 1.002\rres[203] -39.562 26.920 -92.523 -57.243 -39.987 -21.390 11.887 1.002\rres[204] 25.751 26.850 -27.313 8.074 25.467 43.881 77.877 1.002\rres[205] -12.787 26.819 -65.884 -30.405 -13.246 5.237 39.562 1.002\rres[206] -114.094 26.829 -166.864 -131.854 -114.195 -96.423 -61.166 1.002\rres[207] -27.215 26.880 -80.040 -44.971 -27.431 -9.544 26.079 1.002\rres[208] 65.253 26.970 12.034 47.409 65.333 82.874 119.333 1.002\rres[209] 87.094 27.100 33.577 69.194 87.081 104.945 141.467 1.003\rres[210] 148.253 27.269 94.421 130.305 148.113 166.284 203.466 1.002\rres[211] -161.236 27.503 -215.889 -178.736 -161.122 -142.886 -108.375 1.001\rres[212] -129.522 27.348 -183.477 -146.819 -129.294 -111.282 -76.871 1.001\rres[213] -190.682 27.231 -243.930 -208.062 -190.524 -172.562 -138.450 1.001\rres[214] -79.289 27.154 -132.322 -96.647 -79.209 -61.143 -27.250 1.001\rres[215] 47.949 27.116 -4.985 30.474 48.166 66.107 100.695 1.001\rres[216] 114.631 27.119 61.912 96.825 114.839 132.864 167.496 1.001\rres[217] 117.095 27.161 63.624 99.215 117.216 135.276 169.783 1.001\rres[218] 122.954 27.243 68.976 105.183 123.040 140.999 175.833 1.001\rres[219] 57.691 27.364 2.985 40.012 57.620 75.929 110.855 1.001\rres[220] 124.990 27.525 70.453 107.235 124.767 143.105 178.606 1.001\rres[221] -28.457 27.809 -85.147 -46.709 -28.877 -8.749 24.203 1.001\rres[222] -40.195 27.642 -96.634 -58.154 -40.465 -20.895 12.714 1.001\rres[223] -83.163 27.513 -139.662 -101.235 -83.213 -63.910 -30.005 1.001\rres[224] -57.737 27.424 -113.878 -75.793 -57.651 -38.683 -5.179 1.001\rres[225] -60.675 27.373 -116.422 -78.560 -60.376 -41.718 -8.273 1.001\rres[226] -36.237 27.363 -91.706 -54.143 -35.975 -17.495 16.397 1.001\rres[227] -53.528 27.391 -108.591 -71.787 -53.313 -34.530 -0.302 1.001\rres[228] 82.085 27.460 27.176 63.546 82.505 100.972 135.212 1.001\rres[229] 109.582 27.567 54.642 90.994 110.106 128.598 162.713 1.001\rres[230] 190.314 27.713 135.634 171.529 190.765 209.465 244.223 1.001\rres[231] -120.252 27.694 -175.928 -138.875 -119.474 -101.313 -68.003 1.001\rres[232] -100.688 27.530 -155.801 -119.226 -99.978 -81.979 -48.238 1.001\rres[233] -91.964 27.405 -146.450 -110.666 -91.280 -73.148 -39.619 1.001\rres[234] -108.171 27.319 -162.489 -126.710 -107.330 -89.436 -55.884 1.001\rres[235] 21.527 27.273 -32.755 3.074 22.292 40.459 73.543 1.001\rres[236] -18.622 27.266 -73.068 -36.988 -18.139 0.348 33.406 1.001\rres[237] 40.129 27.299 -14.569 21.865 40.523 59.257 92.027 1.001\rres[238] 117.718 27.372 62.996 99.591 118.175 136.830 169.568 1.001\rres[239] 150.564 27.484 95.949 132.308 150.955 169.833 203.577 1.001\rres[240] 128.355 27.634 73.287 109.633 128.714 147.821 181.492 1.001\rres[241] -43.717 26.592 -96.442 -61.945 -43.778 -25.366 7.706 1.002\rres[242] -16.266 26.421 -68.915 -34.345 -16.394 1.657 34.738 1.002\rres[243] -51.824 26.290 -104.250 -69.666 -51.904 -34.191 -0.773 1.002\rres[244] -92.310 26.200 -144.512 -109.770 -92.356 -74.742 -41.007 1.002\rres[245] 53.225 26.151 1.165 35.773 53.155 70.545 104.484 1.002\rres[246] 35.579 26.143 -16.717 18.099 35.633 52.862 86.869 1.001\rres[247] -33.204 26.177 -85.575 -50.833 -33.098 -15.939 18.146 1.001\rres[248] 35.397 26.252 -16.792 17.809 35.174 52.691 87.176 1.001\rres[249] 83.314 26.368 30.491 65.761 83.032 100.631 135.542 1.001\rres[250] 42.630 26.524 -9.672 24.976 42.309 59.916 95.055 1.001\rres[251] -206.923 27.800 -260.529 -225.880 -206.782 -187.968 -153.986 1.001\rres[252] -157.656 27.629 -211.249 -176.383 -157.567 -138.906 -104.887 1.001\rres[253] -51.271 27.496 -104.867 -70.030 -51.271 -32.412 1.659 1.001\rres[254] 12.860 27.402 -40.341 -5.856 13.103 31.625 65.496 1.001\rres[255] 11.898 27.348 -41.371 -7.218 11.887 30.504 64.701 1.001\rres[256] -1.747 27.333 -54.813 -20.847 -1.807 16.587 50.997 1.001\rres[257] 46.786 27.357 -6.081 27.648 46.704 65.069 100.383 1.001\rres[258] 84.114 27.422 31.253 65.110 83.938 102.405 137.813 1.001\rres[259] 141.780 27.525 88.996 122.717 141.448 159.933 195.838 1.001\rres[260] 140.825 27.667 88.214 121.349 140.766 158.831 195.301 1.001\rres[261] -45.758 27.262 -98.677 -63.456 -45.803 -28.120 8.017 1.002\rres[262] -73.536 27.103 -126.119 -91.086 -73.677 -55.880 -19.998 1.001\rres[263] -66.558 26.984 -118.944 -84.155 -66.732 -49.089 -12.830 1.001\rres[264] -3.221 26.904 -55.491 -20.926 -3.268 14.217 50.110 1.001\rres[265] 64.093 26.864 12.275 46.220 64.035 81.487 117.920 1.001\rres[266] -24.292 26.865 -75.837 -42.281 -24.457 -6.832 29.548 1.001\rres[267] 11.470 26.906 -40.398 -6.459 11.267 29.011 65.223 1.001\rres[268] 69.347 26.987 17.413 51.396 68.896 87.025 123.112 1.001\rres[269] 16.854 27.108 -35.179 -1.244 16.533 34.478 70.967 1.001\rres[270] 75.619 27.268 23.067 57.629 75.329 93.227 129.655 1.001\rres[271] -225.643 27.480 -278.886 -243.823 -225.694 -207.386 -171.764 1.003\rres[272] -140.428 27.320 -193.206 -158.567 -140.609 -122.309 -86.874 1.003\rres[273] -92.158 27.198 -145.009 -110.346 -92.252 -74.095 -38.573 1.003\rres[274] -37.826 27.116 -90.730 -55.873 -38.139 -19.735 15.494 1.002\rres[275] -22.591 27.074 -75.243 -40.456 -22.982 -4.368 30.621 1.002\rres[276] 30.930 27.072 -22.094 13.172 30.419 49.127 84.548 1.002\rres[277] 103.777 27.109 50.599 85.820 102.988 121.948 157.742 1.002\rres[278] 195.528 27.187 142.165 177.513 194.832 213.701 249.967 1.002\rres[279] 173.383 27.304 119.414 155.128 172.627 191.796 228.591 1.001\rres[280] 48.822 27.460 -5.443 30.590 47.952 67.271 104.943 1.001\rres[281] -100.761 27.924 -155.664 -119.211 -100.098 -81.104 -47.734 1.001\rres[282] -73.020 27.777 -127.369 -91.266 -72.520 -53.694 -20.563 1.001\rres[283] -64.686 27.669 -119.132 -82.823 -64.075 -45.417 -12.494 1.001\rres[284] -17.650 27.599 -72.206 -35.666 -17.169 1.377 35.020 1.001\rres[285] 46.285 27.569 -7.652 28.252 46.815 65.328 99.260 1.001\rres[286] 25.187 27.577 -28.572 7.101 25.553 44.083 78.569 1.001\rres[287] 23.689 27.625 -29.512 5.427 23.865 42.895 77.192 1.001\rres[288] 61.274 27.713 8.220 42.691 61.404 80.495 114.966 1.001\rres[289] 48.126 27.838 -5.127 29.210 48.155 67.369 102.003 1.001\rres[290] 73.868 28.002 20.444 54.993 73.670 93.109 128.079 1.001\rres[291] -62.043 27.792 -115.822 -80.494 -62.779 -43.673 -4.435 1.001\rres[292] -78.060 27.657 -131.681 -96.527 -78.957 -60.027 -21.083 1.001\rres[293] -97.080 27.560 -150.130 -115.697 -97.882 -79.080 -40.450 1.001\rres[294] -84.403 27.502 -137.305 -103.193 -85.127 -66.419 -27.873 1.001\rres[295] -62.058 27.484 -115.409 -80.974 -62.634 -43.885 -5.395 1.001\rres[296] 13.785 27.505 -39.891 -5.375 13.338 32.209 69.885 1.001\rres[297] 108.636 27.565 55.243 89.343 108.404 127.130 164.689 1.001\rres[298] 103.229 27.665 49.562 83.789 102.942 122.033 159.137 1.001\rres[299] 89.617 27.803 35.620 70.171 89.485 108.312 146.507 1.001\rres[300] 80.672 27.979 26.414 60.959 80.808 99.562 137.744 1.001\rres[301] -141.883 28.305 -196.509 -160.420 -141.920 -123.442 -84.721 1.001\rres[302] -106.210 28.157 -160.120 -124.453 -106.309 -87.823 -49.394 1.001\rres[303] -37.352 28.047 -91.475 -55.335 -37.285 -19.229 19.895 1.001\rres[304] -18.327 27.975 -72.662 -36.286 -18.237 -0.397 38.713 1.001\rres[305] -86.778 27.942 -140.890 -105.114 -86.791 -68.788 -29.209 1.001\rres[306] -37.999 27.947 -91.849 -56.327 -37.867 -20.131 19.461 1.001\rres[307] 69.141 27.991 14.641 50.665 69.178 86.859 126.516 1.001\rres[308] 159.261 28.074 104.599 140.704 159.387 177.180 216.359 1.001\rres[309] 136.516 28.195 80.873 118.244 136.554 154.819 193.892 1.001\rres[310] 84.565 28.354 28.496 66.131 84.693 102.951 142.719 1.002\rres[311] 8.480 27.336 -47.030 -9.792 8.978 27.073 59.498 1.003\rres[312] 11.977 27.128 -43.062 -5.921 12.427 30.119 62.594 1.003\rres[313] 22.210 26.959 -32.858 4.243 22.751 40.219 72.580 1.003\rres[314] 60.484 26.829 5.199 42.716 60.942 78.598 110.330 1.002\rres[315] -6.440 26.739 -61.507 -24.246 -6.091 11.508 43.556 1.002\rres[316] -11.502 26.689 -66.443 -29.364 -11.033 6.418 38.397 1.002\rres[317] -51.977 26.680 -107.165 -69.981 -51.390 -33.782 -1.318 1.002\rres[318] -37.802 26.711 -93.392 -55.747 -37.309 -19.574 12.624 1.002\rres[319] 18.322 26.783 -37.480 0.309 19.012 36.680 68.620 1.001\rres[320] -6.770 26.895 -62.754 -24.603 -5.931 11.437 44.809 1.001\rres[321] -169.878 27.641 -225.412 -188.557 -170.087 -150.812 -115.846 1.003\rres[322] -115.245 27.475 -170.155 -133.481 -115.610 -96.509 -61.338 1.002\rres[323] -127.030 27.348 -181.991 -145.028 -127.405 -108.460 -73.235 1.002\rres[324] -75.745 27.260 -130.493 -94.150 -76.056 -57.347 -21.877 1.002\rres[325] -4.442 27.211 -59.066 -22.893 -4.731 14.091 49.373 1.002\rres[326] 72.438 27.202 17.923 54.115 72.083 90.962 125.624 1.002\rres[327] 74.431 27.233 20.320 56.096 74.054 93.130 128.264 1.001\rres[328] 86.173 27.303 32.254 68.056 86.103 104.893 140.473 1.001\rres[329] 100.100 27.413 46.110 82.027 100.161 118.754 154.311 1.001\rres[330] 191.836 27.562 137.087 173.612 191.900 210.527 246.650 1.001\rres[331] -3.568 27.677 -58.111 -22.218 -3.240 14.872 51.330 1.001\rres[332] 35.490 27.505 -18.504 16.700 35.467 53.896 89.971 1.001\rres[333] 14.836 27.372 -38.705 -3.945 14.883 33.326 69.479 1.001\rres[334] 35.529 27.278 -17.795 16.742 35.624 53.845 89.880 1.001\rres[335] 57.772 27.224 4.745 39.406 57.712 76.032 112.258 1.001\rres[336] 7.505 27.209 -45.201 -11.277 7.241 25.550 61.787 1.001\rres[337] -62.853 27.234 -115.231 -81.656 -63.102 -44.830 -8.470 1.001\rres[338] 3.554 27.299 -48.771 -15.196 3.261 21.590 57.887 1.001\rres[339] -47.691 27.403 -100.181 -66.462 -48.175 -29.624 6.870 1.001\rres[340] -7.951 27.546 -60.486 -26.990 -8.417 10.133 47.240 1.001\rres[341] -167.838 27.534 -221.260 -186.767 -167.997 -149.518 -113.530 1.005\rres[342] -109.245 27.409 -162.241 -128.030 -109.426 -91.138 -55.319 1.005\rres[343] -76.905 27.323 -129.335 -95.553 -76.780 -58.727 -23.086 1.004\rres[344] -92.961 27.276 -144.845 -111.585 -92.688 -74.742 -39.601 1.004\rres[345] -15.658 27.269 -67.414 -34.298 -15.410 2.716 37.316 1.004\rres[346] 87.984 27.302 36.319 69.290 88.171 106.278 140.902 1.003\rres[347] 132.212 27.374 80.204 113.503 132.167 150.380 185.832 1.003\rres[348] 30.540 27.486 -21.770 11.869 30.461 48.883 84.590 1.003\rres[349] 120.550 27.636 67.565 101.960 120.450 138.866 174.815 1.002\rres[350] 123.963 27.825 70.604 105.055 123.691 142.633 178.437 1.002\rsigma 86.053 4.095 78.239 83.300 85.994 88.737 94.268 1.013\rsigma.B 317.817 38.958 252.469 291.832 313.873 339.888 405.125 1.002\rdeviance 4111.820 21.476 4069.687 4097.266 4111.765 4126.251 4152.352 1.008\rn.eff\rbeta[1] 1400\rbeta[2] 230\rgamma[1] 1000\rgamma[2] 3000\rgamma[3] 1300\rgamma[4] 2800\rgamma[5] 3000\rgamma[6] 3000\rgamma[7] 1800\rgamma[8] 3000\rgamma[9] 1800\rgamma[10] 2900\rgamma[11] 3000\rgamma[12] 1800\rgamma[13] 1900\rgamma[14] 3000\rgamma[15] 3000\rgamma[16] 1300\rgamma[17] 1100\rgamma[18] 3000\rgamma[19] 3000\rgamma[20] 3000\rgamma[21] 3000\rgamma[22] 3000\rgamma[23] 1800\rgamma[24] 3000\rgamma[25] 810\rgamma[26] 3000\rgamma[27] 1500\rgamma[28] 570\rgamma[29] 3000\rgamma[30] 3000\rgamma[31] 3000\rgamma[32] 660\rgamma[33] 730\rgamma[34] 3000\rgamma[35] 340\rres[1] 1100\rres[2] 1200\rres[3] 1400\rres[4] 1700\rres[5] 2100\rres[6] 2700\rres[7] 3000\rres[8] 3000\rres[9] 3000\rres[10] 3000\rres[11] 3000\rres[12] 3000\rres[13] 3000\rres[14] 3000\rres[15] 3000\rres[16] 3000\rres[17] 3000\rres[18] 3000\rres[19] 3000\rres[20] 3000\rres[21] 1200\rres[22] 1400\rres[23] 1700\rres[24] 2000\rres[25] 2500\rres[26] 3000\rres[27] 3000\rres[28] 3000\rres[29] 3000\rres[30] 3000\rres[31] 3000\rres[32] 3000\rres[33] 3000\rres[34] 3000\rres[35] 3000\rres[36] 3000\rres[37] 3000\rres[38] 3000\rres[39] 3000\rres[40] 3000\rres[41] 3000\rres[42] 3000\rres[43] 3000\rres[44] 3000\rres[45] 3000\rres[46] 3000\rres[47] 3000\rres[48] 3000\rres[49] 3000\rres[50] 3000\rres[51] 3000\rres[52] 2800\rres[53] 2200\rres[54] 1700\rres[55] 1400\rres[56] 1200\rres[57] 1000\rres[58] 710\rres[59] 630\rres[60] 550\rres[61] 2000\rres[62] 2400\rres[63] 3000\rres[64] 3000\rres[65] 3000\rres[66] 3000\rres[67] 3000\rres[68] 3000\rres[69] 3000\rres[70] 3000\rres[71] 3000\rres[72] 3000\rres[73] 3000\rres[74] 3000\rres[75] 3000\rres[76] 3000\rres[77] 3000\rres[78] 3000\rres[79] 3000\rres[80] 3000\rres[81] 2100\rres[82] 2600\rres[83] 3000\rres[84] 3000\rres[85] 3000\rres[86] 3000\rres[87] 3000\rres[88] 3000\rres[89] 3000\rres[90] 3000\rres[91] 3000\rres[92] 3000\rres[93] 3000\rres[94] 3000\rres[95] 3000\rres[96] 3000\rres[97] 3000\rres[98] 3000\rres[99] 3000\rres[100] 3000\rres[101] 3000\rres[102] 3000\rres[103] 3000\rres[104] 3000\rres[105] 3000\rres[106] 3000\rres[107] 3000\rres[108] 3000\rres[109] 3000\rres[110] 3000\rres[111] 1700\rres[112] 1400\rres[113] 1100\rres[114] 980\rres[115] 840\rres[116] 740\rres[117] 650\rres[118] 580\rres[119] 600\rres[120] 480\rres[121] 1600\rres[122] 2000\rres[123] 2400\rres[124] 3000\rres[125] 3000\rres[126] 3000\rres[127] 3000\rres[128] 3000\rres[129] 3000\rres[130] 3000\rres[131] 3000\rres[132] 3000\rres[133] 3000\rres[134] 3000\rres[135] 3000\rres[136] 3000\rres[137] 3000\rres[138] 3000\rres[139] 2700\rres[140] 3000\rres[141] 3000\rres[142] 3000\rres[143] 3000\rres[144] 3000\rres[145] 3000\rres[146] 3000\rres[147] 3000\rres[148] 3000\rres[149] 3000\rres[150] 3000\rres[151] 1600\rres[152] 1900\rres[153] 2300\rres[154] 3000\rres[155] 3000\rres[156] 3000\rres[157] 3000\rres[158] 3000\rres[159] 3000\rres[160] 3000\rres[161] 1000\rres[162] 1200\rres[163] 1400\rres[164] 1700\rres[165] 2100\rres[166] 2600\rres[167] 3000\rres[168] 3000\rres[169] 3000\rres[170] 3000\rres[171] 3000\rres[172] 3000\rres[173] 3000\rres[174] 3000\rres[175] 3000\rres[176] 3000\rres[177] 2700\rres[178] 2200\rres[179] 2000\rres[180] 1500\rres[181] 3000\rres[182] 3000\rres[183] 3000\rres[184] 3000\rres[185] 3000\rres[186] 3000\rres[187] 3000\rres[188] 3000\rres[189] 3000\rres[190] 3000\rres[191] 3000\rres[192] 3000\rres[193] 3000\rres[194] 3000\rres[195] 3000\rres[196] 2500\rres[197] 2000\rres[198] 1700\rres[199] 1400\rres[200] 1200\rres[201] 3000\rres[202] 3000\rres[203] 2700\rres[204] 2100\rres[205] 1700\rres[206] 1400\rres[207] 1200\rres[208] 1000\rres[209] 890\rres[210] 1100\rres[211] 3000\rres[212] 3000\rres[213] 3000\rres[214] 3000\rres[215] 3000\rres[216] 3000\rres[217] 3000\rres[218] 3000\rres[219] 3000\rres[220] 3000\rres[221] 1800\rres[222] 2200\rres[223] 2800\rres[224] 3000\rres[225] 3000\rres[226] 3000\rres[227] 3000\rres[228] 3000\rres[229] 3000\rres[230] 3000\rres[231] 3000\rres[232] 3000\rres[233] 3000\rres[234] 3000\rres[235] 3000\rres[236] 3000\rres[237] 3000\rres[238] 3000\rres[239] 3000\rres[240] 3000\rres[241] 900\rres[242] 1000\rres[243] 1200\rres[244] 1400\rres[245] 1800\rres[246] 2200\rres[247] 2800\rres[248] 3000\rres[249] 3000\rres[250] 3000\rres[251] 3000\rres[252] 3000\rres[253] 3000\rres[254] 3000\rres[255] 3000\rres[256] 3000\rres[257] 3000\rres[258] 3000\rres[259] 2600\rres[260] 2100\rres[261] 1600\rres[262] 1900\rres[263] 2400\rres[264] 3000\rres[265] 3000\rres[266] 3000\rres[267] 3000\rres[268] 3000\rres[269] 3000\rres[270] 3000\rres[271] 590\rres[272] 650\rres[273] 730\rres[274] 830\rres[275] 960\rres[276] 1100\rres[277] 1200\rres[278] 1500\rres[279] 1800\rres[280] 2500\rres[281] 3000\rres[282] 3000\rres[283] 3000\rres[284] 3000\rres[285] 3000\rres[286] 3000\rres[287] 3000\rres[288] 3000\rres[289] 3000\rres[290] 2800\rres[291] 3000\rres[292] 3000\rres[293] 3000\rres[294] 3000\rres[295] 3000\rres[296] 3000\rres[297] 3000\rres[298] 3000\rres[299] 3000\rres[300] 3000\rres[301] 3000\rres[302] 3000\rres[303] 3000\rres[304] 3000\rres[305] 3000\rres[306] 3000\rres[307] 3000\rres[308] 2700\rres[309] 2300\rres[310] 1700\rres[311] 610\rres[312] 670\rres[313] 760\rres[314] 860\rres[315] 990\rres[316] 1200\rres[317] 1400\rres[318] 1700\rres[319] 2100\rres[320] 2600\rres[321] 770\rres[322] 870\rres[323] 1000\rres[324] 1200\rres[325] 1400\rres[326] 1600\rres[327] 2000\rres[328] 2600\rres[329] 2700\rres[330] 3000\rres[331] 3000\rres[332] 3000\rres[333] 3000\rres[334] 3000\rres[335] 3000\rres[336] 3000\rres[337] 3000\rres[338] 3000\rres[339] 3000\rres[340] 3000\rres[341] 350\rres[342] 380\rres[343] 420\rres[344] 460\rres[345] 510\rres[346] 570\rres[347] 810\rres[348] 740\rres[349] 1200\rres[350] 1500\rsigma 130\rsigma.B 3000\rdeviance 200\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 229.5 and DIC = 4341.4\rDIC is an estimate of expected predictive error (lower deviance is better).\rGiven that Time cannot be randomized, there is likely to be a temporal dependency structure to the data. The above analyses assume no temporal dependency - actually, they assume that the variance-covariance matrix demonstrates a structure known as sphericity. Lets specifically model in a first order autoregressive correlation structure in an attempt to accommodate the expected temporal autocorrelation.\n\u0026gt; modelString3=\u0026quot;\r+ model {\r+ #Likelihood\r+ y[1]~dnorm(mu[1],tau)\r+ mu[1] \u0026lt;- eta1[1]\r+ eta1[1] ~ dnorm(eta[1], taueps)\r+ eta[1] \u0026lt;- inprod(beta[],X[1,]) + gamma[Block[1]]\r+ res[1] \u0026lt;- y[1]-mu[1]\r+ for (i in 2:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- eta1[i]\r+ eta1[i] ~ dnorm(temp[i], taueps)\r+ temp[i] \u0026lt;- eta[i] + -rho*(mu[i-1]-y[i-1])\r+ eta[i] \u0026lt;- inprod(beta[],X[i,]) + gamma[Block[i]]\r+ res[i] \u0026lt;- y[i]-mu[i]\r+ } + beta ~ dmnorm(a0,A0)\r+ for (i in 1:nBlock) {\r+ gamma[i] ~ dnorm(0, tau.B) #prior\r+ }\r+ rho ~ dunif(-1,1)\r+ tau \u0026lt;- pow(sigma,-2)\r+ sigma \u0026lt;- z/sqrt(chSq) + z ~ dnorm(0, 0.0016)I(0,) #1/25^2 = 0.0016\r+ chSq ~ dgamma(0.5, 0.5)\r+ taueps \u0026lt;- pow(sigma.eps,-2)\r+ sigma.eps \u0026lt;- z/sqrt(chSq.eps) + z.eps ~ dnorm(0, 0.0016)I(0,) #1/25^2 = 0.0016\r+ chSq.eps ~ dgamma(0.5, 0.5)\r+ tau.B \u0026lt;- pow(sigma.B,-2)\r+ sigma.B \u0026lt;- z/sqrt(chSq.B) + z.B ~ dnorm(0, 0.0016)I(0,) #1/25^2 = 0.0016\r+ chSq.B ~ dgamma(0.5, 0.5)\r+ sd.y \u0026lt;- sd(res)\r+ sd.block \u0026lt;- sd(gamma)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString3, con = \u0026quot;matrixModel3.txt\u0026quot;)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~Time,data.rm)\r\u0026gt; data.rm.list \u0026lt;- with(data.rm,\r+ list(y=y,\r+ Block=as.numeric(Block),\r+ X=Xmat,\r+ n=nrow(data.rm),\r+ nBlock=length(levels(Block)),\r+ a0=rep(0,ncol(Xmat)), A0=diag(ncol(Xmat))\r+ )\r+ )\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026#39;gamma\u0026#39;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma.B\u0026quot;,\u0026quot;res\u0026quot;,\u0026#39;sigma.eps\u0026#39;,\u0026#39;rho\u0026#39;,\u0026#39;sd.y\u0026#39;,\u0026#39;sd.block\u0026#39;)\r\u0026gt; adaptSteps = 1000\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; data.rm.r2jags.mt \u0026lt;- jags(data = data.rm.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;matrixModel3.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 350\rUnobserved stochastic nodes: 393\rTotal graph size: 3931\rInitializing model\r\u0026gt; \u0026gt; data.rm.mt.mcmc \u0026lt;- data.rm.r2jags.mt$BUGSoutput$sims.matrix\r\u0026gt; summary(as.mcmc(data.rm.mt.mcmc[,grep(\u0026#39;beta|sigma|rho\u0026#39;,colnames(data.rm.mt.mcmc))]))\rIterations = 1:3000\rThinning interval = 1 Number of chains = 1 Sample size per chain = 3000 1. Empirical mean and standard deviation for each variable,\rplus standard error of the mean:\rMean SD Naive SE Time-series SE\rbeta[1] 0.1093 1.0090 0.018422 0.018422\rbeta[2] 9.1917 1.0421 0.019026 0.019026\rrho 0.6991 0.1635 0.002985 0.002985\rsigma 63.1901 7.6813 0.140241 0.140241\rsigma.B 314.5779 38.4731 0.702419 0.702419\rsigma.eps 32.8263 9.4551 0.172625 0.172625\r2. Quantiles for each variable:\r2.5% 25% 50% 75% 97.5%\rbeta[1] -1.8402 -0.5698 0.09792 0.7920 2.0576\rbeta[2] 7.2315 8.4966 9.19938 9.8961 11.2232\rrho 0.4549 0.5508 0.68313 0.8453 0.9746\rsigma 49.8811 56.9814 62.70134 69.5262 77.4328\rsigma.B 251.2601 286.7395 310.40901 338.2187 398.3167\rsigma.eps 14.8598 25.5907 35.65555 40.2488 46.3558\r\u0026gt; \u0026gt; #head(data.rm.r2jags.mt$BUGSoutput$sims.list[[c(\u0026#39;beta\u0026#39;,\u0026#39;rho\u0026#39;,\u0026#39;sigma\u0026#39;)]]) \u0026gt; #print(data.rm.r2jags.mt)\r\u0026gt; data.rm.mcmc.list.mt \u0026lt;- as.mcmc(data.rm.r2jags.mt)\r\u0026gt; Data.Rm.mcmc.list.mt \u0026lt;- data.rm.mcmc.list.mt\r\u0026gt; \u0026gt; # R2 calculations\r\u0026gt; Xmat \u0026lt;- model.matrix(~Time, data.rm)\r\u0026gt; coefs \u0026lt;- data.rm.r2jags.mt$BUGSoutput$sims.list[[\u0026#39;beta\u0026#39;]]\r\u0026gt; fitted \u0026lt;- coefs %*% t(Xmat)\r\u0026gt; X.var \u0026lt;- aaply(fitted,1,function(x){var(x)})\r\u0026gt; X.var[1:10]\r1 2 3 4 5 6 7 8 814.3418 591.0181 634.1438 685.5362 900.1883 740.2397 864.5962 435.7952 9 10 672.4743 584.2064 \u0026gt; \u0026gt; Z.var \u0026lt;- data.rm.r2jags.mt$BUGSoutput$sims.list[[\u0026#39;sd.block\u0026#39;]]^2\r\u0026gt; R.var \u0026lt;- data.rm.r2jags.mt$BUGSoutput$sims.list[[\u0026#39;sd.y\u0026#39;]]^2\r\u0026gt; R2.marginal \u0026lt;- (X.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.marginal \u0026lt;- data.frame(Mean=mean(R2.marginal), Median=median(R2.marginal), HPDinterval(as.mcmc(R2.marginal)))\r\u0026gt; R2.conditional \u0026lt;- (X.var+Z.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.conditional \u0026lt;- data.frame(Mean=mean(R2.conditional),\r+ Median=median(R2.conditional), HPDinterval(as.mcmc(R2.conditional)))\r\u0026gt; R2.block \u0026lt;- (Z.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.block \u0026lt;- data.frame(Mean=mean(R2.block), Median=median(R2.block), HPDinterval(as.mcmc(R2.block)))\r\u0026gt; R2.res\u0026lt;-(R.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.res \u0026lt;- data.frame(Mean=mean(R2.res), Median=median(R2.res), HPDinterval(as.mcmc(R2.res)))\r\u0026gt; \u0026gt; (r2 \u0026lt;- rbind(R2.block=R2.block, R2.marginal=R2.marginal, R2.res=R2.res, R2.conditional=R2.conditional))\rMean Median lower upper\rR2.block 0.52595295 0.52197768 0.40289527 0.6376026\rR2.marginal 0.07190426 0.06983026 0.03887103 0.1087763\rR2.res 0.40214279 0.40351594 0.28261806 0.5200679\rR2.conditional 0.59785721 0.59648406 0.47993214 0.7173819\rIt would appear that the incorporation of a first order autocorrelation structure is indeed appropriate. The degree of correlation between successive points is \\(0.733\\). Let’s have a look at a summary figure.\n\u0026gt; coefs \u0026lt;- data.rm.r2jags.mt$BUGSoutput$sims.list[[\u0026#39;beta\u0026#39;]]\r\u0026gt; newdata \u0026lt;- with(data.rm, data.frame(Time=seq(min(Time, na.rm=TRUE), max(Time, na.rm=TRUE), len=100)))\r\u0026gt; Xmat \u0026lt;- model.matrix(~Time, newdata)\r\u0026gt; pred \u0026lt;- (coefs %*% t(Xmat))\r\u0026gt; pred \u0026lt;- adply(pred, 2, function(x) {\r+ data.frame(Mean=mean(x), Median=median(x, na.rm=TRUE), t(quantile(x,na.rm=TRUE)),\r+ HPDinterval(as.mcmc(x)),HPDinterval(as.mcmc(x),p=0.5))\r+ })\r\u0026gt; newdata \u0026lt;- cbind(newdata, pred)\r\u0026gt; #Also calculate the partial observations\r\u0026gt; Xmat \u0026lt;- model.matrix(~Time, data.rm)\r\u0026gt; pred \u0026lt;- colMeans(as.vector(coefs %*% t(Xmat))+data.rm.r2jags.mt$BUGSoutput$sims.list[[\u0026#39;res\u0026#39;]])\r\u0026gt; part.obs \u0026lt;- cbind(data.rm,Median=pred)\r\u0026gt; \u0026gt; ggplot(newdata, aes(y=Median, x=Time)) +\r+ geom_point(data=part.obs, aes(y=Median))+\r+ geom_ribbon(aes(ymin=lower, ymax=upper), fill=\u0026#39;blue\u0026#39;,alpha=0.2) +\r+ geom_line()+\r+ scale_x_continuous(\u0026#39;Time\u0026#39;) +\r+ scale_y_continuous(\u0026#39;Y\u0026#39;) +\r+ theme_classic() +\r+ theme(axis.title.y = element_text(vjust=2, size=rel(1.2)),\r+ axis.title.x = element_text(vjust=-2, size=rel(1.2)),\r+ plot.margin=unit(c(0.5,0.5,2,2), \u0026#39;lines\u0026#39;))\r\r\rReferences\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1581387194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581387194,"objectID":"8671386bc83d41e93c5b7d21cbed4a70","permalink":"/jags/block-anova-jags/block-anova-jags/","publishdate":"2020-02-10T21:13:14-05:00","relpermalink":"/jags/block-anova-jags/block-anova-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","randomised complete block","anova"],"title":"Randomised Complete Block Anova - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","anova","STAN","randomised complete block"],"content":"\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rIn the previous tutorial (nested ANOVA), we introduced the concept of employing sub-replicates that are nested within the main treatment levels as a means of absorbing some of the unexplained variability that would otherwise arise from designs in which sampling units are selected from amongst highly heterogeneous conditions. Such (nested) designs are useful in circumstances where the levels of the main treatment (such as burnt and un-burnt sites) occur at a much larger temporal or spatial scale than the experimental/sampling units (e.g. vegetation monitoring quadrats). For circumstances in which the main treatments can be applied (or naturally occur) at the same scale as the sampling units (such as whether a stream rock is enclosed by a fish proof fence or not), an alternative design is available. In this design (randomised complete block design), each of the levels of the main treatment factor are grouped (blocked) together (in space and/or time) and therefore, whilst the conditions between the groups (referred to as “blocks”) might vary substantially, the conditions under which each of the levels of the treatment are tested within any given block are far more homogeneous.\nIf any differences between blocks (due to the heterogeneity) can account for some of the total variability between the sampling units (thereby reducing the amount of variability that the main treatment(s) failed to explain), then the main test of treatment effects will be more powerful/sensitive. As an simple example of a randomised complete block (RCB) design, consider an investigation into the roles of different organism scales (microbial, macro invertebrate and vertebrate) on the breakdown of leaf debris packs within streams. An experiment could consist of four treatment levels - leaf packs protected by fish-proof mesh, leaf packs protected by fine macro invertebrate exclusion mesh, leaf packs protected by dissolving antibacterial tablets, and leaf packs relatively unprotected as controls. As an acknowledgement that there are many other unmeasured factors that could influence leaf pack breakdown (such as flow velocity, light levels, etc) and that these are likely to vary substantially throughout a stream, the treatments are to be arranged into groups or “blocks” (each containing a single control, microbial, macro invertebrate and fish protected leaf pack). Blocks of treatment sets are then secured in locations haphazardly selected throughout a particular reach of stream. Importantly, the arrangement of treatments in each block must be randomized to prevent the introduction of some systematic bias - such as light angle, current direction etc.\nBlocking does however come at a cost. The blocks absorb both unexplained variability as well as degrees of freedom from the residuals. Consequently, if the amount of the total unexplained variation that is absorbed by the blocks is not sufficiently large enough to offset the reduction in degrees of freedom (which may result from either less than expected heterogeneity, or due to the scale at which the blocks are established being inappropriate to explain much of the variation), for a given number of sampling units (leaf packs), the tests of main treatment effects will suffer power reductions. Treatments can also be applied sequentially or repeatedly at the scale of the entire block, such that at any single time, only a single treatment level is being applied (see the lower two sub-figures above). Such designs are called repeated measures. A repeated measures ANOVA is to an single factor ANOVA as a paired t-test is to a independent samples t-test. One example of a repeated measures analysis might be an investigation into the effects of a five different diet drugs (four doses and a placebo) on the food intake of lab rats. Each of the rats (“subjects”) is subject to each of the four drugs (within subject effects) which are administered in a random order. In another example, temporal recovery responses of sharks to bi-catch entanglement stresses might be simulated by analyzing blood samples collected from captive sharks (subjects) every half hour for three hours following a stress inducing restraint. This repeated measures design allows the anticipated variability in stress tolerances between individual sharks to be accounted for in the analysis (so as to permit more powerful test of the main treatments). Furthermore, by performing repeated measures on the same subjects, repeated measures designs reduce the number of subjects required for the investigation. Essentially, this is a randomised complete block design except that the within subject (block) effect (e.g. time since stress exposure) cannot be randomised.\nTo suppress contamination effects resulting from the proximity of treatment sampling units within a block, units should be adequately spaced in time and space. For example, the leaf packs should not be so close to one another that the control packs are effected by the antibacterial tablets and there should be sufficient recovery time between subsequent drug administrations. In addition, the order or arrangement of treatments within the blocks must be randomized so as to prevent both confounding as well as computational complications. Whilst this is relatively straight forward for the classic randomized complete block design (such as the leaf packs in streams), it is logically not possible for repeated measures designs. Blocking factors are typically random factors that represent all the possible blocks that could be selected. As such, no individual block can truly be replicated. Randomised complete block and repeated measures designs can therefore also be thought of as un-replicated factorial designs in which there are two or more factors but that the interactions between the blocks and all the within block factors are not replicated.\n\rLinear models\rThe linear models for two and three factor nested design are:\n\\[ y_{ij} = \\mu + \\beta_i + \\alpha_j + \\epsilon_{ij},\\]\n\\[ y_{ijk} = \\mu + \\beta_i + \\alpha_j + \\gamma_k + (\\beta\\alpha)_{ij} + (\\beta\\gamma)_{ik} + (\\alpha\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 1)}\\]\n\\[ y_{ijk} = \\mu + \\beta_i + \\alpha_j + \\gamma_k + (\\alpha\\gamma)_{jk} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 2)},\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\beta\\) is the effect of the Blocking Factor B (\\(\\sum \\beta=0\\)), \\(\\alpha\\) and \\(\\gamma\\) are the effects of withing block Factor A and Factor C, respectively, and \\(\\epsilon \\sim N(0,\\sigma^2)\\) is the random unexplained or residual component.\nTests for the effects of blocks as well as effects within blocks assume that there are no interactions between blocks and the within block effects. That is, it is assumed that any effects are of similar nature within each of the blocks. Whilst this assumption may well hold for experiments that are able to consciously set the scale over which the blocking units are arranged, when designs utilize arbitrary or naturally occurring blocking units, the magnitude and even polarity of the main effects are likely to vary substantially between the blocks. The preferred (non-additive or “Model 1”) approach to un-replicated factorial analysis of some bio-statisticians is to include the block by within subject effect interactions (e.g. \\(\\beta\\alpha\\)). Whilst these interaction effects cannot be formally tested, they can be used as the denominators in F-ratio calculations of their respective main effects tests. Proponents argue that since these blocking interactions cannot be formally tested, there is no sound inferential basis for using these error terms separately. Alternatively, models can be fitted additively (“Model 2”) whereby all the block by within subject effect interactions are pooled into a single residual term (\\(\\epsilon\\)). Although the latter approach is simpler, each of the within subject effects tests do assume that there are no interactions involving the blocks and that perhaps even more restrictively, that sphericity holds across the entire design.\n\rAssumptions\rAs with other ANOVA designs, the reliability of hypothesis tests is dependent on the residuals being:\n\rnormally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator should be used to explore normality. Scale transformations are often useful.\n\requally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\n\rindependent of one another. Although the observations within a block may not strictly be independent, provided the treatments are applied or ordered randomly within each block or subject, within block proximity effects on the residuals should be random across all blocks and thus the residuals should still be independent of one another. Nevertheless, it is important that experimental units within blocks are adequately spaced in space and time so as to suppress contamination or carryover effects.\n\r\r\r\rSimple RCB\rData generation\rImagine we has designed an experiment in which we intend to measure a response (y) to one of treatments (three levels; “a1”, “a2” and “a3”). Unfortunately, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability you decide to apply a design (RCB) in which each of the treatments within each of 35 blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; library(plyr)\r\u0026gt; set.seed(123)\r\u0026gt; nTreat \u0026lt;- 3\r\u0026gt; nBlock \u0026lt;- 35\r\u0026gt; sigma \u0026lt;- 5\r\u0026gt; sigma.block \u0026lt;- 12\r\u0026gt; n \u0026lt;- nBlock*nTreat\r\u0026gt; Block \u0026lt;- gl(nBlock, k=1)\r\u0026gt; A \u0026lt;- gl(nTreat,k=1)\r\u0026gt; dt \u0026lt;- expand.grid(A=A,Block=Block)\r\u0026gt; #Xmat \u0026lt;- model.matrix(~Block + A + Block:A, data=dt)\r\u0026gt; Xmat \u0026lt;- model.matrix(~-1+Block + A, data=dt)\r\u0026gt; block.effects \u0026lt;- rnorm(n = nBlock, mean = 40, sd = sigma.block)\r\u0026gt; A.effects \u0026lt;- c(30,40)\r\u0026gt; all.effects \u0026lt;- c(block.effects,A.effects)\r\u0026gt; lin.pred \u0026lt;- Xmat %*% all.effects\r\u0026gt; \u0026gt; # OR\r\u0026gt; Xmat \u0026lt;- cbind(model.matrix(~-1+Block,data=dt),model.matrix(~-1+A,data=dt))\r\u0026gt; ## Sum to zero block effects\r\u0026gt; block.effects \u0026lt;- rnorm(n = nBlock, mean = 0, sd = sigma.block)\r\u0026gt; A.effects \u0026lt;- c(40,70,80)\r\u0026gt; all.effects \u0026lt;- c(block.effects,A.effects)\r\u0026gt; lin.pred \u0026lt;- Xmat %*% all.effects\r\u0026gt; \u0026gt; \u0026gt; \u0026gt; ## the quadrat observations (within sites) are drawn from\r\u0026gt; ## normal distributions with means according to the site means\r\u0026gt; ## and standard deviations of 5\r\u0026gt; y \u0026lt;- rnorm(n,lin.pred,sigma)\r\u0026gt; data.rcb \u0026lt;- data.frame(y=y, expand.grid(A=A, Block=Block))\r\u0026gt; head(data.rcb) #print out the first six rows of the data set\ry A Block\r1 45.80853 1 1\r2 66.71784 2 1\r3 93.29238 3 1\r4 43.10101 1 2\r5 73.20697 2 2\r6 91.77487 3 2\r\rExploratory data analysis\rNormality and Homogeneity of variance\n\u0026gt; boxplot(y~A, data.rcb)\rConclusions:\n\rthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\n\rthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. . More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\n\r\rObvious violations could be addressed either by:\n\rtransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\r\rBlock by within-Block interaction\n\u0026gt; library(car)\r\u0026gt; with(data.rcb, interaction.plot(A,Block,y))\r\u0026gt; \u0026gt; #OR with ggplot\r\u0026gt; library(ggplot2)\r\u0026gt; ggplot(data.rcb, aes(y=y, x=A, group=Block,color=Block)) + geom_line() +\r+ guides(color=guide_legend(ncol=3))\r\u0026gt; \u0026gt; residualPlots(lm(y~Block+A, data.rcb))\r Test stat Pr(\u0026gt;|Test stat|)\rBlock A Tukey test -1.4163 0.1567\r\u0026gt; \u0026gt; # the Tukey\u0026#39;s non-additivity test by itself can be obtained via an internal function\r\u0026gt; # within the car package\r\u0026gt; car:::tukeyNonaddTest(lm(y~Block+A, data.rcb))\rTest Pvalue -1.4163343 0.1566776 \u0026gt; \u0026gt; # alternatively, there is also a Tukey\u0026#39;s non-additivity test within the\r\u0026gt; # asbio package\r\u0026gt; library(asbio)\r\u0026gt; with(data.rcb,tukey.add.test(y,A,Block))\rTukey\u0026#39;s one df test for additivity F = 2.0060029 Denom df = 67 p-value = 0.1613102\rConclusions:\n\rthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (A). Any trends appear to be reasonably consistent between Blocks.\r\r\rModel fitting\rFull parameterisation\n\\[ y_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\beta_0 + \\beta_i + \\gamma_{j(i)}, \\]\nwhere \\(\\gamma_{ij)} \\sim N(0, \\sigma^2_B)\\), \\(\\beta_0, \\beta_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\beta_0\\)) and two treatment effects (\\(\\beta_i\\), where \\(i\\) is \\(1,2\\)).\nMatrix parameterisation\n\\[ y_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\boldsymbol \\beta \\boldsymbol X + \\gamma_{j(i)}, \\]\nwhere \\(\\gamma_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\boldsymbol \\beta \\sim MVN(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\alpha_0\\)) and two treatment effects (\\(\\beta_i\\), where \\(i\\) is \\(1,2\\)). The matrix parameterisation is a compressed notation, In this parameterisation, there are three alpha parameters (one representing the mean of treatment a1, and the other two representing the treatment effects (differences between a2 and a1 and a3 and a1). In generating priors for each of these three alpha parameters, we could loop through each and define a non-informative normal prior to each (as in the Full parameterisation version). However, it turns out that it is more efficient (in terms of mixing and thus the number of necessary iterations) to define the priors from a multivariate normal distribution. This has as many means as there are parameters to estimate (\\(3\\)) and a \\(3\\times3\\) matrix of zeros and \\(100\\) in the diagonals.\n\\[ \\boldsymbol \\mu =\r\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\;\\;\\; \\sigma^2 \\sim \\begin{bmatrix}\r1000000 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 1000000 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 1000000\r\\end{bmatrix}. \\]\nHierarchical parameterisation\n\\[ y_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}= \\beta_0 + \\beta_i + \\gamma_{j(i)}, \\]\nwhere \\(\\gamma_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\beta_0, \\beta_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\).\nRather than assume a specific variance-covariance structure, just like lme we can incorporate an appropriate structure to account for different dependency/correlation structures in our data. In RCB designs, it is prudent to capture the residuals to allow checks that there are no outstanding dependency issues following model fitting.\n\rFull means parameterisation\r\u0026gt; rstanString=\u0026quot;\r+ data{\r+ int n;\r+ int nA;\r+ int nB;\r+ vector [n] y;\r+ int A[n];\r+ int B[n];\r+ }\r+ + parameters{\r+ real alpha[nA];\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ vector [nB] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma_B;\r+ }\r+ + model{\r+ real mu[n];\r+ + // Priors\r+ alpha ~ normal( 0 , 100 );\r+ beta ~ normal( 0 , sigma_B );\r+ sigma_B ~ cauchy( 0 , 25 );\r+ sigma ~ cauchy( 0 , 25 );\r+ + for ( i in 1:n ) {\r+ mu[i] = alpha[A[i]] + beta[B[i]];\r+ }\r+ y ~ normal( mu , sigma );\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstanString, con = \u0026quot;fullModel.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; data.rcb.list \u0026lt;- with(data.rcb, list(y=y, A=as.numeric(A), B=as.numeric(Block),\r+ n=nrow(data.rcb), nB=length(levels(Block)),nA=length(levels(A))))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;alpha\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma_B\u0026quot;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\rStart the STAN model (check the model, load data into the model, specify the number of chains and compile the model). Load the rstan package.\n\u0026gt; library(rstan)\rNow run the STAN code via the rstan interface.\n\u0026gt; data.rcb.rstan.c \u0026lt;- stan(data = data.rcb.list, file = \u0026quot;fullModel.stan\u0026quot;, + chains = nChains, pars = params, iter = nIter, + warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;fullModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 1: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 1: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 1: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 1: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 1: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 1: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 1: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 1: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 1: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 1: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 1: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.406 seconds (Warm-up)\rChain 1: 0.218 seconds (Sampling)\rChain 1: 0.624 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;fullModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 2: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 2: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 2: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 2: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 2: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 2: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 2: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 2: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 2: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 2: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 2: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.347 seconds (Warm-up)\rChain 2: 0.187 seconds (Sampling)\rChain 2: 0.534 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.rcb.rstan.c, par = c(\u0026quot;alpha\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;sigma_B\u0026quot;))\rInference for Stan model: fullModel.\r2 chains, each with iter=4500; warmup=3000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\ralpha[1] 41.62 0.10 2.22 37.25 40.16 41.60 43.04 46.10 503 1\ralpha[2] 69.56 0.10 2.23 65.09 68.08 69.55 71.00 74.06 501 1\ralpha[3] 81.91 0.10 2.21 77.50 80.46 81.89 83.35 86.41 512 1\rsigma 5.06 0.01 0.45 4.28 4.74 5.03 5.35 6.06 2235 1\rsigma_B 11.71 0.03 1.53 9.19 10.60 11.57 12.67 15.17 3266 1\rSamples were drawn using NUTS(diag_e) at Thu Feb 20 11:13:14 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; data.rcb.rstan.c.df \u0026lt;-as.data.frame(extract(data.rcb.rstan.c))\r\u0026gt; head(data.rcb.rstan.c.df)\ralpha.1 alpha.2 alpha.3 sigma sigma_B lp__\r1 39.79771 67.25749 80.02741 5.282417 12.242788 -322.0344\r2 42.06012 69.10275 82.61692 5.337436 13.564846 -320.3613\r3 44.78961 68.80971 82.67198 5.738811 9.592255 -330.7680\r4 43.87492 71.52632 83.14572 5.623264 12.883006 -329.6773\r5 41.66449 69.57592 82.74876 4.660046 9.676986 -315.6445\r6 44.58965 71.16270 83.85940 5.243767 11.651670 -324.5751\r\u0026gt; \u0026gt; data.rcb.mcmc.c\u0026lt;-rstan:::as.mcmc.list.stanfit(data.rcb.rstan.c)\r\u0026gt; \u0026gt; library(coda)\r\u0026gt; MCMCsum \u0026lt;- function(x) {\r+ data.frame(Median=median(x, na.rm=TRUE), t(quantile(x,na.rm=TRUE)),\r+ HPDinterval(as.mcmc(x)),HPDinterval(as.mcmc(x),p=0.5))\r+ }\r\u0026gt; \u0026gt; plyr:::adply(as.matrix(data.rcb.rstan.c.df),2,MCMCsum)\rX1 Median X0. X25. X50. X75.\r1 alpha.1 41.597151 33.420664 40.156779 41.597151 43.039950\r2 alpha.2 69.545338 62.734285 68.079657 69.545338 70.998104\r3 alpha.3 81.886423 75.169984 80.457712 81.886423 83.351567\r4 sigma 5.031927 3.809447 4.735465 5.031927 5.354484\r5 sigma_B 11.567999 7.856574 10.598430 11.567999 12.672590\r6 lp__ -321.445808 -348.603106 -325.384920 -321.445808 -317.997899\rX100. lower upper lower.1 upper.1\r1 50.023636 37.176909 46.004039 39.946809 42.769474\r2 77.380653 65.407869 74.292016 68.088148 71.000258\r3 89.460777 77.157802 86.028401 80.283264 83.159508\r4 7.032198 4.266709 5.994735 4.675345 5.271463\r5 17.627994 8.771093 14.580594 10.177722 12.169971\r6 -307.471584 -332.225405 -311.436678 -323.628137 -316.377883\r\rFull effect parameterisation\r\u0026gt; rstan2String=\u0026quot;\r+ data{\r+ int n;\r+ int nB;\r+ vector [n] y;\r+ int A2[n];\r+ int A3[n];\r+ int B[n];\r+ }\r+ + parameters{\r+ real alpha0;\r+ real alpha2;\r+ real alpha3;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ vector [nB] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma_B;\r+ }\r+ + model{\r+ real mu[n];\r+ + // Priors\r+ alpha0 ~ normal( 0 , 1000 );\r+ alpha2 ~ normal( 0 , 1000 );\r+ alpha3 ~ normal( 0 , 1000 );\r+ beta ~ normal( 0 , sigma_B );\r+ sigma_B ~ cauchy( 0 , 25 );\r+ sigma ~ cauchy( 0 , 25 );\r+ + for ( i in 1:n ) {\r+ mu[i] = alpha0 + alpha2*A2[i] + + alpha3*A3[i] + beta[B[i]];\r+ }\r+ y ~ normal( mu , sigma );\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstan2String, con = \u0026quot;full2Model.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; A2 \u0026lt;- ifelse(data.rcb$A==\u0026#39;2\u0026#39;,1,0)\r\u0026gt; A3 \u0026lt;- ifelse(data.rcb$A==\u0026#39;3\u0026#39;,1,0)\r\u0026gt; data.rcb.list \u0026lt;- with(data.rcb, list(y=y, A2=A2, A3=A3, B=as.numeric(Block),\r+ n=nrow(data.rcb), nB=length(levels(Block))))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;alpha0\u0026quot;,\u0026quot;alpha2\u0026quot;,\u0026quot;alpha3\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma_B\u0026quot;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\rNow run the STAN code via the rstan interface.\n\u0026gt; data.rcb.rstan.f \u0026lt;- stan(data = data.rcb.list, file = \u0026quot;full2Model.stan\u0026quot;, + chains = nChains, pars = params, iter = nIter, + warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;full2Model\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 1: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 1: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 1: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 1: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 1: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 1: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 1: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 1: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 1: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 1: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 1: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.704 seconds (Warm-up)\rChain 1: 0.233 seconds (Sampling)\rChain 1: 0.937 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;full2Model\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 2: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 2: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 2: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 2: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 2: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 2: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 2: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 2: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 2: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 2: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 2: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.673 seconds (Warm-up)\rChain 2: 0.264 seconds (Sampling)\rChain 2: 0.937 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.rcb.rstan.f, par = c(\u0026quot;alpha0\u0026quot;, \u0026quot;alpha2\u0026quot;, \u0026quot;alpha3\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;sigma_B\u0026quot;))\rInference for Stan model: full2Model.\r2 chains, each with iter=4500; warmup=3000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\ralpha0 41.71 0.15 2.21 37.04 40.32 41.77 43.19 45.96 226 1\ralpha2 27.97 0.03 1.20 25.68 27.16 27.97 28.77 30.41 2096 1\ralpha3 40.29 0.03 1.20 37.88 39.52 40.27 41.07 42.73 2085 1\rsigma 5.08 0.01 0.45 4.32 4.76 5.05 5.35 6.08 1585 1\rsigma_B 11.73 0.03 1.58 9.10 10.62 11.58 12.73 15.13 2104 1\rSamples were drawn using NUTS(diag_e) at Thu Feb 20 11:14:01 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; data.rcb.rstan.f.df \u0026lt;-as.data.frame(extract(data.rcb.rstan.f))\r\u0026gt; head(data.rcb.rstan.f.df)\ralpha0 alpha2 alpha3 sigma sigma_B lp__\r1 39.38308 27.48083 39.10102 5.742481 14.31952 -331.6050\r2 38.29360 28.13918 41.58618 4.724267 11.67688 -315.5252\r3 42.98642 26.90932 39.18832 6.013423 10.67375 -327.8293\r4 43.84156 28.24367 38.43665 4.755819 11.80901 -318.5703\r5 41.76138 30.59659 40.03245 5.299085 11.85943 -321.7613\r6 42.46431 28.85686 39.76470 5.240906 10.37141 -317.9281\r\u0026gt; \u0026gt; data.rcb.mcmc.f\u0026lt;-rstan:::as.mcmc.list.stanfit(data.rcb.rstan.f)\r\u0026gt; \u0026gt; plyr:::adply(as.matrix(data.rcb.rstan.f.df),2,MCMCsum)\rX1 Median X0. X25. X50. X75. X100.\r1 alpha0 41.765430 32.584162 40.32217 41.765430 43.185446 48.91023\r2 alpha2 27.970882 22.825642 27.16208 27.970882 28.768852 32.68851\r3 alpha3 40.269416 35.484427 39.52038 40.269416 41.069445 44.40748\r4 sigma 5.046204 3.830609 4.75756 5.046204 5.345414 6.99463\r5 sigma_B 11.580959 7.573467 10.61634 11.580959 12.733706 20.27037\r6 lp__ -321.442061 -344.215535 -325.37936 -321.442061 -317.729990 -306.64478\rlower upper lower.1 upper.1\r1 37.196783 46.064222 40.350871 43.199889\r2 25.626233 30.328589 27.296888 28.895397\r3 37.695622 42.536499 39.488834 41.025024\r4 4.216449 5.950882 4.696235 5.273486\r5 8.894630 14.798288 10.336185 12.367167\r6 -332.457780 -311.441309 -323.880049 -316.490939\r\rMatrix parameterisation\r\u0026gt; rstanString2=\u0026quot;\r+ data{\r+ int n;\r+ int nX;\r+ int nB;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ int B[n];\r+ }\r+ + parameters{\r+ vector [nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ vector [nB] gamma;\r+ real\u0026lt;lower=0\u0026gt; sigma_B;\r+ }\r+ transformed parameters {\r+ vector[n] mu; + + mu = X*beta;\r+ for (i in 1:n) {\r+ mu[i] = mu[i] + gamma[B[i]];\r+ }\r+ } + model{\r+ // Priors\r+ beta ~ normal( 0 , 100 );\r+ gamma ~ normal( 0 , sigma_B );\r+ sigma_B ~ cauchy( 0 , 25 );\r+ sigma ~ cauchy( 0 , 25 );\r+ + y ~ normal( mu , sigma );\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstanString2, con = \u0026quot;matrixModel.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; Xmat \u0026lt;- model.matrix(~A, data=data.rcb)\r\u0026gt; data.rcb.list \u0026lt;- with(data.rcb, list(y=y, X=Xmat, nX=ncol(Xmat),\r+ B=as.numeric(Block),\r+ n=nrow(data.rcb), nB=length(levels(Block))))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma_B\u0026quot;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\rNow run the STAN code via the rstan interface.\n\u0026gt; data.rcb.rstan.d \u0026lt;- stan(data = data.rcb.list, file = \u0026quot;matrixModel.stan\u0026quot;, + chains = nChains, pars = params, iter = nIter, + warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;matrixModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 1: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 1: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 1: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 1: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 1: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 1: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 1: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 1: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 1: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 1: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 1: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.695 seconds (Warm-up)\rChain 1: 0.234 seconds (Sampling)\rChain 1: 0.929 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;matrixModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 2: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 2: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 2: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 2: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 2: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 2: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 2: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 2: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 2: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 2: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 2: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.704 seconds (Warm-up)\rChain 2: 0.234 seconds (Sampling)\rChain 2: 0.938 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.rcb.rstan.d, par = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;sigma_B\u0026quot;))\rInference for Stan model: matrixModel.\r2 chains, each with iter=4500; warmup=3000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 41.74 0.13 2.08 37.63 40.40 41.71 43.09 46.01 268 1.01\rbeta[2] 27.89 0.03 1.21 25.50 27.09 27.89 28.71 30.26 2295 1.00\rbeta[3] 40.24 0.03 1.21 37.86 39.42 40.27 41.10 42.56 2342 1.00\rsigma 5.07 0.01 0.45 4.29 4.75 5.03 5.34 6.06 1876 1.00\rsigma_B 11.74 0.03 1.60 9.09 10.62 11.55 12.67 15.45 2908 1.00\rSamples were drawn using NUTS(diag_e) at Thu Feb 20 11:14:50 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; data.rcb.rstan.d.df \u0026lt;-as.data.frame(extract(data.rcb.rstan.d))\r\u0026gt; head(data.rcb.rstan.d.df)\rbeta.1 beta.2 beta.3 sigma sigma_B lp__\r1 42.18780 27.75495 38.97017 4.824826 12.43586 -313.5110\r2 42.18060 29.00253 41.20811 5.640763 10.94287 -326.8560\r3 36.71838 27.87852 39.66980 5.854247 10.78857 -328.5481\r4 41.53933 30.06548 40.20779 5.555467 12.24461 -329.5007\r5 41.43148 27.51720 38.67289 4.555229 10.14139 -327.1661\r6 39.34493 28.41662 40.31444 4.276922 13.15484 -313.5802\r\u0026gt; \u0026gt; data.rcb.mcmc.d\u0026lt;-rstan:::as.mcmc.list.stanfit(data.rcb.rstan.d)\r\u0026gt; \u0026gt; plyr:::adply(as.matrix(data.rcb.rstan.d.df),2,MCMCsum)\rX1 Median X0. X25. X50. X75.\r1 beta.1 41.710961 35.434490 40.398391 41.710961 43.09143\r2 beta.2 27.887177 23.330526 27.087464 27.887177 28.71052\r3 beta.3 40.274402 34.983236 39.423162 40.274402 41.10111\r4 sigma 5.032048 3.678731 4.753538 5.032048 5.33909\r5 sigma_B 11.549970 7.650390 10.624816 11.549970 12.66691\r6 lp__ -321.017545 -353.249400 -324.914721 -321.017545 -317.55082\rX100. lower upper lower.1 upper.1\r1 48.927966 37.263192 45.52043 40.240435 42.908363\r2 32.076745 25.707383 30.41474 27.175308 28.782407\r3 45.619411 37.938149 42.59995 39.543821 41.185932\r4 6.986504 4.251539 6.00204 4.714495 5.283777\r5 19.896439 8.775183 14.90410 10.238108 12.186504\r6 -306.873890 -332.837835 -311.51054 -323.070607 -315.824390\r\r\rRCB (repeated measures) - continuous within\rData generation\rImagine now that we has designed an experiment to investigate the effects of a continuous predictor (\\(x\\), for example time) on a response (\\(y\\)). Again, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability, we again decide to apply a design (RCB) in which each of the levels of \\(X\\) (such as time) treatments within each of \\(35\\) blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(123)\r\u0026gt; slope \u0026lt;- 30\r\u0026gt; intercept \u0026lt;- 200\r\u0026gt; nBlock \u0026lt;- 35\r\u0026gt; nTime \u0026lt;- 10\r\u0026gt; sigma \u0026lt;- 50\r\u0026gt; sigma.block \u0026lt;- 30\r\u0026gt; n \u0026lt;- nBlock*nTime\r\u0026gt; Block \u0026lt;- gl(nBlock, k=1)\r\u0026gt; Time \u0026lt;- 1:10\r\u0026gt; rho \u0026lt;- 0.8\r\u0026gt; dt \u0026lt;- expand.grid(Time=Time,Block=Block)\r\u0026gt; Xmat \u0026lt;- model.matrix(~-1+Block + Time, data=dt)\r\u0026gt; block.effects \u0026lt;- rnorm(n = nBlock, mean = intercept, sd = sigma.block)\r\u0026gt; #A.effects \u0026lt;- c(30,40)\r\u0026gt; all.effects \u0026lt;- c(block.effects,slope)\r\u0026gt; lin.pred \u0026lt;- Xmat %*% all.effects\r\u0026gt; \u0026gt; # OR\r\u0026gt; Xmat \u0026lt;- cbind(model.matrix(~-1+Block,data=dt),model.matrix(~Time,data=dt))\r\u0026gt; ## Sum to zero block effects\r\u0026gt; ##block.effects \u0026lt;- rnorm(n = nBlock, mean = 0, sd = sigma.block)\r\u0026gt; ###A.effects \u0026lt;- c(40,70,80)\r\u0026gt; ##all.effects \u0026lt;- c(block.effects,intercept,slope)\r\u0026gt; ##lin.pred \u0026lt;- Xmat %*% all.effects\r\u0026gt; \u0026gt; ## the quadrat observations (within sites) are drawn from\r\u0026gt; ## normal distributions with means according to the site means\r\u0026gt; ## and standard deviations of 5\r\u0026gt; eps \u0026lt;- NULL\r\u0026gt; eps[1] \u0026lt;- 0\r\u0026gt; for (j in 2:n) {\r+ eps[j] \u0026lt;- rho*eps[j-1] #residuals\r+ }\r\u0026gt; y \u0026lt;- rnorm(n,lin.pred,sigma)+eps\r\u0026gt; \u0026gt; #OR\r\u0026gt; eps \u0026lt;- NULL\r\u0026gt; # first value cant be autocorrelated\r\u0026gt; eps[1] \u0026lt;- rnorm(1,0,sigma)\r\u0026gt; for (j in 2:n) {\r+ eps[j] \u0026lt;- rho*eps[j-1] + rnorm(1, mean = 0, sd = sigma) #residuals\r+ }\r\u0026gt; y \u0026lt;- lin.pred + eps\r\u0026gt; data.rm \u0026lt;- data.frame(y=y, dt)\r\u0026gt; head(data.rm) #print out the first six rows of the data set\ry Time Block\r1 282.1142 1 1\r2 321.1404 2 1\r3 278.7700 3 1\r4 285.8709 4 1\r5 336.6390 5 1\r6 333.5961 6 1\r\u0026gt; \u0026gt; ggplot(data.rm, aes(y=y, x=Time)) + geom_smooth(method=\u0026#39;lm\u0026#39;) + geom_point() + facet_wrap(~Block)\r\rExploratory data analysis\rNormality and Homogeneity of variance\n\u0026gt; boxplot(y~Time, data.rm)\r\u0026gt; \u0026gt; ggplot(data.rm, aes(y=y, x=factor(Time))) + geom_boxplot()\rConclusions:\n\rthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\n\rthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\n\r\rObvious violations could be addressed either by:\n\rtransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\r\rBlock by within-Block interaction\n\u0026gt; with(data.rm, interaction.plot(Time,Block,y))\r\u0026gt; \u0026gt; ggplot(data.rm, aes(y=y, x=Time, color=Block, group=Block)) + geom_line() +\r+ guides(color=guide_legend(ncol=3))\r\u0026gt; \u0026gt; residualPlots(lm(y~Block+Time, data.rm))\r Test stat Pr(\u0026gt;|Test stat|)\rBlock Time -0.7274 0.4675\rTukey test -0.9809 0.3267\r\u0026gt; \u0026gt; # the Tukey\u0026#39;s non-additivity test by itself can be obtained via an internal function\r\u0026gt; # within the car package\r\u0026gt; car:::tukeyNonaddTest(lm(y~Block+Time, data.rm))\rTest Pvalue -0.9808606 0.3266615 \u0026gt; \u0026gt; # alternatively, there is also a Tukey\u0026#39;s non-additivity test within the\r\u0026gt; # asbio package\r\u0026gt; with(data.rm,tukey.add.test(y,Time,Block))\rTukey\u0026#39;s one df test for additivity F = 0.3997341 Denom df = 305 p-value = 0.5277003\rConclusions:\n\rthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (Time). Any trends appear to be reasonably consistent between Blocks.\r\rSphericity\nSince the levels of Time cannot be randomly assigned, it is likely that sphericity is not met. We can explore whether there is an auto-correlation patterns in the residuals. Note, as there was only ten time periods, it does not make logical sense to explore lags above \\(10\\).\n\u0026gt; library(nlme)\r\u0026gt; data.rm.lme \u0026lt;- lme(y~Time, random=~1|Block, data=data.rm)\r\u0026gt; acf(resid(data.rm.lme), lag=10)\rConclusions:\nThe autocorrelation factor (ACF) at a range of lags up to \\(10\\), indicate that there is a cyclical pattern of residual auto-correlation. We really should explore incorporating some form of correlation structure into our model.\n\rModel fitting\r\rMatrix parameterisation\r\u0026gt; rstanString2=\u0026quot;\r+ data{\r+ int n;\r+ int nX;\r+ int nB;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ int B[n];\r+ }\r+ + parameters{\r+ vector [nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ vector [nB] gamma;\r+ real\u0026lt;lower=0\u0026gt; sigma_B;\r+ }\r+ transformed parameters {\r+ vector[n] mu; + + mu = X*beta;\r+ for (i in 1:n) {\r+ mu[i] = mu[i] + gamma[B[i]];\r+ }\r+ } + model{\r+ // Priors\r+ beta ~ normal( 0 , 100 );\r+ gamma ~ normal( 0 , sigma_B );\r+ sigma_B ~ cauchy( 0 , 25 );\r+ sigma ~ cauchy( 0 , 25 );\r+ + y ~ normal( mu , sigma );\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstanString2, con = \u0026quot;matrixModel2.stan\u0026quot;)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~Time, data=data.rm)\r\u0026gt; data.rm.list \u0026lt;- with(data.rm, list(y=y, X=Xmat, nX=ncol(Xmat),\r+ B=as.numeric(Block),\r+ n=nrow(data.rm), nB=length(levels(Block))))\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta\u0026#39;,\u0026#39;sigma\u0026#39;,\u0026#39;sigma_B\u0026#39;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; data.rm.rstan.d \u0026lt;- stan(data = data.rm.list, file = \u0026quot;matrixModel2.stan\u0026quot;, + chains = nChains, pars = params, iter = nIter, + warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;matrixModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 1: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 1: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 1: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 1: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 1: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 1: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 1: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 1: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 1: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 1: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 1: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 2.027 seconds (Warm-up)\rChain 1: 0.657 seconds (Sampling)\rChain 1: 2.684 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;matrixModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 2: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 2: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 2: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 2: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 2: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 2: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 2: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 2: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 2: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 2: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 2: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 2.764 seconds (Warm-up)\rChain 2: 0.453 seconds (Sampling)\rChain 2: 3.217 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.rm.rstan.d , par = c(\u0026#39;beta\u0026#39;,\u0026#39;sigma\u0026#39;,\u0026#39;sigma_B\u0026#39;))\rInference for Stan model: matrixModel.\r2 chains, each with iter=4500; warmup=3000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 185.90 0.79 12.77 159.70 177.57 185.87 194.16 211.22 263 1.01\rbeta[2] 30.80 0.02 1.03 28.72 30.13 30.77 31.46 32.80 2854 1.00\rsigma 55.86 0.04 2.20 51.68 54.34 55.75 57.29 60.43 2639 1.00\rsigma_B 64.74 0.21 8.97 50.08 58.37 63.92 70.18 84.68 1816 1.00\rSamples were drawn using NUTS(diag_e) at Thu Feb 20 11:15:03 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\rGiven that Time cannot be randomized, there is likely to be a temporal dependency structure to the data. The above analyses assume no temporal dependency - actually, they assume that the variance-covariance matrix demonstrates a structure known as sphericity. Lets specifically model in a first order autoregressive correlation structure in an attempt to accommodate the expected temporal autocorrelation.\n\u0026gt; rstanString3=\u0026quot;\r+ data{\r+ int n;\r+ int nX;\r+ int nB;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ int B[n];\r+ vector [n] tgroup;\r+ }\r+ + parameters{\r+ vector [nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ vector [nB] gamma;\r+ real\u0026lt;lower=0\u0026gt; sigma_B;\r+ real ar;\r+ }\r+ transformed parameters {\r+ vector[n] mu; + vector[n] E;\r+ vector[n] res;\r+ + mu = X*beta;\r+ for (i in 1:n) {\r+ E[i] = 0;\r+ }\r+ for (i in 1:n) {\r+ mu[i] = mu[i] + gamma[B[i]];\r+ res[i] = y[i] - mu[i];\r+ if(i\u0026gt;0 \u0026amp;\u0026amp; i \u0026lt; n \u0026amp;\u0026amp; tgroup[i+1] == tgroup[i]) {\r+ E[i+1] = res[i];\r+ }\r+ mu[i] = mu[i] + (E[i] * ar);\r+ }\r+ } + model{\r+ // Priors\r+ beta ~ normal( 0 , 100 );\r+ gamma ~ normal( 0 , sigma_B );\r+ sigma_B ~ cauchy( 0 , 25 );\r+ sigma ~ cauchy( 0 , 25 );\r+ + y ~ normal( mu , sigma );\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstanString3, con = \u0026quot;matrixModel3.stan\u0026quot;)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~Time, data=data.rm)\r\u0026gt; data.rm.list \u0026lt;- with(data.rm, list(y=y, X=Xmat, nX=ncol(Xmat),\r+ B=as.numeric(Block),\r+ n=nrow(data.rm), nB=length(levels(Block)),\r+ tgroup=as.numeric(Block)))\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;beta\u0026#39;,\u0026#39;sigma\u0026#39;,\u0026#39;sigma_B\u0026#39;,\u0026#39;ar\u0026#39;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; data.rm.rstan.d \u0026lt;- stan(data = data.rm.list, file = \u0026quot;matrixModel3.stan\u0026quot;, + chains = nChains, pars = params, iter = nIter, + warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;matrixModel3\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 1: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 1: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 1: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 1: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 1: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 1: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 1: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 1: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 1: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 1: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 1: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 4.063 seconds (Warm-up)\rChain 1: 1.041 seconds (Sampling)\rChain 1: 5.104 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;matrixModel3\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 2: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 2: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 2: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 2: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 2: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 2: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 2: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 2: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 2: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 2: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 2: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 3.945 seconds (Warm-up)\rChain 2: 1.015 seconds (Sampling)\rChain 2: 4.96 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.rm.rstan.d , par = c(\u0026#39;beta\u0026#39;,\u0026#39;sigma\u0026#39;,\u0026#39;sigma_B\u0026#39;,\u0026#39;ar\u0026#39;))\rInference for Stan model: matrixModel3.\r2 chains, each with iter=4500; warmup=3000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 179.26 0.24 12.87 153.98 170.50 179.54 187.94 203.70 2981 1\rbeta[2] 31.25 0.02 1.64 28.05 30.14 31.22 32.32 34.54 5531 1\rsigma 48.74 0.03 1.96 45.11 47.43 48.64 50.04 52.87 3537 1\rsigma_B 50.33 0.30 10.54 31.10 43.06 49.68 56.96 72.38 1241 1\rar 0.78 0.00 0.05 0.68 0.75 0.78 0.82 0.87 2773 1\rSamples were drawn using NUTS(diag_e) at Thu Feb 20 11:15:58 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\r\rReferences\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\r\r\r","date":1581387194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581387194,"objectID":"75ddf469b482d9023331bf88e4f5531a","permalink":"/stan/block-anova-stan/block-anova-stan/","publishdate":"2020-02-10T21:13:14-05:00","relpermalink":"/stan/block-anova-stan/block-anova-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","randomised complete block","anova"],"title":"Randomised Complete Block Anova - STAN","type":"STAN"},{"authors":["Andrea Gabrio"],"categories":["R","anova","JAGS","mixed effects model"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rWhen single sampling units are selected amongst highly heterogeneous conditions, it is unlikely that these single units will adequately represent the populations and repeated sampling is likely to yield very different outcomes. For example, if we were investigating the impacts of fuel reduction burning across a highly heterogeneous landscape, our ability to replicate adequately might be limited by the number of burn sites available.\nAlternatively, sub-replicates within each of the sampling units (e.g. sites) can be collected (and averaged) so as to provided better representatives for each of the units and ultimately reduce the unexplained variability of the test of treatments. In essence, the sub-replicates are the replicates of an additional nested factor whose levels are nested within the main treatment factor. A nested factor refers to a factor whose levels are unique within each level of the factor it is nested within and each level is only represented once. For example, the fuel reduction burn study design could consist of three burnt sites and three un-burnt (control) sites each containing four quadrats (replicates of site and sub-replicates of the burn treatment). Each site represents a unique level of a random factor (any given site cannot be both burnt and un-burnt) that is nested within the fire treatment (burned or not).\nA nested design can be thought of as a hierarchical arrangement of factors (hence the alternative name hierarchical designs) whereby a treatment is progressively sub-replicated. As an additional example, imagine an experiment designed to comparing the leaf toughness of a number of tree species. Working down the hierarchy, five individual trees were randomly selected within (nested within) each species, three branches were randomly selected within each tree, two leaves were randomly selected within each branch and the force required to shear the leaf material in half (transversely) was measured in four random locations along the leaf. Clearly any given leaf can only be from a single branch, tree and species. Each level of sub-replication is introduced to further reduce the amount of unexplained variation and thereby increasing the power of the test for the main treatment effect. Additionally, it is possible to investigate which scale has the greatest (or least, etc) degree of variability - the level of the species, individual tree, branch, leaf, leaf region etc.\n\rNested factors are typically random factors, of which the levels are randomly selected to represent all possible levels (e.g. sites). When the main treatment effect (often referred to as Factor A) is a fixed factor, such designs are referred to as a mixed model nested ANOVA, whereas when Factor A is random, the design is referred to as a Model II nested ANOVA.\n\rFixed nested factors are also possible. For example, specific dates (corresponding to particular times during a season) could be nested within season. When all factors are fixed, the design is referred to as a Model I mixed model.\n\rFully nested designs (the topic of this chapter) differ from other multi-factor designs in that all factors within (below) the main treatment factor are nested and thus interactions are un-replicated and cannot be tested. Indeed, interaction effects (interaction between Factor A and site) are assumed to be zero.\n\r\r\rLinear models (frequentist)\rThe linear models for two and three factor nested design are:\n\\[ y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk},\\]\n\\[ y_{ijkl} = \\mu + \\alpha_i + \\beta_{j(i)} + gamma_{k(j(i))} + \\epsilon_{ijkl},\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha\\) is the effect of Factor A, \\(\\beta\\) is the effect of Factor B, \\(\\gamma\\) is the effect of Factor C and \\(\\epsilon\\) is the random unexplained or residual component.\n\rLinear models (Bayesian)\rSo called “random effects” are modelled differently from “fixed effects” in that rather than estimate their individual effects, we instead estimate the variability due to these “random effects”. Since technically all variables in a Bayesian framework are random, some prefer to use the terms ‘fixed effects’ and ‘varying effects’. As random factors typically represent “random” selections of levels (such as a set of randomly selected sites), incorporated in order to account for the dependency structure (observations within sites are more likely to be correlated to one another - not strickly independent) to the data, we are not overly interested in the individual differences between levels of the ‘varying’ (random) factor. Instead (in addition to imposing a separate correlation structure within each nest), we want to know how much variability is attributed to this level of the design. The linear models for two and three factor nested design are:\n\\[ y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk}, \\;\\;\\; \\epsilon_{ijk} \\sim N(0, \\sigma^2), \\;\\;\\; \\beta_{j(i)} \\sim N(0, \\sigma^2_{B}) \\]\n\\[ y_{ijkl} = \\mu + \\alpha_i + \\beta_{j(i)} + \\gamma_{k(j(i))} + \\epsilon_{ijkl}, \\;\\;\\; \\epsilon_{ijkl} \\sim N(0, \\sigma^2), \\;\\;\\; \\beta_{j(i)} \\sim N(0, \\sigma^2_{B}) \\;\\;\\; \\gamma_{k(j(i))} \\sim N(0, \\sigma^2_C) \\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha\\) is the effect of Factor A, \\(\\beta\\) is the variability of Factor B (nested within Factor A), \\(\\gamma\\) is the variability of Factor C (nested within Factor B) and \\(\\epsilon\\) is the random unexplained or residual component that is assumed to be normally distributed with a mean of zero and a constant amount of standard deviation (\\(\\sigma^2\\)). The subscripts are iterators. For example, the \\(i\\) represents the number of effects to be estimated for Factor A. Thus the first formula can be read as the \\(k\\)-th observation of \\(y\\) is drawn from a normal distribution (with a specific level of variability) and mean proposed to be determined by a base mean (\\(\\mu\\) - mean of the first treatment across all nests) plus the effect of the \\(i\\)-th treatment effect plus the variabilitythe model proposes that, given a base mean (\\(\\mu\\)) and knowing the effect of the \\(i\\)-th treatment (factor A) and which of the \\(j\\)-th nests within the treatment the \\(k\\)-th observation from Block \\(j\\) (factor B) within treatment effect.\n\rNull hypotheses\rSeparate null hypotheses are associated with each of the factors, however, nested factors are typically only added to absorb some of the unexplained variability and thus, specific hypotheses tests associated with nested factors are of lesser biological importance. Hence, rather than estimate the effects of random effects, we instead estimate how much variability they contribute.\nFactor A: the main treatment effect (fixed)\n\r\\(H_0(A): \\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\) (the population group means are all equal). That is, that the mean of population \\(1\\) is equal to that of population \\(2\\) and so on, and thus all population means are equal to one another - no effect of the factor on the response. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the mean of the first group (\\(\\alpha_i=\\mu_i-\\mu_1\\)) then the \\(H_0\\) can alternatively be written as:\n\r\\(H_0(A) : \\alpha_1=\\alpha_2=\\ldots=\\alpha_i=0\\) (the effect of each group equals zero). If one or more of the \\(\\alpha_i\\) are different from zero (the response mean for this treatment differs from the overall response mean), there is evidence that the null hypothesis is not true indicating that the factor does affect the response variable.\n\r\rFactor A: the main treatment effect (random)\n\r\\(H_0(A) : \\sigma^2_{\\alpha}=0\\) (population variance equals zero). There is no added variance due to all possible levels of A.\r\rFactor B: the nested effect (random)\n\r\\(H_0(B) : \\sigma^2_{\\beta}=0\\) (population variance equals zero). There is no added variance due to all possible levels of B within the (set or all possible) levels of A.\r\rFactor B: the nested effect (fixed)\n\r\\(H_0(B): \\mu_{1(1)}=\\mu_{2(1)}=\\ldots=\\mu_{j(i)}=\\mu\\) (the population group means of B (within A) are all equal).\n\r\\(H_0(B): \\beta_{1(1)}=\\beta_{2(1)}=\\ldots=\\beta_{j(i)}=0\\) (the effect of each chosen B group equals zero).\n\r\r\rAnalysis of variance\rAnalysis of variance sequentially partitions the total variability in the response variable into components explained by each of the factors (starting with the factors lowest down in the hierarchy - the most deeply nested) and the components unexplained by each factor. Explained variability is calculated by subtracting the amount unexplained by the factor from the amount unexplained by a reduced model that does not contain the factor. When the null hypothesis for a factor is true (no effect or added variability), the ratio of explained and unexplained components for that factor (F-ratio) should follow a theoretical F-distribution with an expected value less than 1. The appropriate unexplained residuals and therefore the appropriate F-ratios for each factor differ according to the different null hypotheses associated with different combinations of fixed and random factors in a nested linear model (see Table below).\n\u0026gt; fact_anova_table\rdf MS F-ratio (B random) Var comp (B random) A \u0026quot;a-1\u0026quot; \u0026quot;MS A\u0026quot; \u0026quot;(MS A)/(MS B\u0026#39;(A))\u0026quot; \u0026quot;((MS A) - (MS B\u0026#39;(A)))/nb\u0026quot; B\u0026#39;(A) \u0026quot;(b-1)a\u0026quot; \u0026quot;MS B\u0026#39;(A)\u0026quot; \u0026quot;(MS B\u0026#39;(A))/(MS res)\u0026quot; \u0026quot;((MS B\u0026#39;(A)) - (MS res))/n\u0026quot;\rRes \u0026quot;(n-1)ba\u0026quot; \u0026quot;MS res\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; F-ratio (B fixed) Var comp (B fixed) A \u0026quot;(MS A)/(MS res)\u0026quot; \u0026quot;((MS A) - (MS res))/nb\u0026quot; B\u0026#39;(A) \u0026quot;(MS B\u0026#39;(A))/(MS res)\u0026quot; \u0026quot;((MS B\u0026#39;(A)) - (MS res))/n\u0026quot;\rRes \u0026quot;\u0026quot; \u0026quot;\u0026quot; \rThe corresponding R syntax is given below.\n\u0026gt; #A fixed/random, B random (balanced)\r\u0026gt; summary(aov(y~A+Error(B), data))\r\u0026gt; VarCorr(lme(y~A,random=1|B, data))\r\u0026gt; \u0026gt; #A fixed/random, B random (unbalanced)\r\u0026gt; anova(lme(y~A,random=1|B, data), type=\u0026#39;marginal\u0026#39;)\r\u0026gt; \u0026gt; #A fixed/random, B fixed(balanced)\r\u0026gt; summary(aov(y~A+B, data))\r\u0026gt; \u0026gt; #A fixed/random, B fixed (unbalanced)\r\u0026gt; contrasts(data$B) \u0026lt;- contr.sum\r\u0026gt; Anova(aov(y~A/B, data), type=\u0026#39;III\u0026#39;)\r\rVariance components\rAs previously alluded to, it can often be useful to determine the relative contribution (to explaining the unexplained variability) of each of the factors as this provides insights into the variability at each different scales. These contributions are known as Variance components and are estimates of the added variances due to each of the factors. For consistency with leading texts on this topic, I have included estimated variance components for various balanced nested ANOVA designs in the above table. However, variance components based on a modified version of the maximum likelihood iterative model fitting procedure (REML) is generally recommended as this accommodates both balanced and unbalanced designs. While there are no numerical differences in the calculations of variance components for fixed and random factors, fixed factors are interpreted very differently and arguably have little clinical meaning (other to infer relative contribution). For fixed factors, variance components estimate the variance between the means of the specific populations that are represented by the selected levels of the factor and therefore represent somewhat arbitrary and artificial populations. For random factors, variance components estimate the variance between means of all possible populations that could have been selected and thus represents the true population variance.\n\rAssumptions\rAn F-distribution represents the relative frequencies of all the possible F-ratio’s when a given null hypothesis is true and certain assumptions about the residuals (denominator in the F-ratio calculation) hold. Consequently, it is also important that diagnostics associated with a particular hypothesis test reflect the denominator for the appropriate F-ratio. For example, when testing the null hypothesis that there is no effect of Factor A (\\(H_0(A):\\alpha_i=0\\)) in a mixed nested ANOVA, the means of each level of Factor B are used as the replicates of Factor A. As with single factor anova, hypothesis testing for nested ANOVA assumes the residuals are:\n\rnormally distributed. Factors higher up in the hierarchy of a nested model are based on means (or means of means) of lower factors and thus the Central Limit Theory would predict that normality will usually be satisfied for the higher level factors. Nevertheless, boxplots using the appropriate scale of replication should be used to explore normality. Scale transformations are often useful.\n\requally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\n\rindependent of one another - this requires special consideration so as to ensure that the scale at which sub-replicates are measured is still great enough to enable observations to be independent.\n\r\r\rUnbalanced nested designs\rDesigns that incorporate fixed and random factors (either nested or factorial), involve F-ratio calculations in which the denominators are themselves random factors other than the overall residuals. Many statisticians argue that when such denominators are themselves not statistically significant (at the \\(0.25\\) level), there are substantial power benefits from pooling together successive non-significant denominator terms. Thus an F-ratio for a particular factor might be recalculated after pooling together its original denominator with its denominators denominator and so on. The conservative \\(0.25\\) is used instead of the usual 0.05 to reduce further the likelihood of Type II errors (falsely concluding an effect is non-significant - that might result from insufficient power).\nFor a simple completely balanced nested ANOVA, it is possible to pool together (calculate their mean) each of the sub-replicates within each nest (site) and then perform single factor ANOVA on those aggregates. Indeed, for a balanced design, the estimates and hypothesis for Factor A will be identical to that produced via nested ANOVA. However, if there are an unequal number of sub-replicates within each nest, then the single factor ANOVA will be less powerful that a proper nested ANOVA. Unbalanced designs are those designs in which sample (subsample) sizes for each level of one or more factors differ. These situations are relatively common in biological research, however such imbalance has some important implications for nested designs.\nFirstly, hypothesis tests are more robust to the assumptions of normality and equal variance when the design is balanced. Secondly (and arguably, more importantly), the model contrasts are not orthogonal (independent) and the sums of squares component attributed to each of the model terms cannot be calculated by simple additive partitioning of the total sums of squares. In such situations, exact F-ratios cannot be constructed (at least in theory), variance components calculations are more complicated and significance tests cannot be computed. The denominator MS in an F-ratio is determined by examining the expected value of the mean squares of each term in a model. Unequal sample sizes result in expected means squares for which there are no obvious logical comparators that enable the impact of an individual model term to be isolated. The severity of this issue depends on which scale of the sub-sampling hierarchy the unbalance(s) occurs as well whether the unbalance occurs in the replication of a fixed or random factor. For example, whilst unequal levels of the first nesting factor (e.g. unequal number of burn vs un-burnt sites) has no effect on F-ratio construction or hypothesis testing for the top level factor (irrespective of whether either of the factors are fixed or random), unequal sub-sampling (replication) at the level of a random (but not fixed) nesting factor will impact on the ability to construct F-ratios and variance components of all terms above it in the hierarchy. There are a number of alternative ways of dealing with unbalanced nested designs. All alternatives assume that the imbalance is not a direct result of the treatments themselves. Such outcomes are more appropriately analysed by modelling the counts of surviving observations via frequency analysis.\n\rSplit the analysis up into separate smaller simple ANOVA’s each using the means of the nesting factor to reflect the appropriate scale of replication. As the resulting sums of squares components are thereby based on an aggregated dataset the analyses then inherit the procedures and requirements of single ANOVA.\rAdopt mixed-modelling techniques.\r\rWe note that, in a Bayesian framework, issues of design balance essentially evaporate.\n\rLinear mixed effects models\rAlthough the term “mixed-effects” can be used to refer to any design that incorporates both fixed and random predictors, its use is more commonly restricted to designs in which factors are nested or grouped within other factors. Typical examples include nested, longitudinal (measurements repeated over time) data, repeated measures and blocking designs. Furthermore, rather than basing parameter estimations on observed and expected mean squares or error strata (as outline above), mixed-effects models estimate parameters via maximum likelihood (ML) or residual maximum likelihood (REML). In so doing, mixed-effects models more appropriately handle estimation of parameters, effects and variance components of unbalanced designs (particularly for random effects). Resulting fitted (or expected) values of each level of a factor (for example, the expected population site means) are referred to as Best Linear Unbiased Predictors (BLUP’s). As an acknowledgement that most estimated site means will be more extreme than the underlying true population means they estimate (based on the principle that smaller sample sizes result in greater chances of more extreme observations and that nested sub-replicates are also likely to be highly intercorrelated), BLUP’s are less spread from the overall mean than are simple site means. In addition, mixed-effects models naturally model the “within-block” correlation structure that complicates many longitudinal designs.\nWhilst the basic concepts of mixed-effects models have been around for a long time, recent computing advances and adoptions have greatly boosted the popularity of these procedures. Linear mixed effects models are currently at the forefront of statistical development, and as such, are very much a work in progress - both in theory and in practice. Recent developments have seen a further shift away from the traditional practices associated with degrees of freedom, probability distribution and p-value calculations. The traditional approach to inference testing is to compare the fit of an alternative (full) model to a null (reduced) model (via an F-ratio). When assumptions of normality and homogeneity of variance apply, the degrees of freedom are easily computed and the F-ratio has an exact F-distribution to which it can be compared. However, this approach introduces two additional problematic assumptions when estimating fixed effects in a mixed effects model. Firstly, when estimating the effects of one factor, the parameter estimates associated with other factor(s) are assumed to be the true values of those parameters (not estimates). Whilst this assumption is reasonable when all factors are fixed, as random factors are selected such that they represent one possible set of levels drawn from an entire population of possible levels for the random factor, it is unlikely that the associated parameter estimates accurately reflect the true values. Consequently, there is not necessarily an appropriate F-distribution. Furthermore, determining the appropriate degrees of freedom (nominally, the number of independent observations on which estimates are based) for models that incorporate a hierarchical structure is only possible under very specific circumstances (such as completely balanced designs). Degrees of freedom is a somewhat arbitrary defined concept used primarily to select a theoretical probability distribution on which a statistic can be compared. Arguably, however, it is a concept that is overly simplistic for complex hierarchical designs. Most statistical applications continue to provide the “approximate” solutions (as did earlier versions within R). However, R linear mixed effects development leaders argue strenuously that given the above shortcomings, such approximations are variably inappropriate and are thus omitted.\nMarkov chain Monte Carlo (MCMC) sampling methods provide a Bayesian-like alternative for inference testing. Markov chains use the mixed model parameter estimates to generate posterior probability distributions of each parameter from which Monte Carlo sampling methods draw a large set of parameter samples. These parameter samples can then be used to calculate highest posterior density (HPD) intervals (also known as Bayesian credible intervals). Such intervals indicate the interval in which there is a specified probability (typically \\(95\\)%) that the true population parameter lies. Furthermore, whilst technically against the spirit of the Bayesian philosophy, it is also possible to generate P values on which to base inferences.\n\r\rData generation\rImagine we has designed an experiment in which we intend to measure a response (\\(y\\)) to one of treatments (three levels; “a1”, “a2” and “a3”). The treatments occur at a spatial scale (over an area) that far exceeds the logistical scale of sampling units (it would take too long to sample at the scale at which the treatments were applied). The treatments occurred at the scale of hectares whereas it was only feasible to sample y using 1m quadrats. Given that the treatments were naturally occurring (such as soil type), it was not possible to have more than five sites of each treatment type, yet prior experience suggested that the sites in which you intended to sample were very uneven and patchy with respect to \\(y\\). In an attempt to account for this inter-site variability (and thus maximize the power of the test for the effect of treatment, you decided to employ a nested design in which 10 quadrats were randomly located within each of the five replicate sites per three treatments. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; library(plyr)\r\u0026gt; set.seed(123)\r\u0026gt; nTreat \u0026lt;- 3\r\u0026gt; nSites \u0026lt;- 15\r\u0026gt; nSitesPerTreat \u0026lt;- nSites/nTreat\r\u0026gt; nQuads \u0026lt;- 10\r\u0026gt; site.sigma \u0026lt;- 12\r\u0026gt; sigma \u0026lt;- 5\r\u0026gt; n \u0026lt;- nSites * nQuads\r\u0026gt; sites \u0026lt;- gl(n=nSites,k=nQuads, lab=paste0(\u0026#39;S\u0026#39;,1:nSites))\r\u0026gt; A \u0026lt;- gl(nTreat, nSitesPerTreat*nQuads, n, labels=c(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;))\r\u0026gt; a.means \u0026lt;- c(40,70,80)\r\u0026gt; ## the site means (treatment effects) are drawn from normal distributions\r\u0026gt; ## with means of 40, 70 and 80 and standard deviations of 12\r\u0026gt; A.effects \u0026lt;- rnorm(nSites, rep(a.means,each=nSitesPerTreat),site.sigma)\r\u0026gt; #A.effects \u0026lt;- a.means %*% t(model.matrix(~A, data.frame(A=gl(nTreat,nSitesPerTreat,nSites))))+rnorm(nSites,0,site.sigma)\r\u0026gt; Xmat \u0026lt;- model.matrix(~sites -1)\r\u0026gt; lin.pred \u0026lt;- Xmat %*% c(A.effects)\r\u0026gt; ## the quadrat observations (within sites) are drawn from\r\u0026gt; ## normal distributions with means according to the site means\r\u0026gt; ## and standard deviations of 5\r\u0026gt; y \u0026lt;- rnorm(n,lin.pred,sigma)\r\u0026gt; data.nest \u0026lt;- data.frame(y=y, A=A, Sites=sites,Quads=1:length(y))\r\u0026gt; head(data.nest) #print out the first six rows of the data set\ry A Sites Quads\r1 42.20886 a1 S1 1\r2 35.76354 a1 S1 2\r3 23.44121 a1 S1 3\r4 36.78107 a1 S1 4\r5 30.91034 a1 S1 5\r6 27.93517 a1 S1 6\r\u0026gt; \u0026gt; library(ggplot2)\r\u0026gt; ggplot(data.nest, aes(y=y, x=1)) + geom_boxplot() + facet_grid(.~Sites)\rExploratory data analysis\rNormality and Homogeneity of variance\n\u0026gt; #Effects of treatment\r\u0026gt; boxplot(y~A, ddply(data.nest, ~A+Sites,numcolwise(mean, na.rm=T)))\r\u0026gt; \u0026gt; #Site effects\r\u0026gt; boxplot(y~Sites, ddply(data.nest, ~A+Sites+Quads,numcolwise(mean, na.rm=T)))\r\u0026gt; \u0026gt; ## with ggplot2\r\u0026gt; ggplot(ddply(data.nest, ~A+Sites,numcolwise(mean, na.rm=T)), aes(y=y, x=A)) +\r+ geom_boxplot()\rConclusions:\n\rthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\n\rthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the y-axis. Hence it there is no evidence of non-homogeneity.\n\r\rObvious violations could be addressed either by:\n\rtransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\r\r\r\rModel fitting\rFor non-hierarchical linear models, uniform priors on variance (standard deviation) parameters seem to work reasonably well. Gelman and others (2006) warns that the use of the inverse-gamma family of non-informative priors are very sensitive to ϵ particularly when variance is close to zero and this may lead to unintentionally informative priors. When the number of groups (treatments or varying/random effects) is large (more than \\(5\\)), Gelman and others (2006) advocated the use of either uniform or half-cauchy priors. Yet when the number of groups is low, Gelman and others (2006) indicates that uniform priors have a tendency to result in inflated variance estimates. Consequently, half-cauchy priors are generally recommended for variances.\nFull parameterisation\n\\[ y_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\alpha_0 + \\alpha_i + \\beta_{j(i)}, \\]\nwhere \\(\\beta_{ij)} \\sim N(0, \\sigma^2_B)\\), \\(\\alpha_0, \\alpha_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\alpha_0\\)) and two treatment effects (\\(\\alpha_i\\), where \\(i\\) is \\(1,2\\)).\nMatrix parameterisation\n\\[ y_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\boldsymbol \\alpha \\boldsymbol X + \\beta_{j(i)}, \\]\nwhere \\(\\beta_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\boldsymbol \\alpha \\sim MVN(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\alpha_0\\)) and two treatment effects (\\(\\alpha_i\\), where \\(i\\) is \\(1,2\\)). The matrix parameterisation is a compressed notation, In this parameterisation, there are three alpha parameters (one representing the mean of treatment a1, and the other two representing the treatment effects (differences between a2 and a1 and a3 and a1). In generating priors for each of these three alpha parameters, we could loop through each and define a non-informative normal prior to each (as in the Full parameterisation version). However, it turns out that it is more efficient (in terms of mixing and thus the number of necessary iterations) to define the priors from a multivariate normal distribution. This has as many means as there are parameters to estimate (\\(3\\)) and a \\(3\\times3\\) matrix of zeros and \\(100\\) in the diagonals.\n\\[ \\boldsymbol \\mu =\r\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\;\\;\\; \\sigma^2 \\sim \\begin{bmatrix}\r1000000 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 1000000 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 1000000\r\\end{bmatrix}. \\]\nHierarchical parameterisation\n\\[ y_{ijk} \\sim N(\\beta_{i(j)}, \\sigma^2), \\;\\;\\; \\beta_{i(j)}\\sim N(\\mu_i, \\sigma^2_B), \\]\nwhere \\(\\mu_i = \\boldsymbol \\alpha \\boldsymbol X\\), \\(\\alpha_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). In the heirarchical parameterisation, we are indicating two residual layers - one representing the variability in the observed data between individual observations (within sites) and the second representing the variability between site means (within the three treatments).\nFull effect parameterisation\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- alpha0 + alpha[A[i]] + beta[site[i]]\r+ }\r+ + #Priors\r+ alpha0 ~ dnorm(0, 1.0E-6)\r+ alpha[1] \u0026lt;- 0\r+ for (i in 2:nA) {\r+ alpha[i] ~ dnorm(0, 1.0E-6) #prior\r+ }\r+ for (i in 1:nSite) {\r+ beta[i] ~ dnorm(0, tau.B) #prior\r+ }\r+ tau \u0026lt;- pow(sigma,-2)\r+ sigma \u0026lt;-z/sqrt(chSq)\r+ z ~ dnorm(0, .0016)I(0,)\r+ chSq ~ dgamma(0.5, 0.5)\r+ + tau.B \u0026lt;- pow(sigma.B,-2)\r+ sigma.B \u0026lt;-z/sqrt(chSq.B)\r+ z.B ~ dnorm(0, .0016)I(0,)\r+ chSq.B ~ dgamma(0.5, 0.5)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;fullModel.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; data.nest.list \u0026lt;- with(data.nest,\r+ list(y=y,\r+ site=as.numeric(Sites),\r+ A=as.numeric(A),\r+ n=nrow(data.nest),\r+ nSite=length(levels(Sites)),\r+ nA = length(levels(A))\r+ )\r+ )\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;alpha0\u0026quot;,\u0026quot;alpha\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma.B\u0026quot;)\r\u0026gt; adaptSteps = 1000\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\u0026gt; library(R2jags)\rNow run the JAGS code via the R2jags interface.\n\u0026gt; data.nest.r2jags.f \u0026lt;- jags(data = data.nest.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;fullModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 150\rUnobserved stochastic nodes: 22\rTotal graph size: 502\rInitializing model\r\u0026gt; \u0026gt; print(data.nest.r2jags.f)\rInference for Bugs model at \u0026quot;fullModel.txt\u0026quot;, fit using jags,\r2 chains, each with 4500 iterations (first 3000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\ralpha[1] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000 1\ralpha[2] 27.388 7.149 13.085 22.881 27.312 31.980 41.230 1.001 3000\ralpha[3] 40.839 7.083 26.936 36.251 40.800 45.412 55.107 1.002 3000\ralpha0 42.325 4.978 32.452 39.136 42.215 45.422 52.310 1.002 3000\rsigma 5.069 0.307 4.530 4.851 5.051 5.265 5.722 1.002 3000\rsigma.B 10.990 2.527 7.168 9.260 10.656 12.306 17.136 1.009 190\rdeviance 909.635 5.937 899.898 905.400 908.952 913.145 923.175 1.001 3000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 17.6 and DIC = 927.3\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMatrix parameterisation\r\u0026gt; modelString2=\u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- inprod(alpha[],X[i,]) + inprod(beta[], Z[i,])\r+ } + + #Priors\r+ alpha ~ dmnorm(a0,A0)\r+ for (i in 1:nZ) {\r+ beta[i] ~ dnorm(0, tau.B) #prior\r+ }\r+ tau \u0026lt;- pow(sigma,-2)\r+ sigma \u0026lt;-z/sqrt(chSq)\r+ z ~ dnorm(0, .0016)I(0,)\r+ chSq ~ dgamma(0.5, 0.5)\r+ + tau.B \u0026lt;- pow(sigma.B,-2)\r+ sigma.B \u0026lt;-z/sqrt(chSq.B)\r+ z.B ~ dnorm(0, .0016)I(0,)\r+ chSq.B ~ dgamma(0.5, 0.5)\r+ + }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString2, con = \u0026quot;matrixModel.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; A.Xmat \u0026lt;- model.matrix(~A,data.nest)\r\u0026gt; Zmat \u0026lt;- model.matrix(~-1+Sites, data.nest)\r\u0026gt; data.nest.list \u0026lt;- with(data.nest,\r+ list(y=y,\r+ X=A.Xmat,\r+ n=nrow(data.nest),\r+ Z=Zmat, nZ=ncol(Zmat),\r+ a0=rep(0,3), A0=diag(3)\r+ )\r+ )\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;alpha\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma.B\u0026quot;,\u0026#39;beta\u0026#39;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\rNow run the JAGS code via the R2jags interface.\n\u0026gt; data.nest.r2jags.m \u0026lt;- jags(data = data.nest.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;matrixModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 150\rUnobserved stochastic nodes: 20\rTotal graph size: 3231\rInitializing model\r\u0026gt; \u0026gt; print(data.nest.r2jags.m)\rInference for Bugs model at \u0026quot;matrixModel.txt\u0026quot;, fit using jags,\r2 chains, each with 4500 iterations (first 3000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\ralpha[1] 0.201 1.016 -1.750 -0.474 0.215 0.872 2.161 1.001 3000\ralpha[2] 0.082 0.972 -1.835 -0.585 0.092 0.730 1.954 1.003 2000\ralpha[3] 0.077 1.005 -1.867 -0.608 0.075 0.771 2.055 1.001 3000\rbeta[1] 31.532 1.871 27.942 30.237 31.536 32.794 35.248 1.001 3000\rbeta[2] 38.069 1.911 34.289 36.788 38.125 39.343 41.817 1.001 3000\rbeta[3] 59.346 1.872 55.692 58.089 59.346 60.579 63.088 1.001 3000\rbeta[4] 40.644 1.936 36.885 39.378 40.659 41.960 44.321 1.002 1400\rbeta[5] 40.506 1.855 36.802 39.248 40.492 41.750 44.199 1.001 3000\rbeta[6] 90.495 2.131 86.451 89.013 90.489 91.970 94.602 1.001 3000\rbeta[7] 75.252 2.114 71.007 73.850 75.238 76.681 79.322 1.002 1200\rbeta[8] 57.061 2.180 52.888 55.574 57.032 58.568 61.289 1.001 2400\rbeta[9] 61.336 2.171 57.214 59.855 61.372 62.822 65.415 1.001 3000\rbeta[10] 62.816 2.159 58.580 61.353 62.774 64.268 67.144 1.001 3000\rbeta[11] 93.379 2.134 89.192 91.945 93.374 94.750 97.533 1.001 3000\rbeta[12] 83.011 2.161 78.822 81.508 83.024 84.486 87.245 1.001 3000\rbeta[13] 82.765 2.202 78.398 81.292 82.774 84.252 87.054 1.001 3000\rbeta[14] 81.140 2.165 76.775 79.675 81.185 82.598 85.236 1.001 3000\rbeta[15] 74.041 2.119 70.008 72.616 74.027 75.493 78.245 1.001 3000\rsigma 5.058 0.306 4.499 4.844 5.049 5.255 5.710 1.002 1200\rsigma.B 68.791 13.133 48.825 59.338 66.869 75.995 98.963 1.002 3000\rdeviance 909.431 6.235 899.560 905.043 908.621 913.008 923.865 1.003 810\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 19.4 and DIC = 928.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rHierarchical parameterisation\r\u0026gt; modelString3=\u0026quot;\r+ model {\r+ #Likelihood (esimating site means (gamma.site)\r+ for (i in 1:n) {\r+ y[i]~dnorm(quad.means[i],tau)\r+ quad.means[i] \u0026lt;- gamma.site[site[i]]\r+ }\r+ for (i in 1:s) {\r+ gamma.site[i] ~ dnorm(site.means[i], tau.site)\r+ site.means[i] \u0026lt;- inprod(beta[],A.Xmat[i,])\r+ }\r+ #Priors\r+ for (i in 1:a) {\r+ beta[i] ~ dnorm(0, 1.0E-6) #prior\r+ }\r+ tau \u0026lt;- pow(sigma,-2)\r+ sigma \u0026lt;-z/sqrt(chSq)\r+ z ~ dnorm(0, .0016)I(0,)\r+ chSq ~ dgamma(0.5, 0.5)\r+ + tau.B \u0026lt;- pow(sigma.B,-2)\r+ sigma.B \u0026lt;-z/sqrt(chSq.B)\r+ z.B ~ dnorm(0, .0016)I(0,)\r+ chSq.B ~ dgamma(0.5, 0.5)\r+ + tau.site \u0026lt;- pow(sigma.site,-2)\r+ sigma.site \u0026lt;-z/sqrt(chSq.site)\r+ z.site ~ dnorm(0, .0016)I(0,)\r+ chSq.site ~ dgamma(0.5, 0.5)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString3, con = \u0026quot;hierarchicalModel.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; A.Xmat \u0026lt;- model.matrix(~A,ddply(data.nest,~Sites,catcolwise(unique)))\r\u0026gt; data.nest.list \u0026lt;- with(data.nest,\r+ list(y=y,\r+ site=Sites,\r+ A.Xmat= A.Xmat,\r+ n=nrow(data.nest),\r+ s=length(levels(Sites)),\r+ a = ncol(A.Xmat)\r+ )\r+ )\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma.site\u0026quot;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\rNow run the JAGS code via the R2jags interface.\n\u0026gt; data.nest.r2jags.h \u0026lt;- jags(data = data.nest.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;hierarchicalModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 150\rUnobserved stochastic nodes: 24\rTotal graph size: 406\rInitializing model\r\u0026gt; \u0026gt; print(data.nest.r2jags.h)\rInference for Bugs model at \u0026quot;hierarchicalModel.txt\u0026quot;, fit using jags,\r2 chains, each with 4500 iterations (first 3000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 42.139 4.991 32.186 38.913 42.226 45.346 51.751 1.001 3000\rbeta[2] 27.611 6.859 13.692 23.437 27.617 31.993 41.118 1.001 3000\rbeta[3] 41.048 7.032 26.813 36.805 41.067 45.316 55.566 1.002 1200\rsigma 5.058 0.315 4.483 4.841 5.036 5.257 5.763 1.001 3000\rsigma.site 10.889 2.386 7.235 9.269 10.578 12.125 16.695 1.005 3000\rdeviance 909.557 6.168 899.915 905.154 908.708 913.153 923.686 1.001 1900\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 19.0 and DIC = 928.6\rDIC is an estimate of expected predictive error (lower deviance is better).\rIf you want to include finite-population standard deviations in the model you can use the following code.\n\u0026gt; modelString4=\u0026quot;\r+ model {\r+ #Likelihood (esimating site means (gamma.site)\r+ for (i in 1:n) {\r+ y[i]~dnorm(quad.means[i],tau)\r+ quad.means[i] \u0026lt;- gamma.site[site[i]]\r+ y.err[i]\u0026lt;- quad.means[i]-y[i]\r+ }\r+ for (i in 1:s) {\r+ gamma.site[i] ~ dnorm(site.means[i], tau.site)\r+ site.means[i] \u0026lt;- inprod(beta[],A.Xmat[i,])\r+ site.err[i] \u0026lt;- site.means[i] - gamma.site[i]\r+ }\r+ #Priors\r+ for (i in 1:a) {\r+ beta[i] ~ dnorm(0, 1.0E-6) #prior\r+ }\r+ tau \u0026lt;- pow(sigma,-2)\r+ sigma \u0026lt;-z/sqrt(chSq)\r+ z ~ dnorm(0, .0016)I(0,)\r+ chSq ~ dgamma(0.5, 0.5)\r+ + tau.site \u0026lt;- pow(sigma.site,-2)\r+ sigma.site \u0026lt;-z/sqrt(chSq.site)\r+ z.site ~ dnorm(0, .0016)I(0,)\r+ chSq.site ~ dgamma(0.5, 0.5)\r+ + sd.y \u0026lt;- sd(y.err)\r+ sd.site \u0026lt;- sd(site.err)\r+ sd.A \u0026lt;- sd(beta)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString4, con = \u0026quot;SDModel.txt\u0026quot;)\r\u0026gt; \u0026gt; #data list\r\u0026gt; A.Xmat \u0026lt;- model.matrix(~A,ddply(data.nest,~Sites,catcolwise(unique)))\r\u0026gt; data.nest.list \u0026lt;- with(data.nest,\r+ list(y=y,\r+ site=Sites,\r+ A.Xmat= A.Xmat,\r+ n=nrow(data.nest),\r+ s=length(levels(Sites)),\r+ a = ncol(A.Xmat)\r+ )\r+ )\r\u0026gt; \u0026gt; #parameters and chain details\r\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sd.y\u0026quot;,\u0026#39;sd.site\u0026#39;,\u0026#39;sd.A\u0026#39;,\u0026#39;sigma.site\u0026#39;)\r\u0026gt; adaptSteps = 1000\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; data.nest.r2jags.SD \u0026lt;- jags(data = data.nest.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;SDModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 150\rUnobserved stochastic nodes: 22\rTotal graph size: 571\rInitializing model\r\u0026gt; \u0026gt; print(data.nest.r2jags.SD)\rInference for Bugs model at \u0026quot;SDModel.txt\u0026quot;, fit using jags,\r2 chains, each with 4500 iterations (first 3000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 42.336 5.027 32.564 39.187 42.338 45.373 52.570 1.004 420\rbeta[2] 27.417 7.290 12.457 22.904 27.308 31.955 42.039 1.001 2100\rbeta[3] 40.862 7.164 26.163 36.386 40.920 45.444 55.173 1.007 770\rsd.A 10.042 4.276 2.657 7.162 9.646 12.369 19.900 1.001 2200\rsd.site 10.592 1.057 9.214 9.909 10.354 11.029 13.276 1.010 280\rsd.y 4.999 0.095 4.852 4.929 4.987 5.058 5.219 1.003 770\rsigma 5.047 0.309 4.489 4.830 5.029 5.257 5.705 1.005 310\rsigma.site 11.003 2.465 7.419 9.295 10.610 12.282 16.704 1.004 480\rdeviance 909.411 6.011 899.576 904.938 908.750 913.034 922.925 1.003 630\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 18.0 and DIC = 927.5\rDIC is an estimate of expected predictive error (lower deviance is better).\rCalculate \\(R^2\\) from the posterior of the model.\n\u0026gt; data.nest.mcmc.listSD \u0026lt;- as.mcmc(data.nest.r2jags.SD)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~A, data.nest)\r\u0026gt; coefs \u0026lt;- data.nest.r2jags.SD$BUGSoutput$sims.list[[\u0026#39;beta\u0026#39;]]\r\u0026gt; fitted \u0026lt;- coefs %*% t(Xmat)\r\u0026gt; X.var \u0026lt;- aaply(fitted,1,function(x){var(x)})\r\u0026gt; Z.var \u0026lt;- data.nest.r2jags.SD$BUGSoutput$sims.list[[\u0026#39;sd.site\u0026#39;]]^2\r\u0026gt; R.var \u0026lt;- data.nest.r2jags.SD$BUGSoutput$sims.list[[\u0026#39;sd.y\u0026#39;]]^2\r\u0026gt; R2.marginal \u0026lt;- (X.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.marginal \u0026lt;- data.frame(Mean=mean(R2.marginal), Median=median(R2.marginal), HPDinterval(as.mcmc(R2.marginal)))\r\u0026gt; R2.conditional \u0026lt;- (X.var+Z.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.conditional \u0026lt;- data.frame(Mean=mean(R2.conditional),\r+ Median=median(R2.conditional), HPDinterval(as.mcmc(R2.conditional)))\r\u0026gt; R2.site \u0026lt;- (Z.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.site \u0026lt;- data.frame(Mean=mean(R2.site), Median=median(R2.site), HPDinterval(as.mcmc(R2.site)))\r\u0026gt; R2.res\u0026lt;-(R.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.res \u0026lt;- data.frame(Mean=mean(R2.res), Median=median(R2.res), HPDinterval(as.mcmc(R2.res)))\r\u0026gt; \u0026gt; rbind(R2.site=R2.site, R2.marginal=R2.marginal, R2.res=R2.res, R2.conditional=R2.conditional)\rMean Median lower upper\rR2.site 0.26437322 0.2428822 0.16881028 0.41958555\rR2.marginal 0.67674004 0.6992418 0.49930501 0.78437310\rR2.res 0.05888674 0.0584191 0.03459529 0.08514432\rR2.conditional 0.94111326 0.9415809 0.91485568 0.96540471\r\rGraphical summaries\r\u0026gt; newdata \u0026lt;- with(data.nest, data.frame(A=levels(A)))\r\u0026gt; Xmat \u0026lt;- model.matrix(~A, newdata)\r\u0026gt; coefs \u0026lt;- data.nest.r2jags.m$BUGSoutput$sims.list[[\u0026#39;alpha\u0026#39;]]\r\u0026gt; fit \u0026lt;- coefs %*% t(Xmat)\r\u0026gt; newdata \u0026lt;- cbind(newdata,\r+ adply(fit, 2, function(x) {\r+ data.frame(Mean=mean(x), Median=median(x), HPDinterval(as.mcmc(x)),\r+ HPDinterval(as.mcmc(x), p=0.68))\r+ })\r+ )\r\u0026gt; \u0026gt; \u0026gt; library(ggplot2)\r\u0026gt; library(gridExtra)\r\u0026gt; library(grid)\r\u0026gt; p1 \u0026lt;- ggplot(newdata, aes(y=Median, x=A)) +\r+ geom_errorbar(aes(ymin=lower, ymax=upper), width=0.01, size=1) +\r+ geom_errorbar(aes(ymin=lower.1, ymax=upper.1), width=0, size=2) +\r+ geom_point(size=4, shape=21, fill=\u0026#39;white\u0026#39;)+\r+ scale_y_continuous(\u0026#39;Y\u0026#39;)+\r+ scale_x_discrete(\u0026#39;X\u0026#39;)+\r+ theme_classic()+\r+ theme(axis.title.y=element_text(vjust=2, size=rel(1.25)),\r+ axis.title.x=element_text(vjust=-2, size=rel(1.25)),\r+ plot.margin=unit(c(0.5,0.5,2,2), \u0026#39;lines\u0026#39;)\r+ )\r\u0026gt; \u0026gt; p1\r\r\rData generation - second example\rNow imagine a similar experiment in which we intend to measure a response (\\(y\\)) to one of treatments (three levels; “a1”, “a2” and “a3”). As with the previous design, we decided to establish a nested design in which there are sub-replicate (\\(1\\)m Quadrats) within each Site. In the current design, we have decided to further sub-replicate. Within each of the \\(5\\) Quadrats, we are going to randomly place \\(2\\times10\\)cm pit traps. Now we have Sites nested within Treatments, Quadrats nested within Sites AND, Pits nested within Sites. The latter of these (Pits nested within Sites) are the observations (\\(y\\)). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(123)\r\u0026gt; nTreat \u0026lt;- 3\r\u0026gt; nSites \u0026lt;- 15\r\u0026gt; nSitesPerTreat \u0026lt;- nSites/nTreat\r\u0026gt; nQuads \u0026lt;- 5\r\u0026gt; nPits \u0026lt;- 2\r\u0026gt; site.sigma \u0026lt;- 10 # sd within between sites within treatment\r\u0026gt; quad.sigma \u0026lt;- 10\r\u0026gt; sigma \u0026lt;- 7.5\r\u0026gt; n \u0026lt;- nSites * nQuads * nPits\r\u0026gt; sites \u0026lt;- gl(n=nSites,n/nSites,n, lab=paste(\u0026quot;site\u0026quot;,1:nSites))\r\u0026gt; A \u0026lt;- gl(nTreat, n/nTreat, n, labels=c(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;))\r\u0026gt; a.means \u0026lt;- c(40,70,80)\r\u0026gt; \u0026gt; #A\u0026lt;-gl(nTreat,nSites/nTreat,nSites,labels=c(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;))\r\u0026gt; a\u0026lt;-gl(nTreat,1,nTreat,labels=c(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;))\r\u0026gt; a.X \u0026lt;- model.matrix(~a, expand.grid(a))\r\u0026gt; a.eff \u0026lt;- as.vector(solve(a.X,a.means))\r\u0026gt; site.means \u0026lt;- rnorm(nSites,a.X %*% a.eff,site.sigma)\r\u0026gt; \u0026gt; A \u0026lt;- gl(nTreat,nSites/nTreat,nSites,labels=c(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;))\r\u0026gt; A.X \u0026lt;- model.matrix(~A, expand.grid(A))\r\u0026gt; #a.X \u0026lt;- model.matrix(~A, expand.grid(A=gl(nTreat,nSites/nTreat,nSites,labels=c(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;))))\r\u0026gt; site.means \u0026lt;- rnorm(nSites,A.X %*% a.eff,site.sigma)\r\u0026gt; \u0026gt; SITES \u0026lt;- gl(nSites,(nSites*nQuads)/nSites,nSites*nQuads,labels=paste(\u0026#39;site\u0026#39;,1:nSites))\r\u0026gt; sites.X \u0026lt;- model.matrix(~SITES-1)\r\u0026gt; quad.means \u0026lt;- rnorm(nSites*nQuads,sites.X %*% site.means,quad.sigma)\r\u0026gt; \u0026gt; #SITES \u0026lt;- gl(nSites,1,nSites,labels=paste(\u0026#39;site\u0026#39;,1:nSites))\r\u0026gt; #sites.X \u0026lt;- model.matrix(~SITES-1)\r\u0026gt; #quad.means \u0026lt;- rnorm(nSites*nQuads,sites.X %*% site.means,quad.sigma)\r\u0026gt; \u0026gt; QUADS \u0026lt;- gl(nSites*nQuads,n/(nSites*nQuads),n,labels=paste(\u0026#39;quad\u0026#39;,1:(nSites*nQuads)))\r\u0026gt; quads.X \u0026lt;- model.matrix(~QUADS-1)\r\u0026gt; #quads.eff \u0026lt;- as.vector(solve(quads.X,quad.means))\r\u0026gt; #pit.means \u0026lt;- rnorm(n,quads.eff %*% t(quads.X),sigma)\r\u0026gt; pit.means \u0026lt;- rnorm(n,quads.X %*% quad.means,sigma)\r\u0026gt; \u0026gt; PITS \u0026lt;- gl(nPits*nSites*nQuads,1, n, labels=paste(\u0026#39;pit\u0026#39;,1:(nPits*nSites*nQuads)))\r\u0026gt; data.nest1\u0026lt;-data.frame(Pits=PITS,Quads=QUADS,Sites=rep(SITES,each=2), A=rep(A,each=nQuads*nPits),y=pit.means)\r\u0026gt; #data.nest1\u0026lt;-data.nest1[order(data.nest1$A,data.nest1$Sites,data.nest1$Quads),]\r\u0026gt; head(data.nest1) #print out the first six rows of the data set\rPits Quads Sites A y\r1 pit 1 quad 1 site 1 a1 61.79607\r2 pit 2 quad 1 site 1 a1 56.24699\r3 pit 3 quad 2 site 1 a1 42.40885\r4 pit 4 quad 2 site 1 a1 52.06672\r5 pit 5 quad 3 site 1 a1 73.71286\r6 pit 6 quad 3 site 1 a1 62.50529\r\u0026gt; \u0026gt; ggplot(data.nest1, aes(y=y, x=1)) + geom_boxplot() + facet_grid(.~Quads)\rExploratory data analysis\rNormality and Homogeneity of variance\n\u0026gt; #Effects of treatment\r\u0026gt; boxplot(y~A, ddply(data.nest1, ~A+Sites,numcolwise(mean, na.rm=T)))\r\u0026gt; \u0026gt; #Site effects\r\u0026gt; boxplot(y~Sites, ddply(data.nest1, ~A+Sites+Quads,numcolwise(mean, na.rm=T)))\r\u0026gt; \u0026gt; #Quadrat effects\r\u0026gt; boxplot(y~Quads, ddply(data.nest1, ~A+Sites+Quads+Pits,numcolwise(mean, na.rm=T)))\rConclusions:\n\rthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\n\rthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity.\n\rit is a little difficult to assess normality/homogeneity of variance of quadrats since there are only two pits per quadrat. Nevertheless, there is no suggestion that variance increases with increasing mean.\n\r\rObvious violations could be addressed either by:\n\rtransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\r\r\r\rModel fitting\rFrequentist for comparison\r\u0026gt; library(nlme)\r\u0026gt; d.lme \u0026lt;- lme(y ~ A, random=~1|Sites/Quads,data=data.nest1)\r\u0026gt; summary(d.lme)\rLinear mixed-effects model fit by REML\rData: data.nest1 AIC BIC logLik\r1137.994 1155.937 -562.997\rRandom effects:\rFormula: ~1 | Sites\r(Intercept)\rStdDev: 10.38248\rFormula: ~1 | Quads %in% Sites\r(Intercept) Residual\rStdDev: 8.441615 7.161178\rFixed effects: y ~ A Value Std.Error DF t-value p-value\r(Intercept) 41.38646 5.04334 75 8.206160 0.0000\rAa2 21.36271 7.13236 12 2.995181 0.0112\rAa3 39.14584 7.13236 12 5.488483 0.0001\rCorrelation: (Intr) Aa2 Aa2 -0.707 Aa3 -0.707 0.500\rStandardized Within-Group Residuals:\rMin Q1 Med Q3 Max -2.11852493 -0.54600763 -0.03428569 0.53382444 2.26256381 Number of Observations: 150\rNumber of Groups: Sites Quads %in% Sites 15 75 \u0026gt; \u0026gt; anova(d.lme)\rnumDF denDF F-value p-value\r(Intercept) 1 75 446.9152 \u0026lt;.0001\rA 2 12 15.1037 5e-04\r\rFull effect parameterisation\r\u0026gt; modelString=\u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- alpha0 + alpha[A[i]] + beta.site[site[i]] + beta.quad[quad[i]]\r+ }\r+ + #Priors\r+ alpha0 ~ dnorm(0, 1.0E-6)\r+ alpha[1] \u0026lt;- 0\r+ for (i in 2:nA) {\r+ alpha[i] ~ dnorm(0, 1.0E-6) #prior\r+ }\r+ for (i in 1:nSite) {\r+ beta.site[i] ~ dnorm(0, tau.Bs) #prior\r+ }\r+ for (i in 1:nQuad) {\r+ beta.quad[i] ~ dnorm(0, tau.Bq) #prior\r+ }\r+ tau \u0026lt;- pow(sigma,-2)\r+ sigma \u0026lt;-z/sqrt(chSq)\r+ z ~ dnorm(0, .0016)I(0,)\r+ chSq ~ dgamma(0.5, 0.5)\r+ + tau.Bs \u0026lt;- pow(sigma.Bs,-2)\r+ sigma.Bs \u0026lt;-z/sqrt(chSq.Bs)\r+ z.Bs ~ dnorm(0, .0016)I(0,)\r+ chSq.Bs ~ dgamma(0.5, 0.5)\r+ + tau.Bq \u0026lt;- pow(sigma.Bq,-2)\r+ sigma.Bq \u0026lt;-z/sqrt(chSq.Bq)\r+ z.Bq ~ dnorm(0, .0016)I(0,)\r+ chSq.Bq ~ dgamma(0.5, 0.5)\r+ + }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;fullModel2.txt\u0026quot;)\r\u0026gt; \u0026gt; data.nest.list \u0026lt;- with(data.nest1,\r+ list(y=y,\r+ site=as.numeric(Sites),\r+ A=as.numeric(A),\r+ n=nrow(data.nest1),\r+ nSite=length(levels(Sites)),\r+ nA = length(levels(A)),\r+ nQuad=length(levels(Quads)),\r+ quad = as.numeric(Quads)\r+ )\r+ )\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026quot;alpha0\u0026quot;,\u0026quot;alpha\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma.Bs\u0026quot;,\u0026quot;sigma.Bq\u0026quot;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; data.nest.r2jags.f2 \u0026lt;- jags(data = data.nest.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;fullModel2.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 150\rUnobserved stochastic nodes: 99\rTotal graph size: 793\rInitializing model\r\u0026gt; \u0026gt; print(data.nest.r2jags.f2)\rInference for Bugs model at \u0026quot;fullModel2.txt\u0026quot;, fit using jags,\r2 chains, each with 4500 iterations (first 3000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat\ralpha[1] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000\ralpha[2] 21.147 7.532 6.252 16.137 21.140 25.968 35.890 1.001\ralpha[3] 38.985 7.635 23.341 34.130 39.120 43.879 53.757 1.001\ralpha0 41.541 5.460 30.677 37.967 41.659 45.032 52.383 1.001\rsigma 7.294 0.604 6.238 6.870 7.264 7.664 8.580 1.003\rsigma.Bq 8.433 1.132 6.355 7.650 8.378 9.175 10.757 1.005\rsigma.Bs 10.779 2.673 6.704 8.951 10.409 12.219 17.127 1.017\rdeviance 1020.495 17.724 988.898 1007.948 1019.500 1032.389 1056.708 1.005\rn.eff\ralpha[1] 1\ralpha[2] 3000\ralpha[3] 3000\ralpha0 3000\rsigma 970\rsigma.Bq 420\rsigma.Bs 100\rdeviance 510\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 156.8 and DIC = 1177.3\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMatrix parameterisation\r\u0026gt; modelString2=\u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- inprod(alpha[], X[i,]) + inprod(beta.site[],Z.site[i,]) + inprod(beta.quad[],Z.quad[i,])\r+ y.err[i] \u0026lt;- y[i]-mu[i]\r+ }\r+ + #Priors\r+ for (i in 1:nX) {\r+ alpha[i] ~ dnorm(0, 1.0E-6) #prior\r+ }\r+ for (i in 1:nSite) {\r+ beta.site[i] ~ dnorm(0, tau.Bs) #prior\r+ }\r+ for (i in 1:nQuad) {\r+ beta.quad[i] ~ dnorm(0, tau.Bq) #prior\r+ }\r+ tau \u0026lt;- pow(sigma,-2)\r+ sigma \u0026lt;-z/sqrt(chSq)\r+ z ~ dnorm(0, .0016)I(0,)\r+ chSq ~ dgamma(0.5, 0.5)\r+ + tau.Bs \u0026lt;- pow(sigma.Bs,-2)\r+ sigma.Bs \u0026lt;-z/sqrt(chSq.Bs)\r+ z.Bs ~ dnorm(0, .0016)I(0,)\r+ chSq.Bs ~ dgamma(0.5, 0.5)\r+ + tau.Bq \u0026lt;- pow(sigma.Bq,-2)\r+ sigma.Bq \u0026lt;-z/sqrt(chSq.Bq)\r+ z.Bq ~ dnorm(0, .0016)I(0,)\r+ chSq.Bq ~ dgamma(0.5, 0.5)\r+ + sd.res \u0026lt;- sd(y.err[])\r+ sd.site \u0026lt;- sd(beta.site[])\r+ sd.quad \u0026lt;- sd(beta.quad[]) + }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString2, con = \u0026quot;matrixModel2.txt\u0026quot;)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~A, data=data.nest1)\r\u0026gt; Zsite \u0026lt;- model.matrix(~-1+Sites, data=data.nest1)\r\u0026gt; Zquad \u0026lt;- model.matrix(~-1+Quads, data=data.nest1)\r\u0026gt; \u0026gt; data.nest.list \u0026lt;- with(data.nest1,\r+ list(y=y,\r+ n=nrow(data.nest1),\r+ X=Xmat, nX=ncol(Xmat),\r+ Z.site=Zsite, nSite=ncol(Zsite),\r+ Z.quad=Zquad, nQuad=ncol(Zquad)\r+ )\r+ )\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026quot;alpha\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma.Bs\u0026quot;,\u0026quot;sigma.Bq\u0026quot;,\u0026#39;sd.res\u0026#39;,\u0026#39;sd.site\u0026#39;,\u0026#39;sd.quad\u0026#39;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; data.nest.r2jags.m2 \u0026lt;- jags(data = data.nest.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;matrixModel2.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 150\rUnobserved stochastic nodes: 99\rTotal graph size: 14993\rInitializing model\r\u0026gt; \u0026gt; print(data.nest.r2jags.m2)\rInference for Bugs model at \u0026quot;matrixModel2.txt\u0026quot;, fit using jags,\r2 chains, each with 4500 iterations (first 3000 discarded)\rn.sims = 3000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat\ralpha[1] 41.247 5.438 30.494 37.721 41.227 44.692 52.262 1.002\ralpha[2] 21.535 7.824 6.537 16.541 21.439 26.473 37.416 1.003\ralpha[3] 39.276 7.723 24.165 34.357 39.319 44.191 54.637 1.001\rsd.quad 8.427 0.828 6.866 7.889 8.420 8.956 10.131 1.001\rsd.res 7.221 0.420 6.500 6.924 7.186 7.486 8.137 1.010\rsd.site 10.263 1.703 7.202 9.180 10.187 11.240 13.917 1.002\rsigma 7.261 0.598 6.189 6.845 7.209 7.631 8.540 1.010\rsigma.Bq 8.514 1.064 6.557 7.776 8.454 9.189 10.801 1.001\rsigma.Bs 10.703 2.802 6.379 8.805 10.283 12.108 17.304 1.001\rdeviance 1019.366 17.429 987.783 1007.166 1018.196 1030.618 1056.340 1.010\rn.eff\ralpha[1] 3000\ralpha[2] 3000\ralpha[3] 2500\rsd.quad 3000\rsd.res 150\rsd.site 3000\rsigma 160\rsigma.Bq 3000\rsigma.Bs 3000\rdeviance 160\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 151.0 and DIC = 1170.4\rDIC is an estimate of expected predictive error (lower deviance is better).\rIf we use the JAGS matrix parameterisation model from above, the JAGS model is already complete (as we defined the sd components in that model already).\n\u0026gt; data.nest1.mcmc.listSD \u0026lt;- as.mcmc(data.nest.r2jags.m2)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~A, data.nest1)\r\u0026gt; coefs \u0026lt;- data.nest.r2jags.m2$BUGSoutput$sims.list[[\u0026#39;alpha\u0026#39;]]\r\u0026gt; fitted \u0026lt;- coefs %*% t(Xmat)\r\u0026gt; X.var \u0026lt;- aaply(fitted,1,function(x){var(x)})\r\u0026gt; Z.var \u0026lt;- data.nest.r2jags.m2$BUGSoutput$sims.list[[\u0026#39;sd.site\u0026#39;]]^2\r\u0026gt; R.var \u0026lt;- data.nest.r2jags.m2$BUGSoutput$sims.list[[\u0026#39;sd.res\u0026#39;]]^2\r\u0026gt; R2.marginal \u0026lt;- (X.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.marginal \u0026lt;- data.frame(Mean=mean(R2.marginal), Median=median(R2.marginal), HPDinterval(as.mcmc(R2.marginal)))\r\u0026gt; R2.conditional \u0026lt;- (X.var+Z.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.conditional \u0026lt;- data.frame(Mean=mean(R2.conditional),\r+ Median=median(R2.conditional), HPDinterval(as.mcmc(R2.conditional)))\r\u0026gt; R2.site \u0026lt;- (Z.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.site \u0026lt;- data.frame(Mean=mean(R2.site), Median=median(R2.site), HPDinterval(as.mcmc(R2.site)))\r\u0026gt; R2.res\u0026lt;-(R.var)/(X.var+Z.var+R.var)\r\u0026gt; R2.res \u0026lt;- data.frame(Mean=mean(R2.res), Median=median(R2.res), HPDinterval(as.mcmc(R2.res)))\r\u0026gt; \u0026gt; rbind(R2.site=R2.site, R2.marginal=R2.marginal, R2.res=R2.res, R2.conditional=R2.conditional)\rMean Median lower upper\rR2.site 0.2537842 0.2373232 0.1145934 0.4450797\rR2.marginal 0.6199972 0.6408875 0.4077973 0.7873383\rR2.res 0.1262186 0.1233096 0.0646023 0.1907540\rR2.conditional 0.8737814 0.8766904 0.8092460 0.9353977\r\r\rReferences\rGelman, Andrew, and others. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).” Bayesian Analysis 1 (3): 515–34.\n\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1581300794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581300794,"objectID":"1e40228f571d277dad5ac75ca2071006","permalink":"/jags/nested-anova-jags/netsed-anova-jags/","publishdate":"2020-02-09T21:13:14-05:00","relpermalink":"/jags/nested-anova-jags/netsed-anova-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","mixed effects model","anova"],"title":"Nested Anova - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","anova","STAN","mixed effects model"],"content":"\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rWhen single sampling units are selected amongst highly heterogeneous conditions, it is unlikely that these single units will adequately represent the populations and repeated sampling is likely to yield very different outcomes. For example, if we were investigating the impacts of fuel reduction burning across a highly heterogeneous landscape, our ability to replicate adequately might be limited by the number of burn sites available.\nAlternatively, sub-replicates within each of the sampling units (e.g. sites) can be collected (and averaged) so as to provided better representatives for each of the units and ultimately reduce the unexplained variability of the test of treatments. In essence, the sub-replicates are the replicates of an additional nested factor whose levels are nested within the main treatment factor. A nested factor refers to a factor whose levels are unique within each level of the factor it is nested within and each level is only represented once. For example, the fuel reduction burn study design could consist of three burnt sites and three un-burnt (control) sites each containing four quadrats (replicates of site and sub-replicates of the burn treatment). Each site represents a unique level of a random factor (any given site cannot be both burnt and un-burnt) that is nested within the fire treatment (burned or not).\nA nested design can be thought of as a hierarchical arrangement of factors (hence the alternative name hierarchical designs) whereby a treatment is progressively sub-replicated. As an additional example, imagine an experiment designed to comparing the leaf toughness of a number of tree species. Working down the hierarchy, five individual trees were randomly selected within (nested within) each species, three branches were randomly selected within each tree, two leaves were randomly selected within each branch and the force required to shear the leaf material in half (transversely) was measured in four random locations along the leaf. Clearly any given leaf can only be from a single branch, tree and species. Each level of sub-replication is introduced to further reduce the amount of unexplained variation and thereby increasing the power of the test for the main treatment effect. Additionally, it is possible to investigate which scale has the greatest (or least, etc) degree of variability - the level of the species, individual tree, branch, leaf, leaf region etc.\n\rNested factors are typically random factors, of which the levels are randomly selected to represent all possible levels (e.g. sites). When the main treatment effect (often referred to as Factor A) is a fixed factor, such designs are referred to as a mixed model nested ANOVA, whereas when Factor A is random, the design is referred to as a Model II nested ANOVA.\n\rFixed nested factors are also possible. For example, specific dates (corresponding to particular times during a season) could be nested within season. When all factors are fixed, the design is referred to as a Model I mixed model.\n\rFully nested designs (the topic of this chapter) differ from other multi-factor designs in that all factors within (below) the main treatment factor are nested and thus interactions are un-replicated and cannot be tested. Indeed, interaction effects (interaction between Factor A and site) are assumed to be zero.\n\r\r\rLinear models (frequentist)\rThe linear models for two and three factor nested design are:\n\\[ y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk},\\]\n\\[ y_{ijkl} = \\mu + \\alpha_i + \\beta_{j(i)} + gamma_{k(j(i))} + \\epsilon_{ijkl},\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha\\) is the effect of Factor A, \\(\\beta\\) is the effect of Factor B, \\(\\gamma\\) is the effect of Factor C and \\(\\epsilon\\) is the random unexplained or residual component.\n\rLinear models (Bayesian)\rSo called “random effects” are modelled differently from “fixed effects” in that rather than estimate their individual effects, we instead estimate the variability due to these “random effects”. Since technically all variables in a Bayesian framework are random, some prefer to use the terms ‘fixed effects’ and ‘varying effects’. As random factors typically represent “random” selections of levels (such as a set of randomly selected sites), incorporated in order to account for the dependency structure (observations within sites are more likely to be correlated to one another - not strickly independent) to the data, we are not overly interested in the individual differences between levels of the ‘varying’ (random) factor. Instead (in addition to imposing a separate correlation structure within each nest), we want to know how much variability is attributed to this level of the design. The linear models for two and three factor nested design are:\n\\[ y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk}, \\;\\;\\; \\epsilon_{ijk} \\sim N(0, \\sigma^2), \\;\\;\\; \\beta_{j(i)} \\sim N(0, \\sigma^2_{B}) \\]\n\\[ y_{ijkl} = \\mu + \\alpha_i + \\beta_{j(i)} + \\gamma_{k(j(i))} + \\epsilon_{ijkl}, \\;\\;\\; \\epsilon_{ijkl} \\sim N(0, \\sigma^2), \\;\\;\\; \\beta_{j(i)} \\sim N(0, \\sigma^2_{B}) \\;\\;\\; \\gamma_{k(j(i))} \\sim N(0, \\sigma^2_C) \\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha\\) is the effect of Factor A, \\(\\beta\\) is the variability of Factor B (nested within Factor A), \\(\\gamma\\) is the variability of Factor C (nested within Factor B) and \\(\\epsilon\\) is the random unexplained or residual component that is assumed to be normally distributed with a mean of zero and a constant amount of standard deviation (\\(\\sigma^2\\)). The subscripts are iterators. For example, the \\(i\\) represents the number of effects to be estimated for Factor A. Thus the first formula can be read as the \\(k\\)-th observation of \\(y\\) is drawn from a normal distribution (with a specific level of variability) and mean proposed to be determined by a base mean (\\(\\mu\\) - mean of the first treatment across all nests) plus the effect of the \\(i\\)-th treatment effect plus the variabilitythe model proposes that, given a base mean (\\(\\mu\\)) and knowing the effect of the \\(i\\)-th treatment (factor A) and which of the \\(j\\)-th nests within the treatment the \\(k\\)-th observation from Block \\(j\\) (factor B) within treatment effect.\n\rNull hypotheses\rSeparate null hypotheses are associated with each of the factors, however, nested factors are typically only added to absorb some of the unexplained variability and thus, specific hypotheses tests associated with nested factors are of lesser biological importance. Hence, rather than estimate the effects of random effects, we instead estimate how much variability they contribute.\nFactor A: the main treatment effect (fixed)\n\r\\(H_0(A): \\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\) (the population group means are all equal). That is, that the mean of population \\(1\\) is equal to that of population \\(2\\) and so on, and thus all population means are equal to one another - no effect of the factor on the response. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the mean of the first group (\\(\\alpha_i=\\mu_i-\\mu_1\\)) then the \\(H_0\\) can alternatively be written as:\n\r\\(H_0(A) : \\alpha_1=\\alpha_2=\\ldots=\\alpha_i=0\\) (the effect of each group equals zero). If one or more of the \\(\\alpha_i\\) are different from zero (the response mean for this treatment differs from the overall response mean), there is evidence that the null hypothesis is not true indicating that the factor does affect the response variable.\n\r\rFactor A: the main treatment effect (random)\n\r\\(H_0(A) : \\sigma^2_{\\alpha}=0\\) (population variance equals zero). There is no added variance due to all possible levels of A.\r\rFactor B: the nested effect (random)\n\r\\(H_0(B) : \\sigma^2_{\\beta}=0\\) (population variance equals zero). There is no added variance due to all possible levels of B within the (set or all possible) levels of A.\r\rFactor B: the nested effect (fixed)\n\r\\(H_0(B): \\mu_{1(1)}=\\mu_{2(1)}=\\ldots=\\mu_{j(i)}=\\mu\\) (the population group means of B (within A) are all equal).\n\r\\(H_0(B): \\beta_{1(1)}=\\beta_{2(1)}=\\ldots=\\beta_{j(i)}=0\\) (the effect of each chosen B group equals zero).\n\r\r\rAnalysis of variance\rAnalysis of variance sequentially partitions the total variability in the response variable into components explained by each of the factors (starting with the factors lowest down in the hierarchy - the most deeply nested) and the components unexplained by each factor. Explained variability is calculated by subtracting the amount unexplained by the factor from the amount unexplained by a reduced model that does not contain the factor. When the null hypothesis for a factor is true (no effect or added variability), the ratio of explained and unexplained components for that factor (F-ratio) should follow a theoretical F-distribution with an expected value less than 1. The appropriate unexplained residuals and therefore the appropriate F-ratios for each factor differ according to the different null hypotheses associated with different combinations of fixed and random factors in a nested linear model (see Table below).\n\u0026gt; fact_anova_table\rdf MS F-ratio (B random) Var comp (B random) A \u0026quot;a-1\u0026quot; \u0026quot;MS A\u0026quot; \u0026quot;(MS A)/(MS B\u0026#39;(A))\u0026quot; \u0026quot;((MS A) - (MS B\u0026#39;(A)))/nb\u0026quot; B\u0026#39;(A) \u0026quot;(b-1)a\u0026quot; \u0026quot;MS B\u0026#39;(A)\u0026quot; \u0026quot;(MS B\u0026#39;(A))/(MS res)\u0026quot; \u0026quot;((MS B\u0026#39;(A)) - (MS res))/n\u0026quot;\rRes \u0026quot;(n-1)ba\u0026quot; \u0026quot;MS res\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; F-ratio (B fixed) Var comp (B fixed) A \u0026quot;(MS A)/(MS res)\u0026quot; \u0026quot;((MS A) - (MS res))/nb\u0026quot; B\u0026#39;(A) \u0026quot;(MS B\u0026#39;(A))/(MS res)\u0026quot; \u0026quot;((MS B\u0026#39;(A)) - (MS res))/n\u0026quot;\rRes \u0026quot;\u0026quot; \u0026quot;\u0026quot; \rThe corresponding R syntax is given below.\n\u0026gt; #A fixed/random, B random (balanced)\r\u0026gt; summary(aov(y~A+Error(B), data))\r\u0026gt; VarCorr(lme(y~A,random=1|B, data))\r\u0026gt; \u0026gt; #A fixed/random, B random (unbalanced)\r\u0026gt; anova(lme(y~A,random=1|B, data), type=\u0026#39;marginal\u0026#39;)\r\u0026gt; \u0026gt; #A fixed/random, B fixed(balanced)\r\u0026gt; summary(aov(y~A+B, data))\r\u0026gt; \u0026gt; #A fixed/random, B fixed (unbalanced)\r\u0026gt; contrasts(data$B) \u0026lt;- contr.sum\r\u0026gt; Anova(aov(y~A/B, data), type=\u0026#39;III\u0026#39;)\r\rVariance components\rAs previously alluded to, it can often be useful to determine the relative contribution (to explaining the unexplained variability) of each of the factors as this provides insights into the variability at each different scales. These contributions are known as Variance components and are estimates of the added variances due to each of the factors. For consistency with leading texts on this topic, I have included estimated variance components for various balanced nested ANOVA designs in the above table. However, variance components based on a modified version of the maximum likelihood iterative model fitting procedure (REML) is generally recommended as this accommodates both balanced and unbalanced designs. While there are no numerical differences in the calculations of variance components for fixed and random factors, fixed factors are interpreted very differently and arguably have little clinical meaning (other to infer relative contribution). For fixed factors, variance components estimate the variance between the means of the specific populations that are represented by the selected levels of the factor and therefore represent somewhat arbitrary and artificial populations. For random factors, variance components estimate the variance between means of all possible populations that could have been selected and thus represents the true population variance.\n\rAssumptions\rAn F-distribution represents the relative frequencies of all the possible F-ratio’s when a given null hypothesis is true and certain assumptions about the residuals (denominator in the F-ratio calculation) hold. Consequently, it is also important that diagnostics associated with a particular hypothesis test reflect the denominator for the appropriate F-ratio. For example, when testing the null hypothesis that there is no effect of Factor A (\\(H_0(A):\\alpha_i=0\\)) in a mixed nested ANOVA, the means of each level of Factor B are used as the replicates of Factor A. As with single factor anova, hypothesis testing for nested ANOVA assumes the residuals are:\n\rnormally distributed. Factors higher up in the hierarchy of a nested model are based on means (or means of means) of lower factors and thus the Central Limit Theory would predict that normality will usually be satisfied for the higher level factors. Nevertheless, boxplots using the appropriate scale of replication should be used to explore normality. Scale transformations are often useful.\n\requally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\n\rindependent of one another - this requires special consideration so as to ensure that the scale at which sub-replicates are measured is still great enough to enable observations to be independent.\n\r\r\rUnbalanced nested designs\rDesigns that incorporate fixed and random factors (either nested or factorial), involve F-ratio calculations in which the denominators are themselves random factors other than the overall residuals. Many statisticians argue that when such denominators are themselves not statistically significant (at the \\(0.25\\) level), there are substantial power benefits from pooling together successive non-significant denominator terms. Thus an F-ratio for a particular factor might be recalculated after pooling together its original denominator with its denominators denominator and so on. The conservative \\(0.25\\) is used instead of the usual 0.05 to reduce further the likelihood of Type II errors (falsely concluding an effect is non-significant - that might result from insufficient power).\nFor a simple completely balanced nested ANOVA, it is possible to pool together (calculate their mean) each of the sub-replicates within each nest (site) and then perform single factor ANOVA on those aggregates. Indeed, for a balanced design, the estimates and hypothesis for Factor A will be identical to that produced via nested ANOVA. However, if there are an unequal number of sub-replicates within each nest, then the single factor ANOVA will be less powerful that a proper nested ANOVA. Unbalanced designs are those designs in which sample (subsample) sizes for each level of one or more factors differ. These situations are relatively common in biological research, however such imbalance has some important implications for nested designs.\nFirstly, hypothesis tests are more robust to the assumptions of normality and equal variance when the design is balanced. Secondly (and arguably, more importantly), the model contrasts are not orthogonal (independent) and the sums of squares component attributed to each of the model terms cannot be calculated by simple additive partitioning of the total sums of squares. In such situations, exact F-ratios cannot be constructed (at least in theory), variance components calculations are more complicated and significance tests cannot be computed. The denominator MS in an F-ratio is determined by examining the expected value of the mean squares of each term in a model. Unequal sample sizes result in expected means squares for which there are no obvious logical comparators that enable the impact of an individual model term to be isolated. The severity of this issue depends on which scale of the sub-sampling hierarchy the unbalance(s) occurs as well whether the unbalance occurs in the replication of a fixed or random factor. For example, whilst unequal levels of the first nesting factor (e.g. unequal number of burn vs un-burnt sites) has no effect on F-ratio construction or hypothesis testing for the top level factor (irrespective of whether either of the factors are fixed or random), unequal sub-sampling (replication) at the level of a random (but not fixed) nesting factor will impact on the ability to construct F-ratios and variance components of all terms above it in the hierarchy. There are a number of alternative ways of dealing with unbalanced nested designs. All alternatives assume that the imbalance is not a direct result of the treatments themselves. Such outcomes are more appropriately analysed by modelling the counts of surviving observations via frequency analysis.\n\rSplit the analysis up into separate smaller simple ANOVA’s each using the means of the nesting factor to reflect the appropriate scale of replication. As the resulting sums of squares components are thereby based on an aggregated dataset the analyses then inherit the procedures and requirements of single ANOVA.\rAdopt mixed-modelling techniques.\r\rWe note that, in a Bayesian framework, issues of design balance essentially evaporate.\n\rLinear mixed effects models\rAlthough the term “mixed-effects” can be used to refer to any design that incorporates both fixed and random predictors, its use is more commonly restricted to designs in which factors are nested or grouped within other factors. Typical examples include nested, longitudinal (measurements repeated over time) data, repeated measures and blocking designs. Furthermore, rather than basing parameter estimations on observed and expected mean squares or error strata (as outline above), mixed-effects models estimate parameters via maximum likelihood (ML) or residual maximum likelihood (REML). In so doing, mixed-effects models more appropriately handle estimation of parameters, effects and variance components of unbalanced designs (particularly for random effects). Resulting fitted (or expected) values of each level of a factor (for example, the expected population site means) are referred to as Best Linear Unbiased Predictors (BLUP’s). As an acknowledgement that most estimated site means will be more extreme than the underlying true population means they estimate (based on the principle that smaller sample sizes result in greater chances of more extreme observations and that nested sub-replicates are also likely to be highly intercorrelated), BLUP’s are less spread from the overall mean than are simple site means. In addition, mixed-effects models naturally model the “within-block” correlation structure that complicates many longitudinal designs.\nWhilst the basic concepts of mixed-effects models have been around for a long time, recent computing advances and adoptions have greatly boosted the popularity of these procedures. Linear mixed effects models are currently at the forefront of statistical development, and as such, are very much a work in progress - both in theory and in practice. Recent developments have seen a further shift away from the traditional practices associated with degrees of freedom, probability distribution and p-value calculations. The traditional approach to inference testing is to compare the fit of an alternative (full) model to a null (reduced) model (via an F-ratio). When assumptions of normality and homogeneity of variance apply, the degrees of freedom are easily computed and the F-ratio has an exact F-distribution to which it can be compared. However, this approach introduces two additional problematic assumptions when estimating fixed effects in a mixed effects model. Firstly, when estimating the effects of one factor, the parameter estimates associated with other factor(s) are assumed to be the true values of those parameters (not estimates). Whilst this assumption is reasonable when all factors are fixed, as random factors are selected such that they represent one possible set of levels drawn from an entire population of possible levels for the random factor, it is unlikely that the associated parameter estimates accurately reflect the true values. Consequently, there is not necessarily an appropriate F-distribution. Furthermore, determining the appropriate degrees of freedom (nominally, the number of independent observations on which estimates are based) for models that incorporate a hierarchical structure is only possible under very specific circumstances (such as completely balanced designs). Degrees of freedom is a somewhat arbitrary defined concept used primarily to select a theoretical probability distribution on which a statistic can be compared. Arguably, however, it is a concept that is overly simplistic for complex hierarchical designs. Most statistical applications continue to provide the “approximate” solutions (as did earlier versions within R). However, R linear mixed effects development leaders argue strenuously that given the above shortcomings, such approximations are variably inappropriate and are thus omitted.\nMarkov chain Monte Carlo (MCMC) sampling methods provide a Bayesian-like alternative for inference testing. Markov chains use the mixed model parameter estimates to generate posterior probability distributions of each parameter from which Monte Carlo sampling methods draw a large set of parameter samples. These parameter samples can then be used to calculate highest posterior density (HPD) intervals (also known as Bayesian credible intervals). Such intervals indicate the interval in which there is a specified probability (typically \\(95\\)%) that the true population parameter lies. Furthermore, whilst technically against the spirit of the Bayesian philosophy, it is also possible to generate P values on which to base inferences.\n\r\rData generation\rImagine we has designed an experiment in which we intend to measure a response (\\(y\\)) to one of treatments (three levels; “a1”, “a2” and “a3”). The treatments occur at a spatial scale (over an area) that far exceeds the logistical scale of sampling units (it would take too long to sample at the scale at which the treatments were applied). The treatments occurred at the scale of hectares whereas it was only feasible to sample y using 1m quadrats. Given that the treatments were naturally occurring (such as soil type), it was not possible to have more than five sites of each treatment type, yet prior experience suggested that the sites in which you intended to sample were very uneven and patchy with respect to \\(y\\). In an attempt to account for this inter-site variability (and thus maximize the power of the test for the effect of treatment, you decided to employ a nested design in which 10 quadrats were randomly located within each of the five replicate sites per three treatments. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; library(plyr)\r\u0026gt; set.seed(123)\r\u0026gt; nTreat \u0026lt;- 3\r\u0026gt; nSites \u0026lt;- 15\r\u0026gt; nSitesPerTreat \u0026lt;- nSites/nTreat\r\u0026gt; nQuads \u0026lt;- 10\r\u0026gt; site.sigma \u0026lt;- 12\r\u0026gt; sigma \u0026lt;- 5\r\u0026gt; n \u0026lt;- nSites * nQuads\r\u0026gt; sites \u0026lt;- gl(n=nSites,k=nQuads, lab=paste0(\u0026#39;S\u0026#39;,1:nSites))\r\u0026gt; A \u0026lt;- gl(nTreat, nSitesPerTreat*nQuads, n, labels=c(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;))\r\u0026gt; a.means \u0026lt;- c(40,70,80)\r\u0026gt; ## the site means (treatment effects) are drawn from normal distributions\r\u0026gt; ## with means of 40, 70 and 80 and standard deviations of 12\r\u0026gt; A.effects \u0026lt;- rnorm(nSites, rep(a.means,each=nSitesPerTreat),site.sigma)\r\u0026gt; #A.effects \u0026lt;- a.means %*% t(model.matrix(~A, data.frame(A=gl(nTreat,nSitesPerTreat,nSites))))+rnorm(nSites,0,site.sigma)\r\u0026gt; Xmat \u0026lt;- model.matrix(~sites -1)\r\u0026gt; lin.pred \u0026lt;- Xmat %*% c(A.effects)\r\u0026gt; ## the quadrat observations (within sites) are drawn from\r\u0026gt; ## normal distributions with means according to the site means\r\u0026gt; ## and standard deviations of 5\r\u0026gt; y \u0026lt;- rnorm(n,lin.pred,sigma)\r\u0026gt; data.nest \u0026lt;- data.frame(y=y, A=A, Sites=sites,Quads=1:length(y))\r\u0026gt; head(data.nest) #print out the first six rows of the data set\ry A Sites Quads\r1 42.20886 a1 S1 1\r2 35.76354 a1 S1 2\r3 23.44121 a1 S1 3\r4 36.78107 a1 S1 4\r5 30.91034 a1 S1 5\r6 27.93517 a1 S1 6\r\u0026gt; \u0026gt; library(ggplot2)\r\u0026gt; ggplot(data.nest, aes(y=y, x=1)) + geom_boxplot() + facet_grid(.~Sites)\rExploratory data analysis\rNormality and Homogeneity of variance\n\u0026gt; #Effects of treatment\r\u0026gt; boxplot(y~A, ddply(data.nest, ~A+Sites,numcolwise(mean, na.rm=T)))\r\u0026gt; \u0026gt; #Site effects\r\u0026gt; boxplot(y~Sites, ddply(data.nest, ~A+Sites+Quads,numcolwise(mean, na.rm=T)))\r\u0026gt; \u0026gt; ## with ggplot2\r\u0026gt; ggplot(ddply(data.nest, ~A+Sites,numcolwise(mean, na.rm=T)), aes(y=y, x=A)) +\r+ geom_boxplot()\rConclusions:\n\rthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\n\rthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the y-axis. Hence it there is no evidence of non-homogeneity.\n\r\rObvious violations could be addressed either by:\n\rtransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\r\r\r\rModel fitting\rFor non-hierarchical linear models, uniform priors on variance (standard deviation) parameters seem to work reasonably well. Gelman and others (2006) warns that the use of the inverse-gamma family of non-informative priors are very sensitive to ϵ particularly when variance is close to zero and this may lead to unintentionally informative priors. When the number of groups (treatments or varying/random effects) is large (more than \\(5\\)), Gelman and others (2006) advocated the use of either uniform or half-cauchy priors. Yet when the number of groups is low, Gelman and others (2006) indicates that uniform priors have a tendency to result in inflated variance estimates. Consequently, half-cauchy priors are generally recommended for variances.\nFull parameterisation\n\\[ y_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\alpha_0 + \\alpha_i + \\beta_{j(i)}, \\]\nwhere \\(\\beta_{ij)} \\sim N(0, \\sigma^2_B)\\), \\(\\alpha_0, \\alpha_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\alpha_0\\)) and two treatment effects (\\(\\alpha_i\\), where \\(i\\) is \\(1,2\\)).\nMatrix parameterisation\n\\[ y_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\boldsymbol \\alpha \\boldsymbol X + \\beta_{j(i)}, \\]\nwhere \\(\\beta_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\boldsymbol \\alpha \\sim MVN(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\alpha_0\\)) and two treatment effects (\\(\\alpha_i\\), where \\(i\\) is \\(1,2\\)). The matrix parameterisation is a compressed notation, In this parameterisation, there are three alpha parameters (one representing the mean of treatment a1, and the other two representing the treatment effects (differences between a2 and a1 and a3 and a1). In generating priors for each of these three alpha parameters, we could loop through each and define a non-informative normal prior to each (as in the Full parameterisation version). However, it turns out that it is more efficient (in terms of mixing and thus the number of necessary iterations) to define the priors from a multivariate normal distribution. This has as many means as there are parameters to estimate (\\(3\\)) and a \\(3\\times3\\) matrix of zeros and \\(100\\) in the diagonals.\n\\[ \\boldsymbol \\mu =\r\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\;\\;\\; \\sigma^2 \\sim \\begin{bmatrix}\r1000000 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 1000000 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 1000000\r\\end{bmatrix}. \\]\nHierarchical parameterisation\n\\[ y_{ijk} \\sim N(\\beta_{i(j)}, \\sigma^2), \\;\\;\\; \\beta_{i(j)}\\sim N(\\mu_i, \\sigma^2_B), \\]\nwhere \\(\\mu_i = \\boldsymbol \\alpha \\boldsymbol X\\), \\(\\alpha_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). In the heirarchical parameterisation, we are indicating two residual layers - one representing the variability in the observed data between individual observations (within sites) and the second representing the variability between site means (within the three treatments).\nFull means parameterisation\r\u0026gt; rstanString=\u0026quot;\r+ data{\r+ int n;\r+ int nA;\r+ int nB;\r+ vector [n] y;\r+ int A[n];\r+ int B[n];\r+ }\r+ + parameters{\r+ real alpha[nA];\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ vector [nB] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma_B;\r+ }\r+ + model{\r+ real mu[n];\r+ + // Priors\r+ alpha ~ normal( 0 , 100 );\r+ beta ~ normal( 0 , sigma_B );\r+ sigma_B ~ cauchy( 0 , 25 );\r+ sigma ~ cauchy( 0 , 25 );\r+ + for ( i in 1:n ) {\r+ mu[i] = alpha[A[i]] + beta[B[i]];\r+ }\r+ y ~ normal( mu , sigma );\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstanString, con = \u0026quot;fullModel.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; data.nest.list \u0026lt;- with(data.nest, list(y=y, A=as.numeric(A), B=as.numeric(Sites),\r+ n=nrow(data.nest), nB=length(levels(Sites)),nA=length(levels(A))))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;alpha\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma_B\u0026quot;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\rStart the STAN model (check the model, load data into the model, specify the number of chains and compile the model). Load the rstan package.\n\u0026gt; library(rstan)\rNow run the STAN code via the rstan interface.\n\u0026gt; data.nest.rstan.c \u0026lt;- stan(data = data.nest.list, file = \u0026quot;fullModel.stan\u0026quot;, chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;fullModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 1: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 1: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 1: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 1: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 1: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 1: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 1: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 1: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 1: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 1: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 1: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 1 seconds (Warm-up)\rChain 1: 0.532 seconds (Sampling)\rChain 1: 1.532 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;fullModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 2: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 2: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 2: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 2: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 2: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 2: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 2: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 2: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 2: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 2: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 2: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 1.046 seconds (Warm-up)\rChain 2: 0.515 seconds (Sampling)\rChain 2: 1.561 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.nest.rstan.c, par = c(\u0026quot;alpha\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;sigma_B\u0026quot;))\rInference for Stan model: fullModel.\r2 chains, each with iter=4500; warmup=3000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\ralpha[1] 42.13 0.17 5.42 31.72 38.59 42.04 45.54 52.73 1058 1\ralpha[2] 69.78 0.16 5.18 58.88 66.67 70.02 73.13 79.77 1031 1\ralpha[3] 83.44 0.16 5.32 73.39 80.08 83.32 86.79 93.71 1128 1\rsigma 5.04 0.01 0.31 4.49 4.82 5.03 5.24 5.71 1681 1\rsigma_B 11.53 0.07 2.64 7.64 9.65 11.05 13.00 17.73 1347 1\rSamples were drawn using NUTS(diag_e) at Wed Feb 19 18:54:36 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; data.nest.rstan.c.df \u0026lt;-as.data.frame(extract(data.nest.rstan.c))\r\u0026gt; head(data.nest.rstan.c.df)\ralpha.1 alpha.2 alpha.3 sigma sigma_B lp__\r1 39.90783 67.92721 85.21329 5.001354 10.311217 -356.3867\r2 43.82811 70.24443 84.04493 5.063975 8.728672 -355.7535\r3 40.70837 86.85136 68.64037 5.418739 17.424519 -364.8486\r4 44.81747 68.68533 81.60515 5.133029 11.324690 -356.2288\r5 36.25821 69.65002 79.77226 5.401904 11.885382 -358.2225\r6 39.98730 75.26225 83.81045 5.215396 10.654035 -355.1518\r\rFull effect parameterisation\r\u0026gt; rstan2String=\u0026quot;\r+ data{\r+ int n;\r+ int nB;\r+ vector [n] y;\r+ int A2[n];\r+ int A3[n];\r+ int B[n];\r+ }\r+ + parameters{\r+ real alpha0;\r+ real alpha2;\r+ real alpha3;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ vector [nB] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma_B;\r+ }\r+ + model{\r+ real mu[n];\r+ + // Priors\r+ alpha0 ~ normal( 0 , 100 );\r+ alpha2 ~ normal( 0 , 100 );\r+ alpha3 ~ normal( 0 , 100 );\r+ beta ~ normal( 0 , sigma_B );\r+ sigma_B ~ cauchy( 0 , 25 );\r+ sigma ~ cauchy( 0 , 25 );\r+ + for ( i in 1:n ) {\r+ mu[i] = alpha0 + alpha2*A2[i] + + alpha3*A3[i] + beta[B[i]];\r+ }\r+ y ~ normal( mu , sigma );\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstan2String, con = \u0026quot;full2Model.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; A2 \u0026lt;- ifelse(data.nest$A==\u0026#39;a2\u0026#39;,1,0)\r\u0026gt; A3 \u0026lt;- ifelse(data.nest$A==\u0026#39;a3\u0026#39;,1,0)\r\u0026gt; data.nest.list \u0026lt;- with(data.nest, list(y=y, A2=A2, A3=A3, B=as.numeric(Sites),\r+ n=nrow(data.nest), nB=length(levels(Sites))))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;alpha0\u0026quot;,\u0026quot;alpha2\u0026quot;,\u0026quot;alpha3\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma_B\u0026quot;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\rNow run the STAN code via the rstan interface.\n\u0026gt; data.nest.rstan.c2 \u0026lt;- stan(data = data.nest.list, file = \u0026quot;full2Model.stan\u0026quot;, chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;full2Model\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 1: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 1: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 1: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 1: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 1: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 1: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 1: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 1: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 1: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 1: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 1: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 2.155 seconds (Warm-up)\rChain 1: 1.313 seconds (Sampling)\rChain 1: 3.468 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;full2Model\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 2: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 2: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 2: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 2: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 2: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 2: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 2: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 2: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 2: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 2: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 2: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 2.344 seconds (Warm-up)\rChain 2: 1.687 seconds (Sampling)\rChain 2: 4.031 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.nest.rstan.c2, par = c(\u0026quot;alpha0\u0026quot;, \u0026quot;alpha2\u0026quot;, \u0026quot;alpha3\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;sigma_B\u0026quot;))\rInference for Stan model: full2Model.\r2 chains, each with iter=4500; warmup=3000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\ralpha0 42.24 0.19 5.22 32.07 38.86 42.23 45.49 52.71 754 1\ralpha2 27.74 0.24 7.22 12.83 23.19 27.95 32.39 41.86 930 1\ralpha3 41.08 0.26 7.57 26.40 36.24 40.91 45.91 56.62 832 1\rsigma 5.06 0.01 0.31 4.49 4.84 5.04 5.26 5.70 1614 1\rsigma_B 11.50 0.07 2.62 7.48 9.61 11.18 12.87 17.56 1473 1\rSamples were drawn using NUTS(diag_e) at Wed Feb 19 18:55:27 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; data.nest.rstan.c2.df \u0026lt;-as.data.frame(extract(data.nest.rstan.c2))\r\u0026gt; head(data.nest.rstan.c2.df)\ralpha0 alpha2 alpha3 sigma sigma_B lp__\r1 43.44920 25.05917 38.82597 4.987000 8.342916 -352.4477\r2 37.88972 26.37229 52.49395 5.060172 14.993315 -359.3865\r3 44.42550 31.08944 44.58649 4.560923 12.629643 -356.8136\r4 44.91839 23.67947 47.86579 5.243758 11.344471 -360.4625\r5 36.77499 30.43252 49.29206 4.755335 12.472457 -360.6371\r6 41.89213 22.74346 43.71447 5.184679 10.054922 -356.3188\r\rMatrix parameterisation\r\u0026gt; rstanString2=\u0026quot;\r+ data{\r+ int n;\r+ int nB;\r+ int nA;\r+ vector [n] y;\r+ matrix [n,nA] X;\r+ int B[n];\r+ vector [nA] a0;\r+ matrix [nA,nA] A0;\r+ }\r+ + parameters{\r+ vector [nA] alpha;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ vector [nB] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma_B;\r+ }\r+ + model{\r+ real mu[n];\r+ + // Priors\r+ //alpha ~ normal( 0 , 100 );\r+ alpha ~ multi_normal(a0,A0);\r+ beta ~ normal( 0 , sigma_B );\r+ sigma_B ~ cauchy( 0 , 25);\r+ sigma ~ cauchy( 0 , 25 );\r+ + for ( i in 1:n ) {\r+ mu[i] = dot_product(X[i],alpha) + beta[B[i]];\r+ }\r+ y ~ normal( mu , sigma );\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstanString2, con = \u0026quot;matrixModel.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; X \u0026lt;- model.matrix(~A, data.nest)\r\u0026gt; nA \u0026lt;- ncol(X)\r\u0026gt; data.nest.list \u0026lt;- with(data.nest, list(y=y, X=X, B=as.numeric(Sites),\r+ n=nrow(data.nest), nB=length(levels(Sites)), nA=nA,\r+ a0=rep(0,nA), A0=diag(100000,nA)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;alpha\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma_B\u0026quot;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\rNow run the STAN code via the rstan interface.\n\u0026gt; data.nest.rstan.m \u0026lt;- stan(data = data.nest.list, file = \u0026quot;matrixModel.stan\u0026quot;, chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;matrixModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 1: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 1: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 1: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 1: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 1: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 1: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 1: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 1: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 1: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 1: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 1: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 8.409 seconds (Warm-up)\rChain 1: 6.17 seconds (Sampling)\rChain 1: 14.579 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;matrixModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 2: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 2: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 2: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 2: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 2: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 2: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 2: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 2: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 2: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 2: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 2: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 9.595 seconds (Warm-up)\rChain 2: 6.436 seconds (Sampling)\rChain 2: 16.031 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.nest.rstan.m, par = c(\u0026quot;alpha\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;sigma_B\u0026quot;))\rInference for Stan model: matrixModel.\r2 chains, each with iter=4500; warmup=3000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\ralpha[1] 42.36 0.16 5.19 31.70 39.08 42.45 45.71 52.79 1069 1\ralpha[2] 27.59 0.23 7.45 13.12 22.64 27.47 32.31 43.07 1037 1\ralpha[3] 41.00 0.23 7.53 25.73 36.29 41.13 45.57 55.83 1083 1\rsigma 5.04 0.01 0.31 4.48 4.83 5.02 5.22 5.69 2053 1\rsigma_B 11.51 0.07 2.61 7.71 9.66 11.09 12.97 17.82 1545 1\rSamples were drawn using NUTS(diag_e) at Wed Feb 19 18:56:43 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; data.nest.rstan.m.df \u0026lt;-as.data.frame(extract(data.nest.rstan.m))\r\u0026gt; head(data.nest.rstan.m.df)\ralpha.1 alpha.2 alpha.3 sigma sigma_B lp__\r1 41.40841 32.47143 57.93674 4.737729 11.199844 -364.6446\r2 39.59351 24.92710 43.71482 5.359000 9.763771 -358.0532\r3 51.54847 28.01489 31.29953 5.424494 10.611593 -357.9738\r4 33.53023 43.54390 53.51006 4.879110 12.370616 -351.2236\r5 41.44793 19.43188 41.36733 5.279320 10.059042 -360.4873\r6 31.05092 29.80598 56.34603 4.882690 15.141476 -358.0164\r\rHierarchical parameterisation\r\u0026gt; rstanString3=\u0026quot;\r+ data{\r+ int n;\r+ int nA;\r+ int nSites;\r+ vector [n] y;\r+ matrix [nSites,nA] X;\r+ matrix [n,nSites] Z;\r+ }\r+ + parameters{\r+ vector[nA] beta;\r+ vector[nSites] gamma;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ real\u0026lt;lower=0\u0026gt; sigma_S;\r+ + }\r+ + model{\r+ vector [n] mu_site;\r+ vector [nSites] mu;\r+ + // Priors\r+ beta ~ normal( 0 , 1000 );\r+ gamma ~ normal( 0 , 1000 );\r+ sigma ~ cauchy( 0 , 25 );\r+ sigma_S~ cauchy( 0 , 25 );\r+ + mu_site = Z*gamma;\r+ y ~ normal( mu_site , sigma );\r+ mu = X*beta;\r+ gamma ~ normal(mu, sigma_S);\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstanString3, con = \u0026quot;hierarchicalModel.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; dt.A \u0026lt;- ddply(data.nest,~Sites,catcolwise(unique))\r\u0026gt; X\u0026lt;-model.matrix(~A, dt.A)\r\u0026gt; Z\u0026lt;-model.matrix(~Sites-1, data.nest)\r\u0026gt; data.nest.list \u0026lt;- list(y=data.nest$y, X=X, Z=Z, n=nrow(data.nest),\r+ nSites=nrow(X),nA=ncol(X))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;sigma_S\u0026quot;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\rNow run the STAN code via the rstan interface.\n\u0026gt; data.nest.rstan.h \u0026lt;- stan(data = data.nest.list, file = \u0026quot;hierarchicalModel.stan\u0026quot;, chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;hierarchicalModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 1: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 1: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 1: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 1: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 1: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 1: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 1: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 1: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 1: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 1: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 1: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.668 seconds (Warm-up)\rChain 1: 0.234 seconds (Sampling)\rChain 1: 0.902 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;hierarchicalModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 2: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 2: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 2: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 2: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 2: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 2: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 2: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 2: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 2: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 2: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 2: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.703 seconds (Warm-up)\rChain 2: 0.328 seconds (Sampling)\rChain 2: 1.031 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.nest.rstan.h, par = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;sigma_S\u0026quot;))\rInference for Stan model: hierarchicalModel.\r2 chains, each with iter=4500; warmup=3000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 42.00 0.12 5.42 31.11 38.53 41.97 45.38 52.92 1950 1\rbeta[2] 27.78 0.16 7.78 12.51 22.96 27.60 32.61 43.48 2223 1\rbeta[3] 41.25 0.16 7.60 25.98 36.43 41.22 46.13 56.47 2297 1\rsigma 5.05 0.01 0.31 4.51 4.83 5.03 5.24 5.69 3535 1\rsigma_S 11.60 0.06 2.77 7.65 9.69 11.12 13.00 18.21 2293 1\rSamples were drawn using NUTS(diag_e) at Wed Feb 19 18:57:31 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; data.nest.rstan.h.df \u0026lt;-as.data.frame(extract(data.nest.rstan.h))\r\u0026gt; head(data.nest.rstan.h.df)\rbeta.1 beta.2 beta.3 sigma sigma_S lp__\r1 46.23354 28.84100 35.42376 5.036956 11.894690 -358.2262\r2 45.88700 23.99846 40.07971 5.376392 10.841072 -354.2937\r3 49.82263 21.87036 29.82862 5.212592 13.817988 -356.6391\r4 52.75362 19.51831 26.12392 5.023215 11.063866 -357.1474\r5 49.27716 26.58538 25.05783 5.633089 13.477152 -357.9141\r6 46.81798 29.23605 31.25244 4.968886 8.497519 -353.4571\rIf you want to include finite-population standard deviations in the model you can use the following code.\n\u0026gt; rstanString4=\u0026quot;\r+ data{\r+ int n;\r+ int nA;\r+ int nSites;\r+ vector [n] y;\r+ matrix [nSites,nA] X;\r+ matrix [n,nSites] Z;\r+ }\r+ + parameters{\r+ vector[nA] beta;\r+ vector[nSites] gamma;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ real\u0026lt;lower=0\u0026gt; sigma_S;\r+ + }\r+ + model{\r+ vector [n] mu_site;\r+ vector [nSites] mu;\r+ + // Priors\r+ beta ~ normal( 0 , 1000 );\r+ gamma ~ normal( 0 , 1000 );\r+ sigma ~ cauchy( 0 , 25 );\r+ sigma_S~ cauchy( 0 , 25 );\r+ + mu_site = Z*gamma;\r+ y ~ normal( mu_site , sigma );\r+ mu = X*beta;\r+ gamma ~ normal(mu, sigma_S);\r+ }\r+ + generated quantities {\r+ vector [n] mu_site;\r+ vector [nSites] mu;\r+ vector [n] y_err;\r+ real sd_y;\r+ vector [nSites] mu_site_err;\r+ real sd_site;\r+ real sd_A;\r+ + mu_site = Z*gamma;\r+ y_err = mu_site - y;\r+ sd_y = sd(y_err);\r+ + mu = X*beta;\r+ mu_site_err = mu - gamma;\r+ sd_site = sd(mu_site_err);\r+ + sd_A = sd(beta);\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstanString4, con = \u0026quot;SDModel.stan\u0026quot;)\r\u0026gt; \u0026gt; #data list\r\u0026gt; dt.A \u0026lt;- ddply(data.nest,~Sites,catcolwise(unique))\r\u0026gt; X\u0026lt;-model.matrix(~A, dt.A)\r\u0026gt; Z\u0026lt;-model.matrix(~Sites-1, data.nest)\r\u0026gt; data.nest.list \u0026lt;- list(y=data.nest$y, X=X, Z=Z, n=nrow(data.nest),\r+ nSites=nrow(X),nA=ncol(X))\r\u0026gt; \u0026gt; #parameters and chain details\r\u0026gt; params \u0026lt;- c(\u0026#39;beta\u0026#39;,\u0026#39;sigma\u0026#39;,\u0026#39;sigma_S\u0026#39;,\u0026#39;sd_A\u0026#39;,\u0026#39;sd_site\u0026#39;,\u0026#39;sd_y\u0026#39;)\r\u0026gt; adaptSteps = 1000\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; data.nest.rstan.SD \u0026lt;- stan(data = data.nest.list, file = \u0026quot;SDModel.stan\u0026quot;, chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;SDModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 1: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 1: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 1: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 1: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 1: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 1: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 1: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 1: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 1: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 1: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 1: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.853 seconds (Warm-up)\rChain 1: 0.344 seconds (Sampling)\rChain 1: 1.197 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;SDModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 2: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 2: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 2: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 2: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 2: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 2: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 2: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 2: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 2: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 2: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 2: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.703 seconds (Warm-up)\rChain 2: 0.433 seconds (Sampling)\rChain 2: 1.136 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.nest.rstan.SD, par = c(\u0026#39;beta\u0026#39;,\u0026#39;sigma\u0026#39;,\u0026#39;sigma_S\u0026#39;,\u0026#39;sd_A\u0026#39;,\u0026#39;sd_site\u0026#39;,\u0026#39;sd_y\u0026#39;))\rInference for Stan model: SDModel.\r2 chains, each with iter=4500; warmup=3000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 42.04 0.13 5.72 29.86 38.36 42.12 45.76 53.10 2010 1\rbeta[2] 27.62 0.16 8.05 12.00 22.54 27.60 32.69 43.90 2627 1\rbeta[3] 41.07 0.17 8.06 25.43 36.00 41.02 46.12 57.14 2344 1\rsigma 5.04 0.00 0.31 4.48 4.82 5.02 5.24 5.69 4394 1\rsigma_S 11.70 0.06 2.81 7.63 9.75 11.25 13.12 18.91 2345 1\rsd_A 10.44 0.10 4.59 2.83 7.28 9.90 13.05 21.10 1968 1\rsd_site 10.77 0.03 1.22 9.20 9.99 10.50 11.24 13.94 1343 1\rsd_y 5.00 0.00 0.10 4.85 4.93 4.98 5.05 5.21 1220 1\rSamples were drawn using NUTS(diag_e) at Wed Feb 19 18:58:19 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; data.nest.rstan.SD.df \u0026lt;-as.data.frame(extract(data.nest.rstan.SD))\r\u0026gt; head(data.nest.rstan.SD.df)\rbeta.1 beta.2 beta.3 sigma sigma_S sd_A sd_site sd_y\r1 36.80986 28.70439 51.46066 5.609824 7.880194 11.533954 10.530131 4.955827\r2 43.21227 24.45470 36.88674 4.686372 12.168378 9.543014 9.421704 4.951786\r3 44.79516 23.02770 33.81295 5.312964 13.073682 10.883877 9.541595 5.250076\r4 37.08413 32.27076 52.57465 4.924647 13.462061 10.609527 10.778432 5.041660\r5 47.78030 20.76704 31.80325 4.758630 11.249078 13.581725 11.035733 4.947235\r6 47.58350 19.70850 37.81088 5.630886 12.222762 14.143406 10.171348 5.088340\rlp__\r1 -356.9475\r2 -354.0613\r3 -361.8290\r4 -357.0632\r5 -353.8392\r6 -360.0825\r\r\rData generation - second example\rNow imagine a similar experiment in which we intend to measure a response (\\(y\\)) to one of treatments (three levels; “a1”, “a2” and “a3”). As with the previous design, we decided to establish a nested design in which there are sub-replicate (\\(1\\)m Quadrats) within each Site. In the current design, we have decided to further sub-replicate. Within each of the \\(5\\) Quadrats, we are going to randomly place \\(2\\times10\\)cm pit traps. Now we have Sites nested within Treatments, Quadrats nested within Sites AND, Pits nested within Sites. The latter of these (Pits nested within Sites) are the observations (\\(y\\)). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(123)\r\u0026gt; nTreat \u0026lt;- 3\r\u0026gt; nSites \u0026lt;- 15\r\u0026gt; nSitesPerTreat \u0026lt;- nSites/nTreat\r\u0026gt; nQuads \u0026lt;- 5\r\u0026gt; nPits \u0026lt;- 2\r\u0026gt; site.sigma \u0026lt;- 10 # sd within between sites within treatment\r\u0026gt; quad.sigma \u0026lt;- 10\r\u0026gt; sigma \u0026lt;- 7.5\r\u0026gt; n \u0026lt;- nSites * nQuads * nPits\r\u0026gt; sites \u0026lt;- gl(n=nSites,n/nSites,n, lab=paste(\u0026quot;site\u0026quot;,1:nSites))\r\u0026gt; A \u0026lt;- gl(nTreat, n/nTreat, n, labels=c(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;))\r\u0026gt; a.means \u0026lt;- c(40,70,80)\r\u0026gt; \u0026gt; #A\u0026lt;-gl(nTreat,nSites/nTreat,nSites,labels=c(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;))\r\u0026gt; a\u0026lt;-gl(nTreat,1,nTreat,labels=c(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;))\r\u0026gt; a.X \u0026lt;- model.matrix(~a, expand.grid(a))\r\u0026gt; a.eff \u0026lt;- as.vector(solve(a.X,a.means))\r\u0026gt; site.means \u0026lt;- rnorm(nSites,a.X %*% a.eff,site.sigma)\r\u0026gt; \u0026gt; A \u0026lt;- gl(nTreat,nSites/nTreat,nSites,labels=c(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;))\r\u0026gt; A.X \u0026lt;- model.matrix(~A, expand.grid(A))\r\u0026gt; #a.X \u0026lt;- model.matrix(~A, expand.grid(A=gl(nTreat,nSites/nTreat,nSites,labels=c(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;))))\r\u0026gt; site.means \u0026lt;- rnorm(nSites,A.X %*% a.eff,site.sigma)\r\u0026gt; \u0026gt; SITES \u0026lt;- gl(nSites,(nSites*nQuads)/nSites,nSites*nQuads,labels=paste(\u0026#39;site\u0026#39;,1:nSites))\r\u0026gt; sites.X \u0026lt;- model.matrix(~SITES-1)\r\u0026gt; quad.means \u0026lt;- rnorm(nSites*nQuads,sites.X %*% site.means,quad.sigma)\r\u0026gt; \u0026gt; #SITES \u0026lt;- gl(nSites,1,nSites,labels=paste(\u0026#39;site\u0026#39;,1:nSites))\r\u0026gt; #sites.X \u0026lt;- model.matrix(~SITES-1)\r\u0026gt; #quad.means \u0026lt;- rnorm(nSites*nQuads,sites.X %*% site.means,quad.sigma)\r\u0026gt; \u0026gt; QUADS \u0026lt;- gl(nSites*nQuads,n/(nSites*nQuads),n,labels=paste(\u0026#39;quad\u0026#39;,1:(nSites*nQuads)))\r\u0026gt; quads.X \u0026lt;- model.matrix(~QUADS-1)\r\u0026gt; #quads.eff \u0026lt;- as.vector(solve(quads.X,quad.means))\r\u0026gt; #pit.means \u0026lt;- rnorm(n,quads.eff %*% t(quads.X),sigma)\r\u0026gt; pit.means \u0026lt;- rnorm(n,quads.X %*% quad.means,sigma)\r\u0026gt; \u0026gt; PITS \u0026lt;- gl(nPits*nSites*nQuads,1, n, labels=paste(\u0026#39;pit\u0026#39;,1:(nPits*nSites*nQuads)))\r\u0026gt; data.nest1\u0026lt;-data.frame(Pits=PITS,Quads=QUADS,Sites=rep(SITES,each=2), A=rep(A,each=nQuads*nPits),y=pit.means)\r\u0026gt; #data.nest1\u0026lt;-data.nest1[order(data.nest1$A,data.nest1$Sites,data.nest1$Quads),]\r\u0026gt; head(data.nest1) #print out the first six rows of the data set\rPits Quads Sites A y\r1 pit 1 quad 1 site 1 a1 61.79607\r2 pit 2 quad 1 site 1 a1 56.24699\r3 pit 3 quad 2 site 1 a1 42.40885\r4 pit 4 quad 2 site 1 a1 52.06672\r5 pit 5 quad 3 site 1 a1 73.71286\r6 pit 6 quad 3 site 1 a1 62.50529\r\u0026gt; \u0026gt; ggplot(data.nest1, aes(y=y, x=1)) + geom_boxplot() + facet_grid(.~Quads)\rExploratory data analysis\rNormality and Homogeneity of variance\n\u0026gt; #Effects of treatment\r\u0026gt; boxplot(y~A, ddply(data.nest1, ~A+Sites,numcolwise(mean, na.rm=T)))\r\u0026gt; \u0026gt; #Site effects\r\u0026gt; boxplot(y~Sites, ddply(data.nest1, ~A+Sites+Quads,numcolwise(mean, na.rm=T)))\r\u0026gt; \u0026gt; #Quadrat effects\r\u0026gt; boxplot(y~Quads, ddply(data.nest1, ~A+Sites+Quads+Pits,numcolwise(mean, na.rm=T)))\rConclusions:\n\rthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\n\rthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity.\n\rit is a little difficult to assess normality/homogeneity of variance of quadrats since there are only two pits per quadrat. Nevertheless, there is no suggestion that variance increases with increasing mean.\n\r\rObvious violations could be addressed either by:\n\rtransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\r\r\r\rModel fitting\rFrequentist for comparison\r\u0026gt; library(nlme)\r\u0026gt; d.lme \u0026lt;- lme(y ~ A, random=~1|Sites/Quads,data=data.nest1)\r\u0026gt; summary(d.lme)\rLinear mixed-effects model fit by REML\rData: data.nest1 AIC BIC logLik\r1137.994 1155.937 -562.997\rRandom effects:\rFormula: ~1 | Sites\r(Intercept)\rStdDev: 10.38248\rFormula: ~1 | Quads %in% Sites\r(Intercept) Residual\rStdDev: 8.441615 7.161178\rFixed effects: y ~ A Value Std.Error DF t-value p-value\r(Intercept) 41.38646 5.04334 75 8.206160 0.0000\rAa2 21.36271 7.13236 12 2.995181 0.0112\rAa3 39.14584 7.13236 12 5.488483 0.0001\rCorrelation: (Intr) Aa2 Aa2 -0.707 Aa3 -0.707 0.500\rStandardized Within-Group Residuals:\rMin Q1 Med Q3 Max -2.11852493 -0.54600763 -0.03428569 0.53382444 2.26256381 Number of Observations: 150\rNumber of Groups: Sites Quads %in% Sites 15 75 \u0026gt; \u0026gt; anova(d.lme)\rnumDF denDF F-value p-value\r(Intercept) 1 75 446.9152 \u0026lt;.0001\rA 2 12 15.1037 5e-04\r\rFull effect parameterisation\r\u0026gt; rstanString=\u0026quot;\r+ data{\r+ int n;\r+ int nSite;\r+ int nQuad;\r+ vector [n] y;\r+ int A2[n];\r+ int A3[n];\r+ int Site[n];\r+ int Quad[n];\r+ }\r+ + parameters{\r+ real alpha0;\r+ real alpha2;\r+ real alpha3;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ vector [nSite] beta_Site;\r+ real\u0026lt;lower=0\u0026gt; sigma_Site;\r+ vector [nQuad] beta_Quad;\r+ real\u0026lt;lower=0\u0026gt; sigma_Quad;\r+ }\r+ + model{\r+ real mu[n];\r+ + // Priors\r+ alpha0 ~ normal( 0 , 100 );\r+ alpha2 ~ normal( 0 , 100 );\r+ alpha3 ~ normal( 0 , 100 );\r+ beta_Site~ normal( 0 , sigma_Site );\r+ sigma_Site ~ cauchy( 0 , 25 );\r+ beta_Quad~ normal( 0 , sigma_Quad );\r+ sigma_Quad ~ cauchy( 0 , 25 );\r+ sigma ~ cauchy( 0 , 25 );\r+ + for ( i in 1:n ) {\r+ mu[i] = alpha0 + alpha2*A2[i] + + alpha3*A3[i] + beta_Site[Site[i]] + beta_Quad[Quad[i]];\r+ }\r+ y ~ normal( mu , sigma );\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstanString, con = \u0026quot;fullModel2.stan\u0026quot;)\r\u0026gt; \u0026gt; A2 \u0026lt;- ifelse(data.nest1$A==\u0026#39;a2\u0026#39;,1,0)\r\u0026gt; A3 \u0026lt;- ifelse(data.nest1$A==\u0026#39;a3\u0026#39;,1,0)\r\u0026gt; data.nest.list \u0026lt;- with(data.nest1, list(y=y, A2=A2, A3=A3, Site=as.numeric(Sites),\r+ n=nrow(data.nest1), nSite=length(levels(Sites)),\r+ nQuad=length(levels(Quads)), Quad=as.numeric(Quads)))\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;alpha0\u0026#39;,\u0026#39;alpha2\u0026#39;,\u0026#39;alpha3\u0026#39;,\u0026#39;sigma\u0026#39;,\u0026#39;sigma_Site\u0026#39;, \u0026#39;sigma_Quad\u0026#39;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; data.nest1.rstan.f \u0026lt;- stan(data = data.nest.list, file = \u0026quot;fullModel2.stan\u0026quot;, chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;fullModel2\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 1: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 1: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 1: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 1: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 1: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 1: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 1: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 1: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 1: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 1: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 1: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 3.327 seconds (Warm-up)\rChain 1: 4.345 seconds (Sampling)\rChain 1: 7.672 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;fullModel2\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 2: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 2: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 2: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 2: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 2: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 2: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 2: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 2: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 2: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 2: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 2: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 4.045 seconds (Warm-up)\rChain 2: 4.161 seconds (Sampling)\rChain 2: 8.206 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.nest1.rstan.f, par = c(\u0026#39;alpha0\u0026#39;,\u0026#39;alpha2\u0026#39;,\u0026#39;alpha3\u0026#39;,\u0026#39;sigma\u0026#39;,\u0026#39;sigma_Site\u0026#39;, \u0026#39;sigma_Quad\u0026#39;))\rInference for Stan model: fullModel2.\r2 chains, each with iter=4500; warmup=3000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\ralpha0 41.21 0.12 5.53 30.40 37.59 41.13 44.76 52.32 2302 1\ralpha2 21.55 0.17 7.92 5.66 16.63 21.47 26.71 37.38 2167 1\ralpha3 38.98 0.16 7.60 23.46 34.19 39.32 43.83 53.42 2249 1\rsigma 7.29 0.02 0.61 6.19 6.86 7.25 7.67 8.59 1527 1\rsigma_Site 11.26 0.08 3.03 6.67 9.13 10.87 12.93 18.76 1571 1\rsigma_Quad 8.58 0.03 1.14 6.51 7.81 8.53 9.32 11.04 1656 1\rSamples were drawn using NUTS(diag_e) at Wed Feb 19 18:59:26 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; data.nest1.rstan.f.df \u0026lt;-as.data.frame(extract(data.nest1.rstan.f))\r\u0026gt; head(data.nest1.rstan.f.df)\ralpha0 alpha2 alpha3 sigma sigma_Site sigma_Quad lp__\r1 40.99894 18.4408136 42.31045 7.968007 8.827050 8.460864 -613.2249\r2 53.13787 0.7837646 25.07146 7.354129 20.331472 7.205399 -607.1231\r3 37.57365 19.9305037 39.18817 7.851311 6.937956 9.572309 -607.2551\r4 44.25290 26.5530610 41.77012 7.238055 11.726263 10.298161 -608.2911\r5 43.04766 30.5539155 30.54624 7.949620 9.587351 9.579669 -615.0313\r6 45.62270 15.5490676 23.57169 7.407616 15.972144 7.886300 -610.6864\r\rMatrix parameterisation\r\u0026gt; rstanString2=\u0026quot;\r+ data{\r+ int n;\r+ int nSite;\r+ int nQuad;\r+ int nA;\r+ vector [n] y;\r+ matrix [n,nA] X;\r+ int Site[n];\r+ int Quad[n];\r+ vector [nA] a0;\r+ matrix [nA,nA] A0;\r+ }\r+ + parameters{\r+ vector [nA] alpha;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ vector [nSite] beta_Site;\r+ real\u0026lt;lower=0\u0026gt; sigma_Site;\r+ vector [nQuad] beta_Quad;\r+ real\u0026lt;lower=0\u0026gt; sigma_Quad;\r+ }\r+ + model{\r+ real mu[n];\r+ + // Priors\r+ //alpha ~ normal( 0 , 100 );\r+ alpha ~ multi_normal(a0,A0);\r+ beta_Site ~ normal( 0 , sigma_Site );\r+ sigma_Site ~ cauchy( 0 , 25);\r+ beta_Quad ~ normal( 0 , sigma_Quad );\r+ sigma_Quad ~ cauchy( 0 , 25);\r+ sigma ~ cauchy( 0 , 25 );\r+ + for ( i in 1:n ) {\r+ mu[i] = dot_product(X[i],alpha) + beta_Site[Site[i]] + beta_Quad[Quad[i]];\r+ }\r+ y ~ normal( mu , sigma );\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(rstanString2, con = \u0026quot;matrixModel2.stan\u0026quot;)\r\u0026gt; \u0026gt; X \u0026lt;- model.matrix(~A, data.nest)\r\u0026gt; nA \u0026lt;- ncol(X)\r\u0026gt; data.nest.list \u0026lt;- with(data.nest1, list(y=y, X=X, Site=as.numeric(Sites),\r+ Quad=as.numeric(Quads),\r+ n=nrow(data.nest1), nSite=length(levels(Sites)),\r+ nQuad=length(levels(Quads)),\r+ nA=nA,\r+ a0=rep(0,nA), A0=diag(100000,nA)))\r\u0026gt; \u0026gt; params \u0026lt;- c(\u0026#39;alpha\u0026#39;,\u0026#39;sigma\u0026#39;,\u0026#39;sigma_Site\u0026#39;, \u0026#39;sigma_Quad\u0026#39;)\r\u0026gt; burnInSteps = 3000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\r\u0026gt; \u0026gt; data.nest1.rstan.m2 \u0026lt;- stan(data = data.nest.list, file = \u0026quot;matrixModel2.stan\u0026quot;, chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;matrixModel2\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 1: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 1: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 1: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 1: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 1: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 1: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 1: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 1: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 1: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 1: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 1: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 10.174 seconds (Warm-up)\rChain 1: 14.97 seconds (Sampling)\rChain 1: 25.144 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;matrixModel2\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 4500 [ 0%] (Warmup)\rChain 2: Iteration: 450 / 4500 [ 10%] (Warmup)\rChain 2: Iteration: 900 / 4500 [ 20%] (Warmup)\rChain 2: Iteration: 1350 / 4500 [ 30%] (Warmup)\rChain 2: Iteration: 1800 / 4500 [ 40%] (Warmup)\rChain 2: Iteration: 2250 / 4500 [ 50%] (Warmup)\rChain 2: Iteration: 2700 / 4500 [ 60%] (Warmup)\rChain 2: Iteration: 3001 / 4500 [ 66%] (Sampling)\rChain 2: Iteration: 3450 / 4500 [ 76%] (Sampling)\rChain 2: Iteration: 3900 / 4500 [ 86%] (Sampling)\rChain 2: Iteration: 4350 / 4500 [ 96%] (Sampling)\rChain 2: Iteration: 4500 / 4500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 13.182 seconds (Warm-up)\rChain 2: 10.54 seconds (Sampling)\rChain 2: 23.722 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.nest1.rstan.m2, par = c(\u0026#39;alpha\u0026#39;,\u0026#39;sigma\u0026#39;,\u0026#39;sigma_Site\u0026#39;, \u0026#39;sigma_Quad\u0026#39;))\rInference for Stan model: matrixModel2.\r2 chains, each with iter=4500; warmup=3000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\ralpha[1] 41.39 0.13 5.70 30.15 37.79 41.38 44.91 52.83 1882 1\ralpha[2] 21.32 0.17 7.96 5.50 16.25 21.52 26.33 36.79 2123 1\ralpha[3] 39.34 0.17 7.90 23.39 34.35 39.23 44.45 54.87 2103 1\rsigma 7.31 0.01 0.61 6.23 6.87 7.27 7.70 8.60 1917 1\rsigma_Site 11.43 0.07 3.03 6.65 9.35 11.01 13.05 18.81 1720 1\rsigma_Quad 8.54 0.03 1.13 6.44 7.76 8.52 9.28 10.89 1786 1\rSamples were drawn using NUTS(diag_e) at Wed Feb 19 19:01:01 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; data.nest1.rstan.m2.df \u0026lt;-as.data.frame(extract(data.nest1.rstan.m2))\r\u0026gt; head(data.nest1.rstan.m2.df)\ralpha.1 alpha.2 alpha.3 sigma sigma_Site sigma_Quad lp__\r1 44.95385 11.48209 37.04594 7.045977 17.024677 10.216341 -616.7436\r2 34.74513 32.79135 44.50707 7.688600 9.246435 8.342615 -604.2742\r3 39.99199 21.01827 41.38576 7.014567 13.747544 10.353393 -606.2766\r4 41.24520 30.12204 36.86309 6.652121 10.653882 8.080439 -588.9044\r5 47.31591 14.78743 29.89851 6.686367 8.986245 9.646120 -610.5245\r6 41.36124 16.08883 34.86754 6.258581 12.465772 10.688584 -606.7448\r\r\rReferences\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rGelman, Andrew, and others. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).” Bayesian Analysis 1 (3): 515–34.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\r\r\r","date":1581300794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581300794,"objectID":"b915ae6ed709fa80773dee12d3ae44d1","permalink":"/stan/nested-anova-stan/nested-anova-stan/","publishdate":"2020-02-09T21:13:14-05:00","relpermalink":"/stan/nested-anova-stan/nested-anova-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","mixed effects model","anova"],"title":"Nested Anova - STAN","type":"STAN"},{"authors":["Andrea Gabrio"],"categories":["news","Midia","PRIMENT","HEART"],"content":"The moment for the second edition of the HEART\u0026rsquo;s one-day introductory course to health economics arrived at last! The course, led by Rachael Hunter and called \u0026ldquo;Understanding health economics in clinical trials\u0026rdquo;, took place on Tuesday 11 February and was prepared in collaboration between the HEART group and the Institute of Clinical Trials and Methodology (ICTM). I believe this second edition of the course was a success both in terms of the quality/quantity of the material covered during the six sessions throughout the day, as well as in terms of the positive feedback we received from the participants. Also, this time a new HEART member (Marie) joined the group and very nicely delivered the session about patient reported outcome measures (PROMs), engaging in nice discussions with the audience.\nFinally, a couple of personal notes.\n I recently attended a very interesting meeting about missing data methodology which was held by an international group of very talented senior and junior statisticians from different universities, including people like Ian White and James Carpenter from UCL and the LSHTM. It was really an amazing experience to meet so many people working in different stats area but with a common passion about missing data methods (also mine!). From what I understood this series of meetings (called \u0026ldquo;MiDIA\u0026rdquo;) have been held since years but do not have a very regular schedule due to people being busy I guess, which makes totally sense. Not sure when the next one will be held but now I am definitely looking forward to the next meetings!   I would also like to highlight a recent tweet from UCL PRIMENT CTU  #Vacancy Come join our team of health economists! Closing date: 15 March 2020\nApply here: https://t.co/IUg8JIeANX #clinicaltrials #healtheconomics\n\u0026mdash; Priment Clinical Trials Unit (@PRIMENT_UCL) February 19, 2020  which advertises a new position as health economist in our HEART group for performing health economics using data from clinical trials. I would encourage anyone interested in some good applied health economic work to apply for this position. Deadline 15 March 2020.\n To conclude, I would also like to say that I have done some updates to this website. From the inclusion of new tutorials on the use of JAGS and STAN on different statistical topics, to a restyle of the website. In particular I had fun by playing around with some Markdown code to add new features, e.g. customised alert notes and emoji, for example. Somthing like this:  \u0026#9819; Queen's note! \u0026#9819; \r\r\rI 👍 $\\rm \\LaTeX$ very much.\r\r\r\rThis took me so much time but I am quite satisfied with the result if I may say so. You really never stop learning new things!\n","date":1581292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581334443,"objectID":"1c19d34a10187c1d1d07db11e33f759e","permalink":"/post/update2-february/","publishdate":"2020-02-10T00:00:00Z","relpermalink":"/post/update2-february/","section":"post","summary":"The moment for the second edition of the HEART\u0026rsquo;s one-day introductory course to health economics arrived at last! The course, led by Rachael Hunter and called \u0026ldquo;Understanding health economics in clinical trials\u0026rdquo;, took place on Tuesday 11 February and was prepared in collaboration between the HEART group and the Institute of Clinical Trials and Methodology (ICTM). I believe this second edition of the course was a success both in terms of the quality/quantity of the material covered during the six sessions throughout the day, as well as in terms of the positive feedback we received from the participants.","tags":["News","Academic"],"title":"Something old, something new, something borrowed, something blue","type":"post"},{"authors":["Andrea Gabrio"],"categories":["R","autocorrelation","JAGS"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rUp until now (in the proceeding tutorials), the focus has been on models that adhere to specific assumptions about the underlying populations (and data). Indeed, both before and immediately after fitting these models, I have stressed the importance of evaluating and validating the proposed and fitted models to ensure reliability of the models. It is now worth us revisiting those fundamental assumptions as well as exploring the options that are available when the populations (data) do not conform. Let’s explore a simple linear regression model to see how each of the assumptions relate to the model.\n\\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0, \\sigma^2).\\]\nThe above simple statistical model models the linear relationship of \\(y_i\\) against \\(x_i\\). The residuals (\\(\\epsilon\\)) are assumed to be normally distributed with a mean of zero and a constant (yet unknown) variance (\\(\\sigma\\), homogeneity of variance). The residuals (and thus observations) are also assumed to all be independent.\nHomogeneity of variance and independence are encapsulated within the single symbol for variance (\\(\\sigma^2\\)). In assuming equal variances and independence, we are actually making an assumption about the variance-covariance structure of the populations (and thus residuals). Specifically, we assume that all populations are equally varied and thus can be represented well by a single variance term (all diagonal values in a \\(N\\times N\\) covariance matrix are the same, \\(\\sigma^2\\)) and the covariances between each population are zero (off diagonals). In simple regression, each observation (data point) represents a single observation drawn (sampled) from an entire population of possible observations. The above covariance structure thus assumes that the covariance between each population (observation) is zero - that is, each observation is completely independent of each other observation. Whilst it is mathematically convenient when data conform to these conditions (normality, homogeneity of variance, independence and linearity), data often violate one or more of these assumptions. In the following, I want to discuss and explore the causes and options for dealing with non-compliance to each of these conditions. By gaining a better understanding of how the various model fitting engines perform their task, we are better equipped to accommodate aspects of the data that don’t otherwise conform to the simple regression assumptions. In this tutorial we specifically focus on the topic of heterogeneity of the variance.\nIn order that the estimated parameters represent the underlying populations in an unbiased manner, the residuals (and thus each each observation) must be independent. However, what if we were sampling a population over time and we were interested in investigating how changes in a response relate to changes in a predictor (such as rainfall). For any response that does not “reset” itself on a regular basis, the state of the population (the value of its response) at a given time is likely to be at least partly dependent on the state of the population at the sampling time before. We can further generalise the above into:\n\\[ y_i \\sim Dist(\\mu_i),\\]\nwhere \\(\\mu_i=\\boldsymbol X \\boldsymbol \\beta + \\boldsymbol Z \\boldsymbol \\gamma\\), with \\(\\boldsymbol X\\) and \\(\\boldsymbol \\beta\\) representing the fixed data structure and fixed effects, respectively, while with \\(\\boldsymbol Z\\) and \\(\\boldsymbol \\gamma\\) represent the varying data structure and varying effects, respectively. In simple regression, there are no “varying” effects, and thus:\n\\[ \\boldsymbol \\gamma \\sim MVN(\\boldsymbol 0, \\boldsymbol \\Sigma),\\]\nwhere \\(\\boldsymbol \\Sigma\\) is a variance-covariance matrix of the form\n\\[ \\boldsymbol \\Sigma = \\frac{\\sigma^2}{1-\\rho^2}\r\\begin{bmatrix}\r1 \u0026amp; \\rho^{\\phi_{1,2}} \u0026amp; \\ldots \u0026amp; \\rho^{\\phi_{1,n}} \\\\\r\\rho^{\\phi_{2,1}} \u0026amp; 1 \u0026amp; \\ldots \u0026amp; \\vdots\\\\\r\\vdots \u0026amp; \\ldots \u0026amp; 1 \u0026amp; \\vdots\\\\\r\\rho^{\\phi_{n,1}} \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; 1\r\\end{bmatrix}. \\]\nNotice that this introduces a very large number of additional parameters that require estimating: \\(\\sigma^2\\) (error variance), \\(\\rho\\) (base autocorrelation) and each of the individual covariances (\\(\\rho^{\\phi_{n,n}}\\)). Hence, there are always going to be more parameters to estimate than there are date avaiable to use to estimate these paramters. We typically make one of a number of alternative assumptions so as to make this task more manageable.\n\rWhen we assume that all residuals are independent (regular regression), i.e. \\(\\rho=0\\), \\(\\boldsymbol \\Sigma\\) is essentially equal to \\(\\sigma^2 \\boldsymbol I\\) and we simply use:\r\r\\[ \\boldsymbol \\gamma \\sim N( 0,\\sigma^2).\\]\n\rWe could assume there is a reasonably simple pattern of correlation that declines over time. The simplest of these is a first order autoregressive (AR1) structure in which exponent on the correlation declines linearly according to the time lag (\\(\\mid t - s\\mid\\)).\r\r\\[ \\boldsymbol \\Sigma = \\frac{\\sigma^2}{1-\\rho^2}\r\\begin{bmatrix}\r1 \u0026amp; \\rho \u0026amp; \\ldots \u0026amp; \\rho^{\\mid t-s \\mid} \\\\\r\\rho \u0026amp; 1 \u0026amp; \\ldots \u0026amp; \\vdots\\\\\r\\vdots \u0026amp; \\ldots \u0026amp; 1 \u0026amp; \\vdots\\\\\r\\rho^{\\mid t-s \\mid } \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; 1\r\\end{bmatrix}. \\]\nNote, in making this assumption, we are also assuming that the degree of correlation is dependent only on the lag and not on when the lag occurs (stationarity). That is all lag 1 residual pairs will have the same degree of correlation, all the lag \\(2\\) pairs will have the same correlation and so on.\n\r\rFirst order autocorrelation\rConsider an example, in which the number of individuals at time \\(2\\) will be partly dependent on the number of individuals present at time \\(1\\). Clearly then, the observations (and thus residuals) are not fully independent - there is an auto-regressive correlation dependency structure. We could accommodate this lack of independence by fitting a model that incorporates a AR1 variance-covariance structure. Alternatively, we fit the following model:\n\\[ y_{it} \\sim Dist(\\mu_{it}),\\]\nwhere\n\\[\\mu_{it}=\\boldsymbol X \\boldsymbol \\beta + \\rho \\epsilon_{i,t-1} + \\gamma_{it},\\]\nand where \\(\\gamma \\sim N(0, \\sigma^2)\\). In this version of the model, we are stating that the expected value of an observation is equal to the regular linear predictor plus the autocorrelation parameter (\\(\\rho\\)) multipled by the residual associated with the previous observation plus the regular independently distributed noise (\\(\\sigma^2\\)). Such a model is substantially faster to fit, although along with stationarity assumes in estimating the autocorrelation parameter, only the smallest lags are used. To see this in action, we will first generate some temporally auto-correlated data.\n\u0026gt; set.seed(126)\r\u0026gt; n = 50\r\u0026gt; a \u0026lt;- 20 #intercept\r\u0026gt; b \u0026lt;- 0.2 #slope\r\u0026gt; x \u0026lt;- round(runif(n, 1, n), 1) #values of the year covariate\r\u0026gt; year \u0026lt;- 1:n\r\u0026gt; sigma \u0026lt;- 20\r\u0026gt; rho \u0026lt;- 0.8\r\u0026gt; \u0026gt; library(nlme)\r\u0026gt; ## define a constructor for a first-order\r\u0026gt; ## correlation structure\r\u0026gt; ar1 \u0026lt;- corAR1(form = ~year, value = rho)\r\u0026gt; ## initialize this constructor against our data\r\u0026gt; AR1 \u0026lt;- Initialize(ar1, data = data.frame(year))\r\u0026gt; ## generate a correlation matrix\r\u0026gt; V \u0026lt;- corMatrix(AR1)\r\u0026gt; ## Cholesky factorization of V\r\u0026gt; Cv \u0026lt;- chol(V)\r\u0026gt; ## simulate AR1 errors\r\u0026gt; e \u0026lt;- t(Cv) %*% rnorm(n, 0, sigma) # cov(e) = V * sig^2\r\u0026gt; ## generate response\r\u0026gt; y \u0026lt;- a + b * x + e\r\u0026gt; data.temporalCor = data.frame(y = y, x = x, year = year)\r\u0026gt; write.table(data.temporalCor, file = \u0026quot;data.temporalCor.csv\u0026quot;,\r+ sep = \u0026quot;,\u0026quot;, quote = F, row.names = FALSE)\r\u0026gt; \u0026gt; pairs(data.temporalCor)\rWe will now proceed to analyse these data via both of the above techniques for JAGS:\n\rincorporating AR1 residual autocorrelation structure\n\rincorporating lagged residuals into the model\n\r\r\rIncorporating lagged residuals\rModel fitting\rWe proceed to code the model into JAGS (remember that in this software normal distribution are parameterised in terms of precisions \\(\\tau\\) rather than variances, where \\(\\tau=\\frac{1}{\\sigma^2}\\)). Define the model.\n\u0026gt; modelString = \u0026quot; + model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ fit[i] \u0026lt;- inprod(beta[],X[i,])\r+ y[i] ~ dnorm(mu[i],tau.cor)\r+ }\r+ e[1] \u0026lt;- (y[1] - fit[1])\r+ mu[1] \u0026lt;- fit[1]\r+ for (i in 2:n) {\r+ e[i] \u0026lt;- (y[i] - fit[i]) #- phi*e[i-1]\r+ mu[i] \u0026lt;- fit[i] + phi * e[i-1]\r+ }\r+ #Priors\r+ phi ~ dunif(-1,1)\r+ for (i in 1:nX) {\r+ beta[i] ~ dnorm(0,1.0E-6)\r+ }\r+ sigma \u0026lt;- z/sqrt(chSq) # prior for sigma; cauchy = normal/sqrt(chi^2)\r+ z ~ dnorm(0, 0.04)I(0,)\r+ chSq ~ dgamma(0.5, 0.5) # chi^2 with 1 d.f.\r+ tau \u0026lt;- pow(sigma, -2)\r+ tau.cor \u0026lt;- tau #* (1- phi*phi)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;tempModel.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; Xmat = model.matrix(~x, data.temporalCor)\r\u0026gt; data.temporalCor.list \u0026lt;- with(data.temporalCor, list(y = y, X = Xmat,\r+ n = nrow(data.temporalCor), nX = ncol(Xmat)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 10000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10000\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\u0026gt; library(R2jags)\rNow run the JAGS code via the R2jags interface.\n\u0026gt; data.temporalCor.r2jags \u0026lt;- jags(data = data.temporalCor.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;tempModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 50\rUnobserved stochastic nodes: 5\rTotal graph size: 413\rInitializing model\r\u0026gt; \u0026gt; print(data.temporalCor.r2jags)\rInference for Bugs model at \u0026quot;tempModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 30.841 11.858 8.852 22.556 30.505 38.559 55.177 1.001 10000\rbeta[2] 0.225 0.100 0.028 0.159 0.225 0.292 0.422 1.001 3800\rphi 0.913 0.054 0.793 0.879 0.919 0.954 0.994 1.001 3400\rsigma 12.133 1.253 9.967 11.253 12.034 12.902 14.828 1.001 7300\rdeviance 391.602 2.641 388.354 389.656 390.985 392.927 398.180 1.001 9200\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.5 and DIC = 395.1\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMCMC diagnostics\r\u0026gt; library(mcmcplots)\r\u0026gt; denplot(data.temporalCor.r2jags, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;))\r\u0026gt; traplot(data.temporalCor.r2jags, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;))\r\u0026gt; data.mcmc = as.mcmc(data.temporalCor.r2jags)\r\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(data.mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 2 3930 3746 1.05 beta[2] 2 3866 3746 1.03 deviance 2 3866 3746 1.03 phi 7 7397 3746 1.97 sigma 4 4636 3746 1.24 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 3 4062 3746 1.080 beta[2] 2 3620 3746 0.966 deviance 2 3803 3746 1.020 phi 6 6878 3746 1.840 sigma 4 4713 3746 1.260 \r\u0026gt; #Autocorrelation diagnostic\r\u0026gt; autocorr.diag(data.mcmc)\rbeta[1] beta[2] deviance phi sigma\rLag 0 1.000000000 1.000000000 1.000000000 1.000000000 1.000000000\rLag 1 0.174857318 -0.006205038 0.164212015 0.398270011 0.166634323\rLag 5 0.017823932 0.002140092 -0.016470982 0.017851360 0.011892997\rLag 10 0.004107514 0.010910488 0.020001216 -0.005693854 0.007020861\rLag 50 0.002176470 0.016102607 0.008360988 0.002061169 -0.007663541\rAll diagnostics seem fine.\n\rModel validation\rWhenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by sigma.\n\\[ Res_i = Y_i - \\mu_i\\]\n\\[ Res_{i+1} = Res_{i+1} - \\rho Res_i\\]\n\\[ Res_i = \\frac{Res_i}{\\sigma} \\]\n\u0026gt; mcmc = data.temporalCor.r2jags$BUGSoutput$sims.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.temporalCor\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = -1 * sweep(fit, 2, data.temporalCor$y, \u0026quot;-\u0026quot;)\r\u0026gt; n = ncol(resid)\r\u0026gt; resid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, \u0026quot;phi\u0026quot;])\r\u0026gt; resid = apply(resid, 2, median)/median(mcmc[, \u0026quot;sigma\u0026quot;])\r\u0026gt; fit = apply(fit, 2, median)\r\u0026gt; \u0026gt; library(ggplot2)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\r\u0026gt; \u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()\r\u0026gt; \u0026gt; ggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +\r+ geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;)\r\u0026gt; \u0026gt; plot(acf(resid, lag = 40))\rNo obvious autocorrelation or other issues with residuals remaining.\n\rParameter estimates\rExplore parameter estimates.\n\u0026gt; library(broom)\r\u0026gt; tidyMCMC(as.mcmc(data.temporalCor.r2jags), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 30.8 11.9 7.36 53.5 2 beta[2] 0.225 0.100 0.0321 0.425\r3 deviance 392. 2.64 388. 397. 4 phi 0.913 0.0537 0.813 1.000\r5 sigma 12.1 1.25 9.91 14.7 \r\r\rIncorporating AR1 residual autocorrelation structure\rModel fitting\rWe proceed to code the model into JAGS (remember that in this software normal distribution are parameterised in terms of precisions \\(\\tau\\) rather than variances, where \\(\\tau=\\frac{1}{\\sigma^2}\\)). Define the model.\n\u0026gt; modelString2 = \u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ mu[i] \u0026lt;- inprod(beta[],X[i,])\r+ }\r+ y[1:n] ~ dmnorm(mu[1:n],Omega)\r+ for (i in 1:n) {\r+ for (j in 1:n) {\r+ Sigma[i,j] \u0026lt;- sigma2*(equals(i,j) + (1-equals(i,j))*pow(phi,abs(i-j))) + }\r+ }\r+ Omega \u0026lt;- inverse(Sigma)\r+ + #Priors\r+ phi ~ dunif(-1,1)\r+ for (i in 1:nX) {\r+ beta[i] ~ dnorm(0,1.0E-6)\r+ }\r+ sigma \u0026lt;- z/sqrt(chSq) # prior for sigma; cauchy = normal/sqrt(chi^2)\r+ z ~ dnorm(0, 0.04)I(0,)\r+ chSq ~ dgamma(0.5, 0.5) # chi^2 with 1 d.f.\r+ sigma2 = pow(sigma,2)\r+ #tau.cor \u0026lt;- tau #* (1- phi*phi)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString2, con = \u0026quot;tempModel2.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; Xmat = model.matrix(~x, data.temporalCor)\r\u0026gt; data.temporalCor.list \u0026lt;- with(data.temporalCor, list(y = y, X = Xmat,\r+ n = nrow(data.temporalCor), nX = ncol(Xmat)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 5000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 10000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10000\rNow run the JAGS code via the R2jags interface.\n\u0026gt; data.temporalCor2.r2jags \u0026lt;- jags(data = data.temporalCor.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;tempModel2.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 1\rUnobserved stochastic nodes: 5\rTotal graph size: 5566\rInitializing model\r\u0026gt; \u0026gt; print(data.temporalCor2.r2jags)\rInference for Bugs model at \u0026quot;tempModel2.txt\u0026quot;, fit using jags,\r2 chains, each with 10000 iterations (first 5000 discarded)\rn.sims = 10000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 19.926 24.597 -19.141 9.722 18.990 29.365 64.348 1.014 10000\rbeta[2] 0.225 0.100 0.028 0.159 0.227 0.291 0.421 1.001 10000\rphi 0.890 0.055 0.773 0.854 0.895 0.930 0.980 1.011 160\rsigma 30.352 15.780 18.171 22.799 26.810 32.951 61.419 1.010 410\rdeviance 392.642 2.706 389.232 390.628 392.029 394.019 399.490 1.001 2900\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.7 and DIC = 396.3\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMCMC diagnostics\r\u0026gt; denplot(data.temporalCor2.r2jags, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;))\r\u0026gt; traplot(data.temporalCor2.r2jags, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;))\r\u0026gt; data.mcmc = as.mcmc(data.temporalCor2.r2jags)\r\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(data.mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 15 14982 3746 4.00 beta[2] 2 3866 3746 1.03 deviance 2 3995 3746 1.07 phi 9 9308 3746 2.48 sigma 8 10294 3746 2.75 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 4 4955 3746 1.320 beta[2] 2 3620 3746 0.966 deviance 2 3930 3746 1.050 phi 12 12162 3746 3.250 sigma 8 10644 3746 2.840 \r\u0026gt; #Autocorrelation diagnostic\r\u0026gt; autocorr.diag(data.mcmc)\rbeta[1] beta[2] deviance phi sigma\rLag 0 1.000000000 1.000000000 1.00000000 1.0000000 1.00000000\rLag 1 0.023745389 -0.007088969 0.19477040 0.8775299 0.95206712\rLag 5 0.019171996 0.008569178 0.08589717 0.5774327 0.80961727\rLag 10 -0.009155805 0.008682983 0.06468974 0.3677587 0.64495814\rLag 50 0.012167974 0.014954099 0.01686647 0.0317406 0.04466731\rAll diagnostics seem fine.\n\rModel validation\rWhenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by sigma.\n\\[ Res_i = Y_i - \\mu_i\\]\n\\[ Res_{i+1} = Res_{i+1} - \\rho Res_i\\]\n\\[ Res_i = \\frac{Res_i}{\\sigma} \\]\n\u0026gt; mcmc = data.temporalCor2.r2jags$BUGSoutput$sims.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.temporalCor\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = -1 * sweep(fit, 2, data.temporalCor$y, \u0026quot;-\u0026quot;)\r\u0026gt; n = ncol(resid)\r\u0026gt; resid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, \u0026quot;phi\u0026quot;])\r\u0026gt; resid = apply(resid, 2, median)/median(mcmc[, \u0026quot;sigma\u0026quot;])\r\u0026gt; fit = apply(fit, 2, median)\r\u0026gt; \u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\r\u0026gt; \u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()\r\u0026gt; \u0026gt; ggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +\r+ geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;)\r\u0026gt; \u0026gt; plot(acf(resid, lag = 40))\rNo obvious autocorrelation or other issues with residuals remaining\n\rParameter estimates\rExplore parameter estimates.\n\u0026gt; tidyMCMC(as.mcmc(data.temporalCor2.r2jags), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 19.9 24.6 -16.6 66.3 2 beta[2] 0.225 0.0997 0.0313 0.423\r3 deviance 393. 2.71 389. 398. 4 phi 0.890 0.0546 0.780 0.984\r5 sigma 30.4 15.8 16.2 51.2 \r\r\rReferences\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1581214394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581214394,"objectID":"f6b45dba87b1b50584d3c435c238c60d","permalink":"/jags/autocorrelation-jags/autocorrelation-jags/","publishdate":"2020-02-08T21:13:14-05:00","relpermalink":"/jags/autocorrelation-jags/autocorrelation-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","autocorrelation"],"title":"Temporal Autocorrelation - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","autocorrelation","STAN"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rUp until now (in the proceeding tutorials), the focus has been on models that adhere to specific assumptions about the underlying populations (and data). Indeed, both before and immediately after fitting these models, I have stressed the importance of evaluating and validating the proposed and fitted models to ensure reliability of the models. It is now worth us revisiting those fundamental assumptions as well as exploring the options that are available when the populations (data) do not conform. Let’s explore a simple linear regression model to see how each of the assumptions relate to the model.\n\\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0, \\sigma^2).\\]\nThe above simple statistical model models the linear relationship of \\(y_i\\) against \\(x_i\\). The residuals (\\(\\epsilon\\)) are assumed to be normally distributed with a mean of zero and a constant (yet unknown) variance (\\(\\sigma\\), homogeneity of variance). The residuals (and thus observations) are also assumed to all be independent.\nHomogeneity of variance and independence are encapsulated within the single symbol for variance (\\(\\sigma^2\\)). In assuming equal variances and independence, we are actually making an assumption about the variance-covariance structure of the populations (and thus residuals). Specifically, we assume that all populations are equally varied and thus can be represented well by a single variance term (all diagonal values in a \\(N\\times N\\) covariance matrix are the same, \\(\\sigma^2\\)) and the covariances between each population are zero (off diagonals). In simple regression, each observation (data point) represents a single observation drawn (sampled) from an entire population of possible observations. The above covariance structure thus assumes that the covariance between each population (observation) is zero - that is, each observation is completely independent of each other observation. Whilst it is mathematically convenient when data conform to these conditions (normality, homogeneity of variance, independence and linearity), data often violate one or more of these assumptions. In the following, I want to discuss and explore the causes and options for dealing with non-compliance to each of these conditions. By gaining a better understanding of how the various model fitting engines perform their task, we are better equipped to accommodate aspects of the data that don’t otherwise conform to the simple regression assumptions. In this tutorial we specifically focus on the topic of heterogeneity of the variance.\nIn order that the estimated parameters represent the underlying populations in an unbiased manner, the residuals (and thus each each observation) must be independent. However, what if we were sampling a population over time and we were interested in investigating how changes in a response relate to changes in a predictor (such as rainfall). For any response that does not “reset” itself on a regular basis, the state of the population (the value of its response) at a given time is likely to be at least partly dependent on the state of the population at the sampling time before. We can further generalise the above into:\n\\[ y_i \\sim Dist(\\mu_i),\\]\nwhere \\(\\mu_i=\\boldsymbol X \\boldsymbol \\beta + \\boldsymbol Z \\boldsymbol \\gamma\\), with \\(\\boldsymbol X\\) and \\(\\boldsymbol \\beta\\) representing the fixed data structure and fixed effects, respectively, while with \\(\\boldsymbol Z\\) and \\(\\boldsymbol \\gamma\\) represent the varying data structure and varying effects, respectively. In simple regression, there are no “varying” effects, and thus:\n\\[ \\boldsymbol \\gamma \\sim MVN(\\boldsymbol 0, \\boldsymbol \\Sigma),\\]\nwhere \\(\\boldsymbol \\Sigma\\) is a variance-covariance matrix of the form\n\\[ \\boldsymbol \\Sigma = \\frac{\\sigma^2}{1-\\rho^2}\r\\begin{bmatrix}\r1 \u0026amp; \\rho^{\\phi_{1,2}} \u0026amp; \\ldots \u0026amp; \\rho^{\\phi_{1,n}} \\\\\r\\rho^{\\phi_{2,1}} \u0026amp; 1 \u0026amp; \\ldots \u0026amp; \\vdots\\\\\r\\vdots \u0026amp; \\ldots \u0026amp; 1 \u0026amp; \\vdots\\\\\r\\rho^{\\phi_{n,1}} \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; 1\r\\end{bmatrix}. \\]\nNotice that this introduces a very large number of additional parameters that require estimating: \\(\\sigma^2\\) (error variance), \\(\\rho\\) (base autocorrelation) and each of the individual covariances (\\(\\rho^{\\phi_{n,n}}\\)). Hence, there are always going to be more parameters to estimate than there are date avaiable to use to estimate these paramters. We typically make one of a number of alternative assumptions so as to make this task more manageable.\n\rWhen we assume that all residuals are independent (regular regression), i.e. \\(\\rho=0\\), \\(\\boldsymbol \\Sigma\\) is essentially equal to \\(\\sigma^2 \\boldsymbol I\\) and we simply use:\r\r\\[ \\boldsymbol \\gamma \\sim N( 0,\\sigma^2).\\]\n\rWe could assume there is a reasonably simple pattern of correlation that declines over time. The simplest of these is a first order autoregressive (AR1) structure in which exponent on the correlation declines linearly according to the time lag (\\(\\mid t - s\\mid\\)).\r\r\\[ \\boldsymbol \\Sigma = \\frac{\\sigma^2}{1-\\rho^2}\r\\begin{bmatrix}\r1 \u0026amp; \\rho \u0026amp; \\ldots \u0026amp; \\rho^{\\mid t-s \\mid} \\\\\r\\rho \u0026amp; 1 \u0026amp; \\ldots \u0026amp; \\vdots\\\\\r\\vdots \u0026amp; \\ldots \u0026amp; 1 \u0026amp; \\vdots\\\\\r\\rho^{\\mid t-s \\mid } \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; 1\r\\end{bmatrix}. \\]\nNote, in making this assumption, we are also assuming that the degree of correlation is dependent only on the lag and not on when the lag occurs (stationarity). That is all lag 1 residual pairs will have the same degree of correlation, all the lag \\(2\\) pairs will have the same correlation and so on.\n\r\rFirst order autocorrelation\rConsider an example, in which the number of individuals at time \\(2\\) will be partly dependent on the number of individuals present at time \\(1\\). Clearly then, the observations (and thus residuals) are not fully independent - there is an auto-regressive correlation dependency structure. We could accommodate this lack of independence by fitting a model that incorporates a AR1 variance-covariance structure. Alternatively, we fit the following model:\n\\[ y_{it} \\sim Dist(\\mu_{it}),\\]\nwhere\n\\[\\mu_{it}=\\boldsymbol X \\boldsymbol \\beta + \\rho \\epsilon_{i,t-1} + \\gamma_{it},\\]\nand where \\(\\gamma \\sim N(0, \\sigma^2)\\). In this version of the model, we are stating that the expected value of an observation is equal to the regular linear predictor plus the autocorrelation parameter (\\(\\rho\\)) multipled by the residual associated with the previous observation plus the regular independently distributed noise (\\(\\sigma^2\\)). Such a model is substantially faster to fit, although along with stationarity assumes in estimating the autocorrelation parameter, only the smallest lags are used. To see this in action, we will first generate some temporally auto-correlated data.\n\u0026gt; set.seed(126)\r\u0026gt; n = 50\r\u0026gt; a \u0026lt;- 20 #intercept\r\u0026gt; b \u0026lt;- 0.2 #slope\r\u0026gt; x \u0026lt;- round(runif(n, 1, n), 1) #values of the year covariate\r\u0026gt; year \u0026lt;- 1:n\r\u0026gt; sigma \u0026lt;- 20\r\u0026gt; rho \u0026lt;- 0.8\r\u0026gt; \u0026gt; library(nlme)\r\u0026gt; ## define a constructor for a first-order\r\u0026gt; ## correlation structure\r\u0026gt; ar1 \u0026lt;- corAR1(form = ~year, value = rho)\r\u0026gt; ## initialize this constructor against our data\r\u0026gt; AR1 \u0026lt;- Initialize(ar1, data = data.frame(year))\r\u0026gt; ## generate a correlation matrix\r\u0026gt; V \u0026lt;- corMatrix(AR1)\r\u0026gt; ## Cholesky factorization of V\r\u0026gt; Cv \u0026lt;- chol(V)\r\u0026gt; ## simulate AR1 errors\r\u0026gt; e \u0026lt;- t(Cv) %*% rnorm(n, 0, sigma) # cov(e) = V * sig^2\r\u0026gt; ## generate response\r\u0026gt; y \u0026lt;- a + b * x + e\r\u0026gt; data.temporalCor = data.frame(y = y, x = x, year = year)\r\u0026gt; write.table(data.temporalCor, file = \u0026quot;data.temporalCor.csv\u0026quot;,\r+ sep = \u0026quot;,\u0026quot;, quote = F, row.names = FALSE)\r\u0026gt; \u0026gt; pairs(data.temporalCor)\rWe will now proceed to analyse these data via both of the above techniques for JAGS:\n\rincorporating AR1 residual autocorrelation structure\n\rincorporating lagged residuals into the model\n\r\r\rIncorporating lagged residuals\rModel fitting\rWe proceed to code the model into JAGS (remember that in this software normal distribution are parameterised in terms of precisions \\(\\tau\\) rather than variances, where \\(\\tau=\\frac{1}{\\sigma^2}\\)). Define the model.\n\u0026gt; stanString = \u0026quot;\r+ data {\r+ int\u0026lt;lower=1\u0026gt; n;\r+ vector [n] y;\r+ int\u0026lt;lower=1\u0026gt; nX;\r+ matrix[n,nX] X;\r+ }\r+ transformed data {\r+ }\r+ parameters {\r+ vector[nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ real\u0026lt;lower=-1,upper=1\u0026gt; phi;\r+ }\r+ transformed parameters {\r+ vector[n] mu;\r+ vector[n] epsilon;\r+ mu = X*beta;\r+ epsilon[1] = y[1] - mu[1];\r+ for (i in 2:n) {\r+ epsilon[i] = (y[i] - mu[i]);\r+ mu[i] = mu[i] + phi*epsilon[i-1];\r+ }\r+ }\r+ model {\r+ phi ~ uniform(-1,1);\r+ beta ~ normal(0,100);\r+ sigma ~ cauchy(0,5);\r+ y ~ normal(mu, sigma);\r+ }\r+ generated quantities {\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(stanString, con = \u0026quot;tempModel.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; Xmat = model.matrix(~x, data.temporalCor)\r\u0026gt; data.temporalCor.list \u0026lt;- with(data.temporalCor, list(y = y, X = Xmat,\r+ n = nrow(data.temporalCor), nX = ncol(Xmat)))\rDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1500\rNow compile and run the Stan code via the rstan interface.\n\u0026gt; library(rstan)\rDuring the warmup stage, the No-U-Turn sampler (NUTS) attempts to determine the optimum stepsize - the stepsize that achieves the target acceptance rate (\\(0.8\\) or \\(80\\)% by default) without divergence (occurs when the stepsize is too large relative to the curvature of the log posterior and results in approximations that are likely to diverge and be biased) - and without hitting the maximum treedepth (\\(10\\)). At each iteration of the NUTS algorithm, the number of leapfrog steps doubles (as it increases the treedepth) and only terminates when either the NUTS criterion are satisfied or the tree depth reaches the maximum (\\(10\\) by default).\n\u0026gt; data.temporalCor.rstan \u0026lt;- stan(data = data.temporalCor.list, file = \u0026quot;tempModel.stan\u0026quot;, chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;tempModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 1: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 1: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 1: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 1: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 1: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 1: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 1: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 1: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 1: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 1: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 1: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.087 seconds (Warm-up)\rChain 1: 0.052 seconds (Sampling)\rChain 1: 0.139 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;tempModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 2: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 2: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 2: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 2: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 2: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 2: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 2: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 2: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 2: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 2: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 2: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.084 seconds (Warm-up)\rChain 2: 0.055 seconds (Sampling)\rChain 2: 0.139 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.temporalCor.rstan, par = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;))\rInference for Stan model: tempModel.\r2 chains, each with iter=1500; warmup=500; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 30.80 0.30 11.73 9.28 22.78 30.20 38.06 55.53 1518 1\rbeta[2] 0.22 0.00 0.10 0.02 0.16 0.22 0.29 0.43 1362 1\rsigma 12.06 0.03 1.20 10.04 11.22 11.96 12.83 14.61 1245 1\rphi 0.92 0.00 0.05 0.80 0.88 0.92 0.96 1.00 898 1\rSamples were drawn using NUTS(diag_e) at Tue Feb 18 16:51:59 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\rMCMC diagnostics\r\u0026gt; library(mcmcplots)\r\u0026gt; mcmc = As.mcmc.list(data.temporalCor.rstan)\r\u0026gt; denplot(mcmc, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;))\r\u0026gt; traplot(mcmc, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;))\r\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r[[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r\u0026gt; #Autocorrelation diagnostic\r\u0026gt; autocorr.diag(mcmc)\rbeta[1] beta[2] sigma phi lp__\rLag 0 1.00000000 1.000000000 1.000000000 1.00000000 1.00000000\rLag 1 0.11945325 0.130376690 0.227794064 0.17925033 0.53885687\rLag 5 0.03437963 0.007224984 -0.042874656 0.07298004 0.09845135\rLag 10 0.03127466 0.025645224 0.006317540 0.01226778 -0.02012550\rLag 50 -0.05003287 0.024817286 -0.001190987 0.01445287 -0.02401952\r\u0026gt; stan_ac(data.temporalCor.rstan)\r\u0026gt; stan_rhat(data.temporalCor.rstan)\r\u0026gt; stan_ess(data.temporalCor.rstan)\rAll diagnostics seem fine.\n\rModel validation\rWhenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by sigma.\n\\[ Res_i = Y_i - \\mu_i\\]\n\\[ Res_{i+1} = Res_{i+1} - \\rho Res_i\\]\n\\[ Res_i = \\frac{Res_i}{\\sigma} \\]\n\u0026gt; mcmc = as.matrix(data.temporalCor.rstan)\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data.temporalCor$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = -1 * sweep(fit, 2, data.temporalCor$y, \u0026quot;-\u0026quot;)\r\u0026gt; n = ncol(resid)\r\u0026gt; resid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, \u0026quot;phi\u0026quot;])\r\u0026gt; resid = apply(resid, 2, median)/median(mcmc[, \u0026quot;sigma\u0026quot;])\r\u0026gt; fit = apply(fit, 2, median)\r\u0026gt; \u0026gt; library(ggplot2)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\r\u0026gt; \u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()\r\u0026gt; \u0026gt; ggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +\r+ geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;) + theme_classic()\r\u0026gt; \u0026gt; plot(acf(resid, lag = 40))\r\u0026gt; \u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data.temporalCor),\r+ fit[i, ], mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; ggplot() + geom_density(data = NULL, aes(x = as.vector(yRep),\r+ fill = \u0026quot;Model\u0026quot;), alpha = 0.5) + geom_density(data = data.temporalCor,\r+ aes(x = y, fill = \u0026quot;Obs\u0026quot;), alpha = 0.5) + theme_classic()\rNo obvious autocorrelation or other issues with residuals remaining.\n\rParameter estimates\rExplore parameter estimates.\n\u0026gt; library(broom)\r\u0026gt; tidyMCMC(data.temporalCor.rstan, par = c(\u0026quot;beta\u0026quot;, \u0026quot;phi\u0026quot;, \u0026quot;sigma\u0026quot;),\r+ conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;, rhat = TRUE,\r+ ess = TRUE)\r# A tibble: 4 x 7\rterm estimate std.error conf.low conf.high rhat ess\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r1 beta[1] 30.8 11.7 8.41 54.2 1.00 1518\r2 beta[2] 0.223 0.101 0.0145 0.417 1.000 1362\r3 phi 0.915 0.0519 0.825 1.000 1.00 898\r4 sigma 12.1 1.20 10.1 14.6 1.000 1245\r\r\rIncorporating AR1 residual autocorrelation structure\rModel fitting\rWe proceed to code the model into JAGS (remember that in this software normal distribution are parameterised in terms of precisions \\(\\tau\\) rather than variances, where \\(\\tau=\\frac{1}{\\sigma^2}\\)). Define the model.\n\u0026gt; stanString = \u0026quot;\r+ functions { + matrix cov_matrix_ar1(real ar, real sigma, int nrows) { + matrix[nrows, nrows] mat; + vector[nrows - 1] gamma; + mat = diag_matrix(rep_vector(1, nrows)); + for (i in 2:nrows) { + gamma[i - 1] = pow(ar, i - 1); + for (j in 1:(i - 1)) { + mat[i, j] = gamma[i - j]; + mat[j, i] = gamma[i - j]; + } + } + return sigma^2 / (1 - ar^2) * mat; + }\r+ } + + data { + int\u0026lt;lower=1\u0026gt; n; // total number of observations + vector[n] y; // response variable\r+ int\u0026lt;lower=1\u0026gt; nX;\r+ matrix[n,nX] X;\r+ } + transformed data {\r+ vector[n] se2 = rep_vector(0, n); + } + parameters { + vector[nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma; // residual SD + real \u0026lt;lower=-1,upper=1\u0026gt; phi; // autoregressive effects + } + transformed parameters { + } + model {\r+ matrix[n, n] res_cov_matrix;\r+ matrix[n, n] Sigma; + vector[n] mu = X*beta;\r+ res_cov_matrix = cov_matrix_ar1(phi, sigma, n);\r+ Sigma = res_cov_matrix + diag_matrix(se2);\r+ Sigma = cholesky_decompose(Sigma); + + // priors including all constants\r+ beta ~ student_t(3,30,30);\r+ sigma ~ cauchy(0,5);\r+ y ~ multi_normal_cholesky(mu,Sigma);\r+ } + generated quantities { + }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(stanString, con = \u0026quot;tempModel2.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; Xmat = model.matrix(~x, data.temporalCor)\r\u0026gt; data.temporalCor.list \u0026lt;- with(data.temporalCor, list(y = y, X = Xmat,\r+ n = nrow(data.temporalCor), nX = ncol(Xmat)))\rDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1500\rNow compile and run the Stan code via the rstan interface.\n\u0026gt; data.temporalCor2.rstan \u0026lt;- stan(data = data.temporalCor.list, file = \u0026quot;tempModel2.stan\u0026quot;, chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;tempModel2\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 1: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 1: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 1: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 1: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 1: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 1: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 1: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 1: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 1: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 1: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 1: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 2.143 seconds (Warm-up)\rChain 1: 1.194 seconds (Sampling)\rChain 1: 3.337 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;tempModel2\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0.001 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 2: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 2: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 2: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 2: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 2: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 2: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 2: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 2: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 2: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 2: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 2: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 2.478 seconds (Warm-up)\rChain 2: 1.287 seconds (Sampling)\rChain 2: 3.765 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.temporalCor2.rstan, par = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;))\rInference for Stan model: tempModel2.\r2 chains, each with iter=1500; warmup=500; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 21.73 0.53 16.53 -7.44 12.46 20.90 29.75 55.90 957 1\rbeta[2] 0.23 0.00 0.10 0.02 0.16 0.23 0.30 0.42 1523 1\rsigma 12.02 0.03 1.23 9.93 11.13 11.95 12.80 14.60 1552 1\rphi 0.89 0.00 0.06 0.77 0.86 0.90 0.93 0.99 781 1\rSamples were drawn using NUTS(diag_e) at Tue Feb 18 16:52:55 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\rMCMC diagnostics\r\u0026gt; mcmc = As.mcmc.list(data.temporalCor2.rstan)\r\u0026gt; denplot(mcmc, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;))\r\u0026gt; traplot(mcmc, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;phi\u0026quot;))\r\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r[[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r\u0026gt; #Autocorrelation diagnostic\r\u0026gt; autocorr.diag(mcmc)\rbeta[1] beta[2] sigma phi lp__\rLag 0 1.000000000 1.000000000 1.000000000 1.000000000 1.000000000\rLag 1 0.248972079 0.105529288 0.061621949 0.168454454 0.557890281\rLag 5 -0.001409062 0.006290841 0.018746956 0.075009305 0.115634020\rLag 10 0.036198498 0.007579889 -0.001415388 0.013955579 -0.033295103\rLag 50 0.033443833 -0.040041892 -0.005562613 -0.004995361 0.004248614\r\u0026gt; stan_ac(data.temporalCor2.rstan)\r\u0026gt; stan_rhat(data.temporalCor2.rstan)\r\u0026gt; stan_ess(data.temporalCor2.rstan)\rAll diagnostics seem fine.\n\rModel validation\rWhenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by sigma.\n\\[ Res_i = Y_i - \\mu_i\\]\n\\[ Res_{i+1} = Res_{i+1} - \\rho Res_i\\]\n\\[ Res_i = \\frac{Res_i}{\\sigma} \\]\n\u0026gt; mcmc = as.matrix(data.temporalCor2.rstan)\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data.temporalCor$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = -1 * sweep(fit, 2, data.temporalCor$y, \u0026quot;-\u0026quot;)\r\u0026gt; n = ncol(resid)\r\u0026gt; resid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, \u0026quot;phi\u0026quot;])\r\u0026gt; resid = apply(resid, 2, median)/median(mcmc[, \u0026quot;sigma\u0026quot;])\r\u0026gt; fit = apply(fit, 2, median)\r\u0026gt; \u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\r\u0026gt; \u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()\r\u0026gt; \u0026gt; ggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +\r+ geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;) + theme_classic()\r\u0026gt; \u0026gt; plot(acf(resid, lag = 40))\r\u0026gt; \u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data.temporalCor),\r+ fit[i, ], mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; ggplot() + geom_density(data = NULL, aes(x = as.vector(yRep),\r+ fill = \u0026quot;Model\u0026quot;), alpha = 0.5) + geom_density(data = data.temporalCor,\r+ aes(x = y, fill = \u0026quot;Obs\u0026quot;), alpha = 0.5) + theme_classic()\rNo obvious autocorrelation or other issues with residuals remaining.\n\rParameter estimates\rExplore parameter estimates.\n\u0026gt; tidyMCMC(data.temporalCor2.rstan, par = c(\u0026quot;beta\u0026quot;, \u0026quot;phi\u0026quot;, \u0026quot;sigma\u0026quot;),\r+ conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;, rhat = TRUE,\r+ ess = TRUE)\r# A tibble: 4 x 7\rterm estimate std.error conf.low conf.high rhat ess\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r1 beta[1] 21.7 16.5 -8.48 54.2 1.00 957\r2 beta[2] 0.227 0.103 0.0405 0.439 1.00 1523\r3 phi 0.893 0.0566 0.790 0.995 0.999 781\r4 sigma 12.0 1.23 9.78 14.4 0.999 1552\r\r\rReferences\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\r\r\r","date":1581214394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581214394,"objectID":"edf045829f625881a266c84248d9bc8c","permalink":"/stan/autocorrelation-stan/autocorrelation-stan/","publishdate":"2020-02-08T21:13:14-05:00","relpermalink":"/stan/autocorrelation-stan/autocorrelation-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","autocorrelation"],"title":"Temporal Autocorrelation - STAN","type":"STAN"},{"authors":["Andrea Gabrio"],"categories":["R","heterogeneity","JAGS","heteroskedasticity"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rUp until now (in the proceeding tutorials), the focus has been on models that adhere to specific assumptions about the underlying populations (and data). Indeed, both before and immediately after fitting these models, I have stressed the importance of evaluating and validating the proposed and fitted models to ensure reliability of the models. It is now worth us revisiting those fundamental assumptions as well as exploring the options that are available when the populations (data) do not conform. Let’s explore a simple linear regression model to see how each of the assumptions relate to the model.\n\\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0, \\sigma^2).\\]\nThe above simple statistical model models the linear relationship of \\(y_i\\) against \\(x_i\\). The residuals (\\(\\epsilon\\)) are assumed to be normally distributed with a mean of zero and a constant (yet unknown) variance (\\(\\sigma\\), homogeneity of variance). The residuals (and thus observations) are also assumed to all be independent.\nHomogeneity of variance and independence are encapsulated within the single symbol for variance (\\(\\sigma^2\\)). In assuming equal variances and independence, we are actually making an assumption about the variance-covariance structure of the populations (and thus residuals). Specifically, we assume that all populations are equally varied and thus can be represented well by a single variance term (all diagonal values in a \\(N\\times N\\) covariance matrix are the same, \\(\\sigma^2\\)) and the covariances between each population are zero (off diagonals). In simple regression, each observation (data point) represents a single observation drawn (sampled) from an entire population of possible observations. The above covariance structure thus assumes that the covariance between each population (observation) is zero - that is, each observation is completely independent of each other observation. Whilst it is mathematically convenient when data conform to these conditions (normality, homogeneity of variance, independence and linearity), data often violate one or more of these assumptions. In the following, I want to discuss and explore the causes and options for dealing with non-compliance to each of these conditions. By gaining a better understanding of how the various model fitting engines perform their task, we are better equipped to accommodate aspects of the data that don’t otherwise conform to the simple regression assumptions. In this tutorial we specifically focus on the topic of heterogeneity of the variance.\n\r\rDealing with heterogeneity\rThe validity and reliability of the above linear models are very much dependent on variance homogeneity. In particular, variances that increase (or decrease) with a change in expected values are substantial violations. Whilst non-normality can also be a source of heterogeneity and therefore normalising can address both issues, heterogeneity can also be independent of normality. Similarly, generalised linear models (that accommodate alternative residual distributions - such as Poisson, Binomial, Gamma, etc) can be useful for more appropriate modelling of both the distribution and variance of a model. However, for Gaussian (normal) models in which there is evidence of heterogeneity of variance, yet no evidence of non-normality, it is also possible to specifically model in an alternative variance structure. For example, we can elect to allow variance to increase proportionally to a covariate. To assist us in the following demonstration, we will generate another data set - one that has heteroskedasticity (unequal variance) by design. Rather than draw each residual (and thus observation) from a normal distribution with a constant standard deviation), we will draw the residuals from normal distributions whose variance is proportional to the \\(X\\) predictor.\n\u0026gt; set.seed(126)\r\u0026gt; n \u0026lt;- 16\r\u0026gt; a \u0026lt;- 40 #intercept\r\u0026gt; b \u0026lt;- 1.5 #slope\r\u0026gt; x \u0026lt;- 1:n #values of the year covariate\r\u0026gt; sigma \u0026lt;- 1.5 * x\r\u0026gt; sigma\r[1] 1.5 3.0 4.5 6.0 7.5 9.0 10.5 12.0 13.5 15.0 16.5 18.0 19.5 21.0 22.5\r[16] 24.0\r\u0026gt; \u0026gt; eps \u0026lt;- rnorm(n, mean = 0, sd = sigma) #residuals\r\u0026gt; y \u0026lt;- a + b * x + eps #response variable\r\u0026gt; # OR\r\u0026gt; y \u0026lt;- (model.matrix(~x) %*% c(a, b)) + eps\r\u0026gt; data.het \u0026lt;- data.frame(y = round(y, 1), x) #dataset\r\u0026gt; head(data.het) #print out the first six rows of the data set\ry x\r1 42.1 1\r2 44.2 2\r3 41.2 3\r4 51.7 4\r5 43.5 5\r6 48.3 6\r\u0026gt; \u0026gt; # scatterplot of y against x\r\u0026gt; library(car)\r\u0026gt; scatterplot(y ~ x, data.het)\r\u0026gt; \u0026gt; # regular simple linear regression\r\u0026gt; data.het.lm \u0026lt;- lm(y ~ x, data.het)\r\u0026gt; \u0026gt; # plot of standardised residuals\r\u0026gt; plot(rstandard(data.het.lm) ~ fitted(data.het.lm))\r\u0026gt; \u0026gt; # plot of standardized residuals against the predictor\r\u0026gt; plot(rstandard(data.het.lm) ~ x)\rThe above scatterplot suggests that variance may increase with increasing \\(X\\). The residual plot (using standardised residuals) suggests that mean and variance could be related - there is a hint of a wedge-shaped pattern. Importantly, the plot of standardised residuals against the predictor shows the same pattern as the residual plot implying that heterogeneity is likely to be due a relationship between variance \\(X\\). That is, an increase in \\(X\\) is associated with an increase in variance. In response to this, we could incorporate an alternative variance structure. The simple model we fit earlier assumed that the expected values were all drawn from normal distributions with the same level of precision \\(\\tau\\) and therefore variance. This assumption is often summarised as:\n\\[ \\boldsymbol V = \\sigma^2 \\times \\boldsymbol I,\\]\nwhere \\(\\boldsymbol I\\) is the \\(N \\times N\\) identity matrix (elements on the main diagonal are one and zero outside) which multipled by the constant value \\(\\sigma^2\\) produces the homoskedastic covariance matrix \\(\\boldsymbol V\\) (elements on the main diagonal are \\(\\sigma^2\\) and zero outside). If, instead, we consider an heteroskedastic covariance matrix then, for example, we could assume that the variance is proportional to the level of the covariate. This assumption can be summarised as:\n\\[ \\boldsymbol V = \\sigma^2 \\times X \\times \\boldsymbol I,\\]\nwhere the product of the identity matrix \\(\\boldsymbol I\\) and the covariate-specific values \\(\\sigma^2 \\times X\\) produces the heteroskedastic covariance matrix \\(\\boldsymbol V\\) (elements on the main diagonal are \\(\\sigma^2 \\times X\\) and zero outside). With a couple of small adjustments, we can modify the JAGS code in order to accommodate a variance structure in which variance is proportional to the predictor variable. Note that since JAGS works with precision (\\(\\tau=\\frac{1}{\\sigma^2}\\)), I have elected to express the predictor as \\(\\frac{1}{x}\\). This way the weightings are compatible with precision rather than variance. In previous tutorials, we have used a flat, uniform distribution \\([0,100]\\) for variance priors. Whilst this is a reasonable choice for a non-informative prior, Gelman and others (2006) suggest that half-cauchy priors are more appropriate when the number of groups is low.\n\rModel fitting\rThe observed response (\\(y_i\\)) are assumed to be drawn from a normal distribution with a given mean (\\(\\mu\\)) and standard deviation weighted by \\(1\\) on the value of the covariate (\\(\\sigma \\times \\omega\\)). The expected values (\\(\\mu\\)) are themselves determined by the linear predictor (\\(\\beta_0 + \\beta_1x\\)). In this case, \\(\\beta_0\\) represents the mean of the first group and the set of \\(\\beta\\)’s represent the differences between each other group and the first group. MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying ‘uninformative’ priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (\\(100\\)) for both the intercept and the treatment effect and a wide half-cauchy (\\(\\text{scale}=5\\)) for the standard deviation.\n\\[y_i \\sim N(\\mu_i,\\sigma \\times \\omega), \\]\nwhere \\(\\mu_i=\\beta_0 +\\boldsymbol \\beta \\boldsymbol X\\). The assumed priors are: \\(\\beta \\sim N(0,100)\\) and \\(\\sigma \\sim \\text{Cauchy}(0,5)\\). We note that we can also indirectly specify the prior on \\(\\sigma\\) by expressing the standard deviation as the ratio between two variable: \\(\\sigma=\\frac{z}{\\sqrt{\\chi}}\\). The numerator is a zero-truncated normally distributed variable \\(z \\sim N(0, 0.04) I(0,)\\), while the denominator is the square root of a variable distributed according to a Gamma distribution \\(\\chi \\sim \\text{Gamma}(0.5,0.5)\\) (equivalent to a \\(\\chi^2\\) distribution with \\(1\\) degrees of freedom).\nWe proceed to code the model into JAGS (remember that in this software normal distribution are parameterised in terms of precisions \\(\\tau\\) rather than variances, where \\(\\tau=\\frac{1}{\\sigma^2}\\)). Note the following example as group means calculated as derived posteriors.\n\u0026gt; modelString = \u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau*(1/x[i]))\r+ mu[i] \u0026lt;- beta0+beta1*x[i]\r+ }\r+ + #Priors and derivatives\r+ beta0 ~ dnorm(0,1.0E-6)\r+ beta1 ~ dnorm(0,1.0E-6)\r+ + sigma \u0026lt;- z/sqrt(chSq) # prior for sigma; cauchy = normal/sqrt(chi^2)\r+ z ~ dnorm(0, 0.04)I(0,)\r+ chSq ~ dgamma(0.5, 0.5) # chi^2 with 1 d.f.\r+ tau \u0026lt;- pow(sigma, -2)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;heteroskModel.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; data.het.list \u0026lt;- with(data.het, list(y = y, x = x, n = nrow(data.het)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;, \u0026quot;sigma\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10500\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\u0026gt; library(R2jags)\rNow run the JAGS code via the R2jags interface. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; data.het.r2jags \u0026lt;- jags(data = data.het.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;heteroskModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 16\rUnobserved stochastic nodes: 4\rTotal graph size: 111\rInitializing model\r\u0026gt; \u0026gt; print(data.het.r2jags)\rInference for Bugs model at \u0026quot;heteroskModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 41.492 2.571 36.510 39.844 41.466 43.160 46.599 1.001 15000\rbeta1 1.114 0.401 0.313 0.857 1.112 1.371 1.913 1.001 15000\rsigma 3.070 0.629 2.119 2.627 2.969 3.410 4.592 1.002 1300\rdeviance 110.901 2.744 107.742 108.871 110.200 112.216 117.874 1.002 2800\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.8 and DIC = 114.7\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots for the effects model as an example. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters, e.g. \\(\\boldsymbol \\beta\\).\n\u0026gt; library(mcmcplots)\r\u0026gt; denplot(data.het.r2jags, parms = c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;))\r\u0026gt; traplot(data.het.r2jags, parms = c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;))\rTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters (such as \\(\\beta\\)s).\n\u0026gt; data.mcmc = as.mcmc(data.het.r2jags)\r\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(data.mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 2 3938 3746 1.050 beta1 2 3729 3746 0.995 deviance 2 3770 3746 1.010 sigma 4 4643 3746 1.240 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 2 3853 3746 1.030 beta1 2 3895 3746 1.040 deviance 2 3729 3746 0.995 sigma 3 4346 3746 1.160 \rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; autocorr.diag(data.mcmc)\rbeta0 beta1 deviance sigma\rLag 0 1.000000000 1.0000000000 1.000000000 1.000000000\rLag 1 0.011777589 0.0071404620 0.229687388 0.247278554\rLag 5 0.006349593 0.0032513419 -0.000699578 0.011972761\rLag 10 -0.001248639 -0.0002634626 -0.010327446 -0.001271626\rLag 50 0.018019858 -0.0055775204 -0.013066989 0.010275604\rA lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. For more complex models (those that contain multiple effects), it is also advisable to plot the residuals against each of the individual predictors. For sampling designs that involve sample collection over space or time, it is also a good idea to explore whether there are any temporal or spatial patterns in the residuals.\nThere are numerous situations (e.g. when applying specific variance-covariance structures to a model) where raw residuals do not reflect the interior workings of the model. Typically, this is because they do not take into account the variance-covariance matrix or assume a very simple variance-covariance matrix. Since the purpose of exploring residuals is to evaluate the model, for these cases, it is arguably better to draw conclusions based on standardized (or studentised) residuals. Unfortunately the definitions of standardised and studentised residuals appears to vary and the two terms get used interchangeably. I will adopt the following definitions:\n\rStandardised residuals. The raw residuals divided by the true standard deviation of the residuals (which of course is rarely known).\n\rStudentised residuals. The raw residuals divided by the standard deviation of the residuals. Note that externally studentised residuals are calculated by dividing the raw residuals by a unique standard deviation for each observation that is calculated from regressions having left each successive observation out.\n\rPearson residuals. The raw residuals divided by the standard deviation of the response variable.\n\r\rhe mark of a good model is being able to predict well. In an ideal world, we would have sufficiently large sample size as to permit us to hold a fraction (such as \\(25\\)%) back thereby allowing us to train the model on \\(75\\)% of the data and then see how well the model can predict the withheld \\(25\\)%. Unfortunately, such a luxury is still rare. The next best option is to see how well the model can predict the observed data. Models tend to struggle most with the extremes of trends and have particular issues when the extremes approach logical boundaries (such as zero for count data and standard deviations). We can use the fitted model to generate random predicted observations and then explore some properties of these compared to the actual observed data.\nRather than dublicate this for both additive and multiplicative models, we will only explore the multiplicative model. Residuals are not computed directly within JAGS. However, we can calculate them manually form the posteriors.\n\u0026gt; mcmc = data.het.r2jags$BUGSoutput$sims.matrix[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;)]\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data.het$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc, 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data.het$y - fit\r\u0026gt; \u0026gt; library(ggplot2)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rThe above residual plot would make us believe that we had a homogeneity of variance issue (which we thought we were addressing by defining a model that allowed the variance to be proportional to the predictor). This is because we have plotted the raw residuals rather than residuals that have been standardized by the variances. The above plot is also what the residual plot would look like if we had not made any attempt to define a model in which the variance was related to the predictor. Whenever we fit a model that incorporates changes to the variance-covariance structures, we should explore standardised residuals. In this case, we should divide the residuals by sigma and then divide by the square-root of the weights.\n\\[ Res_i = \\frac{Y_i - \\mu_i}{\\sigma \\times \\sqrt{\\omega}}\\]\n\u0026gt; library(dplyr)\r\u0026gt; library(tidyr)\r\u0026gt; mcmc = data.het.r2jags$BUGSoutput$sims.matrix\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;)]\r\u0026gt; Xmat = model.matrix(~x, data.het)\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = -1 * sweep(fit, 2, data.het$y, \u0026quot;-\u0026quot;)\r\u0026gt; resid = apply(resid, 2, median)/(median(mcmc[, \u0026quot;sigma\u0026quot;]) * sqrt(data.het$x))\r\u0026gt; fit = apply(fit, 2, median)\r\u0026gt; \u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rThis is certainly an improvement. Nevertheless, there is still an indication of a relationship between mean and variance. We could attempt to further address this by refining \\(\\omega\\) in the Bayesian model. That is, rather than indicate that variance is proportional to \\(x\\), we could indicate that variance is proportional to \\(x^2\\) (as an example) - we will leave this as an exercise for the reader. Residuals against predictors.\n\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data.het$x)) + theme_classic()\rLets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = data.het.r2jags$BUGSoutput$sims.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~x, data.het)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data.het), fit[i, ],\r+ mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; ggplot() + geom_density(data = NULL, aes(x = as.vector(yRep), fill = \u0026quot;Model\u0026quot;),\r+ alpha = 0.5) + geom_density(data = data.het, aes(x = y, fill = \u0026quot;Obs\u0026quot;),\r+ alpha = 0.5) + theme_classic()\r\rParameter estimates\rFirst, we look at the results from the model.\n\u0026gt; print(data.het.r2jags)\rInference for Bugs model at \u0026quot;heteroskModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 41.492 2.571 36.510 39.844 41.466 43.160 46.599 1.001 15000\rbeta1 1.114 0.401 0.313 0.857 1.112 1.371 1.913 1.001 15000\rsigma 3.070 0.629 2.119 2.627 2.969 3.410 4.592 1.002 1300\rdeviance 110.901 2.744 107.742 108.871 110.200 112.216 117.874 1.002 2800\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.8 and DIC = 114.7\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(as.mcmc(data.het.r2jags), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 4 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta0 41.5 2.57 36.5 46.6 2 beta1 1.11 0.401 0.338 1.94\r3 deviance 111. 2.74 108. 116. 4 sigma 3.07 0.629 2.00 4.35\rConclusions\nA one unit increase in \\(x\\) is associated with a \\(1.11\\) change in \\(y\\). That is, \\(y\\) declines at a rate of \\(1.11\\) per unit increase in \\(x\\). The \\(95\\)% confidence interval for the slope does not overlap with \\(0\\) implying a significant effect of \\(x\\) on \\(y\\). While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\r\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(data.het.r2jags$BUGSoutput$sims.matrix[, c(\u0026quot;beta1\u0026quot;)])\r[1] 0.0092\rWith a p-value of essentially \\(0\\), we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship.\n\rGraphical summaries\rA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group.\n\u0026gt; mcmc = data.het.r2jags$BUGSoutput$sims.matrix\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = data.frame(x = seq(min(data.het$x, na.rm = TRUE), max(data.het$x,\r+ na.rm = TRUE), len = 1000))\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X\u0026quot;) + theme_classic()\rIf you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.\n\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data.het,\r+ aes(y = y, x = x), color = \u0026quot;gray\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X\u0026quot;) + theme_classic()\rA more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since \\(\\text{resid}=\\text{obs}−\\text{fitted}\\) and the fitted values depend only on the single predictor we are interested in.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = data.het\r\u0026gt; fMat = rMat = model.matrix(~x, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data.het$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\r+ color = \u0026quot;gray\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low, ymax = conf.high),\r+ fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_continuous(\u0026quot;X\u0026quot;) +\r+ theme_classic()\r\rR squared\rIn a frequentist context, the \\(R^2\\) value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, \\(R^2\\) is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an \\(R^2\\) greater than \\(100\\)%. Gelman et al. (2019) proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.\nSo in the standard regression model notation of:\n\\[ y_i \\sim \\text{Normal}(\\boldsymbol X \\boldsymbol \\beta, \\sigma),\\]\nthe \\(R^2\\) could be formulated as\n\\[ R^2 = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sigma^2_e},\\]\nwhere \\(\\sigma^2_f=\\text{var}(\\boldsymbol X \\boldsymbol \\beta)\\), and for normal models \\(\\sigma^2_e=\\text{var}(y-\\boldsymbol X \\boldsymbol \\beta)\\)\n\u0026gt; mcmc \u0026lt;- data.het.r2jags$BUGSoutput$sims.matrix\r\u0026gt; Xmat = model.matrix(~x, data.het)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = sweep(fit, 2, data.het$y, \u0026quot;-\u0026quot;)\r\u0026gt; var_f = apply(fit, 1, var)\r\u0026gt; var_e = apply(resid, 1, var)\r\u0026gt; R2 = var_f/(var_f + var_e)\r\u0026gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.244 0.108 0.0309 0.440\r\u0026gt; \u0026gt; # for comparison with frequentist summary(lm(y ~ x, data.het))\r\rHeteroskedasticity with categorical predictors\rFor regression models that include a categorical variable (e.g. ANOVA), heterogeneity manifests as vastly different variances for different levels (treatment groups) of the categorical variable. Recall, that this is diagnosed from the relative size of boxplots. Whilst, the degree of group variability may not be related to the means of the groups, having wildly different variances does lead to an increase in standard errors and thus a lowering of power. In such cases, we would like to be able to indicate that the variances should be estimated separately for each group. That is the variance term is multiplied by a different number for each group. The appropriate matrix is referred to as an Identity matrix. Again, to assist in the explanation some fabricated ANOVA data - data that has heteroscadasticity by design - will be useful.\n\u0026gt; set.seed(126)\r\u0026gt; ngroups \u0026lt;- 5 #number of populations\r\u0026gt; nsample \u0026lt;- 10 #number of reps in each\r\u0026gt; pop.means \u0026lt;- c(40, 45, 55, 40, 30) #population mean length\r\u0026gt; sigma \u0026lt;- rep(c(6, 4, 2, 0.5, 1), each = nsample) #residual standard deviation\r\u0026gt; n \u0026lt;- ngroups * nsample #total sample size\r\u0026gt; eps \u0026lt;- rnorm(n, 0, sigma) #residuals\r\u0026gt; x \u0026lt;- gl(ngroups, nsample, n, lab = LETTERS[1:5]) #factor\r\u0026gt; means \u0026lt;- rep(pop.means, rep(nsample, ngroups))\r\u0026gt; X \u0026lt;- model.matrix(~x - 1) #create a design matrix\r\u0026gt; y \u0026lt;- as.numeric(X %*% pop.means + eps)\r\u0026gt; data.het1 \u0026lt;- data.frame(y, x)\r\u0026gt; boxplot(y ~ x, data.het1)\r\u0026gt; \u0026gt; plot(lm(y ~ x, data.het1), which = 3)\rIt is clear that there is gross heteroskedasticity. The residuals are obviously more spread in some groups than others yet there is no real pattern with means (the residual plot does not show an obvious wedge). Note, for assessing homogeneity of variance, it is best to use the standardised residuals. It turns out that if we switch over to maximum (log) likelihood estimation methods, we can model in a within-group heteroskedasticity structure rather than just assume one very narrow form of variance structure. Lets take a step back and reflect on our simple ANOVA (regression) model that has five groups each with \\(10\\) observations:\n\\[ y_i = \\mu + \\alpha_i + \\epsilon, \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon \\sim N(0, \\sigma^2). \\]\nThis is shorthand notation to indicate that the response variable is being modelled against a specific linear predictor and that the residuals follow a normal distribution with a certain variance (that is the same for each group). Rather than assume that the variance of each group is the same, we could relax this a little so as to permit different levels of variance per group:\n\\[ \\epsilon \\sim N(0, \\sigma^2_i).\\]\nTo achieve this, we actually multiply the variance matrix by a weighting matrix, where the weights associated with each group are determined by the inverse of the ratio of each group to the first (reference) group:\n\\[ \\epsilon \\sim N(0, \\sigma^2_i \\times \\omega).\\]\nSo returning to our five groups of \\(10\\) observations example, the weights would be determined as:\n\u0026gt; data.het1.sd \u0026lt;- with(data.het1, tapply(y, x, sd))\r\u0026gt; 1/(data.het1.sd[1]/data.het1.sd)\rA B C D E 1.0000000 0.6909012 0.4140893 0.1426207 0.3012881 \rThe weights determine the relative amount of each observation that goes into calculating variances. The basic premise is that those with lower variances are likely to be more precise and therefore should have greatest contribution to variance calculations.\nModel fitting\r\u0026gt; modelString2 = \u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau[x[i]])\r+ mu[i] \u0026lt;- inprod(beta[],X[i,])\r+ }\r+ + #Priors and derivatives\r+ for (i in 1:ngroups) {\r+ beta[i] ~ dnorm(0,1.0E-6)\r+ + sigma[i] \u0026lt;- z[i]/sqrt(chSq[i])\r+ z[i] ~ dnorm(0, 0.04)I(0,)\r+ chSq[i] ~ dgamma(0.5, 0.5)\r+ tau[i] \u0026lt;- pow(sigma[i], -2)\r+ }\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString2, con = \u0026quot;heteroskModel2.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; X = model.matrix(~x, data.het1)\r\u0026gt; data.het1.list \u0026lt;- with(data.het1, list(y = y, x = as.numeric(x), X = X,\r+ n = nrow(data.het1), ngroups = ncol(X)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10500\rNow run the JAGS code via the R2jags interface. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; data.het1.r2jags \u0026lt;- jags(data = data.het1.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;heteroskModel2.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 50\rUnobserved stochastic nodes: 15\rTotal graph size: 444\rInitializing model\r\u0026gt; \u0026gt; print(data.het1.r2jags)\rInference for Bugs model at \u0026quot;heteroskModel2.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 40.282 1.227 37.861 39.518 40.278 41.044 42.731 1.001 11000\rbeta[2] 4.088 1.508 1.063 3.115 4.095 5.059 7.063 1.001 5000\rbeta[3] 14.553 1.336 11.874 13.714 14.566 15.402 17.177 1.001 5600\rbeta[4] -0.655 1.242 -3.118 -1.425 -0.656 0.118 1.804 1.001 11000\rbeta[5] -10.364 1.286 -12.875 -11.173 -10.353 -9.550 -7.830 1.001 12000\rsigma[1] 3.748 0.971 2.378 3.062 3.583 4.231 6.071 1.001 13000\rsigma[2] 2.647 0.729 1.640 2.143 2.504 2.995 4.461 1.001 5400\rsigma[3] 1.629 0.456 1.001 1.314 1.541 1.846 2.767 1.001 4000\rsigma[4] 0.570 0.169 0.346 0.454 0.537 0.647 1.001 1.001 3500\rsigma[5] 1.181 0.336 0.727 0.950 1.118 1.342 2.021 1.001 7100\rdeviance 182.822 5.288 174.824 178.961 182.076 185.810 195.061 1.001 11000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 14.0 and DIC = 196.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMCMC diagnostics\r\u0026gt; library(mcmcplots)\r\u0026gt; denplot(data.het1.r2jags, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\r\u0026gt; traplot(data.het1.r2jags, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots.\n\r\rModel validation\r\u0026gt; mcmc = data.het1.r2jags$BUGSoutput$sims.matrix\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data.het1$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, wch], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data.het1$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rThe above residual plot would make us believe that we had a homogeneity of variance issue (which we thought we were addressing by defining a model that allowed the variance to be proportional to the predictor). This is because we have plotted the raw residuals rather than residuals that have been standardized by the variances. The above plot is also what the residual plot would look like if we had not made any attempt to define a model in which the variance was related to the predictor. Whenever we fit a model that incorporates changes to the variance-covariance structures, we should explore standardized residuals. In this case, we should divide the residuals by the appropriate sigma for associated with that group (level of predictor).\n\\[ Res_{ij} = \\frac{Y_{ij} - \\mu_j}{\\sigma_j}\\]\n\u0026gt; mcmc = data.het1.r2jags$BUGSoutput$sims.matrix\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; Xmat = model.matrix(~x, data.het1)\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = -1 * sweep(fit, 2, data.het1$y, \u0026quot;-\u0026quot;)\r\u0026gt; wch = grep(\u0026quot;sigma\u0026quot;, colnames(mcmc))\r\u0026gt; resid = apply(resid, 2, median)/rep(apply(mcmc[, wch], 2, median), table(data.het1$x))\r\u0026gt; # resid = apply(resid,2,median)/(median(mcmc[,\u0026#39;sigma\u0026#39;]) * sqrt(data.het1$x))\r\u0026gt; fit = apply(fit, 2, median)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rThis is certainly an improvement. Nevertheless, there is still an indication of a relationship between mean and variance. We could attempt to further address this by refining \\(\\omega\\) in the Bayesian model. That is, rather than indicate that variance is proportional to \\(x\\), we could indicate that variance is proportional to \\(x^2\\) (as an example) - we will leave this as an exercise for the reader. Residuals against predictors.\n\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data.het1$x)) + theme_classic()\rLets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = data.het1.r2jags$BUGSoutput$sims.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~x, data.het1)\r\u0026gt; ## get median parameter estimates\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; wch = grep(\u0026quot;sigma\u0026quot;, colnames(mcmc))\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data.het1), fit[i, ],\r+ mcmc[i, wch[as.numeric(data.het1$x[i])]]))\r\u0026gt; newdata = data.frame(x = data.het1$x, yRep) %\u0026gt;% gather(key = Sample, value = Value,\r+ -x)\r\u0026gt; ggplot(newdata) + geom_violin(aes(y = Value, x = x, fill = \u0026quot;Model\u0026quot;), alpha = 0.5) +\r+ geom_violin(data = data.het1, aes(y = y, x = x, fill = \u0026quot;Obs\u0026quot;), alpha = 0.5) +\r+ geom_point(data = data.het1, aes(y = y, x = x), position = position_jitter(width = 0.1,\r+ height = 0), color = \u0026quot;black\u0026quot;) + theme_classic()\rParameter estimates\rFirst, we look at the results from the model.\n\u0026gt; print(data.het1.r2jags)\rInference for Bugs model at \u0026quot;heteroskModel2.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 40.282 1.227 37.861 39.518 40.278 41.044 42.731 1.001 11000\rbeta[2] 4.088 1.508 1.063 3.115 4.095 5.059 7.063 1.001 5000\rbeta[3] 14.553 1.336 11.874 13.714 14.566 15.402 17.177 1.001 5600\rbeta[4] -0.655 1.242 -3.118 -1.425 -0.656 0.118 1.804 1.001 11000\rbeta[5] -10.364 1.286 -12.875 -11.173 -10.353 -9.550 -7.830 1.001 12000\rsigma[1] 3.748 0.971 2.378 3.062 3.583 4.231 6.071 1.001 13000\rsigma[2] 2.647 0.729 1.640 2.143 2.504 2.995 4.461 1.001 5400\rsigma[3] 1.629 0.456 1.001 1.314 1.541 1.846 2.767 1.001 4000\rsigma[4] 0.570 0.169 0.346 0.454 0.537 0.647 1.001 1.001 3500\rsigma[5] 1.181 0.336 0.727 0.950 1.118 1.342 2.021 1.001 7100\rdeviance 182.822 5.288 174.824 178.961 182.076 185.810 195.061 1.001 11000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 14.0 and DIC = 196.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; # OR\r\u0026gt; tidyMCMC(as.mcmc(data.het1.r2jags), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 11 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 40.3 1.23 37.9 42.7 2 beta[2] 4.09 1.51 0.980 6.97 3 beta[3] 14.6 1.34 12.0 17.3 4 beta[4] -0.655 1.24 -3.15 1.76 5 beta[5] -10.4 1.29 -12.9 -7.86 6 deviance 183. 5.29 174. 194. 7 sigma[1] 3.75 0.971 2.23 5.72 8 sigma[2] 2.65 0.729 1.53 4.12 9 sigma[3] 1.63 0.456 0.906 2.54 10 sigma[4] 0.570 0.169 0.313 0.905\r11 sigma[5] 1.18 0.336 0.656 1.82 \rConclusions\n\rthe mean of the first group (A) is \\(40.3\\)\n\rthe mean of the second group (B) is \\(4.12\\) units greater than (A)\n\rthe mean of the third group (C) is \\(14.6\\) units greater than (A)\n\rthe mean of the forth group (D) is \\(-0.637\\) units greater (i.e. less) than (A)\n\rthe mean of the fifth group (E) is \\(-10.3\\) units greater (i.e. less) than (A)\n\r\rThe \\(95\\)% confidence interval for the effects of B, C and E do not overlap with \\(0\\) implying a significant difference between group A and groups B, C and E. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\r\u0026gt; ## since values are less than zero\r\u0026gt; mcmc = data.het1.r2jags$BUGSoutput$sims.matrix\r\u0026gt; for (i in grep(\u0026quot;beta\u0026quot;, colnames(mcmc), value = TRUE)) print(paste(i, mcmcpvalue(mcmc[,i])))\r[1] \u0026quot;beta[1] 0\u0026quot;\r[1] \u0026quot;beta[2] 0.0116\u0026quot;\r[1] \u0026quot;beta[3] 0\u0026quot;\r[1] \u0026quot;beta[4] 0.567133333333333\u0026quot;\r[1] \u0026quot;beta[5] 0\u0026quot;\r\u0026gt; mcmcpvalue(mcmc[, grep(\u0026quot;beta\u0026quot;, colnames(mcmc))])\r[1] 0\rWith a p-value of essentially \\(0\\), we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship.\n\rGraphical summaries\r\u0026gt; mcmc = data.het1.r2jags$BUGSoutput$sims.matrix\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = data.frame(x = levels(data.het1$x))\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) +\r+ theme_classic()\rIf you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.\n\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data.het1,\r+ aes(y = y, x = x), color = \u0026quot;gray\u0026quot;) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) +\r+ theme_classic()\rA more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since \\(\\text{resid}=\\text{obs}−\\text{fitted}\\) and the fitted values depend only on the single predictor we are interested in.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = data.het1\r\u0026gt; fMat = rMat = model.matrix(~x, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data.het1$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\r+ color = \u0026quot;gray\u0026quot;) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\r+ scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) + theme_classic()\r\r\rReferences\rGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9.\n\rGelman, Andrew, and others. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).” Bayesian Analysis 1 (3): 515–34.\n\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1581127994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581127994,"objectID":"c011c205dd76be6207cf502b38fae179","permalink":"/jags/heterogeneity-jags/heterogeneity-jags/","publishdate":"2020-02-07T21:13:14-05:00","relpermalink":"/jags/heterogeneity-jags/heterogeneity-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","heterogeneity","heteroskedasticity"],"title":"Variance Heterogeneity - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","heterogeneity","STAN","heteroskedasticity"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rUp until now (in the proceeding tutorials), the focus has been on models that adhere to specific assumptions about the underlying populations (and data). Indeed, both before and immediately after fitting these models, I have stressed the importance of evaluating and validating the proposed and fitted models to ensure reliability of the models. It is now worth us revisiting those fundamental assumptions as well as exploring the options that are available when the populations (data) do not conform. Let’s explore a simple linear regression model to see how each of the assumptions relate to the model.\n\\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0, \\sigma^2).\\]\nThe above simple statistical model models the linear relationship of \\(y_i\\) against \\(x_i\\). The residuals (\\(\\epsilon\\)) are assumed to be normally distributed with a mean of zero and a constant (yet unknown) variance (\\(\\sigma\\), homogeneity of variance). The residuals (and thus observations) are also assumed to all be independent.\nHomogeneity of variance and independence are encapsulated within the single symbol for variance (\\(\\sigma^2\\)). In assuming equal variances and independence, we are actually making an assumption about the variance-covariance structure of the populations (and thus residuals). Specifically, we assume that all populations are equally varied and thus can be represented well by a single variance term (all diagonal values in a \\(N\\times N\\) covariance matrix are the same, \\(\\sigma^2\\)) and the covariances between each population are zero (off diagonals). In simple regression, each observation (data point) represents a single observation drawn (sampled) from an entire population of possible observations. The above covariance structure thus assumes that the covariance between each population (observation) is zero - that is, each observation is completely independent of each other observation. Whilst it is mathematically convenient when data conform to these conditions (normality, homogeneity of variance, independence and linearity), data often violate one or more of these assumptions. In the following, I want to discuss and explore the causes and options for dealing with non-compliance to each of these conditions. By gaining a better understanding of how the various model fitting engines perform their task, we are better equipped to accommodate aspects of the data that don’t otherwise conform to the simple regression assumptions. In this tutorial we specifically focus on the topic of heterogeneity of the variance.\n\r\rDealing with heterogeneity\rThe validity and reliability of the above linear models are very much dependent on variance homogeneity. In particular, variances that increase (or decrease) with a change in expected values are substantial violations. Whilst non-normality can also be a source of heterogeneity and therefore normalising can address both issues, heterogeneity can also be independent of normality. Similarly, generalised linear models (that accommodate alternative residual distributions - such as Poisson, Binomial, Gamma, etc) can be useful for more appropriate modelling of both the distribution and variance of a model. However, for Gaussian (normal) models in which there is evidence of heterogeneity of variance, yet no evidence of non-normality, it is also possible to specifically model in an alternative variance structure. For example, we can elect to allow variance to increase proportionally to a covariate. To assist us in the following demonstration, we will generate another data set - one that has heteroskedasticity (unequal variance) by design. Rather than draw each residual (and thus observation) from a normal distribution with a constant standard deviation), we will draw the residuals from normal distributions whose variance is proportional to the \\(X\\) predictor.\n\u0026gt; set.seed(126)\r\u0026gt; n \u0026lt;- 16\r\u0026gt; a \u0026lt;- 40 #intercept\r\u0026gt; b \u0026lt;- 1.5 #slope\r\u0026gt; x \u0026lt;- 1:n #values of the year covariate\r\u0026gt; sigma \u0026lt;- 1.5 * x\r\u0026gt; sigma\r[1] 1.5 3.0 4.5 6.0 7.5 9.0 10.5 12.0 13.5 15.0 16.5 18.0 19.5 21.0 22.5\r[16] 24.0\r\u0026gt; \u0026gt; eps \u0026lt;- rnorm(n, mean = 0, sd = sigma) #residuals\r\u0026gt; y \u0026lt;- a + b * x + eps #response variable\r\u0026gt; # OR\r\u0026gt; y \u0026lt;- (model.matrix(~x) %*% c(a, b)) + eps\r\u0026gt; data.het \u0026lt;- data.frame(y = round(y, 1), x) #dataset\r\u0026gt; head(data.het) #print out the first six rows of the data set\ry x\r1 42.1 1\r2 44.2 2\r3 41.2 3\r4 51.7 4\r5 43.5 5\r6 48.3 6\r\u0026gt; \u0026gt; # scatterplot of y against x\r\u0026gt; library(car)\r\u0026gt; scatterplot(y ~ x, data.het)\r\u0026gt; \u0026gt; # regular simple linear regression\r\u0026gt; data.het.lm \u0026lt;- lm(y ~ x, data.het)\r\u0026gt; \u0026gt; # plot of standardised residuals\r\u0026gt; plot(rstandard(data.het.lm) ~ fitted(data.het.lm))\r\u0026gt; \u0026gt; # plot of standardized residuals against the predictor\r\u0026gt; plot(rstandard(data.het.lm) ~ x)\rThe above scatterplot suggests that variance may increase with increasing \\(X\\). The residual plot (using standardised residuals) suggests that mean and variance could be related - there is a hint of a wedge-shaped pattern. Importantly, the plot of standardised residuals against the predictor shows the same pattern as the residual plot implying that heterogeneity is likely to be due a relationship between variance \\(X\\). That is, an increase in \\(X\\) is associated with an increase in variance. In response to this, we could incorporate an alternative variance structure. The simple model we fit earlier assumed that the expected values were all drawn from normal distributions with the same level of precision \\(\\tau\\) and therefore variance. This assumption is often summarised as:\n\\[ \\boldsymbol V = \\sigma^2 \\times \\boldsymbol I,\\]\nwhere \\(\\boldsymbol I\\) is the \\(N \\times N\\) identity matrix (elements on the main diagonal are one and zero outside) which multipled by the constant value \\(\\sigma^2\\) produces the homoskedastic covariance matrix \\(\\boldsymbol V\\) (elements on the main diagonal are \\(\\sigma^2\\) and zero outside). If, instead, we consider an heteroskedastic covariance matrix then, for example, we could assume that the variance is proportional to the level of the covariate. This assumption can be summarised as:\n\\[ \\boldsymbol V = \\sigma^2 \\times X \\times \\boldsymbol I,\\]\nwhere the product of the identity matrix \\(\\boldsymbol I\\) and the covariate-specific values \\(\\sigma^2 \\times X\\) produces the heteroskedastic covariance matrix \\(\\boldsymbol V\\) (elements on the main diagonal are \\(\\sigma^2 \\times X\\) and zero outside). With a couple of small adjustments, we can modify the JAGS code in order to accommodate a variance structure in which variance is proportional to the predictor variable. Note that since JAGS works with precision (\\(\\tau=\\frac{1}{\\sigma^2}\\)), I have elected to express the predictor as \\(\\frac{1}{x}\\). This way the weightings are compatible with precision rather than variance. In previous tutorials, we have used a flat, uniform distribution \\([0,100]\\) for variance priors. Whilst this is a reasonable choice for a non-informative prior, Gelman and others (2006) suggest that half-cauchy priors are more appropriate when the number of groups is low.\n\rModel fitting\rThe observed response (\\(y_i\\)) are assumed to be drawn from a normal distribution with a given mean (\\(\\mu\\)) and standard deviation weighted by \\(1\\) on the value of the covariate (\\(\\sigma \\times \\omega\\)). The expected values (\\(\\mu\\)) are themselves determined by the linear predictor (\\(\\beta_0 + \\beta_1x\\)). In this case, \\(\\beta_0\\) represents the mean of the first group and the set of \\(\\beta\\)’s represent the differences between each other group and the first group. MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying ‘uninformative’ priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (\\(100\\)) for both the intercept and the treatment effect and a wide half-cauchy (\\(\\text{scale}=5\\)) for the standard deviation.\n\\[y_i \\sim N(\\mu_i,\\sigma \\times \\omega), \\]\nwhere \\(\\mu_i=\\beta_0 +\\boldsymbol \\beta \\boldsymbol X\\). The assumed priors are: \\(\\beta \\sim N(0,100)\\) and \\(\\sigma \\sim \\text{Cauchy}(0,5)\\). We proceed to code the model into STAN\n\u0026gt; modelString = \u0026quot;\r+ data {\r+ int\u0026lt;lower=1\u0026gt; n;\r+ int\u0026lt;lower=1\u0026gt; nX;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ vector [n] w;\r+ }\r+ parameters {\r+ vector[nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ }\r+ transformed parameters {\r+ vector[n] mu;\r+ + mu = X*beta;\r+ }\r+ model {\r+ // Likelihood\r+ y~normal(mu,sigma*w);\r+ + // Priors\r+ beta ~ normal(0,1000);\r+ sigma~cauchy(0,5);\r+ }\r+ generated quantities {\r+ vector[n] log_lik;\r+ + for (i in 1:n) {\r+ log_lik[i] = normal_lpdf(y[i] | mu[i], sigma*w[i]); + }\r+ }\r+ + \u0026quot;\r\u0026gt; ## write the model to a stan file \u0026gt; writeLines(modelString, con = \u0026quot;heteroskModel.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; Xmat \u0026lt;- model.matrix(~x, data.het)\r\u0026gt; data.het.list \u0026lt;- with(data.het, list(y = y, X = Xmat, w = sqrt(data.het$x),\r+ n = nrow(data.het), nX = ncol(Xmat)))\rDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;log_lik\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1500\rNow compile and run the Stan code via the rstan interface. Note that the first time stan is run after the rstan package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; library(rstan)\rDuring the warmup stage, the No-U-Turn sampler (NUTS) attempts to determine the optimum stepsize - the stepsize that achieves the target acceptance rate (\\(0.8\\) or \\(80\\)% by default) without divergence (occurs when the stepsize is too large relative to the curvature of the log posterior and results in approximations that are likely to diverge and be biased) - and without hitting the maximum treedepth (\\(10\\)). At each iteration of the NUTS algorithm, the number of leapfrog steps doubles (as it increases the treedepth) and only terminates when either the NUTS criterion are satisfied or the tree depth reaches the maximum (\\(10\\) by default).\n\u0026gt; data.het.rstan \u0026lt;- stan(data = data.het.list, file = \u0026quot;heteroskModel.stan\u0026quot;, chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;heteroskModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 1: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 1: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 1: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 1: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 1: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 1: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 1: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 1: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 1: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 1: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 1: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.058 seconds (Warm-up)\rChain 1: 0.077 seconds (Sampling)\rChain 1: 0.135 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;heteroskModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 2: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 2: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 2: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 2: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 2: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 2: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 2: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 2: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 2: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 2: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 2: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.057 seconds (Warm-up)\rChain 2: 0.079 seconds (Sampling)\rChain 2: 0.136 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.het.rstan, par = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: heteroskModel.\r2 chains, each with iter=1500; warmup=500; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 41.43 0.09 2.57 36.19 39.83 41.49 43.09 46.48 855 1\rbeta[2] 1.13 0.01 0.41 0.31 0.87 1.13 1.39 1.98 806 1\rsigma 3.06 0.02 0.62 2.13 2.63 2.98 3.36 4.57 934 1\rSamples were drawn using NUTS(diag_e) at Tue Feb 18 15:33:02 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots.\n\u0026gt; library(mcmcplots)\r\u0026gt; mcmc = As.mcmc.list(data.het.rstan)\r\u0026gt; denplot(mcmc, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\r\u0026gt; traplot(mcmc, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rThese plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.\n\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r[[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; stan_ac(data.het.rstan, pars = c(\u0026quot;beta\u0026quot;))\rA lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).\n\u0026gt; stan_rhat(data.het.rstan, pars = c(\u0026quot;beta\u0026quot;))\r\u0026gt; stan_ess(data.het.rstan, pars = c(\u0026quot;beta\u0026quot;))\rRhat and effective sample size. In this instance, most of the parameters have reasonably high effective samples and thus there is likely to be a good range of values from which to estimate paramter properties.\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. Ideally, a good model should also be able to predict the data used to fit the model. Residuals are not computed directly within rstan However, we can calculate them manually form the posteriors.\n\u0026gt; mcmc = as.matrix(data.het.rstan)[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data.het$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc, 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data.het$y - fit\r\u0026gt; \u0026gt; library(ggplot2)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rThe above residual plot would make us believe that we had a homogeneity of variance issue (which we thought we were addressing by defining a model that allowed the variance to be proportional to the predictor). This is because we have plotted the raw residuals rather than residuals that have been standardized by the variances. The above plot is also what the residual plot would look like if we had not made any attempt to define a model in which the variance was related to the predictor. Whenever we fit a model that incorporates changes to the variance-covariance structures, we should explore standardised residuals. In this case, we should divide the residuals by sigma and then divide by the square-root of the weights.\n\\[ Res_i = \\frac{Y_i - \\mu_i}{\\sigma \\times \\sqrt{\\omega}}\\]\n\u0026gt; library(dplyr)\r\u0026gt; library(tidyr)\r\u0026gt; mcmc = as.matrix(data.het.rstan)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; Xmat = model.matrix(~x, data.het)\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = -1 * sweep(fit, 2, data.het$y, \u0026quot;-\u0026quot;)\r\u0026gt; resid = apply(resid, 2, median)/(median(mcmc[, \u0026quot;sigma\u0026quot;]) * sqrt(data.het$x))\r\u0026gt; fit = apply(fit, 2, median)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rThis is certainly an improvement. Nevertheless, there is still an indication of a relationship between mean and variance. We could attempt to further address this by refining \\(\\omega\\) in the Bayesian model. That is, rather than indicate that variance is proportional to \\(x\\), we could indicate that variance is proportional to \\(x^2\\) (as an example) - we will leave this as an exercise for the reader. Residuals against predictors.\n\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data.het$x)) + theme_classic()\rLets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = as.matrix(data.het.rstan)\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~x, data.het)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data.het), fit[i, ],\r+ mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; ggplot() + geom_density(data = NULL, aes(x = as.vector(yRep), fill = \u0026quot;Model\u0026quot;),\r+ alpha = 0.5) + geom_density(data = data.het, aes(x = y, fill = \u0026quot;Obs\u0026quot;),\r+ alpha = 0.5) + theme_classic()\r\rParameter estimates\rFirst, we look at the results from the model.\n\u0026gt; print(data.het.rstan, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: heteroskModel.\r2 chains, each with iter=1500; warmup=500; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 41.43 0.09 2.57 36.19 39.83 41.49 43.09 46.48 855 1\rbeta[2] 1.13 0.01 0.41 0.31 0.87 1.13 1.39 1.98 806 1\rsigma 3.06 0.02 0.62 2.13 2.63 2.98 3.36 4.57 934 1\rSamples were drawn using NUTS(diag_e) at Tue Feb 18 15:33:02 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(data.het.rstan, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\r# A tibble: 3 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 41.4 2.57 36.6 46.8 2 beta[2] 1.13 0.410 0.277 1.92\r3 sigma 3.06 0.624 2.07 4.40\rConclusions\nA one unit increase in \\(x\\) is associated with a \\(1.11\\) change in \\(y\\). That is, \\(y\\) declines at a rate of \\(1.11\\) per unit increase in \\(x\\). The \\(95\\)% confidence interval for the slope does not overlap with \\(0\\) implying a significant effect of \\(x\\) on \\(y\\). While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\r\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(as.matrix(data.het.rstan)[, c(\u0026quot;beta[2]\u0026quot;)])\r[1] 0.0105\rWith a p-value of essentially \\(0\\), we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship.\n\rGraphical summaries\rA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group.\n\u0026gt; mcmc = as.matrix(data.het.rstan)\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = data.frame(x = seq(min(data.het$x, na.rm = TRUE), max(data.het$x,\r+ na.rm = TRUE), len = 1000))\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X\u0026quot;) + theme_classic()\rIf you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.\n\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data.het,\r+ aes(y = y, x = x), color = \u0026quot;gray\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X\u0026quot;) + theme_classic()\rA more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since \\(\\text{resid}=\\text{obs}−\\text{fitted}\\) and the fitted values depend only on the single predictor we are interested in.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = data.het\r\u0026gt; fMat = rMat = model.matrix(~x, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data.het$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\r+ color = \u0026quot;gray\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low, ymax = conf.high),\r+ fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_continuous(\u0026quot;X\u0026quot;) +\r+ theme_classic()\r\rR squared\rIn a frequentist context, the \\(R^2\\) value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, \\(R^2\\) is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an \\(R^2\\) greater than \\(100\\)%. Gelman et al. (2019) proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.\nSo in the standard regression model notation of:\n\\[ y_i \\sim \\text{Normal}(\\boldsymbol X \\boldsymbol \\beta, \\sigma),\\]\nthe \\(R^2\\) could be formulated as\n\\[ R^2 = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sigma^2_e},\\]\nwhere \\(\\sigma^2_f=\\text{var}(\\boldsymbol X \\boldsymbol \\beta)\\), and for normal models \\(\\sigma^2_e=\\text{var}(y-\\boldsymbol X \\boldsymbol \\beta)\\)\n\u0026gt; mcmc \u0026lt;- as.matrix(data.het.rstan)\r\u0026gt; Xmat = model.matrix(~x, data.het)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta[1]\u0026quot;,\u0026quot;beta[2]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = sweep(fit, 2, data.het$y, \u0026quot;-\u0026quot;)\r\u0026gt; var_f = apply(fit, 1, var)\r\u0026gt; var_e = apply(resid, 1, var)\r\u0026gt; R2 = var_f/(var_f + var_e)\r\u0026gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.248 0.109 0.0101 0.424\r\u0026gt; \u0026gt; # for comparison with frequentist summary(lm(y ~ x, data.het))\r\rHeteroskedasticity with categorical predictors\rFor regression models that include a categorical variable (e.g. ANOVA), heterogeneity manifests as vastly different variances for different levels (treatment groups) of the categorical variable. Recall, that this is diagnosed from the relative size of boxplots. Whilst, the degree of group variability may not be related to the means of the groups, having wildly different variances does lead to an increase in standard errors and thus a lowering of power. In such cases, we would like to be able to indicate that the variances should be estimated separately for each group. That is the variance term is multiplied by a different number for each group. The appropriate matrix is referred to as an Identity matrix. Again, to assist in the explanation some fabricated ANOVA data - data that has heteroscadasticity by design - will be useful.\n\u0026gt; set.seed(126)\r\u0026gt; ngroups \u0026lt;- 5 #number of populations\r\u0026gt; nsample \u0026lt;- 10 #number of reps in each\r\u0026gt; pop.means \u0026lt;- c(40, 45, 55, 40, 30) #population mean length\r\u0026gt; sigma \u0026lt;- rep(c(6, 4, 2, 0.5, 1), each = nsample) #residual standard deviation\r\u0026gt; n \u0026lt;- ngroups * nsample #total sample size\r\u0026gt; eps \u0026lt;- rnorm(n, 0, sigma) #residuals\r\u0026gt; x \u0026lt;- gl(ngroups, nsample, n, lab = LETTERS[1:5]) #factor\r\u0026gt; means \u0026lt;- rep(pop.means, rep(nsample, ngroups))\r\u0026gt; X \u0026lt;- model.matrix(~x - 1) #create a design matrix\r\u0026gt; y \u0026lt;- as.numeric(X %*% pop.means + eps)\r\u0026gt; data.het1 \u0026lt;- data.frame(y, x)\r\u0026gt; boxplot(y ~ x, data.het1)\r\u0026gt; \u0026gt; plot(lm(y ~ x, data.het1), which = 3)\rIt is clear that there is gross heteroskedasticity. The residuals are obviously more spread in some groups than others yet there is no real pattern with means (the residual plot does not show an obvious wedge). Note, for assessing homogeneity of variance, it is best to use the standardised residuals. It turns out that if we switch over to maximum (log) likelihood estimation methods, we can model in a within-group heteroskedasticity structure rather than just assume one very narrow form of variance structure. Lets take a step back and reflect on our simple ANOVA (regression) model that has five groups each with \\(10\\) observations:\n\\[ y_i = \\mu + \\alpha_i + \\epsilon, \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon \\sim N(0, \\sigma^2). \\]\nThis is shorthand notation to indicate that the response variable is being modelled against a specific linear predictor and that the residuals follow a normal distribution with a certain variance (that is the same for each group). Rather than assume that the variance of each group is the same, we could relax this a little so as to permit different levels of variance per group:\n\\[ \\epsilon \\sim N(0, \\sigma^2_i).\\]\nTo achieve this, we actually multiply the variance matrix by a weighting matrix, where the weights associated with each group are determined by the inverse of the ratio of each group to the first (reference) group:\n\\[ \\epsilon \\sim N(0, \\sigma^2_i \\times \\omega).\\]\nSo returning to our five groups of \\(10\\) observations example, the weights would be determined as:\n\u0026gt; data.het1.sd \u0026lt;- with(data.het1, tapply(y, x, sd))\r\u0026gt; 1/(data.het1.sd[1]/data.het1.sd)\rA B C D E 1.0000000 0.6909012 0.4140893 0.1426207 0.3012881 \rThe weights determine the relative amount of each observation that goes into calculating variances. The basic premise is that those with lower variances are likely to be more precise and therefore should have greatest contribution to variance calculations.\nModel fitting\r\u0026gt; modelString2 = \u0026quot;\r+ data {\r+ int\u0026lt;lower=1\u0026gt; n;\r+ int\u0026lt;lower=1\u0026gt; nX;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ }\r+ parameters {\r+ vector[nX] beta;\r+ vector\u0026lt;lower=0\u0026gt;[nX] sigma;\r+ }\r+ transformed parameters {\r+ vector[n] mu;\r+ vector\u0026lt;lower=0\u0026gt;[n] sig;\r+ + mu = X*beta;\r+ sig = X*sigma;\r+ }\r+ model {\r+ // Likelihood\r+ y~normal(mu,sig);\r+ + // Priors\r+ beta ~ normal(0,1000);\r+ sigma~cauchy(0,5);\r+ }\r+ generated quantities {\r+ vector[n] log_lik;\r+ + for (i in 1:n) {\r+ log_lik[i] = normal_lpdf(y[i] | mu[i], sig[i]); + }\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString2, con = \u0026quot;heteroskModel2.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; Xmat \u0026lt;- model.matrix(~x, data.het1)\r\u0026gt; data.het1.list \u0026lt;- with(data.het1, list(y = y, X = Xmat, n = nrow(data.het1),\r+ nX = ncol(Xmat)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;log_lik\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1500\rNow run the STAN code via the rstan interface. Note that the first time STAN is run after the rstan package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; data.het1.rstan \u0026lt;- stan(data = data.het1.list, file = \u0026quot;heteroskModel2.stan\u0026quot;,\r+ chains = nChains, iter = numSavedSteps, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;heteroskModel2\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup)\rChain 1: Iteration: 200 / 2000 [ 10%] (Warmup)\rChain 1: Iteration: 400 / 2000 [ 20%] (Warmup)\rChain 1: Iteration: 501 / 2000 [ 25%] (Sampling)\rChain 1: Iteration: 700 / 2000 [ 35%] (Sampling)\rChain 1: Iteration: 900 / 2000 [ 45%] (Sampling)\rChain 1: Iteration: 1100 / 2000 [ 55%] (Sampling)\rChain 1: Iteration: 1300 / 2000 [ 65%] (Sampling)\rChain 1: Iteration: 1500 / 2000 [ 75%] (Sampling)\rChain 1: Iteration: 1700 / 2000 [ 85%] (Sampling)\rChain 1: Iteration: 1900 / 2000 [ 95%] (Sampling)\rChain 1: Iteration: 2000 / 2000 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.127 seconds (Warm-up)\rChain 1: 0.275 seconds (Sampling)\rChain 1: 0.402 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;heteroskModel2\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup)\rChain 2: Iteration: 200 / 2000 [ 10%] (Warmup)\rChain 2: Iteration: 400 / 2000 [ 20%] (Warmup)\rChain 2: Iteration: 501 / 2000 [ 25%] (Sampling)\rChain 2: Iteration: 700 / 2000 [ 35%] (Sampling)\rChain 2: Iteration: 900 / 2000 [ 45%] (Sampling)\rChain 2: Iteration: 1100 / 2000 [ 55%] (Sampling)\rChain 2: Iteration: 1300 / 2000 [ 65%] (Sampling)\rChain 2: Iteration: 1500 / 2000 [ 75%] (Sampling)\rChain 2: Iteration: 1700 / 2000 [ 85%] (Sampling)\rChain 2: Iteration: 1900 / 2000 [ 95%] (Sampling)\rChain 2: Iteration: 2000 / 2000 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.122 seconds (Warm-up)\rChain 2: 0.253 seconds (Sampling)\rChain 2: 0.375 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.het1.rstan, par = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: heteroskModel2.\r2 chains, each with iter=2000; warmup=500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 40.28 0.02 0.63 39.06 39.87 40.28 40.71 41.53 1429 1\rbeta[2] 4.10 0.03 1.14 1.82 3.35 4.10 4.86 6.33 1903 1\rbeta[3] 14.55 0.02 0.99 12.57 13.91 14.56 15.22 16.45 1810 1\rbeta[4] -0.66 0.02 0.95 -2.54 -1.29 -0.64 -0.01 1.17 1931 1\rbeta[5] -10.35 0.02 0.97 -12.32 -10.97 -10.32 -9.72 -8.51 1806 1\rsigma[1] 1.99 0.01 0.24 1.58 1.83 1.96 2.13 2.53 2072 1\rsigma[2] 0.89 0.01 0.72 0.05 0.37 0.72 1.22 2.68 3042 1\rsigma[3] 0.41 0.01 0.43 0.01 0.12 0.28 0.57 1.60 2468 1\rsigma[4] 0.30 0.01 0.33 0.01 0.08 0.20 0.39 1.18 2654 1\rsigma[5] 0.35 0.01 0.37 0.01 0.10 0.24 0.47 1.36 3076 1\rSamples were drawn using NUTS(diag_e) at Tue Feb 18 15:33:49 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\rMCMC diagnostics\r\u0026gt; library(mcmcplots)\r\u0026gt; mcmc\u0026lt;-As.mcmc.list(data.het1.rstan)\r\u0026gt; denplot(mcmc, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\r\u0026gt; traplot(mcmc, parms = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots.\n\r\rModel validation\r\u0026gt; mcmc = as.matrix(data.het.rstan)[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data.het$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc, 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data.het$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rThe above residual plot would make us believe that we had a homogeneity of variance issue (which we thought we were addressing by defining a model that allowed the variance to be proportional to the predictor). This is because we have plotted the raw residuals rather than residuals that have been standardized by the variances. The above plot is also what the residual plot would look like if we had not made any attempt to define a model in which the variance was related to the predictor. Whenever we fit a model that incorporates changes to the variance-covariance structures, we should explore standardized residuals. In this case, we should divide the residuals by the appropriate sigma for associated with that group (level of predictor).\n\\[ Res_{ij} = \\frac{Y_{ij} - \\mu_j}{\\sigma_j}\\]\n\u0026gt; mcmc = as.matrix(data.het1.rstan)\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data.het1$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, wch], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data.het1$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rThis is certainly an improvement. Nevertheless, there is still an indication of a relationship between mean and variance. We could attempt to further address this by refining \\(\\omega\\) in the Bayesian model. That is, rather than indicate that variance is proportional to \\(x\\), we could indicate that variance is proportional to \\(x^2\\) (as an example) - we will leave this as an exercise for the reader. Residuals against predictors.\n\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data.het1$x)) + theme_classic()\rLets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = as.matrix(data.het1.rstan)\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~x, data.het1)\r\u0026gt; ## get median parameter estimates\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; wch = grep(\u0026quot;sigma\u0026quot;, colnames(mcmc))\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data.het1), fit[i, ],\r+ mcmc[i, wch[as.numeric(data.het1$x[i])]]))\r\u0026gt; newdata = data.frame(x = data.het1$x, yRep) %\u0026gt;% gather(key = Sample, value = Value,\r+ -x)\r\u0026gt; ggplot(newdata) + geom_violin(aes(y = Value, x = x, fill = \u0026quot;Model\u0026quot;), alpha = 0.5) +\r+ geom_violin(data = data.het1, aes(y = y, x = x, fill = \u0026quot;Obs\u0026quot;), alpha = 0.5) +\r+ geom_point(data = data.het1, aes(y = y, x = x), position = position_jitter(width = 0.1,\r+ height = 0), color = \u0026quot;black\u0026quot;) + theme_classic()\rParameter estimates\rFirst, we look at the results from the model.\n\u0026gt; print(data.het1.rstan, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: heteroskModel2.\r2 chains, each with iter=2000; warmup=500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 40.28 0.02 0.63 39.06 39.87 40.28 40.71 41.53 1429 1\rbeta[2] 4.10 0.03 1.14 1.82 3.35 4.10 4.86 6.33 1903 1\rbeta[3] 14.55 0.02 0.99 12.57 13.91 14.56 15.22 16.45 1810 1\rbeta[4] -0.66 0.02 0.95 -2.54 -1.29 -0.64 -0.01 1.17 1931 1\rbeta[5] -10.35 0.02 0.97 -12.32 -10.97 -10.32 -9.72 -8.51 1806 1\rsigma[1] 1.99 0.01 0.24 1.58 1.83 1.96 2.13 2.53 2072 1\rsigma[2] 0.89 0.01 0.72 0.05 0.37 0.72 1.22 2.68 3042 1\rsigma[3] 0.41 0.01 0.43 0.01 0.12 0.28 0.57 1.60 2468 1\rsigma[4] 0.30 0.01 0.33 0.01 0.08 0.20 0.39 1.18 2654 1\rsigma[5] 0.35 0.01 0.37 0.01 0.10 0.24 0.47 1.36 3076 1\rSamples were drawn using NUTS(diag_e) at Tue Feb 18 15:33:49 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; # OR\r\u0026gt; tidyMCMC(data.het1.rstan, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;,\r+ rhat = TRUE, ess = TRUE)\r# A tibble: 10 x 7\rterm estimate std.error conf.low conf.high rhat ess\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r1 beta[1] 40.3 0.629 39.1 41.5 1.00 1429\r2 beta[2] 4.10 1.14 1.82 6.33 1.00 1903\r3 beta[3] 14.6 0.985 12.7 16.5 1.00 1810\r4 beta[4] -0.663 0.953 -2.54 1.17 1.000 1931\r5 beta[5] -10.4 0.973 -12.3 -8.53 1.00 1806\r6 sigma[1] 1.99 0.242 1.56 2.50 1.00 2072\r7 sigma[2] 0.887 0.719 0.000922 2.30 1.000 3042\r8 sigma[3] 0.414 0.429 0.000139 1.29 1.000 2468\r9 sigma[4] 0.295 0.325 0.00000841 0.914 1.00 2654\r10 sigma[5] 0.350 0.369 0.000149 1.07 1.00 3076\rConclusions\n\rthe mean of the first group (A) is \\(40.3\\)\n\rthe mean of the second group (B) is \\(4.12\\) units greater than (A)\n\rthe mean of the third group (C) is \\(14.6\\) units greater than (A)\n\rthe mean of the forth group (D) is \\(-0.637\\) units greater (i.e. less) than (A)\n\rthe mean of the fifth group (E) is \\(-10.3\\) units greater (i.e. less) than (A)\n\r\rThe \\(95\\)% confidence interval for the effects of B, C and E do not overlap with \\(0\\) implying a significant difference between group A and groups B, C and E. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\r\u0026gt; ## since values are less than zero\r\u0026gt; mcmc = as.matrix(data.het1.rstan)\r\u0026gt; for (i in grep(\u0026quot;beta\u0026quot;, colnames(mcmc), value = TRUE)) print(paste(i, mcmcpvalue(mcmc[,\r+ i])))\r[1] \u0026quot;beta[1] 0\u0026quot;\r[1] \u0026quot;beta[2] 0.001\u0026quot;\r[1] \u0026quot;beta[3] 0\u0026quot;\r[1] \u0026quot;beta[4] 0.487666666666667\u0026quot;\r[1] \u0026quot;beta[5] 0\u0026quot;\r\u0026gt; mcmcpvalue(mcmc[, grep(\u0026quot;beta\u0026quot;, colnames(mcmc))])\r[1] 0\rWith a p-value of essentially \\(0\\), we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship.\n\rGraphical summaries\r\u0026gt; mcmc = as.matrix(data.het1.rstan)\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = data.frame(x = levels(data.het1$x))\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) +\r+ theme_classic()\rIf you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.\n\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data.het1,\r+ aes(y = y, x = x), color = \u0026quot;gray\u0026quot;) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) +\r+ theme_classic()\rA more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since \\(\\text{resid}=\\text{obs}−\\text{fitted}\\) and the fitted values depend only on the single predictor we are interested in.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = data.het1\r\u0026gt; fMat = rMat = model.matrix(~x, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data.het1$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\r+ color = \u0026quot;gray\u0026quot;) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\r+ scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) + theme_classic()\r\r\rReferences\rGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9.\n\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rGelman, Andrew, and others. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).” Bayesian Analysis 1 (3): 515–34.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\r\r\r","date":1581127994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581127994,"objectID":"530b114f7e8d277341b7c1588ed6f39c","permalink":"/stan/heterogeneity-stan/heterogeneity-stan/","publishdate":"2020-02-07T21:13:14-05:00","relpermalink":"/stan/heterogeneity-stan/heterogeneity-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","heterogeneity","heteroskedasticity"],"title":"Variance Heterogeneity - STAN","type":"STAN"},{"authors":["Andrea Gabrio"],"categories":["R","anova","JAGS","factor analysis","factorial designs"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rFactorial designs are an extension of single factor ANOVA designs in which additional factors are added such that each level of one factor is applied to all levels of the other factor(s) and these combinations are replicated. For example, we might design an experiment in which the effects of temperature (high vs low) and fertiliser (added vs not added) on the growth rate of seedlings are investigated by growing seedlings under the different temperature and fertilizer combinations. In addition to investigating the impacts of the main factors, factorial designs allow us to investigate whether the effects of one factor are consistent across levels of another factor. For example, is the effect of temperature on growth rate the same for both fertilised and unfertilized seedlings and similarly, does the impact of fertiliser treatment depend on the temperature under which the seedlings are grown?\nArguably, these interactions give more sophisticated insights into the dynamics of the system we are investigating. Hence, we could add additional main effects, such as soil pH, amount of water, etc, along with all the two way (temp:fert, temp:pH, temp:water, etc), three-way (temp:fert:pH, temp:pH:water), four-way (and so on) interactions in order to explore how these various factors interact with one another to effect the response. However, the more interactions, the more complex the model becomes to specify, compute and interpret - not to mention the rate at which the number of required observations increases. Factorial designs can consist:\n\rentirely of crossed fixed factors (Model I ANOVA - most common) in which conclusions are restricted to the specific combinations of levels selected for the experiment.\n\rentirely of crossed random factors (Model II ANOVA).\n\ra mixture of crossed fixed and random factors (Model III ANOVA).\n\r\rThe latter are useful for investigating the generality of a main treatment effect (fixed) over broad spatial, temporal or clinical levels of organisation. That is whether the observed effects of temperature and/or fertiliser (for example) are observed across the entire genera or country.\n\rLinear model\rAs with single factor ANOVA, the linear model could be constructed as either effects or means parameterisation, although only effects parameterisation will be considered here. The linear models for two and three factor design are\n\\[ y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk},\\]\nand\n\\[ y_{ijkl} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijkl},\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha\\) is the effect of Factor A, \\(\\beta\\) is the effect of Factor B, \\(\\gamma\\) is the effect of Factor C and \\(\\epsilon\\) is the random unexplained or residual component. Note that although the linear models for Model I, Model II and Model III designs are identical, the interpretation of terms (and thus null hypothesis) differ. Recall from the tutorial on single factor ANOVA, that categorical variables in linear models are actually re-parameterised dummy codes - and thus the \\(\\alpha\\) term above, actually represents one or more dummy codes. Thus, if we actually had two levels of Factor A (A1 and A2) and three levels of Factor B (B1, B2, B3), then the fully parameterised linear model would be:\n\\[ y=\\beta_{A1B1}+\\beta_{A2B1−A1B1}+\\beta_{A1B2−A1B1}+\\beta_{A1B3−A1B1}+\\beta_{A2B2−A1B2−A2B1−A1B1}+\\beta_{A2B3−A1B3−A2B1−A1B1}.\\]\nThus, such a model would have six parameters to estimate (in addition to the variance).\n\rNull hypothesis\rThere are separate null hypothesis associated with each of the main effects and the interaction terms.\nModel 1 - fixed effects\rFactor A\n\r\\(H_0(A):\\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\)\r\rThe population group means are all equal. The mean of population \\(1\\) is equal to that of population \\(2\\) and so on, and thus all population means are equal to an overall mean. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the overall mean (\\(\\alpha_i=\\mu_i-\\mu\\)) then the \\(H_0\\) can alternatively be written as:\n\r\\(H_0(A):\\alpha_1=\\alpha_2=\\ldots=\\alpha_i=0\\)\r\rThe effect of each group equals zero. If one or more of the \\(\\alpha_i\\) are different from zero (the response mean for this treatment differs from the overall response mean), the null hypothesis is rejected indicating that the treatment has been found to affect the response variable. Note, as with multiple regression models, these “effects” represent partial effects. In the above, the effect of Factor A is actually the effect of Factor A at the first level of the Factor(s).\nFactor B\n\r\\(H_0(B):\\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\)\r\rThe population group means are all equal - at the first level of Factor A. Equivalent interpretation to Factor A above.\nFactor AB: interaction\n\r\\(H_0(AB):\\mu_{ij}=\\mu_i+\\mu_j-\\mu\\)\r\rThe population group means are all equal. For any given combination of factor levels, the population group mean will be equal to the difference between the overall population mean and the simple additive effects of the individual factor group means. That is, the effects of the main treatment factors are purely additive and independent of one another. This is equivalent to \\(H_0(AB): \\alpha\\beta_{ij}=0\\), no interaction between Factor A and Factor B.\n\rModel 2 - random effects\rFactor A\n\r\\(H_0(A):\\sigma^2_{\\alpha}=0\\)\r\rThe population variance equals zero. There is no added variance due to all possible levels of A.\nFactor B\n\r\\(H_0(B):\\sigma^2_{\\beta}=0\\)\r\rThe population variance equals zero. There is no added variance due to all possible levels of B.\nFactor AB: interaction\n\r\\(H_0(AB):\\sigma^2_{\\alpha\\beta}=0\\)\r\rThere is no added variance due to all possible interactions between all possible levels of A and B.\n\rModel 3 - mixed effects\rFixed factor - e.g. A\n\r\\(H_0(A):\\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\)\r\rThe population group means are all equal. The mean of population \\(1\\) (pooled over all levels of the random factor) is equal to that of population \\(2\\) and so on, and thus all population means are equal to an overall mean pooling over all possible levels of the random factor. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the overall mean (\\(\\alpha_i=\\mu_i-\\mu\\)) then the \\(H_0\\) can alternatively be written as:\n\r\\(H_0(A):\\alpha_1=\\alpha_2=\\ldots=\\alpha_i=0\\)\r\rNo effect of any level of this factor pooled over all possible levels of the random factor.\nRandom factor - e.g. B\n\r\\(H_0(B):\\sigma^2_{\\beta}=0\\)\r\rThe population variance equals zero. There is no added variance due to all possible levels of B.\nFactor AB: interaction\nThe interaction of a fixed and random factor is always considered a random factor.\n\r\\(H_0(AB):\\sigma^2_{\\alpha\\beta}=0\\)\r\rThe population variance equals zero. There is no added variance due to all possible interactions between all possible levels of A and B.\n\r\rAnalysis of variance\rWhen fixed factorial designs are balanced, the total variance in the response variable can be sequentially partitioned into what is explained by each of the model terms (factors and their interactions) and what is left unexplained. For each of the specific null hypotheses, the overall unexplained variability is used as the denominator in F-ratio calculations, and when a null hypothesis is true, an F-ratio should follow an F distribution with an expected value less than \\(1\\). Random factors are added to provide greater generality of conclusions. That is, to enable us to make conclusions about the effect of one factor (such as whether or not fertiliser is added) over all possible levels (not just those sampled) of a random factor (such as all possible locations, seasons, varieties, etc). In order to expand our conclusions beyond the specific levels used in the design, the hypothesis tests (and thus F-ratios) must reflect this extra generality by being more conservative.\nThe appropriate F-ratios for fixed, random and mixed factorial designs are presented below. Generally, once the terms (factors and interactions) have been ordered into a hierarchy (single factors at the top, highest level interactions at the bottom and terms of same order given equivalent positions in the hierarchy), the denominator for any term is selected as the next appropriate random term (an interaction that includes the term to be tested) encountered lower in the hierarchy. Interaction terms that contain one or more random factors are considered themselves to be random terms, as is the overall residual term (as all observations are assumed to be random representations of the entire population(s)). Note, when designs include a mixture of fixed and random crossed effects, exact denominator degrees of freedoms for certain F-ratios are undefined and traditional approaches adopt rather inexact estimated approximate or “Quasi” F-ratios. Pooling of non-significant F-ratio denominator terms, in which lower random terms are added to the denominator (provided \\(\\alpha \u0026gt; 0.25\\)), may also be useful. For random factors within mixed models, selecting F-ratio denominators that are appropriate for the intended hypothesis tests is a particularly complex and controversial issue. Traditionally, there are two alternative approaches and whilst the statistical resumes of each are complicated, essentially they differ in whether or not the interaction term is constrained for the test of the random factor.\nThe constrained or restricted method (Model I), stipulates that for the calculation of a random factor F-ratio (which investigates the added variance added due to the random factor), the overall effect of the interaction is treated as zero. Consequently, the random factor is tested against the residual term. The unconstrained or unrestrained method (Model II) however, does not set the interaction effect to zero and therefore the interaction term is used as the random factor F-ratio denominator. This method assumes that the interaction terms for each level of the random factor are completely independent (correlations between the fixed factor must be consistent across all levels of the random factor). Some statisticians maintain that the independence of the interaction term is difficult to assess for clinical data and therefore, the restricted approach is more appropriate. However, others have suggested that the restricted method is only appropriate for balanced designs.\n\rQuasi F-ratios\rAn additional complication for three or more factor models that contain two or more random factors, is that there may not be a single appropriate interaction term to use as the denominator for many of the main effects F-ratios. For example, if Factors A and B are random and C is fixed, then there are two random interaction terms of equivalent level under Factor C (\\(A^\\prime \\times C\\) and \\(B^\\prime \\times C\\)). As a result, the value of the of the Mean Squares (MS) expected when the null hypothesis is true cannot be easily defined. The solutions for dealing with such situations (quasi F-ratios) involve adding (and subtracting) terms together to create approximate estimates of F-ratio denominators. Alternatively, for random factors, variance components with confidence intervals can be used. These solutions are sufficiently unsatisfying as to lead many statisticians to recommend that factorial designs with two or more random factors should avoided if possible. Arguably however, linear mixed effects models offer more appropriate solutions to the above issues as they are more robust for unbalanced designs, accommodate covariates and provide a more comprehensive treatment and overview of all the underlying data structures.\n\u0026gt; fact_anova_table\rdf MS A,B fixed A,B random A \u0026quot;a-1\u0026quot; \u0026quot;MS A\u0026quot; \u0026quot;(MS A)/(MS res)\u0026quot; \u0026quot;(MS A)/(MS AB)\u0026quot; B \u0026quot;b-1\u0026quot; \u0026quot;MS B\u0026quot; \u0026quot;(MS B)/(MS res)\u0026quot; \u0026quot;(MS B)/(MS AB)\u0026quot; AB \u0026quot;(b-1)(a-1)\u0026quot; \u0026quot;MS AB\u0026quot; \u0026quot;(MS AB)/(MS res)\u0026quot; \u0026quot;(MS AB)/(MS AB)\u0026quot;\rRes \u0026quot;(n-1)ba\u0026quot; \u0026quot;MS res\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; A fixed B random (model I) A fixed B random (model II)\rA \u0026quot;(MS A)/(MS AB)\u0026quot; \u0026quot;(MS A)/(MS AB)\u0026quot; B \u0026quot;(MS B)/(MS res)\u0026quot; \u0026quot;(MS B)/(MS AB)\u0026quot; AB \u0026quot;(MS AB)/(MS res)\u0026quot; \u0026quot;(MS AB)/(MS res)\u0026quot; Res \u0026quot;\u0026quot; \u0026quot;\u0026quot; \rThe corresponding R syntax is given below.\n\u0026gt; #Type I SS (Balanced)\r\u0026gt; anova(lm(y ~ A * B, data))\r\u0026gt; \u0026gt; #Type II SS (Unbalanced)\r\u0026gt; Anova(lm(y ~ A * B, data), type = \u0026quot;II\u0026quot;)\r\u0026gt; \u0026gt; #Type III SS (Unbalanced)\r\u0026gt; Anova(lm(y ~ A * B, data), type = \u0026quot;III\u0026quot;)\r\u0026gt; \u0026gt; #Variance components\r\u0026gt; summary(lmer(y ~ 1 + (1 | A) + (1 | B) + (1 | A:B), data))\rNote that for fixed factor models, when null hypotheses of interactions are rejected, the null hypothesis of the individual constituent factors are unlikely to represent the true nature of the effects and thus are of little value. The nature of such interactions are further explored by fitting simpler linear models (containing at least one less factor) separately for each of the levels of the other removed factor(s). Such Main effects tests are based on a subset of the data, and therefore estimates of the overall residual (unexplained) variability are unlikely to be as precise as the estimates based on the global model. Consequently, F-ratios involving MSResid should use the estimate of MSResid from the global model rather than that based on the smaller, theoretically less precise subset of data. For random and mixed models, since the objective is to generalise the effect of one factor over and above any interactions with other factors, the main factor effects can be interpreted even in the presence of significant interactions. Nevertheless, it should be noted that when a significant interaction is present in a mixed model, the power of the main fixed effects will be reduced (since the amount of variability explained by the interaction term will be relatively high, and this term is used as the denominator for the F-ratio calculation).\n\rAssumptions\rHypothesis tests assume that the residuals are:\n\rnormally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator (see table above) should be used to explore normality. Scale transformations are often useful.\n\requally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\n\rindependent of one another.\n\r\rPlanned and unplanned comparisons\nAs with single factor analysis of variance, planned and unplanned multiple comparisons (such as Tukey’s test) can be incorporated into or follow the linear model respectively so as to further investigate any patterns or trends within the main factors and/or the interactions. As with single factor analysis of variance, the contrasts must be defined prior to fitting the linear model, and no more than \\(p−1\\) (where \\(p\\) is the number of levels of the factor) contrasts can be defined for a factor.\nUnbalanced designs\nA factorial design can be thought of as a table made up of rows (representing the levels of one factor), columns (levels of another factor), and cells (the individual combinations of the set of factors). Whilst the middle left table does not have equal sample sizes in each cell, the sample sizes are in proportion and as such, does not present the issues discussed below for unbalanced designs.\nIn addition to impacting on normality and homogeneity of variance, unequal sample sizes in factorial designs have major implications for the partitioning of the total sums of squares into each of the model components. For balanced designs, the total sums of squares (SSTotal) is equal to the additive sums of squares of each of the components (including the residual). For example, in a two factor balanced design, SSTotal=SSA+SSB+SSAB+SSResid. This can be represented diagrammatically by a Venn Diagram in which each of the SS for the term components butt against one another and are surrounded by the SSResid. However, in unbalanced designs, the sums of squares will be non-orthogonal and the sum of the individual components does not add up to the total sums of squares. Diagrammatically, the SS of the terms intersect or are separated.\nIn regular sequential sums of squares (Type I SS), the sum of the individual sums of squares must be equal to the total sums of squares, the sums of squares of the last factor to be estimated will be calculated as the difference between the total sums of squares and what has already been accounted for by other components. Consequently, the order in which factors are specified in the model (and thus estimated) will alter their sums of squares and therefore their F-ratios. To overcome this problem, traditionally there are two other alternative methods of calculating sums of squares.\n\rType II (hierarchical) SS estimate the sums of squares of each term as the improvement it contributes upon addition of that term to a model of greater complexity and lower in the hierarchy (recall that the hierarchical structure descends from the simplest model down to the fully populated model). The SS for the interaction as well as the first factor to be estimated are the same as for Type I SS. Type II SS estimate the contribution of a factor over and above the contributions of other factors of equal or lower complexity but not above the contributions of the interaction terms or terms nested within the factor. However, these sums of squares are weighted by the sample sizes of each level and therefore are biased towards the trends produced by the groups (levels) that have higher sample sizes. As a result of the weightings, Type II SS actually test hypotheses about really quite complex combinations of factor levels. Rather than test a hypothesis that \\(\\mu_{High}=\\mu_{Medium}=\\mu_{Low}\\), Type II SS might be testing that \\(4\\times\\mu_{High}=1\\times\\mu_{Medium}=0.25\\times\\mu_{Low}\\).\n\rType III (marginal or orthogonal) SS estimate the sums of squares of each term as the improvement based on a comparison of models with and without the term and are unweighted by sample sizes. Type III SS essentially measure just the unique contribution of each factor over and above the contributions of the other factors and interactions. For unbalanced designs,Type III SS essentially test equivalent hypotheses to balanced Type I SS and are therefore arguably more appropriate for unbalanced factorial designs than Type II SS. Importantly, Type III SS are only interpretable if they are based on orthogonal contrasts (such as sum or helmert contrasts and not treatment contrasts).\n\r\rThe choice between Type II and III SS clearly depends on the nature of the question. For example, if we had measured the growth rate of seedlings subjected to two factors (temperature and fertiliser), Type II SS could address whether there was an effect of temperature across the level of fertiliser treatment, whereas Type III SS could assess whether there was an effect of temperature within each level of the fertiliser treatment.\nWhen an entire combination, or cell, is missing (perhaps due to unforeseen circumstances) it is not possible to test all the main effects and/or interactions. The bottom right table above depicts such as situation. One solution is to fit a large single factor ANOVA with as many levels as there are cells (this is known as a cell means model) and investigate various factor and interaction effects via specific contrasts (see the following tables). Difficulties in establishing appropriate error terms, makes missing cells in random and mixed factor designs substantially more complex.\n\r\rData generation\rImagine we has designed an experiment in which we had measured the response (\\(y\\)) under a combination of two different potential influences (Factor A: levels a1 and a2; and Factor B: levels b1, b2 and b3), each combination replicated \\(10\\) times (\\(n=10\\)). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(123)\r\u0026gt; nA \u0026lt;- 2 #number of levels of A\r\u0026gt; nB \u0026lt;- 3 #number of levels of B\r\u0026gt; nsample \u0026lt;- 10 #number of reps in each\r\u0026gt; A \u0026lt;- gl(nA, 1, nA, lab = paste(\u0026quot;a\u0026quot;, 1:nA, sep = \u0026quot;\u0026quot;))\r\u0026gt; B \u0026lt;- gl(nB, 1, nB, lab = paste(\u0026quot;b\u0026quot;, 1:nB, sep = \u0026quot;\u0026quot;))\r\u0026gt; data \u0026lt;- expand.grid(A = A, B = B, n = 1:nsample)\r\u0026gt; X \u0026lt;- model.matrix(~A * B, data = data)\r\u0026gt; eff \u0026lt;- c(40, 15, 5, 0, -15, 10)\r\u0026gt; sigma \u0026lt;- 3 #residual standard deviation\r\u0026gt; n \u0026lt;- nrow(data)\r\u0026gt; eps \u0026lt;- rnorm(n, 0, sigma) #residuals\r\u0026gt; data$y \u0026lt;- as.numeric(X %*% eff + eps)\r\u0026gt; head(data) #print out the first six rows of the data set\rA B n y\r1 a1 b1 1 38.31857\r2 a2 b1 1 54.30947\r3 a1 b2 1 49.67612\r4 a2 b2 1 45.21153\r5 a1 b3 1 40.38786\r6 a2 b3 1 70.14519\r\u0026gt; \u0026gt; with(data, interaction.plot(A, B, y))\r\u0026gt; \u0026gt; ## ALTERNATIVELY, we could supply the population means and get the effect parameters from these. To\r\u0026gt; ## correspond to the model matrix, enter the population means in the order of: a1b1, a2b1, a1b1,\r\u0026gt; ## a2b2,a1b3,a2b3\r\u0026gt; pop.means \u0026lt;- as.matrix(c(40, 55, 45, 45, 40, 65), byrow = F)\r\u0026gt; ## Generate a minimum model matrix for the effects\r\u0026gt; XX \u0026lt;- model.matrix(~A * B, expand.grid(A = factor(1:2), B = factor(1:3)))\r\u0026gt; ## Use the solve() function to solve what are effectively simultaneous equations\r\u0026gt; (eff \u0026lt;- as.vector(solve(XX, pop.means)))\r[1] 40 15 5 0 -15 10\r\u0026gt; \u0026gt; data$y \u0026lt;- as.numeric(X %*% eff + eps)\rWith these sort of data, we are primarily interested in investigating whether there is a relationship between the continuous response variable and the treatment type. Does treatment type effect the response?.\nAssumptions\rThe assumptions are:\n\rAll of the observations are independent - this must be addressed at the design and collection stages. Importantly, to be considered independent replicates, the replicates must be made at the same scale at which the treatment is applied. For example, if the experiment involves subjecting organisms housed in tanks to different water temperatures, then the unit of replication is the individual tanks not the individual organisms in the tanks. The individuals in a tank are strictly not independent with respect to the treatment.\n\rThe response variable (and thus the residuals) should be normally distributed for each sampled populations (combination of factors). Boxplots of each treatment combination are useful for diagnosing major issues with normality.\n\rThe response variable should be equally varied (variance should not be related to mean as these are supposed to be estimated separately) for each combination of treatments. Again, boxplots are useful.\n\r\r\rExploratory data analysis\rNormality and Homogeneity of variance\n\u0026gt; boxplot(y ~ A * B, data)\r\u0026gt; \u0026gt; # OR via ggplot2\r\u0026gt; library(ggplot2)\r\u0026gt; ggplot(data, aes(y = y, x = A, fill = B)) + geom_boxplot()\rConclusions\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical. There is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\nObvious violations could be addressed either by:\n\rtransform the scale of the response variables (to address normality etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\r\r\r\rModel fitting\rThe observed response (\\(y_i\\)) are assumed to be drawn from a normal distribution with a given mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The expected values are themselves determined by the linear predictor (\\(\\boldsymbol X \\boldsymbol \\beta\\)). In this case, \\(\\boldsymbol \\beta\\) represents the intercept associated with the first combination of groups, as well as the (effects) differences between this intercept and each other group. \\(\\boldsymbol X\\) is the model matrix. MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying ‘uninformative’ priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (\\(100\\)) for both the intercept and the treatment effect and a wide half-cauchy (\\(\\text{scale}=5\\)) for the standard deviation.\n\\[y_i \\sim N(\\mu_i,\\sigma), \\]\nwhere \\(\\mu_i=\\beta_0 +\\boldsymbol \\beta \\boldsymbol X\\). The assumed priors are: \\(\\beta \\sim N(0,100)\\) and \\(\\sigma \\sim \\text{Cauchy}(0,5)\\). Exploratory data analysis suggests that the intercept and effects could be drawn from similar distributions (with mean in the \\(10\\)’s and variances in the \\(100\\)’s). Whilst we might therefore be tempted to provide different priors for the intercept, compared to the effects, for a simple model such as this, it is unlikely to be necessary. However, for more complex models, where prior specification becomes more critical, separate priors would probably be necessary.\nWe proceed to code the model into JAGS (remember that in this software normal distribution are parameterised in terms of precisions \\(\\tau\\) rather than variances, where \\(\\tau=\\frac{1}{\\sigma^2}\\)). Note the following example as group means calculated as derived posteriors.\n\u0026gt; modelString = \u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mean[i],tau)\r+ mean[i] \u0026lt;- inprod(beta[],X[i,])\r+ }\r+ #Priors\r+ for (i in 1:ngroups) {\r+ beta[i] ~ dnorm(0, 1.0E-6) + }\r+ sigma ~ dunif(0, 100)\r+ tau \u0026lt;- 1 / (sigma * sigma)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;fact_anovaModel.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; X \u0026lt;- model.matrix(~A * B, data)\r\u0026gt; data.list \u0026lt;- with(data, list(y = y, X = X, n = nrow(data), ngroups = ncol(X)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10500\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\u0026gt; library(R2jags)\rNow run the JAGS code via the R2jags interface. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; data.r2jags \u0026lt;- jags(data = data.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;fact_anovaModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 60\rUnobserved stochastic nodes: 7\rTotal graph size: 502\rInitializing model\r\u0026gt; \u0026gt; print(data.r2jags)\rInference for Bugs model at \u0026quot;fact_anovaModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 40.187 0.927 38.381 39.572 40.186 40.810 42.028 1.001 15000\rbeta[2] 14.739 1.297 12.177 13.875 14.741 15.611 17.281 1.001 15000\rbeta[3] 4.997 1.301 2.439 4.127 4.996 5.850 7.555 1.001 6200\rbeta[4] -0.335 1.302 -2.922 -1.201 -0.323 0.523 2.182 1.001 9300\rbeta[5] -14.551 1.831 -18.188 -15.752 -14.535 -13.331 -10.976 1.001 15000\rbeta[6] 11.081 1.823 7.514 9.859 11.073 12.288 14.680 1.001 15000\rsigma 2.909 0.286 2.410 2.707 2.886 3.092 3.525 1.001 3100\rdeviance 296.719 4.003 290.996 293.788 296.037 298.923 306.334 1.001 3000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 8.0 and DIC = 304.7\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots for the effects model as an example. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters, e.g. \\(\\boldsymbol \\beta\\).\n\u0026gt; library(mcmcplots)\r\u0026gt; denplot(data.r2jags, parms = c(\u0026quot;beta\u0026quot;))\r\u0026gt; traplot(data.r2jags, parms = c(\u0026quot;beta\u0026quot;))\rTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters (such as \\(\\beta\\)s).\n\u0026gt; data.mcmc = as.mcmc(data.r2jags)\r\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(data.mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 2 3895 3746 1.040 beta[2] 2 3649 3746 0.974 beta[3] 2 3981 3746 1.060 beta[4] 2 3811 3746 1.020 beta[5] 2 3855 3746 1.030 beta[6] 2 3770 3746 1.010 deviance 2 3981 3746 1.060 sigma 4 5074 3746 1.350 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 2 3729 3746 0.995 beta[2] 2 3853 3746 1.030 beta[3] 2 3649 3746 0.974 beta[4] 2 3770 3746 1.010 beta[5] 2 3853 3746 1.030 beta[6] 2 3770 3746 1.010 deviance 2 3649 3746 0.974 sigma 4 5366 3746 1.430 \rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; autocorr.diag(data.mcmc)\rbeta[1] beta[2] beta[3] beta[4] beta[5]\rLag 0 1.000000000 1.000000000 1.0000000000 1.000000000 1.000000000\rLag 1 -0.002519333 0.009718890 0.0097211169 0.004831644 0.013455394\rLag 5 -0.004466196 0.013453425 0.0012166509 -0.009459535 0.010837730\rLag 10 -0.006418970 -0.004825081 0.0002148708 -0.003297864 -0.004528907\rLag 50 0.004241571 0.010613172 -0.0056258926 -0.002886136 -0.003130607\rbeta[6] deviance sigma\rLag 0 1.000000000 1.000000000 1.000000000\rLag 1 0.004411377 0.194295905 0.335565370\rLag 5 0.004680461 0.011707557 0.003364317\rLag 10 -0.012377072 0.006873975 0.005557072\rLag 50 0.003484518 -0.008999031 -0.012155151\rA lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. For more complex models (those that contain multiple effects), it is also advisable to plot the residuals against each of the individual predictors. For sampling designs that involve sample collection over space or time, it is also a good idea to explore whether there are any temporal or spatial patterns in the residuals.\nThere are numerous situations (e.g. when applying specific variance-covariance structures to a model) where raw residuals do not reflect the interior workings of the model. Typically, this is because they do not take into account the variance-covariance matrix or assume a very simple variance-covariance matrix. Since the purpose of exploring residuals is to evaluate the model, for these cases, it is arguably better to draw conclusions based on standardized (or studentised) residuals. Unfortunately the definitions of standardised and studentised residuals appears to vary and the two terms get used interchangeably. I will adopt the following definitions:\n\rStandardised residuals. The raw residuals divided by the true standard deviation of the residuals (which of course is rarely known).\n\rStudentised residuals. The raw residuals divided by the standard deviation of the residuals. Note that externally studentised residuals are calculated by dividing the raw residuals by a unique standard deviation for each observation that is calculated from regressions having left each successive observation out.\n\rPearson residuals. The raw residuals divided by the standard deviation of the response variable.\n\r\rhe mark of a good model is being able to predict well. In an ideal world, we would have sufficiently large sample size as to permit us to hold a fraction (such as \\(25\\)%) back thereby allowing us to train the model on \\(75\\)% of the data and then see how well the model can predict the withheld \\(25\\)%. Unfortunately, such a luxury is still rare. The next best option is to see how well the model can predict the observed data. Models tend to struggle most with the extremes of trends and have particular issues when the extremes approach logical boundaries (such as zero for count data and standard deviations). We can use the fitted model to generate random predicted observations and then explore some properties of these compared to the actual observed data.\nRather than dublicate this for both additive and multiplicative models, we will only explore the multiplicative model. Residuals are not computed directly within JAGS. However, we can calculate them manually form the posteriors.\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~A * B, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; wch = grep(\u0026quot;beta\\\\[\u0026quot;, colnames(mcmc))\r\u0026gt; wch\r[1] 1 2 3 4 5 6\r\u0026gt; \u0026gt; head(mcmc)\rbeta[1] beta[2] beta[3] beta[4] beta[5] beta[6] deviance\r[1,] 41.07993 14.73872 4.532543 -1.58279310 -14.91723 11.28780 292.8658\r[2,] 40.30651 13.02455 4.475566 -0.86754574 -12.02942 13.36371 295.1239\r[3,] 40.42144 14.71551 5.149725 0.09616707 -14.80497 10.82830 290.7322\r[4,] 39.79269 16.35682 5.776724 -0.53251753 -17.64694 10.59484 295.1674\r[5,] 39.40269 14.69470 5.237430 -0.29022676 -14.12951 12.81751 293.3136\r[6,] 41.27115 12.58706 5.908648 -2.34899624 -13.31913 13.79862 302.0972\rsigma\r[1,] 3.032059\r[2,] 2.467221\r[3,] 2.874167\r[4,] 2.561227\r[5,] 2.841503\r[6,] 3.403891\r\u0026gt; \u0026gt; coefs = apply(mcmc[, wch], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rResiduals against predictors\n\u0026gt; library(dplyr)\r\u0026gt; library(tidyr)\r\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; wch = grep(\u0026quot;beta\\\\[\u0026quot;, colnames(mcmc))\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = newdata\r\u0026gt; Xmat = model.matrix(~A * B, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, wch], 2, median)\r\u0026gt; print(coefs)\rbeta[1] beta[2] beta[3] beta[4] beta[5] beta[6] 40.1859804 14.7407405 4.9960673 -0.3233121 -14.5348136 11.0732139 \u0026gt; \u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; newdata = newdata %\u0026gt;% cbind(fit, resid)\r\u0026gt; ggplot(newdata) + geom_point(aes(y = resid, x = A)) + theme_classic()\r\u0026gt; \u0026gt; ggplot(newdata) + geom_point(aes(y = resid, x = B)) + theme_classic()\rAnd now for studentised residuals\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; wch = grep(\u0026quot;beta\\\\[\u0026quot;, colnames(mcmc))\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~A * B, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, wch], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; sresid = resid/sd(resid)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit)) + theme_classic()\rFor this simple model, the studentised residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; wch = grep(\u0026quot;beta\\\\[\u0026quot;, colnames(mcmc))\r\u0026gt; #generate a model matrix\r\u0026gt; Xmat = model.matrix(~A*B, data)\r\u0026gt; ##get median parameter estimates\r\u0026gt; coefs = mcmc[,wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i,], mcmc[i, \u0026#39;sigma\u0026#39;]))\r\u0026gt; newdata = data.frame(A=data$A, B=data$B, yRep) %\u0026gt;% gather(key=Sample, value=Value,-A,-B)\r\u0026gt; ggplot(newdata) +\r+ geom_violin(aes(y=Value, x=A, fill=\u0026#39;Model\u0026#39;), alpha=0.5)+\r+ geom_violin(data=data, aes(y=y,x=A,fill=\u0026#39;Obs\u0026#39;), alpha=0.5) +\r+ geom_point(data=data, aes(y=y, x=A), position=position_jitter(width=0.1,height=0),\r+ color=\u0026#39;black\u0026#39;) + theme_classic()\r\u0026gt; \u0026gt; ggplot(newdata) +\r+ geom_violin(aes(y=Value, x=B, fill=\u0026#39;Model\u0026#39;, group=B, color=A), alpha=0.5)+\r+ geom_point(data=data, aes(y=y, x=B, group=B,color=A)) + theme_classic()\rThe predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.\n\u0026gt; library(bayesplot)\r\u0026gt; mcmc_intervals(data.r2jags$BUGSoutput$sims.matrix, regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\u0026gt; mcmc_areas(data.r2jags$BUGSoutput$sims.matrix, regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\rParameter estimates\rAlthough all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and \\(95\\)% credibility intervals.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\rFirst, we look at the results from the additive model.\n\u0026gt; print(data.r2jags)\rInference for Bugs model at \u0026quot;fact_anovaModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 40.187 0.927 38.381 39.572 40.186 40.810 42.028 1.001 15000\rbeta[2] 14.739 1.297 12.177 13.875 14.741 15.611 17.281 1.001 15000\rbeta[3] 4.997 1.301 2.439 4.127 4.996 5.850 7.555 1.001 6200\rbeta[4] -0.335 1.302 -2.922 -1.201 -0.323 0.523 2.182 1.001 9300\rbeta[5] -14.551 1.831 -18.188 -15.752 -14.535 -13.331 -10.976 1.001 15000\rbeta[6] 11.081 1.823 7.514 9.859 11.073 12.288 14.680 1.001 15000\rsigma 2.909 0.286 2.410 2.707 2.886 3.092 3.525 1.001 3100\rdeviance 296.719 4.003 290.996 293.788 296.037 298.923 306.334 1.001 3000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 8.0 and DIC = 304.7\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(as.mcmc(data.r2jags), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 8 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 40.2 0.927 38.4 42.0 2 beta[2] 14.7 1.30 12.2 17.3 3 beta[3] 5.00 1.30 2.43 7.55\r4 beta[4] -0.335 1.30 -2.89 2.21\r5 beta[5] -14.6 1.83 -18.2 -11.0 6 beta[6] 11.1 1.82 7.57 14.7 7 deviance 297. 4.00 290. 304. 8 sigma 2.91 0.286 2.37 3.47\rConclusions\n\rThe intercept represents the mean of the first combination Aa1:Bb1 is \\(40.2\\)\n\rAa2:Bb1 is \\(14.7\\) units greater than Aa1:Bb1\n\rAa1:Bb2 is \\(5\\) units greater Aa1:Bb1\n\rAa1:Bb3 is \\(-0.335\\) units greater Aa1:Bb1\n\rAa2:Bb2 is \\(-14.6\\) units greater than the difference between (Aa1:Bb2 + Aa2:Bb1) and (2*Aa1:Bb1)\n\rAa2:Bb3 is \\(11.1\\) units greater than the difference between (Aa1:Bb3 + Aa2:Bb1) and (2*Aa1:Bb1)\n\r\rThe \\(95\\)% credibility interval for both interactive effects (Aa2:Bb2 and Aa2:Bb3) do not contain \\(0\\), implying significant interactions between A and B. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[2]\u0026quot;])\r[1] 0\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[3]\u0026quot;])\r[1] 0.0004666667\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[4]\u0026quot;])\r[1] 0.7912667\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[5]\u0026quot;])\r[1] 0\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[6]\u0026quot;])\r[1] 0\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, c(\u0026quot;beta[5]\u0026quot;, \u0026quot;beta[6]\u0026quot;)])\r[1] 0\rThere is evidence of an interaction between A and B.\n\rGraphical summaries\rA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group.\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; wch = grep(\u0026quot;^beta\u0026quot;, colnames(mcmc))\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = expand.grid(A=levels(data$A), B=levels(data$B))\r\u0026gt; Xmat = model.matrix(~A*B,newdata)\r\u0026gt; coefs = mcmc[,wch]\r\u0026gt; fit=coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int=TRUE, conf.method=\u0026#39;HPDinterval\u0026#39;))\r\u0026gt; newdata\rA B estimate std.error conf.low conf.high\r1 a1 b1 40.18727 0.9270982 38.38136 42.02744\r2 a2 b1 54.92636 0.9160115 53.12047 56.67452\r3 a1 b2 45.18473 0.9196740 43.37733 46.98224\r4 a2 b2 45.37262 0.9197287 43.61538 47.19883\r5 a1 b3 39.85206 0.9156380 38.11053 41.70144\r6 a2 b3 65.67189 0.9209489 63.84038 67.47931\r\u0026gt; \u0026gt; ggplot(newdata, aes(y=estimate, x=B, fill=A)) +\r+ geom_blank() +\r+ geom_line(aes(x=as.numeric(B), linetype=A)) +\r+ geom_linerange(aes(ymin=conf.low, ymax=conf.high))+\r+ geom_point(aes(shape=A), size=3)+\r+ scale_y_continuous(\u0026#39;Y\u0026#39;)+\r+ scale_x_discrete(\u0026#39;B\u0026#39;)+\r+ scale_shape_manual(\u0026#39;A\u0026#39;,values=c(21,16))+\r+ scale_fill_manual(\u0026#39;A\u0026#39;,values=c(\u0026#39;white\u0026#39;,\u0026#39;black\u0026#39;))+\r+ scale_linetype_manual(\u0026#39;A\u0026#39;,values=c(\u0026#39;solid\u0026#39;,\u0026#39;dashed\u0026#39;))+\r+ theme_classic() +\r+ theme(legend.justification=c(0,1), legend.position=c(0.05,1),\r+ axis.title.y=element_text(vjust=2, size=rel(1.25)),\r+ axis.title.x=element_text(vjust=-2, size=rel(1.25)),\r+ plot.margin=unit(c(0.5,0.5,2,2), \u0026#39;lines\u0026#39;),\r+ legend.key.size=unit(1,\u0026#39;cm\u0026#39;)) + theme_classic()\r\rFinite population standard deviations\rVariance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).\nFinite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (Gelman and others (2005)). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.\n# A tibble: 4 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.A 10.4 0.917 8.65 12.3 2 sd.B 3.06 0.640 1.79 4.28\r3 sd.AB 10.4 0.734 9.04 11.9 4 sd.resid 2.84 0.0836 2.72 3.00\r# A tibble: 4 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.A 39.1 1.95 35.2 42.8\r2 sd.B 11.4 1.90 7.60 15.0\r3 sd.AB 39.0 0.947 37.0 40.8\r4 sd.resid 10.6 0.822 9.30 12.3\rApproximately \\(39\\)% of the total finite population standard deviation is due to the interaction between factor A and factor B.\n\rR squared\rIn a frequentist context, the \\(R^2\\) value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, \\(R^2\\) is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an \\(R^2\\) greater than \\(100\\)%. Gelman et al. (2019) proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.\nSo in the standard regression model notation of:\n\\[ y_i \\sim \\text{Normal}(\\boldsymbol X \\boldsymbol \\beta, \\sigma),\\]\nthe \\(R^2\\) could be formulated as\n\\[ R^2 = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sigma^2_e},\\]\nwhere \\(\\sigma^2_f=\\text{var}(\\boldsymbol X \\boldsymbol \\beta)\\), and for normal models \\(\\sigma^2_e=\\text{var}(y-\\boldsymbol X \\boldsymbol \\beta)\\)\n\u0026gt; mcmc \u0026lt;- data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; Xmat = model.matrix(~A * B, data)\r\u0026gt; wch = grep(\u0026quot;^beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = sweep(fit, 2, data$y, \u0026quot;-\u0026quot;)\r\u0026gt; var_f = apply(fit, 1, var)\r\u0026gt; var_e = apply(resid, 1, var)\r\u0026gt; R2 = var_f/(var_f + var_e)\r\u0026gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.913 0.00817 0.897 0.925\r\u0026gt; \u0026gt; # for comparison with frequentist\r\u0026gt; summary(lm(y ~ A * B, data))\rCall:\rlm(formula = y ~ A * B, data = data)\rResiduals:\rMin 1Q Median 3Q Max -6.5694 -1.8517 -0.0589 1.7120 6.5966 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 40.1940 0.8980 44.760 \u0026lt; 2e-16 ***\rAa2 14.7163 1.2700 11.588 2.88e-16 ***\rBb2 4.9823 1.2700 3.923 0.000249 ***\rBb3 -0.3464 1.2700 -0.273 0.786077 Aa2:Bb2 -14.5093 1.7960 -8.079 7.37e-11 ***\rAa2:Bb3 11.1056 1.7960 6.184 8.65e-08 ***\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 2.84 on 54 degrees of freedom\rMultiple R-squared: 0.92, Adjusted R-squared: 0.9125 F-statistic: 124.1 on 5 and 54 DF, p-value: \u0026lt; 2.2e-16\r\rDealing with interactions\rIn the presence of interations, conclusions about the main effects are overly simplistic at best and completely inaccurate at worst. Therefore, in the presense of interactions we should attempt to tease the analysis appart a little. In the current working example, we have identified that there is a significant interaction between Factor A and Factor B. Our exploration of the regression coefficients, indicated that the pattern between b1, b2 and b3 might differ between a1 and a2. Similarly, if we consider the coefficients from the perspective of Factor A, we can see that the patterns between a1 and a2 are similar for b1 and b3, yet very different for b2.\nAt this point, we can then split the two-factor model up into a series of single-factor models, either:\n\rexamining the effects of Factor B separately for each level of Factor A (two single-factor models) or\n\rexamining the effects of Factor A separately for each level of Factor B (three single-factor models)\n\r\rHowever, rather than subset the data and fit isolated smaller models, it is arguably better to treat these explorations as contrasts. As such we could either:\n\rapply specific contrasts to the already fit model\n\rdefine the specific contrasts and use them to refit the model\n\r\rWe will do the former of these options since we have already fit the global model. For this demonstration, we will explore the effect of factor A at each level of factor B. I will illustrate two ways to perform these contrasts on an already fit model:\nBy generating the posteriors of the cell means (means of each factor combination) and then manually compare the appropriate columns for specific levels of factor B.\r\r\u0026gt; mcmc \u0026lt;- data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; wch = grep(\u0026quot;^beta\u0026quot;, colnames(mcmc))\r\u0026gt; newdata = expand.grid(A = levels(data$A), B = levels(data$B))\r\u0026gt; Xmat = model.matrix(~A * B, data = newdata)\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; head(fit)\r1 2 3 4 5 6\r[1,] 41.07993 55.81865 45.61247 45.43397 39.49714 65.52366\r[2,] 40.30651 53.33106 44.78207 45.77720 39.43896 65.82722\r[3,] 40.42144 55.13695 45.57116 45.48170 40.51761 66.06142\r[4,] 39.79269 56.14951 45.56942 44.27930 39.26017 66.21183\r[5,] 39.40269 54.09738 44.64012 45.20530 39.11246 66.62467\r[6,] 41.27115 53.85821 47.17980 46.44773 38.92215 65.30783\r\u0026gt; \u0026gt; ## we want to compare columns 2-1, 4-3 and 6-5\r\u0026gt; comps = fit[, c(2, 4, 6)] - fit[, c(1, 3, 5)]\r\u0026gt; tidyMCMC(comps, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 3 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 2 14.7 1.30 12.2 17.3 2 4 0.188 1.30 -2.30 2.83\r3 6 25.8 1.30 23.2 28.4 \rBy generating the posteriors of the cell means (means of each factor combination) and then manually compare the appropriate columns for specific levels of factor B.\r\r\u0026gt; mcmc \u0026lt;- data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; wch = grep(\u0026quot;^beta\u0026quot;, colnames(mcmc))\r\u0026gt; newdata = expand.grid(A = levels(data$A), B = levels(data$B))\r\u0026gt; Xmat = model.matrix(~A * B, data = newdata)\r\u0026gt; contr = attr(Xmat, \u0026quot;contrasts\u0026quot;)\r\u0026gt; newdata.a1 = model.frame(~A * B, expand.grid(A = levels(data$A)[1], B = levels(data$B)),\r+ xlev = list(A = levels(data$A), B = levels(data$B)))\r\u0026gt; Xmat.a1 = model.matrix(~A * B, data = newdata.a1, contrasts = contr)\r\u0026gt; newdata.a2 = model.frame(~A * B, expand.grid(A = levels(data$A)[2], B = levels(data$B)),\r+ xlev = list(A = levels(data$A), B = levels(data$B)))\r\u0026gt; Xmat.a2 = model.matrix(~A * B, data = newdata.a2, contrasts = contr)\r\u0026gt; Xmat = Xmat.a2 - Xmat.a1\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 3 x 4\restimate std.error conf.low conf.high\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 14.7 1.30 12.2 17.3 2 0.188 1.30 -2.30 2.83\r3 25.8 1.30 23.2 28.4 \r\rReferences\rGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9.\n\rGelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” The Annals of Statistics 33 (1): 1–53.\n\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1581041594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581041594,"objectID":"22906ea3c5761961dbee854640bc0d04","permalink":"/jags/factorial-anova-jags/factorial-anova-jags/","publishdate":"2020-02-06T21:13:14-05:00","relpermalink":"/jags/factorial-anova-jags/factorial-anova-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","anova","factor analysis","factorial designs"],"title":"Factorial Analysis of Variance - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","anova","STAN","factor analysis","factorial designs"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rFactorial designs are an extension of single factor ANOVA designs in which additional factors are added such that each level of one factor is applied to all levels of the other factor(s) and these combinations are replicated. For example, we might design an experiment in which the effects of temperature (high vs low) and fertiliser (added vs not added) on the growth rate of seedlings are investigated by growing seedlings under the different temperature and fertilizer combinations. In addition to investigating the impacts of the main factors, factorial designs allow us to investigate whether the effects of one factor are consistent across levels of another factor. For example, is the effect of temperature on growth rate the same for both fertilised and unfertilized seedlings and similarly, does the impact of fertiliser treatment depend on the temperature under which the seedlings are grown?\nArguably, these interactions give more sophisticated insights into the dynamics of the system we are investigating. Hence, we could add additional main effects, such as soil pH, amount of water, etc, along with all the two way (temp:fert, temp:pH, temp:water, etc), three-way (temp:fert:pH, temp:pH:water), four-way (and so on) interactions in order to explore how these various factors interact with one another to effect the response. However, the more interactions, the more complex the model becomes to specify, compute and interpret - not to mention the rate at which the number of required observations increases. Factorial designs can consist:\n\rentirely of crossed fixed factors (Model I ANOVA - most common) in which conclusions are restricted to the specific combinations of levels selected for the experiment.\n\rentirely of crossed random factors (Model II ANOVA).\n\ra mixture of crossed fixed and random factors (Model III ANOVA).\n\r\rThe latter are useful for investigating the generality of a main treatment effect (fixed) over broad spatial, temporal or clinical levels of organisation. That is whether the observed effects of temperature and/or fertiliser (for example) are observed across the entire genera or country.\n\rLinear model\rAs with single factor ANOVA, the linear model could be constructed as either effects or means parameterisation, although only effects parameterisation will be considered here. The linear models for two and three factor design are\n\\[ y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk},\\]\nand\n\\[ y_{ijkl} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijkl},\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha\\) is the effect of Factor A, \\(\\beta\\) is the effect of Factor B, \\(\\gamma\\) is the effect of Factor C and \\(\\epsilon\\) is the random unexplained or residual component. Note that although the linear models for Model I, Model II and Model III designs are identical, the interpretation of terms (and thus null hypothesis) differ. Recall from the tutorial on single factor ANOVA, that categorical variables in linear models are actually re-parameterised dummy codes - and thus the \\(\\alpha\\) term above, actually represents one or more dummy codes. Thus, if we actually had two levels of Factor A (A1 and A2) and three levels of Factor B (B1, B2, B3), then the fully parameterised linear model would be:\n\\[ y=\\beta_{A1B1}+\\beta_{A2B1−A1B1}+\\beta_{A1B2−A1B1}+\\beta_{A1B3−A1B1}+\\beta_{A2B2−A1B2−A2B1−A1B1}+\\beta_{A2B3−A1B3−A2B1−A1B1}.\\]\nThus, such a model would have six parameters to estimate (in addition to the variance).\n\rNull hypothesis\rThere are separate null hypothesis associated with each of the main effects and the interaction terms.\nModel 1 - fixed effects\rFactor A\n\r\\(H_0(A):\\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\)\r\rThe population group means are all equal. The mean of population \\(1\\) is equal to that of population \\(2\\) and so on, and thus all population means are equal to an overall mean. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the overall mean (\\(\\alpha_i=\\mu_i-\\mu\\)) then the \\(H_0\\) can alternatively be written as:\n\r\\(H_0(A):\\alpha_1=\\alpha_2=\\ldots=\\alpha_i=0\\)\r\rThe effect of each group equals zero. If one or more of the \\(\\alpha_i\\) are different from zero (the response mean for this treatment differs from the overall response mean), the null hypothesis is rejected indicating that the treatment has been found to affect the response variable. Note, as with multiple regression models, these “effects” represent partial effects. In the above, the effect of Factor A is actually the effect of Factor A at the first level of the Factor(s).\nFactor B\n\r\\(H_0(B):\\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\)\r\rThe population group means are all equal - at the first level of Factor A. Equivalent interpretation to Factor A above.\nFactor AB: interaction\n\r\\(H_0(AB):\\mu_{ij}=\\mu_i+\\mu_j-\\mu\\)\r\rThe population group means are all equal. For any given combination of factor levels, the population group mean will be equal to the difference between the overall population mean and the simple additive effects of the individual factor group means. That is, the effects of the main treatment factors are purely additive and independent of one another. This is equivalent to \\(H_0(AB): \\alpha\\beta_{ij}=0\\), no interaction between Factor A and Factor B.\n\rModel 2 - random effects\rFactor A\n\r\\(H_0(A):\\sigma^2_{\\alpha}=0\\)\r\rThe population variance equals zero. There is no added variance due to all possible levels of A.\nFactor B\n\r\\(H_0(B):\\sigma^2_{\\beta}=0\\)\r\rThe population variance equals zero. There is no added variance due to all possible levels of B.\nFactor AB: interaction\n\r\\(H_0(AB):\\sigma^2_{\\alpha\\beta}=0\\)\r\rThere is no added variance due to all possible interactions between all possible levels of A and B.\n\rModel 3 - mixed effects\rFixed factor - e.g. A\n\r\\(H_0(A):\\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\)\r\rThe population group means are all equal. The mean of population \\(1\\) (pooled over all levels of the random factor) is equal to that of population \\(2\\) and so on, and thus all population means are equal to an overall mean pooling over all possible levels of the random factor. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the overall mean (\\(\\alpha_i=\\mu_i-\\mu\\)) then the \\(H_0\\) can alternatively be written as:\n\r\\(H_0(A):\\alpha_1=\\alpha_2=\\ldots=\\alpha_i=0\\)\r\rNo effect of any level of this factor pooled over all possible levels of the random factor.\nRandom factor - e.g. B\n\r\\(H_0(B):\\sigma^2_{\\beta}=0\\)\r\rThe population variance equals zero. There is no added variance due to all possible levels of B.\nFactor AB: interaction\nThe interaction of a fixed and random factor is always considered a random factor.\n\r\\(H_0(AB):\\sigma^2_{\\alpha\\beta}=0\\)\r\rThe population variance equals zero. There is no added variance due to all possible interactions between all possible levels of A and B.\n\r\rAnalysis of variance\rWhen fixed factorial designs are balanced, the total variance in the response variable can be sequentially partitioned into what is explained by each of the model terms (factors and their interactions) and what is left unexplained. For each of the specific null hypotheses, the overall unexplained variability is used as the denominator in F-ratio calculations, and when a null hypothesis is true, an F-ratio should follow an F distribution with an expected value less than \\(1\\). Random factors are added to provide greater generality of conclusions. That is, to enable us to make conclusions about the effect of one factor (such as whether or not fertiliser is added) over all possible levels (not just those sampled) of a random factor (such as all possible locations, seasons, varieties, etc). In order to expand our conclusions beyond the specific levels used in the design, the hypothesis tests (and thus F-ratios) must reflect this extra generality by being more conservative.\nThe appropriate F-ratios for fixed, random and mixed factorial designs are presented below. Generally, once the terms (factors and interactions) have been ordered into a hierarchy (single factors at the top, highest level interactions at the bottom and terms of same order given equivalent positions in the hierarchy), the denominator for any term is selected as the next appropriate random term (an interaction that includes the term to be tested) encountered lower in the hierarchy. Interaction terms that contain one or more random factors are considered themselves to be random terms, as is the overall residual term (as all observations are assumed to be random representations of the entire population(s)). Note, when designs include a mixture of fixed and random crossed effects, exact denominator degrees of freedoms for certain F-ratios are undefined and traditional approaches adopt rather inexact estimated approximate or “Quasi” F-ratios. Pooling of non-significant F-ratio denominator terms, in which lower random terms are added to the denominator (provided \\(\\alpha \u0026gt; 0.25\\)), may also be useful. For random factors within mixed models, selecting F-ratio denominators that are appropriate for the intended hypothesis tests is a particularly complex and controversial issue. Traditionally, there are two alternative approaches and whilst the statistical resumes of each are complicated, essentially they differ in whether or not the interaction term is constrained for the test of the random factor.\nThe constrained or restricted method (Model I), stipulates that for the calculation of a random factor F-ratio (which investigates the added variance added due to the random factor), the overall effect of the interaction is treated as zero. Consequently, the random factor is tested against the residual term. The unconstrained or unrestrained method (Model II) however, does not set the interaction effect to zero and therefore the interaction term is used as the random factor F-ratio denominator. This method assumes that the interaction terms for each level of the random factor are completely independent (correlations between the fixed factor must be consistent across all levels of the random factor). Some statisticians maintain that the independence of the interaction term is difficult to assess for clinical data and therefore, the restricted approach is more appropriate. However, others have suggested that the restricted method is only appropriate for balanced designs.\n\rQuasi F-ratios\rAn additional complication for three or more factor models that contain two or more random factors, is that there may not be a single appropriate interaction term to use as the denominator for many of the main effects F-ratios. For example, if Factors A and B are random and C is fixed, then there are two random interaction terms of equivalent level under Factor C (\\(A^\\prime \\times C\\) and \\(B^\\prime \\times C\\)). As a result, the value of the of the Mean Squares (MS) expected when the null hypothesis is true cannot be easily defined. The solutions for dealing with such situations (quasi F-ratios) involve adding (and subtracting) terms together to create approximate estimates of F-ratio denominators. Alternatively, for random factors, variance components with confidence intervals can be used. These solutions are sufficiently unsatisfying as to lead many statisticians to recommend that factorial designs with two or more random factors should avoided if possible. Arguably however, linear mixed effects models offer more appropriate solutions to the above issues as they are more robust for unbalanced designs, accommodate covariates and provide a more comprehensive treatment and overview of all the underlying data structures.\n\u0026gt; fact_anova_table\rdf MS A,B fixed A,B random A \u0026quot;a-1\u0026quot; \u0026quot;MS A\u0026quot; \u0026quot;(MS A)/(MS res)\u0026quot; \u0026quot;(MS A)/(MS AB)\u0026quot; B \u0026quot;b-1\u0026quot; \u0026quot;MS B\u0026quot; \u0026quot;(MS B)/(MS res)\u0026quot; \u0026quot;(MS B)/(MS AB)\u0026quot; AB \u0026quot;(b-1)(a-1)\u0026quot; \u0026quot;MS AB\u0026quot; \u0026quot;(MS AB)/(MS res)\u0026quot; \u0026quot;(MS AB)/(MS AB)\u0026quot;\rRes \u0026quot;(n-1)ba\u0026quot; \u0026quot;MS res\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; A fixed B random (model I) A fixed B random (model II)\rA \u0026quot;(MS A)/(MS AB)\u0026quot; \u0026quot;(MS A)/(MS AB)\u0026quot; B \u0026quot;(MS B)/(MS res)\u0026quot; \u0026quot;(MS B)/(MS AB)\u0026quot; AB \u0026quot;(MS AB)/(MS res)\u0026quot; \u0026quot;(MS AB)/(MS res)\u0026quot; Res \u0026quot;\u0026quot; \u0026quot;\u0026quot; \rThe corresponding R syntax is given below.\n\u0026gt; #Type I SS (Balanced)\r\u0026gt; anova(lm(y ~ A * B, data))\r\u0026gt; \u0026gt; #Type II SS (Unbalanced)\r\u0026gt; Anova(lm(y ~ A * B, data), type = \u0026quot;II\u0026quot;)\r\u0026gt; \u0026gt; #Type III SS (Unbalanced)\r\u0026gt; Anova(lm(y ~ A * B, data), type = \u0026quot;III\u0026quot;)\r\u0026gt; \u0026gt; #Variance components\r\u0026gt; summary(lmer(y ~ 1 + (1 | A) + (1 | B) + (1 | A:B), data))\rNote that for fixed factor models, when null hypotheses of interactions are rejected, the null hypothesis of the individual constituent factors are unlikely to represent the true nature of the effects and thus are of little value. The nature of such interactions are further explored by fitting simpler linear models (containing at least one less factor) separately for each of the levels of the other removed factor(s). Such Main effects tests are based on a subset of the data, and therefore estimates of the overall residual (unexplained) variability are unlikely to be as precise as the estimates based on the global model. Consequently, F-ratios involving MSResid should use the estimate of MSResid from the global model rather than that based on the smaller, theoretically less precise subset of data. For random and mixed models, since the objective is to generalise the effect of one factor over and above any interactions with other factors, the main factor effects can be interpreted even in the presence of significant interactions. Nevertheless, it should be noted that when a significant interaction is present in a mixed model, the power of the main fixed effects will be reduced (since the amount of variability explained by the interaction term will be relatively high, and this term is used as the denominator for the F-ratio calculation).\n\rAssumptions\rHypothesis tests assume that the residuals are:\n\rnormally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator (see table above) should be used to explore normality. Scale transformations are often useful.\n\requally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\n\rindependent of one another.\n\r\rPlanned and unplanned comparisons\nAs with single factor analysis of variance, planned and unplanned multiple comparisons (such as Tukey’s test) can be incorporated into or follow the linear model respectively so as to further investigate any patterns or trends within the main factors and/or the interactions. As with single factor analysis of variance, the contrasts must be defined prior to fitting the linear model, and no more than \\(p−1\\) (where \\(p\\) is the number of levels of the factor) contrasts can be defined for a factor.\nUnbalanced designs\nA factorial design can be thought of as a table made up of rows (representing the levels of one factor), columns (levels of another factor), and cells (the individual combinations of the set of factors). Whilst the middle left table does not have equal sample sizes in each cell, the sample sizes are in proportion and as such, does not present the issues discussed below for unbalanced designs.\nIn addition to impacting on normality and homogeneity of variance, unequal sample sizes in factorial designs have major implications for the partitioning of the total sums of squares into each of the model components. For balanced designs, the total sums of squares (SSTotal) is equal to the additive sums of squares of each of the components (including the residual). For example, in a two factor balanced design, SSTotal=SSA+SSB+SSAB+SSResid. This can be represented diagrammatically by a Venn Diagram in which each of the SS for the term components butt against one another and are surrounded by the SSResid. However, in unbalanced designs, the sums of squares will be non-orthogonal and the sum of the individual components does not add up to the total sums of squares. Diagrammatically, the SS of the terms intersect or are separated.\nIn regular sequential sums of squares (Type I SS), the sum of the individual sums of squares must be equal to the total sums of squares, the sums of squares of the last factor to be estimated will be calculated as the difference between the total sums of squares and what has already been accounted for by other components. Consequently, the order in which factors are specified in the model (and thus estimated) will alter their sums of squares and therefore their F-ratios. To overcome this problem, traditionally there are two other alternative methods of calculating sums of squares.\n\rType II (hierarchical) SS estimate the sums of squares of each term as the improvement it contributes upon addition of that term to a model of greater complexity and lower in the hierarchy (recall that the hierarchical structure descends from the simplest model down to the fully populated model). The SS for the interaction as well as the first factor to be estimated are the same as for Type I SS. Type II SS estimate the contribution of a factor over and above the contributions of other factors of equal or lower complexity but not above the contributions of the interaction terms or terms nested within the factor. However, these sums of squares are weighted by the sample sizes of each level and therefore are biased towards the trends produced by the groups (levels) that have higher sample sizes. As a result of the weightings, Type II SS actually test hypotheses about really quite complex combinations of factor levels. Rather than test a hypothesis that \\(\\mu_{High}=\\mu_{Medium}=\\mu_{Low}\\), Type II SS might be testing that \\(4\\times\\mu_{High}=1\\times\\mu_{Medium}=0.25\\times\\mu_{Low}\\).\n\rType III (marginal or orthogonal) SS estimate the sums of squares of each term as the improvement based on a comparison of models with and without the term and are unweighted by sample sizes. Type III SS essentially measure just the unique contribution of each factor over and above the contributions of the other factors and interactions. For unbalanced designs,Type III SS essentially test equivalent hypotheses to balanced Type I SS and are therefore arguably more appropriate for unbalanced factorial designs than Type II SS. Importantly, Type III SS are only interpretable if they are based on orthogonal contrasts (such as sum or helmert contrasts and not treatment contrasts).\n\r\rThe choice between Type II and III SS clearly depends on the nature of the question. For example, if we had measured the growth rate of seedlings subjected to two factors (temperature and fertiliser), Type II SS could address whether there was an effect of temperature across the level of fertiliser treatment, whereas Type III SS could assess whether there was an effect of temperature within each level of the fertiliser treatment.\nWhen an entire combination, or cell, is missing (perhaps due to unforeseen circumstances) it is not possible to test all the main effects and/or interactions. The bottom right table above depicts such as situation. One solution is to fit a large single factor ANOVA with as many levels as there are cells (this is known as a cell means model) and investigate various factor and interaction effects via specific contrasts (see the following tables). Difficulties in establishing appropriate error terms, makes missing cells in random and mixed factor designs substantially more complex.\n\r\rData generation\rImagine we has designed an experiment in which we had measured the response (\\(y\\)) under a combination of two different potential influences (Factor A: levels a1 and a2; and Factor B: levels b1, b2 and b3), each combination replicated \\(10\\) times (\\(n=10\\)). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(123)\r\u0026gt; nA \u0026lt;- 2 #number of levels of A\r\u0026gt; nB \u0026lt;- 3 #number of levels of B\r\u0026gt; nsample \u0026lt;- 10 #number of reps in each\r\u0026gt; A \u0026lt;- gl(nA, 1, nA, lab = paste(\u0026quot;a\u0026quot;, 1:nA, sep = \u0026quot;\u0026quot;))\r\u0026gt; B \u0026lt;- gl(nB, 1, nB, lab = paste(\u0026quot;b\u0026quot;, 1:nB, sep = \u0026quot;\u0026quot;))\r\u0026gt; data \u0026lt;- expand.grid(A = A, B = B, n = 1:nsample)\r\u0026gt; X \u0026lt;- model.matrix(~A * B, data = data)\r\u0026gt; eff \u0026lt;- c(40, 15, 5, 0, -15, 10)\r\u0026gt; sigma \u0026lt;- 3 #residual standard deviation\r\u0026gt; n \u0026lt;- nrow(data)\r\u0026gt; eps \u0026lt;- rnorm(n, 0, sigma) #residuals\r\u0026gt; data$y \u0026lt;- as.numeric(X %*% eff + eps)\r\u0026gt; head(data) #print out the first six rows of the data set\rA B n y\r1 a1 b1 1 38.31857\r2 a2 b1 1 54.30947\r3 a1 b2 1 49.67612\r4 a2 b2 1 45.21153\r5 a1 b3 1 40.38786\r6 a2 b3 1 70.14519\r\u0026gt; \u0026gt; with(data, interaction.plot(A, B, y))\r\u0026gt; \u0026gt; ## ALTERNATIVELY, we could supply the population means and get the effect parameters from these. To\r\u0026gt; ## correspond to the model matrix, enter the population means in the order of: a1b1, a2b1, a1b1,\r\u0026gt; ## a2b2,a1b3,a2b3\r\u0026gt; pop.means \u0026lt;- as.matrix(c(40, 55, 45, 45, 40, 65), byrow = F)\r\u0026gt; ## Generate a minimum model matrix for the effects\r\u0026gt; XX \u0026lt;- model.matrix(~A * B, expand.grid(A = factor(1:2), B = factor(1:3)))\r\u0026gt; ## Use the solve() function to solve what are effectively simultaneous equations\r\u0026gt; (eff \u0026lt;- as.vector(solve(XX, pop.means)))\r[1] 40 15 5 0 -15 10\r\u0026gt; \u0026gt; data$y \u0026lt;- as.numeric(X %*% eff + eps)\rWith these sort of data, we are primarily interested in investigating whether there is a relationship between the continuous response variable and the treatment type. Does treatment type effect the response?.\nAssumptions\rThe assumptions are:\n\rAll of the observations are independent - this must be addressed at the design and collection stages. Importantly, to be considered independent replicates, the replicates must be made at the same scale at which the treatment is applied. For example, if the experiment involves subjecting organisms housed in tanks to different water temperatures, then the unit of replication is the individual tanks not the individual organisms in the tanks. The individuals in a tank are strictly not independent with respect to the treatment.\n\rThe response variable (and thus the residuals) should be normally distributed for each sampled populations (combination of factors). Boxplots of each treatment combination are useful for diagnosing major issues with normality.\n\rThe response variable should be equally varied (variance should not be related to mean as these are supposed to be estimated separately) for each combination of treatments. Again, boxplots are useful.\n\r\r\rExploratory data analysis\rNormality and Homogeneity of variance\n\u0026gt; boxplot(y ~ A * B, data)\r\u0026gt; \u0026gt; # OR via ggplot2\r\u0026gt; library(ggplot2)\r\u0026gt; ggplot(data, aes(y = y, x = A, fill = B)) + geom_boxplot()\rConclusions\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical. There is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\nObvious violations could be addressed either by:\n\rtransform the scale of the response variables (to address normality etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\r\r\r\rModel fitting\rThe observed response (\\(y_i\\)) are assumed to be drawn from a normal distribution with a given mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The expected values are themselves determined by the linear predictor (\\(\\boldsymbol X \\boldsymbol \\beta\\)). In this case, \\(\\boldsymbol \\beta\\) represents the intercept associated with the first combination of groups, as well as the (effects) differences between this intercept and each other group. \\(\\boldsymbol X\\) is the model matrix. MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying ‘uninformative’ priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (\\(100\\)) for both the intercept and the treatment effect and a wide half-cauchy (\\(\\text{scale}=5\\)) for the standard deviation.\n\\[y_i \\sim N(\\mu_i,\\sigma), \\]\nwhere \\(\\mu_i=\\beta_0 +\\boldsymbol \\beta \\boldsymbol X\\). The assumed priors are: \\(\\beta \\sim N(0,100)\\) and \\(\\sigma \\sim \\text{Cauchy}(0,5)\\). Exploratory data analysis suggests that the intercept and effects could be drawn from similar distributions (with mean in the \\(10\\)’s and variances in the \\(100\\)’s). Whilst we might therefore be tempted to provide different priors for the intercept, compared to the effects, for a simple model such as this, it is unlikely to be necessary. However, for more complex models, where prior specification becomes more critical, separate priors would probably be necessary. We proceed to code the model into STAN.\n\u0026gt; modelString = \u0026quot;\r+ data {\r+ int\u0026lt;lower=1\u0026gt; n;\r+ int\u0026lt;lower=1\u0026gt; nX;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ }\r+ parameters {\r+ vector[nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ }\r+ transformed parameters {\r+ vector[n] mu;\r+ + mu = X*beta;\r+ }\r+ model {\r+ // Likelihood\r+ y~normal(mu,sigma);\r+ + // Priors\r+ beta ~ normal(0,100);\r+ sigma~cauchy(0,5);\r+ }\r+ generated quantities {\r+ vector[n] log_lik;\r+ + for (i in 1:n) {\r+ log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); + }\r+ }\r+ + \u0026quot;\r\u0026gt; ## write the model to a stan file \u0026gt; writeLines(modelString, con = \u0026quot;fact_anovaModel.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; Xmat \u0026lt;- model.matrix(~A * B, data)\r\u0026gt; data.list \u0026lt;- with(data, list(y = y, X = Xmat, nX = ncol(Xmat), n = nrow(data)))\rDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;log_lik\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1500\rNow compile and run the Stan code via the rstan interface. Note that the first time stan is run after the rstan package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; library(rstan)\rDuring the warmup stage, the No-U-Turn sampler (NUTS) attempts to determine the optimum stepsize - the stepsize that achieves the target acceptance rate (\\(0.8\\) or \\(80\\)% by default) without divergence (occurs when the stepsize is too large relative to the curvature of the log posterior and results in approximations that are likely to diverge and be biased) - and without hitting the maximum treedepth (\\(10\\)). At each iteration of the NUTS algorithm, the number of leapfrog steps doubles (as it increases the treedepth) and only terminates when either the NUTS criterion are satisfied or the tree depth reaches the maximum (\\(10\\) by default).\n\u0026gt; data.rstan \u0026lt;- stan(data = data.list, file = \u0026quot;fact_anovaModel.stan\u0026quot;, chains = nChains, pars = params,\r+ iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;fact_anovaModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 1: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 1: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 1: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 1: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 1: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 1: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 1: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 1: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 1: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 1: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 1: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.064 seconds (Warm-up)\rChain 1: 0.085 seconds (Sampling)\rChain 1: 0.149 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;fact_anovaModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 2: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 2: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 2: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 2: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 2: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 2: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 2: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 2: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 2: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 2: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 2: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.07 seconds (Warm-up)\rChain 2: 0.087 seconds (Sampling)\rChain 2: 0.157 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.rstan, par = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: fact_anovaModel.\r2 chains, each with iter=1500; warmup=500; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 40.19 0.03 0.90 38.43 39.57 40.17 40.81 41.99 710 1.00\rbeta[2] 14.71 0.05 1.33 12.05 13.83 14.71 15.56 17.39 655 1.01\rbeta[3] 4.99 0.04 1.29 2.42 4.11 5.03 5.88 7.35 845 1.00\rbeta[4] -0.35 0.04 1.29 -2.80 -1.22 -0.37 0.53 2.15 907 1.00\rbeta[5] -14.51 0.07 1.86 -18.22 -15.78 -14.49 -13.21 -10.84 772 1.00\rbeta[6] 11.12 0.07 1.85 7.41 9.93 11.15 12.42 14.68 791 1.01\rsigma 2.89 0.01 0.28 2.41 2.70 2.88 3.07 3.48 1521 1.00\rSamples were drawn using NUTS(diag_e) at Tue Feb 18 14:32:34 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots.\n\u0026gt; library(mcmcplots)\r\u0026gt; s = as.array(data.rstan)\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, dimnames(s)$parameters)\r\u0026gt; s = s[, , wch]\r\u0026gt; mcmc \u0026lt;- do.call(mcmc.list, plyr:::alply(s[, , -(length(s[1, 1, ]))], 2, as.mcmc))\r\u0026gt; denplot(mcmc, parms = c(\u0026quot;beta\u0026quot;))\r\u0026gt; traplot(mcmc, parms = c(\u0026quot;beta\u0026quot;))\rThese plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.\n\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(mcmc)\r$`1`\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r$`2`\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; stan_ac(data.rstan, pars = c(\u0026quot;beta\u0026quot;))\rA lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).\n\u0026gt; stan_rhat(data.rstan, pars = c(\u0026quot;beta\u0026quot;))\r\u0026gt; stan_ess(data.rstan, pars = c(\u0026quot;beta\u0026quot;))\rRhat and effective sample size. In this instance, most of the parameters have reasonably high effective samples and thus there is likely to be a good range of values from which to estimate paramter properties.\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. Ideally, a good model should also be able to predict the data used to fit the model. Residuals are not computed directly within rstan However, we can calculate them manually form the posteriors.\n\u0026gt; mcmc = as.matrix(data.rstan)\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~A * B, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; wch = grep(\u0026quot;beta\\\\[\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = apply(mcmc[, wch], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rResiduals against predictors\n\u0026gt; library(dplyr)\r\u0026gt; library(tidyr)\r\u0026gt; mcmc = as.matrix(data.rstan)\r\u0026gt; wch = grep(\u0026quot;beta\\\\[\u0026quot;, colnames(mcmc))\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = newdata\r\u0026gt; Xmat = model.matrix(~A * B, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, wch], 2, median)\r\u0026gt; print(coefs)\rbeta[1] beta[2] beta[3] beta[4] beta[5] beta[6] 40.1693379 14.7128347 5.0341367 -0.3693605 -14.4877785 11.1514524 \u0026gt; \u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; newdata = newdata %\u0026gt;% cbind(fit, resid)\r\u0026gt; ggplot(newdata) + geom_point(aes(y = resid, x = A)) + theme_classic()\r\u0026gt; \u0026gt; ggplot(newdata) + geom_point(aes(y = resid, x = B)) + theme_classic()\rAnd now for studentised residuals\n\u0026gt; mcmc = as.matrix(data.rstan)\r\u0026gt; wch = grep(\u0026quot;beta\\\\[\u0026quot;, colnames(mcmc))\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~A * B, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, wch], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; sresid = resid/sd(resid)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit)) + theme_classic()\rFor this simple model, the studentized residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = as.matrix(data.rstan)\r\u0026gt; wch = grep(\u0026quot;beta\\\\[\u0026quot;, colnames(mcmc))\r\u0026gt; #generate a model matrix\r\u0026gt; Xmat = model.matrix(~A*B, data)\r\u0026gt; ##get median parameter estimates\r\u0026gt; coefs = mcmc[,wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i,], mcmc[i, \u0026#39;sigma\u0026#39;]))\r\u0026gt; newdata = data.frame(A=data$A, B=data$B, yRep) %\u0026gt;% gather(key=Sample, value=Value,-A,-B)\r\u0026gt; ggplot(newdata) +\r+ geom_violin(aes(y=Value, x=A, fill=\u0026#39;Model\u0026#39;), alpha=0.5)+\r+ geom_violin(data=data, aes(y=y,x=A,fill=\u0026#39;Obs\u0026#39;), alpha=0.5) +\r+ geom_point(data=data, aes(y=y, x=A), position=position_jitter(width=0.1,height=0),\r+ color=\u0026#39;black\u0026#39;) + theme_classic()\r\u0026gt; \u0026gt; ggplot(newdata) +\r+ geom_violin(aes(y=Value, x=B, fill=\u0026#39;Model\u0026#39;, group=B, color=A), alpha=0.5)+\r+ geom_point(data=data, aes(y=y, x=B, group=B,color=A)) + theme_classic()\rThe predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.\n\u0026gt; library(bayesplot)\r\u0026gt; mcmc_intervals(as.matrix(data.rstan), regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\u0026gt; mcmc_areas(as.matrix(data.rstan), regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\rParameter estimates\rAlthough all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and \\(95\\)% credibility intervals.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\rFirst, we look at the results from the additive model.\n\u0026gt; print(data.rstan, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: fact_anovaModel.\r2 chains, each with iter=1500; warmup=500; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 40.19 0.03 0.90 38.43 39.57 40.17 40.81 41.99 710 1.00\rbeta[2] 14.71 0.05 1.33 12.05 13.83 14.71 15.56 17.39 655 1.01\rbeta[3] 4.99 0.04 1.29 2.42 4.11 5.03 5.88 7.35 845 1.00\rbeta[4] -0.35 0.04 1.29 -2.80 -1.22 -0.37 0.53 2.15 907 1.00\rbeta[5] -14.51 0.07 1.86 -18.22 -15.78 -14.49 -13.21 -10.84 772 1.00\rbeta[6] 11.12 0.07 1.85 7.41 9.93 11.15 12.42 14.68 791 1.01\rsigma 2.89 0.01 0.28 2.41 2.70 2.88 3.07 3.48 1521 1.00\rSamples were drawn using NUTS(diag_e) at Tue Feb 18 14:32:34 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(data.rstan, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\r# A tibble: 7 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 40.2 0.905 38.5 42.0 2 beta[2] 14.7 1.33 12.2 17.5 3 beta[3] 4.99 1.29 2.35 7.29\r4 beta[4] -0.349 1.29 -2.78 2.16\r5 beta[5] -14.5 1.86 -18.4 -11.1 6 beta[6] 11.1 1.85 7.40 14.7 7 sigma 2.89 0.279 2.38 3.43\rConclusions\n\rThe intercept represents the mean of the first combination Aa1:Bb1 is \\(40.2\\)\n\rAa2:Bb1 is \\(14.7\\) units greater than Aa1:Bb1\n\rAa1:Bb2 is \\(5\\) units greater Aa1:Bb1\n\rAa1:Bb3 is \\(-0.335\\) units greater Aa1:Bb1\n\rAa2:Bb2 is \\(-14.6\\) units greater than the difference between (Aa1:Bb2 + Aa2:Bb1) and (2*Aa1:Bb1)\n\rAa2:Bb3 is \\(11.1\\) units greater than the difference between (Aa1:Bb3 + Aa2:Bb1) and (2*Aa1:Bb1)\n\r\rThe \\(95\\)% credibility interval for both interactive effects (Aa2:Bb2 and Aa2:Bb3) do not contain \\(0\\), implying significant interactions between A and B. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, \u0026quot;beta[2]\u0026quot;])\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, \u0026quot;beta[3]\u0026quot;])\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, \u0026quot;beta[4]\u0026quot;])\r[1] 0.777\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, \u0026quot;beta[5]\u0026quot;])\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, \u0026quot;beta[6]\u0026quot;])\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, c(\u0026quot;beta[5]\u0026quot;, \u0026quot;beta[6]\u0026quot;)])\r[1] 0\rThere is evidence of an interaction between A and B. In a Bayesian context, we can compare models using the leave-one-out cross-validation statistics. Leave-one-out (LOO) cross-validation explores how well a series of models can predict withheld values Vehtari, Gelman, and Gabry (2017). The LOO Information Criterion (LOOIC) is analogous to the AIC except that the LOOIC takes priors into consideration, does not assume that the posterior distribution is drawn from a multivariate normal and integrates over parameter uncertainty so as to yield a distribution of looic rather than just a point estimate. The LOOIC does however assume that all observations are equally influential (it does not matter which observations are left out). This assumption can be examined via the Pareto \\(k\\) estimate (values greater than \\(0.5\\) or more conservatively \\(0.75\\) are considered overly influential). We can compute LOOIC if we store the loglikelihood from our STAN model, which can then be extracted to compute the information criterion using the package loo.\n\u0026gt; library(loo)\r\u0026gt; (full = loo(extract_log_lik(data.rstan)))\rComputed from 2000 by 60 log-likelihood matrix\rEstimate SE\relpd_loo -151.8 5.2\rp_loo 6.3 1.1\rlooic 303.5 10.5\r------\rMonte Carlo SE of elpd_loo is 0.1.\rAll Pareto k estimates are good (k \u0026lt; 0.5).\rSee help(\u0026#39;pareto-k-diagnostic\u0026#39;) for details.\r\u0026gt; \u0026gt; # now fit a model without main factor\r\u0026gt; modelString2 = \u0026quot;\r+ data {\r+ int\u0026lt;lower=1\u0026gt; n;\r+ int\u0026lt;lower=1\u0026gt; nX;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ }\r+ parameters {\r+ vector[nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ }\r+ transformed parameters {\r+ vector[n] mu;\r+ + mu = X*beta;\r+ }\r+ model {\r+ // Likelihood\r+ y~normal(mu,sigma);\r+ + // Priors\r+ beta ~ normal(0,1000);\r+ sigma~cauchy(0,5);\r+ }\r+ generated quantities {\r+ vector[n] log_lik;\r+ + for (i in 1:n) {\r+ log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); + }\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a stan file \u0026gt; writeLines(modelString2, con = \u0026quot;fact_anovaModel2.stan\u0026quot;)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~A + B, data)\r\u0026gt; data.list \u0026lt;- with(data, list(y = y, X = Xmat, n = nrow(data), nX = ncol(Xmat)))\r\u0026gt; data.rstan.red \u0026lt;- stan(data = data.list, file = \u0026quot;fact_anovaModel2.stan\u0026quot;, chains = nChains,\r+ iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;fact_anovaModel2\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 1: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 1: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 1: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 1: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 1: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 1: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 1: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 1: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 1: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 1: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 1: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.061 seconds (Warm-up)\rChain 1: 0.068 seconds (Sampling)\rChain 1: 0.129 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;fact_anovaModel2\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 2: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 2: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 2: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 2: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 2: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 2: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 2: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 2: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 2: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 2: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 2: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.064 seconds (Warm-up)\rChain 2: 0.062 seconds (Sampling)\rChain 2: 0.126 seconds (Total)\rChain 2: \u0026gt; \u0026gt; (reduced = loo(extract_log_lik(data.rstan.red)))\rComputed from 2000 by 60 log-likelihood matrix\rEstimate SE\relpd_loo -196.6 3.9\rp_loo 4.4 0.5\rlooic 393.2 7.7\r------\rMonte Carlo SE of elpd_loo is 0.1.\rAll Pareto k estimates are good (k \u0026lt; 0.5).\rSee help(\u0026#39;pareto-k-diagnostic\u0026#39;) for details.\r\u0026gt; \u0026gt; par(mfrow = 1:2, mar = c(5, 3.8, 1, 0) + 0.1, las = 3)\r\u0026gt; plot(full, label_points = TRUE)\r\u0026gt; plot(reduced, label_points = TRUE)\rthe expected out-of-sample predictive accuracy is substantially lower for the model that includes the interaction (full model).\n\rGraphical summaries\rA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group.\n\u0026gt; mcmc = as.matrix(data.rstan)\r\u0026gt; wch = grep(\u0026quot;^beta\u0026quot;, colnames(mcmc))\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = expand.grid(A=levels(data$A), B=levels(data$B))\r\u0026gt; Xmat = model.matrix(~A*B,newdata)\r\u0026gt; coefs = mcmc[,wch]\r\u0026gt; fit=coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int=TRUE, conf.method=\u0026#39;HPDinterval\u0026#39;))\r\u0026gt; newdata\rA B estimate std.error conf.low conf.high\r1 a1 b1 40.18899 0.9047706 38.49347 42.01653\r2 a2 b1 54.89674 0.9051450 53.09543 56.61192\r3 a1 b2 45.17758 0.9211897 43.52976 47.13596\r4 a2 b2 45.37979 0.8898798 43.70321 47.20260\r5 a1 b3 39.84017 0.9091679 38.15775 41.68992\r6 a2 b3 65.67259 0.9105872 63.89731 67.49493\r\u0026gt; \u0026gt; ggplot(newdata, aes(y=estimate, x=B, fill=A)) +\r+ geom_blank() +\r+ geom_line(aes(x=as.numeric(B), linetype=A)) +\r+ geom_linerange(aes(ymin=conf.low, ymax=conf.high))+\r+ geom_point(aes(shape=A), size=3)+\r+ scale_y_continuous(\u0026#39;Y\u0026#39;)+\r+ scale_x_discrete(\u0026#39;B\u0026#39;)+\r+ scale_shape_manual(\u0026#39;A\u0026#39;,values=c(21,16))+\r+ scale_fill_manual(\u0026#39;A\u0026#39;,values=c(\u0026#39;white\u0026#39;,\u0026#39;black\u0026#39;))+\r+ scale_linetype_manual(\u0026#39;A\u0026#39;,values=c(\u0026#39;solid\u0026#39;,\u0026#39;dashed\u0026#39;))+\r+ theme_classic() +\r+ theme(legend.justification=c(0,1), legend.position=c(0.05,1),\r+ axis.title.y=element_text(vjust=2, size=rel(1.25)),\r+ axis.title.x=element_text(vjust=-2, size=rel(1.25)),\r+ plot.margin=unit(c(0.5,0.5,2,2), \u0026#39;lines\u0026#39;),\r+ legend.key.size=unit(1,\u0026#39;cm\u0026#39;)) + theme_classic()\r\rFinite population standard deviations\rVariance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).\nFinite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (Gelman and others (2005)). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.\n# A tibble: 4 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.A 10.4 0.942 8.64 12.4 2 sd.B 3.06 0.634 1.81 4.25\r3 sd.AB 10.4 0.734 9.02 11.9 4 sd.resid 2.84 0.0811 2.72 3.00\r# A tibble: 4 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.A 39.1 1.94 35.0 42.6\r2 sd.B 11.5 1.90 7.70 15.1\r3 sd.AB 39.0 0.946 37.2 41.0\r4 sd.resid 10.6 0.862 9.37 12.5\rApproximately \\(39\\)% of the total finite population standard deviation is due to the interaction between factor A and factor B.\n\rR squared\rIn a frequentist context, the \\(R^2\\) value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, \\(R^2\\) is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an \\(R^2\\) greater than \\(100\\)%. Gelman et al. (2019) proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.\nSo in the standard regression model notation of:\n\\[ y_i \\sim \\text{Normal}(\\boldsymbol X \\boldsymbol \\beta, \\sigma),\\]\nthe \\(R^2\\) could be formulated as\n\\[ R^2 = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sigma^2_e},\\]\nwhere \\(\\sigma^2_f=\\text{var}(\\boldsymbol X \\boldsymbol \\beta)\\), and for normal models \\(\\sigma^2_e=\\text{var}(y-\\boldsymbol X \\boldsymbol \\beta)\\)\n\u0026gt; mcmc \u0026lt;- as.matrix(data.rstan)\r\u0026gt; Xmat = model.matrix(~A * B, data)\r\u0026gt; wch = grep(\u0026quot;^beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = sweep(fit, 2, data$y, \u0026quot;-\u0026quot;)\r\u0026gt; var_f = apply(fit, 1, var)\r\u0026gt; var_e = apply(resid, 1, var)\r\u0026gt; R2 = var_f/(var_f + var_e)\r\u0026gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.913 0.00814 0.898 0.925\r\u0026gt; \u0026gt; # for comparison with frequentist\r\u0026gt; summary(lm(y ~ A * B, data))\rCall:\rlm(formula = y ~ A * B, data = data)\rResiduals:\rMin 1Q Median 3Q Max -6.5694 -1.8517 -0.0589 1.7120 6.5966 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 40.1940 0.8980 44.760 \u0026lt; 2e-16 ***\rAa2 14.7163 1.2700 11.588 2.88e-16 ***\rBb2 4.9823 1.2700 3.923 0.000249 ***\rBb3 -0.3464 1.2700 -0.273 0.786077 Aa2:Bb2 -14.5093 1.7960 -8.079 7.37e-11 ***\rAa2:Bb3 11.1056 1.7960 6.184 8.65e-08 ***\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 2.84 on 54 degrees of freedom\rMultiple R-squared: 0.92, Adjusted R-squared: 0.9125 F-statistic: 124.1 on 5 and 54 DF, p-value: \u0026lt; 2.2e-16\r\rDealing with interactions\rIn the presence of interations, conclusions about the main effects are overly simplistic at best and completely inaccurate at worst. Therefore, in the presense of interactions we should attempt to tease the analysis appart a little. In the current working example, we have identified that there is a significant interaction between Factor A and Factor B. Our exploration of the regression coefficients, indicated that the pattern between b1, b2 and b3 might differ between a1 and a2. Similarly, if we consider the coefficients from the perspective of Factor A, we can see that the patterns between a1 and a2 are similar for b1 and b3, yet very different for b2.\nAt this point, we can then split the two-factor model up into a series of single-factor models, either:\n\rexamining the effects of Factor B separately for each level of Factor A (two single-factor models) or\n\rexamining the effects of Factor A separately for each level of Factor B (three single-factor models)\n\r\rHowever, rather than subset the data and fit isolated smaller models, it is arguably better to treat these explorations as contrasts. As such we could either:\n\rapply specific contrasts to the already fit model\n\rdefine the specific contrasts and use them to refit the model\n\r\rWe will do the former of these options since we have already fit the global model. For this demonstration, we will explore the effect of factor A at each level of factor B. I will illustrate two ways to perform these contrasts on an already fit model:\nBy generating the posteriors of the cell means (means of each factor combination) and then manually compare the appropriate columns for specific levels of factor B.\r\r\u0026gt; mcmc \u0026lt;- as.matrix(data.rstan)\r\u0026gt; wch = grep(\u0026quot;^beta\u0026quot;, colnames(mcmc))\r\u0026gt; newdata = expand.grid(A = levels(data$A), B = levels(data$B))\r\u0026gt; Xmat = model.matrix(~A * B, data = newdata)\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; head(fit)\riterations 1 2 3 4 5 6\r[1,] 39.69752 55.00221 44.75174 45.00506 40.68290 66.39012\r[2,] 38.09011 54.98489 46.07381 45.40083 40.12613 65.72993\r[3,] 38.43453 55.88735 45.44974 44.95014 39.88235 65.69477\r[4,] 40.62015 55.15234 45.25819 45.63459 39.89825 65.41017\r[5,] 41.36307 54.24942 45.07712 46.12218 39.55757 64.96870\r[6,] 41.36075 54.50463 45.34008 44.16493 38.64767 66.31968\r\u0026gt; \u0026gt; ## we want to compare columns 2-1, 4-3 and 6-5\r\u0026gt; comps = fit[, c(2, 4, 6)] - fit[, c(1, 3, 5)]\r\u0026gt; tidyMCMC(comps, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 3 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 2 14.7 1.33 12.2 17.5 2 4 0.202 1.29 -2.37 2.74\r3 6 25.8 1.26 23.4 28.3 \rBy generating the posteriors of the cell means (means of each factor combination) and then manually compare the appropriate columns for specific levels of factor B.\r\r\u0026gt; mcmc \u0026lt;- as.matrix(data.rstan)\r\u0026gt; wch = grep(\u0026quot;^beta\u0026quot;, colnames(mcmc))\r\u0026gt; newdata = expand.grid(A = levels(data$A), B = levels(data$B))\r\u0026gt; Xmat = model.matrix(~A * B, data = newdata)\r\u0026gt; contr = attr(Xmat, \u0026quot;contrasts\u0026quot;)\r\u0026gt; newdata.a1 = model.frame(~A * B, expand.grid(A = levels(data$A)[1], B = levels(data$B)),\r+ xlev = list(A = levels(data$A), B = levels(data$B)))\r\u0026gt; Xmat.a1 = model.matrix(~A * B, data = newdata.a1, contrasts = contr)\r\u0026gt; newdata.a2 = model.frame(~A * B, expand.grid(A = levels(data$A)[2], B = levels(data$B)),\r+ xlev = list(A = levels(data$A), B = levels(data$B)))\r\u0026gt; Xmat.a2 = model.matrix(~A * B, data = newdata.a2, contrasts = contr)\r\u0026gt; Xmat = Xmat.a2 - Xmat.a1\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 3 x 4\restimate std.error conf.low conf.high\r\u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 14.7 1.33 12.2 17.5 2 0.202 1.29 -2.37 2.74\r3 25.8 1.26 23.4 28.3 \r\rReferences\rGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9.\n\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rGelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” The Annals of Statistics 33 (1): 1–53.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\rVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and Waic.” Statistics and Computing 27 (5): 1413–32.\n\r\r\r","date":1581041594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581041594,"objectID":"f89993ceecb10d06e4663fd090d8251d","permalink":"/stan/factorial-anova-stan/factorial-anova-stan/","publishdate":"2020-02-06T21:13:14-05:00","relpermalink":"/stan/factorial-anova-stan/factorial-anova-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","anova","factor analysis","factorial designs"],"title":"Factorial Analysis of Variance - STAN","type":"STAN"},{"authors":["Andrea Gabrio"],"categories":["R","ancova","JAGS","factor analysis"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rPrevious tutorials have concentrated on designs for either continuous (Regression) or categorical (ANOVA) predictor variables. Analysis of covariance (ANCOVA) models are essentially ANOVA models that incorporate one or more continuous and categorical variables (covariates). Although the relationship between a response variable and a covariate may itself be of substantial clinical interest, typically covariate(s) are incorporated to reduce the amount of unexplained variability in the model and thereby increase the power of any treatment effects.\nIn ANCOVA, a reduction in unexplained variability is achieved by adjusting the response (to each treatment) according to slight differences in the covariate means as well as accounting for any underlying trends between the response and covariate(s). To do so, the extent to which the within treatment group small differences in covariate means between groups and treatment groups are essentially compared via differences in their \\(y\\)-intercepts. The total variation is thereafter partitioned into explained (using the deviations between the overall trend and trends approximated for each of the treatment groups) and unexplained components (using the deviations between the observations and the approximated within group trends). In this way, ANCOVA can be visualized as a regular ANOVA in which the group and overall means are replaced by group and overall trendlines. Importantly, it should be apparent that ANCOVA is only appropriate when each of the within group trends have the same slope and are thus parallel to one another and the overall trend. Furthermore, ANCOVA is not appropriate when the resulting adjustments must be extrapolated from a linear relationship outside the measured range of the covariate.\nAs an example, an experiment might be set up to investigate the energetic impacts of sexual vs parthenogenetic (egg development without fertilization) reproduction on leaf insect food consumption. To do so, researchers could measure the daily food intake of individual adult female leaf insects from female only (parthenogenetic) and mixed (sexual) populations. Unfortunately, the available individual leaf insects varied substantially in body size which was expected to increase the variability of daily food intake of treatment groups. Consequently, the researchers also measured the body mass of the individuals as a covariate, thereby providing a means by which daily food consumption could be standardized for body mass. ANCOVA attempts to reduce unexplained variability by standardising the response to the treatment by the effects of the specific covariate condition. Thus ANCOVA provides a means of exercising some statistical control over the variability when it is either not possible or not desirable to exercise experimental control (such as blocking or using otherwise homogeneous observations).\n\rNull hypothesis\rFactor A: the main treatment effect\n\r\\(H_0(A):\\mu_1(adj)=\\mu_2(adj)=\\ldots=\\mu_i(adj)=\\mu(adj)\\)\r\rThe adjusted population group means are all equal. The mean of population \\(1\\) adjusted for the covariate is equal to that of population \\(2\\) adjusted for the covariate and so on, and thus all population means adjusted for the covariate are equal to an overall adjusted mean. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group adjusted mean and the overall adjusted mean (\\(\\alpha_i(adj)=\\mu_i(adj)−\\mu(adj)\\)) then the \\(H_0\\) can alternatively be written as:\n\r\\(H_0(A):\\alpha_1(adj)=\\alpha_2(adj)=\\ldots=\\alpha_i(adj)=0\\)\r\rThe effect of each group equals zero. If one or more of the \\(\\alpha_i(adj)\\) are different from zero (the response mean for this treatment differs from the overall response mean), the null hypothesis is not true, indicating that the treatment does affect the response variable.\nFactor B: the covariate effect\n\r\\(H_0(B):\\beta_1(pooled)=0\\)\r\rThe pooled population slope equals zero. Note, that this null hypothesis is rarely of much interest. It is precisely because of this nuisance relationship that ANCOVA designs are applied.\n\rLinear models\rOne or more covariates can be incorporated into single factor, nested, factorial and partly nested designs in order to reduce the unexplained variation. Fundamentally, the covariate(s) are purely used to adjust the response values prior to the regular analysis. The difficulty is in determining the appropriate adjustments. Following is a list of the appropriate linear models and adjusted response calculations for a range of ANCOVA designs. Note that these linear models do not include interactions involving the covariates as these are assumed to be zero. The inclusion of these interaction terms is a useful means of testing the homogeneity of slopes assumption.\n\rSingle categorical and single covariate\n\rLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\beta(x_{ij}-\\bar{x}) + \\epsilon_{ij}\\)\n\rAdjustments: \\(y_{ij(adj)}=y_{ij} - b(x_{ij} - \\bar{x})\\)\n\r\rSingle categorical and two covariates\n\rLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\beta_{YX}(x_{ij}-\\bar{x}) + \\beta_{YZ}(z_{ij}-\\bar{z}) + \\epsilon_{ij}\\)\n\rAdjustments: \\(y_{ij(adj)}=y_{ij} - b_{YX}(x_{ij} - \\bar{x}) - b_{YZ}(z_{ij} - \\bar{z})\\)\n\r\rFactorial designs\n\rLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\gamma_j + (\\alpha\\gamma)_{ij}+ \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijk}\\)\n\rAdjustments: \\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \\bar{x})\\)\n\r\rNested designs\n\rLinear model: \\(y_{ijk}=\\mu + \\alpha_i + \\gamma_{j(i)} + \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijk}\\)\n\rAdjustments: \\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \\bar{x})\\)\n\r\rPartly nested designs\n\rLinear model: \\(y_{ijkl}=\\mu + \\alpha_i + \\gamma_{j(i)} + \\delta_k + (\\alpha\\delta)_{ik} + (\\gamma\\delta)_{j(i)k} + \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijkl}\\)\n\rAdjustments: \\(y_{ijk(adj)}=y_{ijkl} - b_{between}(x_{i} - \\bar{x}) - b_{within}(x_{ijk} - \\bar{x}_i)\\)\n\r\r\r\rAnalysis of variance\rIn ANCOVA, the total variability of the response variable is sequentially partitioned into components explained by each of the model terms, starting with the covariate and is therefore equivalent to performing a regular analysis of variance on the response variables that have been adjusted for the covariate. The appropriate unexplained residuals and therefore the appropriate F-ratios for each factor differ according to the different null hypotheses associated with different linear models as well as combinations of fixed and random factors in the model (see the following tables). Note that since the covariate levels measured are typically different for each group, ANCOVA designs are inherently non-orthogonal (unbalanced). Consequently, sequential (Type I sums of squares) should not be used. For very simple Ancova designs that incorporate a single categorical and single covariate, Type I sums of squares can be used provided the covariate appears in the linear model first (and thus is partitioned out last) as we are typically not interested in estimating this effect.\n\u0026gt; ancova_table\rdf MS F-ratio (A\u0026amp;B fixed) F-ratio (B fixed) Factor A \u0026quot;a-1\u0026quot; \u0026quot;MS A\u0026quot; \u0026quot;(MS A)/(MS res)\u0026quot; \u0026quot;(MS A)/(MS res)\u0026quot; Factor B \u0026quot;1\u0026quot; \u0026quot;MS B\u0026quot; \u0026quot;(MS B)/(MS res)\u0026quot; \u0026quot;(MS B)/(MS res)\u0026quot; Factor AB \u0026quot;a-1\u0026quot; \u0026quot;MS AB\u0026quot; \u0026quot;(MS AB)/(MS res)\u0026quot; \u0026quot;(MS AB)/(MS res)\u0026quot;\rResidual \u0026quot;(n-2)a\u0026quot; \u0026quot;MS res\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \rThe corresponding R syntax is given below.\n\u0026gt; anova(lm(DV ~ B * A, dataset))\r\u0026gt; # OR\r\u0026gt; anova(aov(DV ~ B * A, dataset))\r\u0026gt; # OR (make sure not using treatment contrasts)\r\u0026gt; Anova(lm(DV ~ B * A, dataset), type = \u0026quot;III\u0026quot;)\r\rAssumptions\rAs ANCOVA designs are essentially regular ANOVA designs that are first adjusted (centered) for the covariate(s), ANCOVA designs inherit all of the underlying assumptions of the appropriate ANOVA design. Specifically, hypothesis tests assume that:\n\rThe appropriate residuals are normally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator, see the above tables) should be used to explore normality. Scale transformations are often useful.\n\rThe appropriate residuals are equally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\n\rThe appropriate residuals are independent of one another.\n\rThe relationship between the response variable and the covariate should be linear. Linearity can be explored using scatterplots and residual plots should reveal no patterns.\n\rFor repeated measures and other designs in which treatment levels within blocks can not be be randomly ordered, the variance/covariance matrix is assumed to display sphericity.\n\rFor designs that utilise blocking, it is assumed that there are no block by within block interactions.\n\r\rHomogeneity of Slopes\nIn addition to the above assumptions, ANCOVA designs also assume that slopes of relationships between the response variable and the covariate(s) are the same for each treatment level (group). That is, all the trends are parallel. If the individual slopes deviate substantially from each other (and thus the overall slope), then adjustments made to each of the observations are nonsensical. This situation is analogous to an interaction between two or more factors. In ANCOVA, interactions involving the covariate suggest that the nature of the relationship between the response and the covariate differs between the levels of the categorical treatment. More importantly, they also indicate that whether or not there is an effect of the treatment depends on what range of the covariate you are focussed on. Clearly then, it is not possible to make conclusions about the main effects of treatments in the presence of such interactions. The assumption of homogeneity of slopes can be examined via interaction plots or more formally, by testing hypotheses about the interactions between categorical variables and the covariate(s). There are three broad approaches for dealing with ANCOVA designs with heterogeneous slopes and selection depends on the primary focus of the study.\nWhen the primary objective of the analysis is to investigate the effects of categorical treatments, it is possible to adopt an approach similar to that taken when exploring interactions in multiple regression. The effect of treatments can be examined at specific values of the covariate (such as the mean and \\(\\pm\\) one standard deviation). This approach is really only useful at revealing broad shifts in patterns over the range of the covariate and if the selected values of the covariate do not have some inherent clinical meaning (selected arbitrarily), then the outcomes can be of only limited clinical interest.\n\rAlternatively, the Johnson-Neyman technique (or Wilxon modification thereof) procedure indicates the ranges of the covariate over which the individual regression lines of pairs of treatment groups overlap or cross. Although less powerful than the previous approach, the Wilcox(J-N) procedure has the advantage of revealing the important range (ranges for which the groups are different and not different) of the covariate rather than being constrained by specific levels selected.\n\rUse contrast treatments to split up the interaction term into its constituent contrasts for each level of the treatment. Essentially this compares each of the treatment level slopes to the slope from the “control” group and is useful if the primary focus is on the relationships between the response and the covariate.\n\r\rSimilar covariate ranges\nAdjustments made to the response means in an attempt to statistically account for differences in the covariate involve predicting mean response values along displaced linear relationships between the overall response and covariate variables. The degree of trend displacement for any given group is essentially calculated by multiplying the overall regression slope by the degree of difference between the overall covariate mean and the mean of the covariate for that group. However, when the ranges of the covariate within each of the groups differ substantially from one another, these adjustments are effectively extrapolations and therefore of unknown reliability. If a simple ANOVA of the covariate modelled against the categorical factor indicates that the covariate means differ significantly between groups, it may be necessary to either remove extreme observations or reconsider the analysis.\nRobust ANCOVA\nANCOVA based on rank transformed data can be useful for accommodating data with numerous problematic outliers. Nevertheless, problems about the difficulties of detecting interactions from rank transformed data, obviously have implications for inferential tests of homogeneity of slopes. Randomisation tests that maintain response0covariate pairs and repeatedly randomise these observations amongst the levels of the treatments can also be useful, particularly when there is doubt over the independence of observations. Both planned and unplanned comparisons follow those of other ANOVA chapters without any real additional complications. Notably, recent implementations of the Tukey’s test (within R) accommodate unbalanced designs and thus negate the need for some of the more complicated and specialised techniques that have been highlighted in past texts.\n\r\rData generation\rConsider an experimental design aimed at exploring the effects of a categorical variable with three levels (Group A, Group B and Group C) on a response. From previous studies, we know that the response is influenced by another variable (covariate). Unfortunately, it was not possible to ensure that all sampling units were the same degree of the covariate. Therefore, in an attempt to account for this anticipated extra source of variability, we measured the level of the covariate for each sampling unit. Actually, in allocating treatments to the various treatment groups, we tried to ensure a similar mean and range of the covariate within each group.\n\u0026gt; set.seed(123)\r\u0026gt; n \u0026lt;- 10\r\u0026gt; p \u0026lt;- 3\r\u0026gt; A.eff \u0026lt;- c(40, -15, -20)\r\u0026gt; beta \u0026lt;- -0.45\r\u0026gt; sigma \u0026lt;- 4\r\u0026gt; B \u0026lt;- rnorm(n * p, 0, 15)\r\u0026gt; A \u0026lt;- gl(p, n, lab = paste(\u0026quot;Group\u0026quot;, LETTERS[1:3]))\r\u0026gt; mm \u0026lt;- model.matrix(~A + B)\r\u0026gt; data \u0026lt;- data.frame(A = A, B = B, Y = as.numeric(c(A.eff, beta) %*% t(mm)) + rnorm(n * p, 0, 4))\r\u0026gt; data$B \u0026lt;- data$B + 20\r\u0026gt; head(data)\rA B Y\r1 Group A 11.59287 45.48907\r2 Group A 16.54734 40.37341\r3 Group A 43.38062 33.05922\r4 Group A 21.05763 43.03660\r5 Group A 21.93932 42.41363\r6 Group A 45.72597 31.17787\rExploratory data analysis\r\u0026gt; library(car)\r\u0026gt; scatterplot(Y ~ B | A, data = data)\r\u0026gt; \u0026gt; boxplot(Y ~ A, data)\r\u0026gt; \u0026gt; # OR via ggplot\r\u0026gt; library(ggplot2)\r\u0026gt; ggplot(data, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;)\r\u0026gt; \u0026gt; ggplot(data, aes(y = Y, x = A)) + geom_boxplot()\rConclusions\nThere is no evidence of obvious non-normality. The assumption of linearity seems reasonable. The variability of the three groups seems approximately equal. The slopes (\\(Y\\) vs B trends) appear broadly similar for each treatment group.\nWe can explore inferential evidence of unequal slopes by examining estimated effects of the interaction between the categorical variable and the covariate. Note, pay no attention to the main effects - only the interaction. Even though I intend to illustrate Bayesian analyses here, for such a simple model, it is considerably simpler to use traditional OLS for testing for the presence of an interaction.\n\u0026gt; anova(lm(Y ~ B * A, data = data))\rAnalysis of Variance Table\rResponse: Y\rDf Sum Sq Mean Sq F value Pr(\u0026gt;F) B 1 989.99 989.99 92.6782 1.027e-09 ***\rA 2 2320.05 1160.02 108.5956 9.423e-13 ***\rB:A 2 51.36 25.68 2.4041 0.1118 Residuals 24 256.37 10.68 ---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThere is very little evidence to suggest that the assumption of equal slopes will be inappropriate.\n\r\rModel fitting\rThe observed response (\\(y_i\\)) are assumed to be drawn from a normal distribution with a given mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The expected values are themselves determined by the linear predictor (\\(\\boldsymbol X \\boldsymbol \\beta\\)). In this case, \\(\\boldsymbol \\beta\\) represents the vector of \\(\\beta\\)’s - the intercept associated with the first group, the (effects) differences between this intercept and the intercepts for each other group as well as the slope associated with the continuous covariate. \\(\\boldsymbol X\\) is the model matrix. MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying ‘uninformative’ priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (\\(100\\)) for both the intercept and the treatment effect and a wide half-cauchy (\\(\\text{scale}=5\\)) for the standard deviation.\n\\[y_i \\sim N(\\mu_i,\\sigma), \\]\nwhere \\(\\mu_i=\\beta_0 +\\boldsymbol \\beta \\boldsymbol X\\). The assumed priors are: \\(\\beta \\sim N(0,100)\\) and \\(\\sigma \\sim \\text{Cauchy}(0,5)\\). Note, exploratory data analysis suggests that while the intercept (intercept of Group A) and categorical predictor effects (differences between intercepts of each of the Group and Group A’s intercept) could be drawn from a similar distribution (with mean in the \\(10\\)’s and variances in the \\(100\\)’s), the slope (effect associated with Group A linear relationship) is likely to be an order of magnitude less. We might therefore be tempted to provide different priors for the intercept, categorical effects and slope effect. For a simple model such as this, it is unlikely to be necessary. However, for more complex models, where prior specification becomes more critical, separate priors would probably be necessary.\nWe proceed to code the model into JAGS (remember that in this software normal distribution are parameterised in terms of precisions \\(\\tau\\) rather than variances, where \\(\\tau=\\frac{1}{\\sigma^2}\\)). Note the following example as group means calculated as derived posteriors.\n\u0026gt; modelString = \u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mean[i],tau)\r+ mean[i] \u0026lt;- inprod(beta[],X[i,])\r+ }\r+ #Priors\r+ for (i in 1:ngroups) {\r+ beta[i] ~ dnorm(0, 1.0E-6) + }\r+ sigma ~ dunif(0, 100)\r+ tau \u0026lt;- 1 / (sigma * sigma)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;ancovaModel.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; X \u0026lt;- model.matrix(~A + B, data)\r\u0026gt; data.list \u0026lt;- with(data, list(y = Y, X = X, n = nrow(data), ngroups = ncol(X)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10500\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\u0026gt; library(R2jags)\rNow run the JAGS code via the R2jags interface. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; data.r2jags \u0026lt;- jags(data = data.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;ancovaModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 30\rUnobserved stochastic nodes: 5\rTotal graph size: 224\rInitializing model\r\u0026gt; \u0026gt; print(data.r2jags)\rInference for Bugs model at \u0026quot;ancovaModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 51.001 1.529 47.977 50.009 50.995 52.016 53.980 1.001 15000\rbeta[2] -16.254 1.623 -19.455 -17.342 -16.259 -15.170 -13.090 1.001 10000\rbeta[3] -20.656 1.667 -23.941 -21.752 -20.672 -19.566 -17.330 1.001 15000\rbeta[4] -0.484 0.048 -0.577 -0.516 -0.484 -0.453 -0.389 1.001 15000\rsigma 3.607 0.526 2.740 3.236 3.546 3.912 4.793 1.001 7400\rdeviance 160.601 3.509 155.859 158.002 159.905 162.478 169.218 1.001 15000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 6.2 and DIC = 166.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots for the effects model as an example. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters, e.g. \\(\\boldsymbol \\beta\\).\n\u0026gt; library(mcmcplots)\r\u0026gt; denplot(data.r2jags, parms = c(\u0026quot;beta\u0026quot;))\r\u0026gt; traplot(data.r2jags, parms = c(\u0026quot;beta\u0026quot;))\rTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters (such as \\(\\beta\\)s).\n\u0026gt; data.mcmc = as.mcmc(data.r2jags)\r\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(data.mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 2 3689 3746 0.985 beta[2] 2 3938 3746 1.050 beta[3] 2 3853 3746 1.030 beta[4] 2 3811 3746 1.020 deviance 2 3895 3746 1.040 sigma 5 5552 3746 1.480 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 2 3770 3746 1.010 beta[2] 2 3729 3746 0.995 beta[3] 2 3811 3746 1.020 beta[4] 2 3895 3746 1.040 deviance 2 3855 3746 1.030 sigma 4 5247 3746 1.400 \rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; autocorr.diag(data.mcmc)\rbeta[1] beta[2] beta[3] beta[4] deviance\rLag 0 1.000000000 1.000000000 1.000000000 1.0000000000 1.000000000\rLag 1 0.017910611 -0.003186598 0.009149022 0.0039919666 0.266991768\rLag 5 -0.004399550 -0.002747041 -0.001891657 -0.0213261543 0.005499734\rLag 10 -0.001972741 0.005855050 -0.004887402 0.0186597337 -0.008683579\rLag 50 -0.002269863 0.015348324 -0.001446494 -0.0004828212 -0.010725173\rsigma\rLag 0 1.000000000\rLag 1 0.382742913\rLag 5 0.007377659\rLag 10 -0.001255836\rLag 50 0.003892668\rA lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. For more complex models (those that contain multiple effects), it is also advisable to plot the residuals against each of the individual predictors. For sampling designs that involve sample collection over space or time, it is also a good idea to explore whether there are any temporal or spatial patterns in the residuals.\nThere are numerous situations (e.g. when applying specific variance-covariance structures to a model) where raw residuals do not reflect the interior workings of the model. Typically, this is because they do not take into account the variance-covariance matrix or assume a very simple variance-covariance matrix. Since the purpose of exploring residuals is to evaluate the model, for these cases, it is arguably better to draw conclusions based on standardized (or studentised) residuals. Unfortunately the definitions of standardised and studentised residuals appears to vary and the two terms get used interchangeably. I will adopt the following definitions:\n\rStandardised residuals. The raw residuals divided by the true standard deviation of the residuals (which of course is rarely known).\n\rStudentised residuals. The raw residuals divided by the standard deviation of the residuals. Note that externally studentised residuals are calculated by dividing the raw residuals by a unique standard deviation for each observation that is calculated from regressions having left each successive observation out.\n\rPearson residuals. The raw residuals divided by the standard deviation of the response variable.\n\r\rhe mark of a good model is being able to predict well. In an ideal world, we would have sufficiently large sample size as to permit us to hold a fraction (such as \\(25\\)%) back thereby allowing us to train the model on \\(75\\)% of the data and then see how well the model can predict the withheld \\(25\\)%. Unfortunately, such a luxury is still rare. The next best option is to see how well the model can predict the observed data. Models tend to struggle most with the extremes of trends and have particular issues when the extremes approach logical boundaries (such as zero for count data and standard deviations). We can use the fitted model to generate random predicted observations and then explore some properties of these compared to the actual observed data.\nRather than dublicate this for both additive and multiplicative models, we will only explore the multiplicative model. Residuals are not computed directly within JAGS. However, we can calculate them manually form the posteriors.\n\u0026gt; library(dplyr)\r\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~A + B, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:4], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$Y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rResiduals against predictors\n\u0026gt; library(tidyr)\r\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = newdata\r\u0026gt; Xmat = model.matrix(~A + B, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:4], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$Y - fit\r\u0026gt; newdata = newdata %\u0026gt;% cbind(fit, resid)\r\u0026gt; ggplot(newdata) + geom_point(aes(y = resid, x = A)) + theme_classic()\r\u0026gt; \u0026gt; ggplot(newdata) + geom_point(aes(y = resid, x = B)) + theme_classic()\rAnd now for studentised residuals\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~A + B, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:4], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$Y - fit\r\u0026gt; sresid = resid/sd(resid)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit)) + theme_classic()\rFor this simple model, the studentised residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~A + B, data)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, 1:4]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i,\r+ ], mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; newdata = data.frame(A = data$A, B = data$B, yRep) %\u0026gt;% gather(key = Sample,\r+ value = Value, -A, -B)\r\u0026gt; ggplot(newdata) + geom_violin(aes(y = Value, x = A, fill = \u0026quot;Model\u0026quot;),\r+ alpha = 0.5) + geom_violin(data = data, aes(y = Y, x = A,\r+ fill = \u0026quot;Obs\u0026quot;), alpha = 0.5) + geom_point(data = data, aes(y = Y,\r+ x = A), position = position_jitter(width = 0.1, height = 0),\r+ color = \u0026quot;black\u0026quot;) + theme_classic()\r\u0026gt; \u0026gt; ggplot(newdata) + geom_violin(aes(y = Value, x = B, fill = \u0026quot;Model\u0026quot;,\r+ group = B, color = A), alpha = 0.5) + geom_point(data = data,\r+ aes(y = Y, x = B, group = B, color = A)) + theme_classic()\rThe predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.\n\u0026gt; library(bayesplot)\r\u0026gt; mcmc_intervals(data.r2jags$BUGSoutput$sims.matrix, regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\u0026gt; mcmc_areas(data.r2jags$BUGSoutput$sims.matrix, regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\rParameter estimates\rAlthough all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and \\(95\\)% credibility intervals.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\rFirst, we look at the results from the additive model.\n\u0026gt; print(data.r2jags)\rInference for Bugs model at \u0026quot;ancovaModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 51.001 1.529 47.977 50.009 50.995 52.016 53.980 1.001 15000\rbeta[2] -16.254 1.623 -19.455 -17.342 -16.259 -15.170 -13.090 1.001 10000\rbeta[3] -20.656 1.667 -23.941 -21.752 -20.672 -19.566 -17.330 1.001 15000\rbeta[4] -0.484 0.048 -0.577 -0.516 -0.484 -0.453 -0.389 1.001 15000\rsigma 3.607 0.526 2.740 3.236 3.546 3.912 4.793 1.001 7400\rdeviance 160.601 3.509 155.859 158.002 159.905 162.478 169.218 1.001 15000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 6.2 and DIC = 166.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(as.mcmc(data.r2jags), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 6 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 51.0 1.53 48.0 53.9 2 beta[2] -16.3 1.62 -19.5 -13.1 3 beta[3] -20.7 1.67 -23.9 -17.3 4 beta[4] -0.484 0.0478 -0.577 -0.389\r5 deviance 161. 3.51 155. 167. 6 sigma 3.61 0.526 2.69 4.70 \rConclusions\n\rThe intercept of the first group (Group A) is \\(51\\).\n\rThe mean of the second group (Group B) is \\(-16.3\\) units greater than (A).\n\rThe mean of the third group (Group C) is \\(-20.7\\) units greater than (A).\n\rA one unit increase in B in Group A is associated with a \\(-0.484\\) units increase in \\(Y\\).\n\r\rThe \\(95\\)% confidence interval for the effects of Group B, Group C and the partial slope associated with B do not overlapp with 0 implying a significant difference between group A and groups B, C and a significant negative relationship with B. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[2]\u0026quot;]) # effect of (B-A = 0)\r[1] 0\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[3]\u0026quot;]) # effect of (C-A = 0)\r[1] 0\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[4]\u0026quot;]) # effect of (slope = 0)\r[1] 0\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, 2:4]) # effect of (model)\r[1] 0\rThere is evidence that the reponse differs between the groups.\n\rGraphical summaries\rA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group.\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = expand.grid(A = levels(data$A), B = seq(min(data$B), max(data$B),\r+ len = 100))\r\u0026gt; Xmat = model.matrix(~A + B, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;, \u0026quot;beta[3]\u0026quot;, \u0026quot;beta[4]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = B, fill = A)) + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;B\u0026quot;) + theme_classic()\rAs this is simple single factor ANOVA, we can simple add the raw data to this figure. For more complex designs with additional predictors, it is necessary to plot partial residuals.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = data\r\u0026gt; fMat = rMat = model.matrix(~A + B, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$Y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = B, fill = A)) + geom_point(data = rdata,\r+ aes(y = partial.resid, x = B, color = A)) + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;B\u0026quot;) + theme_classic()\r\rPosteriors\rIn frequentist statistics, when we have more than two groups, we are typically not only interested in whether there is evidence for an overall “effect” of a factor - we are also interested in how various groups compare to one another. To explore these trends, we either compare each group to each other in a pairwise manner (controlling for family-wise Type I error rates) or we explore an independent subset of the possible comparisons. Although these alternate approaches can adequately address a specific research agenda, often they impose severe limitations and compromises on the scope and breadth of questions that can be asked of your data. The reason for these limitations is that in a frequentist framework, any single hypothesis carries with it a (nominally) \\(5\\)% chance of a false rejection (since it is based on long-run frequency). Thus, performing multiple tests are likely to compound this error rate. The point is, that each comparison is compared to its own probability distribution (and each carries a \\(5\\)% error rate). By contrast, in Bayesian statistics, all comparisons (contrasts) are drawn from the one (hopefully stable and convergent) posterior distribution and this posterior is invariant to the type and number of comparisons drawn. Hence, the theory clearly indicates that having generated our posterior distribution, we can then query this distribution in any way that we wish thereby allowing us to explore all of our research questions simultaneously.\nBayesian “contrasts” can be performed either:\n\rwithin the Bayesian sampling model or\n\rconstruct them from the returned MCMC samples (they are drawn from the posteriors)\n\r\rOnly the latter will be demonstrated as it provides a consistent approach across all routines. In order to allow direct comparison to the frequentist equivalents, I will explore the same set of planned and Tukey’s test comparisons described here. For the “planned comparison” we defined two contrasts: 1) group B vs group C; and 2) group A vs the average of groups B and C. Of course each of these could be explored at multiple values of B, however, since we fit an additive model (which assumes that the slopes are homogeneous), the contrasts will be constant throughout the domain of B.\nLets start by comparing each group to each other group in a pairwise manner. Arguably the most elegant way to do this is to generate a Tukey’s contrast matrix. This is a model matrix specific to comparing each group to each other group. Again, since the lines are parallel, it does not really matter what level of B we estimate these efffects at - so lets use the mean B.\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; coefs \u0026lt;- as.matrix(mcmc)[, 1:4]\r\u0026gt; newdata \u0026lt;- data.frame(A = levels(data$A), B = mean(data$B))\r\u0026gt; # A Tukeys contrast matrix\r\u0026gt; library(multcomp)\r\u0026gt; tuk.mat \u0026lt;- contrMat(n = table(newdata$A), type = \u0026quot;Tukey\u0026quot;)\r\u0026gt; Xmat \u0026lt;- model.matrix(~A + B, data = newdata)\r\u0026gt; pairwise.mat \u0026lt;- tuk.mat %*% Xmat\r\u0026gt; pairwise.mat\r(Intercept) AGroup B AGroup C B\rGroup B - Group A 0 1 0 0\rGroup C - Group A 0 0 1 0\rGroup C - Group B 0 -1 1 0\r\u0026gt; \u0026gt; mcmc_areas(coefs %*% t(pairwise.mat))\r\u0026gt; \u0026gt; (comps = tidyMCMC(coefs %*% t(pairwise.mat), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 3 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 Group B - Group A -16.3 1.62 -19.5 -13.1 2 Group C - Group A -20.7 1.67 -23.9 -17.3 3 Group C - Group B -4.40 1.69 -7.68 -1.04\r\u0026gt; \u0026gt; ggplot(comps, aes(y = estimate, x = term)) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;) +\r+ scale_y_continuous(\u0026quot;Effect size\u0026quot;) + scale_x_discrete(\u0026quot;\u0026quot;) + coord_flip() +\r+ theme_classic()\rWith a couple of modifications, we could also express this as percentage changes. A percentage change represents the change (difference between groups) divided by one of the groups (determined by which group you want to express the percentage change to). Hence, we generate an additional mcmc matrix that represents the cell means for the divisor group (group we want to express change relative to). Since the tuk.mat defines comparisons as \\(-1\\) and \\(1\\) pairs, if we simply replace all the \\(-1\\) with \\(0\\), the eventual matrix multiplication will result in estimates of the divisor cell means instread of the difference. We can then divide the original mcmc matrix above with this new divisor mcmc matrix to yeild a mcmc matrix of percentage change.\n\u0026gt; # Modify the tuk.mat to replace -1 with 0. This will allow us to get a\r\u0026gt; # mcmc matrix of ..\r\u0026gt; tuk.mat[tuk.mat == -1] = 0\r\u0026gt; comp.mat \u0026lt;- tuk.mat %*% Xmat\r\u0026gt; comp.mat\r(Intercept) AGroup B AGroup C B\rGroup B - Group A 1 1 0 19.29344\rGroup C - Group A 1 0 1 19.29344\rGroup C - Group B 1 0 1 19.29344\r\u0026gt; \u0026gt; comp.mcmc = 100 * (coefs %*% t(pairwise.mat))/coefs %*% t(comp.mat)\r\u0026gt; (comps = tidyMCMC(comp.mcmc, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 3 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 Group B - Group A -64.3 8.74 -82.4 -48.0 2 Group C - Group A -99.0 12.6 -124. -74.8 3 Group C - Group B -21.4 9.02 -39.2 -4.13\r\u0026gt; \u0026gt; ggplot(comps, aes(y = estimate, x = term)) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;) +\r+ scale_y_continuous(\u0026quot;Effect size (%)\u0026quot;) + scale_x_discrete(\u0026quot;\u0026quot;) + coord_flip() +\r+ theme_classic()\rAnd now for the specific planned comparisons (Group B vs Group C as well as Group A vs the average of Groups B and C). This is achieved by generating our own contrast matrix (defining the contributions of each group to each contrast).\n\u0026gt; c.mat = rbind(c(0, 1, -1), c(1/2, -1/3, -1/3))\r\u0026gt; c.mat\r[,1] [,2] [,3]\r[1,] 0.0 1.0000000 -1.0000000\r[2,] 0.5 -0.3333333 -0.3333333\r\u0026gt; \u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; coefs \u0026lt;- as.matrix(mcmc)[, 1:4]\r\u0026gt; newdata \u0026lt;- data.frame(A = levels(data$A), B = mean(data$B))\r\u0026gt; Xmat \u0026lt;- model.matrix(~A + B, data = newdata)\r\u0026gt; c.mat = c.mat %*% Xmat\r\u0026gt; c.mat\r(Intercept) AGroup B AGroup C B\r[1,] 0.0000000 1.0000000 -1.0000000 0.000000\r[2,] -0.1666667 -0.3333333 -0.3333333 -3.215574\r\u0026gt; \u0026gt; (comps = tidyMCMC(as.mcmc(coefs %*% t(c.mat)), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 4.40 1.69 1.04 7.68\r2 var2 5.36 0.790 3.80 6.93\r\rFinite population standard deviations\rVariance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).\nFinite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (Gelman and others (2005)). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.\n beta[1] beta[2] beta[3] beta[4] deviance sigma\r[1,] 49.12140 -12.79223 -18.26477 -0.4972722 161.2762 3.888826\r[2,] 51.03351 -16.80051 -20.03944 -0.4767683 156.2198 2.958015\r[3,] 51.55756 -16.80292 -20.00531 -0.4479209 161.2724 3.984268\r[4,] 50.15508 -15.15637 -21.01837 -0.4787121 158.5376 3.943798\r[5,] 52.94683 -17.04043 -22.95279 -0.5209229 157.8834 3.194266\r[6,] 52.16920 -17.91313 -23.53270 -0.4678091 159.4251 3.239537\r# A tibble: 3 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.A 3.12 1.18 0.739 5.41\r2 sd.B 7.12 0.703 5.73 8.49\r3 sd.resid 3.46 0.169 3.26 3.79\r# A tibble: 3 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.A 22.9 6.62 8.09 34.1\r2 sd.B 52.3 4.54 43.1 61.2\r3 sd.resid 24.9 3.22 20.8 31.9\rApproximately \\(22.9\\)% of the total finite population standard deviation is due to \\(x\\).\n\rR squared\rIn a frequentist context, the \\(R^2\\) value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, \\(R^2\\) is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an \\(R^2\\) greater than \\(100\\)%. Gelman et al. (2019) proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.\nSo in the standard regression model notation of:\n\\[ y_i \\sim \\text{Normal}(\\boldsymbol X \\boldsymbol \\beta, \\sigma),\\]\nthe \\(R^2\\) could be formulated as\n\\[ R^2 = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sigma^2_e},\\]\nwhere \\(\\sigma^2_f=\\text{var}(\\boldsymbol X \\boldsymbol \\beta)\\), and for normal models \\(\\sigma^2_e=\\text{var}(y-\\boldsymbol X \\boldsymbol \\beta)\\)\n\u0026gt; Xmat = model.matrix(~A + B, data)\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = sweep(fit, 2, data$Y, \u0026quot;-\u0026quot;)\r\u0026gt; var_f = apply(fit, 1, var)\r\u0026gt; var_e = apply(resid, 1, var)\r\u0026gt; R2 = var_f/(var_f + var_e)\r\u0026gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.905 0.0148 0.877 0.922\r\u0026gt; \u0026gt; # for comparison with frequentist\r\u0026gt; summary(lm(Y ~ A + B, data))\rCall:\rlm(formula = Y ~ A + B, data = data)\rResiduals:\rMin 1Q Median 3Q Max -6.4381 -2.2244 -0.6829 2.1732 8.6607 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 51.00608 1.44814 35.22 \u0026lt; 2e-16 ***\rAGroup B -16.25472 1.54125 -10.55 6.92e-11 ***\rAGroup C -20.65596 1.57544 -13.11 5.74e-13 ***\rB -0.48399 0.04526 -10.69 5.14e-11 ***\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 3.44 on 26 degrees of freedom\rMultiple R-squared: 0.9149, Adjusted R-squared: 0.9051 F-statistic: 93.22 on 3 and 26 DF, p-value: 4.901e-14\r\rDealing with heterogeneous slopes\rGenerate the data with heterogeneous slope effects.\n\u0026gt; set.seed(123)\r\u0026gt; n \u0026lt;- 10\r\u0026gt; p \u0026lt;- 3\r\u0026gt; A.eff \u0026lt;- c(40, -15, -20)\r\u0026gt; beta \u0026lt;- c(-0.45, -0.1, 0.5)\r\u0026gt; sigma \u0026lt;- 4\r\u0026gt; B \u0026lt;- rnorm(n * p, 0, 15)\r\u0026gt; A \u0026lt;- gl(p, n, lab = paste(\u0026quot;Group\u0026quot;, LETTERS[1:3]))\r\u0026gt; mm \u0026lt;- model.matrix(~A * B)\r\u0026gt; data1 \u0026lt;- data.frame(A = A, B = B, Y = as.numeric(c(A.eff, beta) %*% t(mm)) + rnorm(n * p, 0, 4))\r\u0026gt; data1$B \u0026lt;- data1$B + 20\r\u0026gt; head(data1)\rA B Y\r1 Group A 11.59287 45.48907\r2 Group A 16.54734 40.37341\r3 Group A 43.38062 33.05922\r4 Group A 21.05763 43.03660\r5 Group A 21.93932 42.41363\r6 Group A 45.72597 31.17787\rExploratory data analysis\r\u0026gt; scatterplot(Y ~ B | A, data = data1)\r\u0026gt; \u0026gt; boxplot(Y ~ A, data1)\r\u0026gt; \u0026gt; # OR via ggplot\r\u0026gt; ggplot(data1, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;)\r\u0026gt; \u0026gt; ggplot(data1, aes(y = Y, x = A)) + geom_boxplot()\rThe slopes (\\(Y\\) vs B trends) do appear to differ between treatment groups - in particular, Group C seems to portray a different trend to Groups A and B.\n\u0026gt; anova(lm(Y ~ B * A, data = data1))\rAnalysis of Variance Table\rResponse: Y\rDf Sum Sq Mean Sq F value Pr(\u0026gt;F) B 1 442.02 442.02 41.380 1.187e-06 ***\rA 2 2760.60 1380.30 129.217 1.418e-13 ***\rB:A 2 285.75 142.87 13.375 0.0001251 ***\rResiduals 24 256.37 10.68 ---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThere is strong evidence to suggest that the assumption of equal slopes is violated.\n\rFitting the model\r\u0026gt; modelString2 = \u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mean[i],tau)\r+ mean[i] \u0026lt;- inprod(beta[],X[i,])\r+ }\r+ #Priors\r+ for (i in 1:ngroups) {\r+ beta[i] ~ dnorm(0, 1.0E-6) + }\r+ sigma ~ dunif(0, 100)\r+ tau \u0026lt;- 1 / (sigma * sigma)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString2, con = \u0026quot;ancovaModel2.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; X \u0026lt;- model.matrix(~A * B, data1)\r\u0026gt; data1.list \u0026lt;- with(data1, list(y = Y, X = X, n = nrow(data1), ngroups = ncol(X)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10500\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model).\n\u0026gt; data1.r2jags \u0026lt;- jags(data = data1.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;ancovaModel2.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 30\rUnobserved stochastic nodes: 7\rTotal graph size: 286\rInitializing model\r\u0026gt; \u0026gt; print(data1.r2jags)\rInference for Bugs model at \u0026quot;ancovaModel2.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 48.194 2.035 44.200 46.864 48.200 49.531 52.217 1.001 15000\rbeta[2] -10.562 2.884 -16.240 -12.453 -10.586 -8.688 -4.814 1.001 8100\rbeta[3] -26.538 2.568 -31.636 -28.207 -26.525 -24.858 -21.431 1.001 15000\rbeta[4] -0.351 0.082 -0.512 -0.404 -0.351 -0.297 -0.188 1.001 15000\rbeta[5] -0.271 0.110 -0.491 -0.344 -0.270 -0.198 -0.055 1.001 15000\rbeta[6] 0.270 0.117 0.039 0.194 0.270 0.346 0.500 1.001 15000\rsigma 3.454 0.535 2.601 3.074 3.396 3.757 4.689 1.002 1800\rdeviance 157.761 4.417 151.465 154.544 156.990 160.166 168.119 1.001 3000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 9.8 and DIC = 167.5\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMCMC diagnostics\r\u0026gt; denplot(data1.r2jags, parms = c(\u0026quot;beta\u0026quot;))\r\u0026gt; traplot(data1.r2jags, parms = c(\u0026quot;beta\u0026quot;))\rTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters (such as \\(\\beta\\)s).\n\u0026gt; data1.mcmc = as.mcmc(data1.r2jags)\r\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(data1.mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 2 3853 3746 1.030 beta[2] 2 3689 3746 0.985 beta[3] 2 3895 3746 1.040 beta[4] 2 3649 3746 0.974 beta[5] 2 3918 3746 1.050 beta[6] 2 3770 3746 1.010 deviance 2 3938 3746 1.050 sigma 4 5018 3746 1.340 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 2 3853 3746 1.030 beta[2] 2 3570 3746 0.953 beta[3] 2 3811 3746 1.020 beta[4] 2 3770 3746 1.010 beta[5] 2 3770 3746 1.010 beta[6] 2 3895 3746 1.040 deviance 2 3981 3746 1.060 sigma 4 5131 3746 1.370 \rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; autocorr.diag(data1.mcmc)\rbeta[1] beta[2] beta[3] beta[4] beta[5]\rLag 0 1.000000000 1.000000000 1.000000000 1.000000000 1.0000000000\rLag 1 -0.002520665 -0.007698073 0.001992162 0.000509790 -0.0005326877\rLag 5 0.001007950 0.009095032 0.001511518 -0.006890623 0.0025773251\rLag 10 -0.011280919 0.007907450 0.005969613 -0.006999313 0.0040454668\rLag 50 -0.012861369 -0.019813696 0.002604518 -0.008791380 -0.0136623372\rbeta[6] deviance sigma\rLag 0 1.000000000 1.000000000 1.0000000000\rLag 1 0.004381248 0.332075434 0.4518687724\rLag 5 -0.001182603 0.032092130 0.0351574955\rLag 10 -0.004191097 0.003338842 0.0005457235\rLag 50 0.002636154 -0.005426687 0.0039447210\r\rModel validation\r\u0026gt; mcmc = data1.r2jags$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata1 = data1\r\u0026gt; Xmat = model.matrix(~A * B, newdata1)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:6], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data1$Y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rResiduals against predictors\n\u0026gt; mcmc = data1.r2jags$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata1 = newdata1\r\u0026gt; Xmat = model.matrix(~A * B, newdata1)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:6], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data1$Y - fit\r\u0026gt; newdata1 = newdata1 %\u0026gt;% cbind(fit, resid)\r\u0026gt; ggplot(newdata1) + geom_point(aes(y = resid, x = A)) + theme_classic()\r\u0026gt; \u0026gt; ggplot(newdata1) + geom_point(aes(y = resid, x = B)) + theme_classic()\rAnd now for studentised residuals\n\u0026gt; mcmc = data1.r2jags$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata1 = data1\r\u0026gt; Xmat = model.matrix(~A * B, newdata1)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:6], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data1$Y - fit\r\u0026gt; sresid = resid/sd(resid)\r\u0026gt; ggplot() + geom_point(data1 = NULL, aes(y = sresid, x = fit)) + theme_classic()\rFor this simple model, the studentised residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = data1.r2jags$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~A * B, data1)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, 1:6]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data1), fit[i,\r+ ], mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; newdata1 = data.frame(A = data1$A, B = data1$B, yRep) %\u0026gt;% gather(key = Sample,\r+ value = Value, -A, -B)\r\u0026gt; ggplot(newdata1) + geom_violin(aes(y = Value, x = A, fill = \u0026quot;Model\u0026quot;),\r+ alpha = 0.5) + geom_violin(data = data1, aes(y = Y, x = A,\r+ fill = \u0026quot;Obs\u0026quot;), alpha = 0.5) + geom_point(data = data1, aes(y = Y,\r+ x = A), position = position_jitter(width = 0.1, height = 0),\r+ color = \u0026quot;black\u0026quot;) + theme_classic()\r\u0026gt; \u0026gt; ggplot(newdata1) + geom_violin(aes(y = Value, x = B, fill = \u0026quot;Model\u0026quot;,\r+ group = B, color = A), alpha = 0.5) + geom_point(data = data1,\r+ aes(y = Y, x = B, group = B, color = A)) + theme_classic()\rThe predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.\n\u0026gt; mcmc_intervals(data1.r2jags$BUGSoutput$sims.matrix, regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\u0026gt; mcmc_areas(data1.r2jags$BUGSoutput$sims.matrix, regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\rParameter estimates\rFirst, we look at the results from the additive model.\n\u0026gt; print(data1.r2jags)\rInference for Bugs model at \u0026quot;ancovaModel2.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 48.194 2.035 44.200 46.864 48.200 49.531 52.217 1.001 15000\rbeta[2] -10.562 2.884 -16.240 -12.453 -10.586 -8.688 -4.814 1.001 8100\rbeta[3] -26.538 2.568 -31.636 -28.207 -26.525 -24.858 -21.431 1.001 15000\rbeta[4] -0.351 0.082 -0.512 -0.404 -0.351 -0.297 -0.188 1.001 15000\rbeta[5] -0.271 0.110 -0.491 -0.344 -0.270 -0.198 -0.055 1.001 15000\rbeta[6] 0.270 0.117 0.039 0.194 0.270 0.346 0.500 1.001 15000\rsigma 3.454 0.535 2.601 3.074 3.396 3.757 4.689 1.002 1800\rdeviance 157.761 4.417 151.465 154.544 156.990 160.166 168.119 1.001 3000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 9.8 and DIC = 167.5\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; # OR\r\u0026gt; tidyMCMC(as.mcmc(data1.r2jags), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 8 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 48.2 2.03 44.2 52.2 2 beta[2] -10.6 2.88 -16.3 -4.94 3 beta[3] -26.5 2.57 -31.6 -21.4 4 beta[4] -0.351 0.0816 -0.510 -0.187 5 beta[5] -0.271 0.110 -0.491 -0.0541\r6 beta[6] 0.270 0.117 0.0436 0.503 7 deviance 158. 4.42 151. 167. 8 sigma 3.45 0.535 2.51 4.50 \rConclusions\n\rThe intercept of the first group (Group A) is \\(48.2\\).\n\rThe mean of the second group (Group B) is \\(-10.6\\) units greater than (A).\n\rThe mean of the third group (Group C) is \\(-26.5\\) units greater than (A).\n\rA one unit increase in B in Group A is associated with a \\(-0.351\\) units increase in \\(Y\\).\n\rdifference in slope between Group B and Group A \\(-0.270\\).\n\rdifference in slope between Group C and Group A \\(0.270\\).\n\r\rThe \\(95\\)% confidence interval for the effects of Group B, Group C and the partial slope associated with B do not overlapp with \\(0\\) implying a significant difference between group A and groups B, C (at the mean level of predictor B) and a significant negative relationship with B (for Group A). The slope associated with Group B was not found to be significantly different from that associated with Group A, however, the slope associated with Group C was found to be significantly less negative than the slope associated with Group A. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[2]\u0026quot;]) # effect of (B-A = 0)\r[1] 0.0009333333\r\u0026gt; mcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[3]\u0026quot;]) # effect of (C-A = 0)\r[1] 0\r\u0026gt; mcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[4]\u0026quot;]) # effect of (slope = 0)\r[1] 0.0003333333\r\u0026gt; mcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[5]\u0026quot;]) # effect of (slopeB - slopeA = 0)\r[1] 0.0152\r\u0026gt; mcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[6]\u0026quot;]) # effect of (slopeC - slopeA = 0)\r[1] 0.0232\r\u0026gt; mcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, 2:6]) # effect of (model)\r[1] 0\rThere is evidence that the reponse differs between the groups.\n\rGraphical summaries\r\u0026gt; mcmc = data1.r2jags$BUGSoutput$sims.matrix\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata1 = expand.grid(A = levels(data1$A), B = seq(min(data1$B), max(data1$B),\r+ len = 100))\r\u0026gt; Xmat = model.matrix(~A * B, newdata1)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;, \u0026quot;beta[3]\u0026quot;, \u0026quot;beta[4]\u0026quot;, \u0026quot;beta[5]\u0026quot;,\r+ \u0026quot;beta[6]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata1 = newdata1 %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; \u0026gt; ggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;B\u0026quot;) + theme_classic()\rAs this is simple single factor ANOVA, we can simple add the raw data to this figure. For more complex designs with additional predictors, it is necessary to plot partial residuals.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata1 = rdata1 = data1\r\u0026gt; fMat = rMat = model.matrix(~A * B, fdata1)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data1$Y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata1 = rdata1 %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; \u0026gt; ggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_point(data = rdata1,\r+ aes(y = partial.resid, x = B, color = A)) + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;B\u0026quot;) + theme_classic()\r\r\rReferences\rGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9.\n\rGelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” The Annals of Statistics 33 (1): 1–53.\n\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1580955194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580955194,"objectID":"7a08fb2f38941abbc899e41074989ac1","permalink":"/jags/ancova-jags/ancova-jags/","publishdate":"2020-02-05T21:13:14-05:00","relpermalink":"/jags/ancova-jags/ancova-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","ancova","factor analysis"],"title":"Analysis of Covariance - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","ancova","STAN","factor analysis"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rPrevious tutorials have concentrated on designs for either continuous (Regression) or categorical (ANOVA) predictor variables. Analysis of covariance (ANCOVA) models are essentially ANOVA models that incorporate one or more continuous and categorical variables (covariates). Although the relationship between a response variable and a covariate may itself be of substantial clinical interest, typically covariate(s) are incorporated to reduce the amount of unexplained variability in the model and thereby increase the power of any treatment effects.\nIn ANCOVA, a reduction in unexplained variability is achieved by adjusting the response (to each treatment) according to slight differences in the covariate means as well as accounting for any underlying trends between the response and covariate(s). To do so, the extent to which the within treatment group small differences in covariate means between groups and treatment groups are essentially compared via differences in their \\(y\\)-intercepts. The total variation is thereafter partitioned into explained (using the deviations between the overall trend and trends approximated for each of the treatment groups) and unexplained components (using the deviations between the observations and the approximated within group trends). In this way, ANCOVA can be visualized as a regular ANOVA in which the group and overall means are replaced by group and overall trendlines. Importantly, it should be apparent that ANCOVA is only appropriate when each of the within group trends have the same slope and are thus parallel to one another and the overall trend. Furthermore, ANCOVA is not appropriate when the resulting adjustments must be extrapolated from a linear relationship outside the measured range of the covariate.\nAs an example, an experiment might be set up to investigate the energetic impacts of sexual vs parthenogenetic (egg development without fertilization) reproduction on leaf insect food consumption. To do so, researchers could measure the daily food intake of individual adult female leaf insects from female only (parthenogenetic) and mixed (sexual) populations. Unfortunately, the available individual leaf insects varied substantially in body size which was expected to increase the variability of daily food intake of treatment groups. Consequently, the researchers also measured the body mass of the individuals as a covariate, thereby providing a means by which daily food consumption could be standardized for body mass. ANCOVA attempts to reduce unexplained variability by standardising the response to the treatment by the effects of the specific covariate condition. Thus ANCOVA provides a means of exercising some statistical control over the variability when it is either not possible or not desirable to exercise experimental control (such as blocking or using otherwise homogeneous observations).\n\rNull hypothesis\rFactor A: the main treatment effect\n\r\\(H_0(A):\\mu_1(adj)=\\mu_2(adj)=\\ldots=\\mu_i(adj)=\\mu(adj)\\)\r\rThe adjusted population group means are all equal. The mean of population \\(1\\) adjusted for the covariate is equal to that of population \\(2\\) adjusted for the covariate and so on, and thus all population means adjusted for the covariate are equal to an overall adjusted mean. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group adjusted mean and the overall adjusted mean (\\(\\alpha_i(adj)=\\mu_i(adj)−\\mu(adj)\\)) then the \\(H_0\\) can alternatively be written as:\n\r\\(H_0(A):\\alpha_1(adj)=\\alpha_2(adj)=\\ldots=\\alpha_i(adj)=0\\)\r\rThe effect of each group equals zero. If one or more of the \\(\\alpha_i(adj)\\) are different from zero (the response mean for this treatment differs from the overall response mean), the null hypothesis is not true, indicating that the treatment does affect the response variable.\nFactor B: the covariate effect\n\r\\(H_0(B):\\beta_1(pooled)=0\\)\r\rThe pooled population slope equals zero. Note, that this null hypothesis is rarely of much interest. It is precisely because of this nuisance relationship that ANCOVA designs are applied.\n\rLinear models\rOne or more covariates can be incorporated into single factor, nested, factorial and partly nested designs in order to reduce the unexplained variation. Fundamentally, the covariate(s) are purely used to adjust the response values prior to the regular analysis. The difficulty is in determining the appropriate adjustments. Following is a list of the appropriate linear models and adjusted response calculations for a range of ANCOVA designs. Note that these linear models do not include interactions involving the covariates as these are assumed to be zero. The inclusion of these interaction terms is a useful means of testing the homogeneity of slopes assumption.\n\rSingle categorical and single covariate\n\rLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\beta(x_{ij}-\\bar{x}) + \\epsilon_{ij}\\)\n\rAdjustments: \\(y_{ij(adj)}=y_{ij} - b(x_{ij} - \\bar{x})\\)\n\r\rSingle categorical and two covariates\n\rLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\beta_{YX}(x_{ij}-\\bar{x}) + \\beta_{YZ}(z_{ij}-\\bar{z}) + \\epsilon_{ij}\\)\n\rAdjustments: \\(y_{ij(adj)}=y_{ij} - b_{YX}(x_{ij} - \\bar{x}) - b_{YZ}(z_{ij} - \\bar{z})\\)\n\r\rFactorial designs\n\rLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\gamma_j + (\\alpha\\gamma)_{ij}+ \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijk}\\)\n\rAdjustments: \\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \\bar{x})\\)\n\r\rNested designs\n\rLinear model: \\(y_{ijk}=\\mu + \\alpha_i + \\gamma_{j(i)} + \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijk}\\)\n\rAdjustments: \\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \\bar{x})\\)\n\r\rPartly nested designs\n\rLinear model: \\(y_{ijkl}=\\mu + \\alpha_i + \\gamma_{j(i)} + \\delta_k + (\\alpha\\delta)_{ik} + (\\gamma\\delta)_{j(i)k} + \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijkl}\\)\n\rAdjustments: \\(y_{ijk(adj)}=y_{ijkl} - b_{between}(x_{i} - \\bar{x}) - b_{within}(x_{ijk} - \\bar{x}_i)\\)\n\r\r\r\rAnalysis of variance\rIn ANCOVA, the total variability of the response variable is sequentially partitioned into components explained by each of the model terms, starting with the covariate and is therefore equivalent to performing a regular analysis of variance on the response variables that have been adjusted for the covariate. The appropriate unexplained residuals and therefore the appropriate F-ratios for each factor differ according to the different null hypotheses associated with different linear models as well as combinations of fixed and random factors in the model (see the following tables). Note that since the covariate levels measured are typically different for each group, ANCOVA designs are inherently non-orthogonal (unbalanced). Consequently, sequential (Type I sums of squares) should not be used. For very simple Ancova designs that incorporate a single categorical and single covariate, Type I sums of squares can be used provided the covariate appears in the linear model first (and thus is partitioned out last) as we are typically not interested in estimating this effect.\n\u0026gt; ancova_table\rdf MS F-ratio (A\u0026amp;B fixed) F-ratio (B fixed) Factor A \u0026quot;a-1\u0026quot; \u0026quot;MS A\u0026quot; \u0026quot;(MS A)/(MS res)\u0026quot; \u0026quot;(MS A)/(MS res)\u0026quot; Factor B \u0026quot;1\u0026quot; \u0026quot;MS B\u0026quot; \u0026quot;(MS B)/(MS res)\u0026quot; \u0026quot;(MS B)/(MS res)\u0026quot; Factor AB \u0026quot;a-1\u0026quot; \u0026quot;MS AB\u0026quot; \u0026quot;(MS AB)/(MS res)\u0026quot; \u0026quot;(MS AB)/(MS res)\u0026quot;\rResidual \u0026quot;(n-2)a\u0026quot; \u0026quot;MS res\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \rThe corresponding R syntax is given below.\n\u0026gt; anova(lm(DV ~ B * A, dataset))\r\u0026gt; # OR\r\u0026gt; anova(aov(DV ~ B * A, dataset))\r\u0026gt; # OR (make sure not using treatment contrasts)\r\u0026gt; Anova(lm(DV ~ B * A, dataset), type = \u0026quot;III\u0026quot;)\r\rAssumptions\rAs ANCOVA designs are essentially regular ANOVA designs that are first adjusted (centered) for the covariate(s), ANCOVA designs inherit all of the underlying assumptions of the appropriate ANOVA design. Specifically, hypothesis tests assume that:\n\rThe appropriate residuals are normally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator, see the above tables) should be used to explore normality. Scale transformations are often useful.\n\rThe appropriate residuals are equally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\n\rThe appropriate residuals are independent of one another.\n\rThe relationship between the response variable and the covariate should be linear. Linearity can be explored using scatterplots and residual plots should reveal no patterns.\n\rFor repeated measures and other designs in which treatment levels within blocks can not be be randomly ordered, the variance/covariance matrix is assumed to display sphericity.\n\rFor designs that utilise blocking, it is assumed that there are no block by within block interactions.\n\r\rHomogeneity of Slopes\nIn addition to the above assumptions, ANCOVA designs also assume that slopes of relationships between the response variable and the covariate(s) are the same for each treatment level (group). That is, all the trends are parallel. If the individual slopes deviate substantially from each other (and thus the overall slope), then adjustments made to each of the observations are nonsensical. This situation is analogous to an interaction between two or more factors. In ANCOVA, interactions involving the covariate suggest that the nature of the relationship between the response and the covariate differs between the levels of the categorical treatment. More importantly, they also indicate that whether or not there is an effect of the treatment depends on what range of the covariate you are focussed on. Clearly then, it is not possible to make conclusions about the main effects of treatments in the presence of such interactions. The assumption of homogeneity of slopes can be examined via interaction plots or more formally, by testing hypotheses about the interactions between categorical variables and the covariate(s). There are three broad approaches for dealing with ANCOVA designs with heterogeneous slopes and selection depends on the primary focus of the study.\nWhen the primary objective of the analysis is to investigate the effects of categorical treatments, it is possible to adopt an approach similar to that taken when exploring interactions in multiple regression. The effect of treatments can be examined at specific values of the covariate (such as the mean and \\(\\pm\\) one standard deviation). This approach is really only useful at revealing broad shifts in patterns over the range of the covariate and if the selected values of the covariate do not have some inherent clinical meaning (selected arbitrarily), then the outcomes can be of only limited clinical interest.\n\rAlternatively, the Johnson-Neyman technique (or Wilxon modification thereof) procedure indicates the ranges of the covariate over which the individual regression lines of pairs of treatment groups overlap or cross. Although less powerful than the previous approach, the Wilcox(J-N) procedure has the advantage of revealing the important range (ranges for which the groups are different and not different) of the covariate rather than being constrained by specific levels selected.\n\rUse contrast treatments to split up the interaction term into its constituent contrasts for each level of the treatment. Essentially this compares each of the treatment level slopes to the slope from the “control” group and is useful if the primary focus is on the relationships between the response and the covariate.\n\r\rSimilar covariate ranges\nAdjustments made to the response means in an attempt to statistically account for differences in the covariate involve predicting mean response values along displaced linear relationships between the overall response and covariate variables. The degree of trend displacement for any given group is essentially calculated by multiplying the overall regression slope by the degree of difference between the overall covariate mean and the mean of the covariate for that group. However, when the ranges of the covariate within each of the groups differ substantially from one another, these adjustments are effectively extrapolations and therefore of unknown reliability. If a simple ANOVA of the covariate modelled against the categorical factor indicates that the covariate means differ significantly between groups, it may be necessary to either remove extreme observations or reconsider the analysis.\nRobust ANCOVA\nANCOVA based on rank transformed data can be useful for accommodating data with numerous problematic outliers. Nevertheless, problems about the difficulties of detecting interactions from rank transformed data, obviously have implications for inferential tests of homogeneity of slopes. Randomisation tests that maintain response0covariate pairs and repeatedly randomise these observations amongst the levels of the treatments can also be useful, particularly when there is doubt over the independence of observations. Both planned and unplanned comparisons follow those of other ANOVA chapters without any real additional complications. Notably, recent implementations of the Tukey’s test (within R) accommodate unbalanced designs and thus negate the need for some of the more complicated and specialised techniques that have been highlighted in past texts.\n\r\rData generation\rConsider an experimental design aimed at exploring the effects of a categorical variable with three levels (Group A, Group B and Group C) on a response. From previous studies, we know that the response is influenced by another variable (covariate). Unfortunately, it was not possible to ensure that all sampling units were the same degree of the covariate. Therefore, in an attempt to account for this anticipated extra source of variability, we measured the level of the covariate for each sampling unit. Actually, in allocating treatments to the various treatment groups, we tried to ensure a similar mean and range of the covariate within each group.\n\u0026gt; set.seed(123)\r\u0026gt; n \u0026lt;- 10\r\u0026gt; p \u0026lt;- 3\r\u0026gt; A.eff \u0026lt;- c(40, -15, -20)\r\u0026gt; beta \u0026lt;- -0.45\r\u0026gt; sigma \u0026lt;- 4\r\u0026gt; B \u0026lt;- rnorm(n * p, 0, 15)\r\u0026gt; A \u0026lt;- gl(p, n, lab = paste(\u0026quot;Group\u0026quot;, LETTERS[1:3]))\r\u0026gt; mm \u0026lt;- model.matrix(~A + B)\r\u0026gt; data \u0026lt;- data.frame(A = A, B = B, Y = as.numeric(c(A.eff, beta) %*% t(mm)) + rnorm(n * p, 0, 4))\r\u0026gt; data$B \u0026lt;- data$B + 20\r\u0026gt; head(data)\rA B Y\r1 Group A 11.59287 45.48907\r2 Group A 16.54734 40.37341\r3 Group A 43.38062 33.05922\r4 Group A 21.05763 43.03660\r5 Group A 21.93932 42.41363\r6 Group A 45.72597 31.17787\rExploratory data analysis\r\u0026gt; library(car)\r\u0026gt; scatterplot(Y ~ B | A, data = data)\r\u0026gt; \u0026gt; boxplot(Y ~ A, data)\r\u0026gt; \u0026gt; # OR via ggplot\r\u0026gt; library(ggplot2)\r\u0026gt; ggplot(data, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;)\r\u0026gt; \u0026gt; ggplot(data, aes(y = Y, x = A)) + geom_boxplot()\rConclusions\nThere is no evidence of obvious non-normality. The assumption of linearity seems reasonable. The variability of the three groups seems approximately equal. The slopes (\\(Y\\) vs B trends) appear broadly similar for each treatment group.\nWe can explore inferential evidence of unequal slopes by examining estimated effects of the interaction between the categorical variable and the covariate. Note, pay no attention to the main effects - only the interaction. Even though I intend to illustrate Bayesian analyses here, for such a simple model, it is considerably simpler to use traditional OLS for testing for the presence of an interaction.\n\u0026gt; anova(lm(Y ~ B * A, data = data))\rAnalysis of Variance Table\rResponse: Y\rDf Sum Sq Mean Sq F value Pr(\u0026gt;F) B 1 989.99 989.99 92.6782 1.027e-09 ***\rA 2 2320.05 1160.02 108.5956 9.423e-13 ***\rB:A 2 51.36 25.68 2.4041 0.1118 Residuals 24 256.37 10.68 ---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThere is very little evidence to suggest that the assumption of equal slopes will be inappropriate.\n\r\rModel fitting\rThe observed response (\\(y_i\\)) are assumed to be drawn from a normal distribution with a given mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The expected values are themselves determined by the linear predictor (\\(\\boldsymbol X \\boldsymbol \\beta\\)). In this case, \\(\\boldsymbol \\beta\\) represents the vector of \\(\\beta\\)’s - the intercept associated with the first group, the (effects) differences between this intercept and the intercepts for each other group as well as the slope associated with the continuous covariate. \\(\\boldsymbol X\\) is the model matrix. MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying ‘uninformative’ priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (\\(100\\)) for both the intercept and the treatment effect and a wide half-cauchy (\\(\\text{scale}=5\\)) for the standard deviation.\n\\[y_i \\sim N(\\mu_i,\\sigma), \\]\nwhere \\(\\mu_i=\\beta_0 +\\boldsymbol \\beta \\boldsymbol X\\). The assumed priors are: \\(\\beta \\sim N(0,100)\\) and \\(\\sigma \\sim \\text{Cauchy}(0,5)\\). Note, exploratory data analysis suggests that while the intercept (intercept of Group A) and categorical predictor effects (differences between intercepts of each of the Group and Group A’s intercept) could be drawn from a similar distribution (with mean in the \\(10\\)’s and variances in the \\(100\\)’s), the slope (effect associated with Group A linear relationship) is likely to be an order of magnitude less. We might therefore be tempted to provide different priors for the intercept, categorical effects and slope effect. For a simple model such as this, it is unlikely to be necessary. However, for more complex models, where prior specification becomes more critical, separate priors would probably be necessary. We proceed to code the model into STAN.\n\u0026gt; modelString = \u0026quot;\r+ data {\r+ int\u0026lt;lower=1\u0026gt; n;\r+ int\u0026lt;lower=1\u0026gt; nX;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ }\r+ parameters {\r+ vector[nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ }\r+ transformed parameters {\r+ vector[n] mu;\r+ + mu = X*beta;\r+ }\r+ model {\r+ //Likelihood\r+ y~normal(mu,sigma);\r+ + //Priors\r+ beta ~ normal(0,100);\r+ sigma~cauchy(0,5);\r+ }\r+ generated quantities {\r+ vector[n] log_lik;\r+ + for (i in 1:n) {\r+ log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); + }\r+ }\r+ + \u0026quot;\r\u0026gt; ## write the model to a stan file \u0026gt; writeLines(modelString, con = \u0026quot;ancovaModel.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; Xmat \u0026lt;- model.matrix(~A + B, data)\r\u0026gt; data.list \u0026lt;- with(data, list(y = Y, X = Xmat, nX = ncol(Xmat), n = nrow(data)))\rDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;log_lik\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1500\rNow compile and run the Stan code via the rstan interface. Note that the first time stan is run after the rstan package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; library(rstan)\rDuring the warmup stage, the No-U-Turn sampler (NUTS) attempts to determine the optimum stepsize - the stepsize that achieves the target acceptance rate (\\(0.8\\) or \\(80\\)% by default) without divergence (occurs when the stepsize is too large relative to the curvature of the log posterior and results in approximations that are likely to diverge and be biased) - and without hitting the maximum treedepth (\\(10\\)). At each iteration of the NUTS algorithm, the number of leapfrog steps doubles (as it increases the treedepth) and only terminates when either the NUTS criterion are satisfied or the tree depth reaches the maximum (\\(10\\) by default).\n\u0026gt; data.rstan \u0026lt;- stan(data = data.list, file = \u0026quot;ancovaModel.stan\u0026quot;, chains = nChains, pars = params,\r+ iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;ancovaModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 1: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 1: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 1: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 1: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 1: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 1: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 1: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 1: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 1: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 1: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 1: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.072 seconds (Warm-up)\rChain 1: 0.059 seconds (Sampling)\rChain 1: 0.131 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;ancovaModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 2: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 2: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 2: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 2: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 2: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 2: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 2: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 2: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 2: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 2: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 2: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.088 seconds (Warm-up)\rChain 2: 0.058 seconds (Sampling)\rChain 2: 0.146 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.rstan, par = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: ancovaModel.\r2 chains, each with iter=1500; warmup=500; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 50.95 0.05 1.46 48.08 50.00 50.93 51.85 53.86 949 1.01\rbeta[2] -16.23 0.04 1.55 -19.25 -17.24 -16.18 -15.24 -13.12 1322 1.00\rbeta[3] -20.60 0.04 1.52 -23.56 -21.58 -20.61 -19.61 -17.55 1430 1.00\rbeta[4] -0.48 0.00 0.05 -0.57 -0.51 -0.48 -0.45 -0.40 1068 1.01\rsigma 3.55 0.02 0.52 2.69 3.17 3.48 3.87 4.67 1175 1.00\rSamples were drawn using NUTS(diag_e) at Tue Feb 18 14:30:06 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots.\n\u0026gt; library(mcmcplots)\r\u0026gt; s = as.array(data.rstan)\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, dimnames(s)$parameters)\r\u0026gt; s = s[, , wch]\r\u0026gt; mcmc \u0026lt;- do.call(mcmc.list, plyr:::alply(s[, , -(length(s[1, 1, ]))], 2, as.mcmc))\r\u0026gt; denplot(mcmc, parms = c(\u0026quot;beta\u0026quot;))\r\u0026gt; traplot(mcmc, parms = c(\u0026quot;beta\u0026quot;))\rThese plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.\n\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(mcmc)\r$`1`\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r$`2`\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; stan_ac(data.rstan, pars = c(\u0026quot;beta\u0026quot;))\rA lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).\n\u0026gt; stan_rhat(data.rstan, pars = c(\u0026quot;beta\u0026quot;))\r\u0026gt; stan_ess(data.rstan, pars = c(\u0026quot;beta\u0026quot;))\rRhat and effective sample size. In this instance, most of the parameters have reasonably high effective samples and thus there is likely to be a good range of values from which to estimate paramter properties.\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. Ideally, a good model should also be able to predict the data used to fit the model. Residuals are not computed directly within rstan However, we can calculate them manually form the posteriors.\n\u0026gt; library(dplyr)\r\u0026gt; mcmc = as.data.frame(data.rstan) %\u0026gt;% dplyr:::select(contains(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~A + B, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:4], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$Y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rResiduals against predictors\n\u0026gt; library(tidyr)\r\u0026gt; mcmc = as.data.frame(data.rstan) %\u0026gt;% dplyr:::select(contains(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = newdata\r\u0026gt; Xmat = model.matrix(~A + B, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:4], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$Y - fit\r\u0026gt; newdata = newdata %\u0026gt;% cbind(fit, resid)\r\u0026gt; ggplot(newdata) + geom_point(aes(y = resid, x = A)) + theme_classic()\r\u0026gt; \u0026gt; ggplot(newdata) + geom_point(aes(y = resid, x = B)) + theme_classic()\rAnd now for studentised residuals\n\u0026gt; mcmc = as.data.frame(data.rstan) %\u0026gt;% dplyr:::select(contains(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~A + B, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:4], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$Y - fit\r\u0026gt; sresid = resid/sd(resid)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit)) + theme_classic()\rFor this simple model, the studentized residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = as.data.frame(data.rstan) %\u0026gt;% dplyr:::select(contains(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~A + B, data)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, 1:4]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i,\r+ ], mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; newdata = data.frame(A = data$A, B = data$B, yRep) %\u0026gt;% gather(key = Sample,\r+ value = Value, -A, -B)\r\u0026gt; ggplot(newdata) + geom_violin(aes(y = Value, x = A, fill = \u0026quot;Model\u0026quot;),\r+ alpha = 0.5) + geom_violin(data = data, aes(y = Y, x = A,\r+ fill = \u0026quot;Obs\u0026quot;), alpha = 0.5) + geom_point(data = data, aes(y = Y,\r+ x = A), position = position_jitter(width = 0.1, height = 0),\r+ color = \u0026quot;black\u0026quot;) + theme_classic()\r\u0026gt; \u0026gt; ggplot(newdata) + geom_violin(aes(y = Value, x = B, fill = \u0026quot;Model\u0026quot;,\r+ group = B, color = A), alpha = 0.5) + geom_point(data = data,\r+ aes(y = Y, x = B, group = B, color = A)) + theme_classic()\rThe predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.\n\u0026gt; library(bayesplot)\r\u0026gt; mcmc_intervals(as.matrix(data.rstan), regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\u0026gt; mcmc_areas(as.matrix(data.rstan), regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\rParameter estimates\rAlthough all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and \\(95\\)% credibility intervals.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\rFirst, we look at the results from the additive model.\n\u0026gt; print(data.rstan, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: ancovaModel.\r2 chains, each with iter=1500; warmup=500; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 50.95 0.05 1.46 48.08 50.00 50.93 51.85 53.86 949 1.01\rbeta[2] -16.23 0.04 1.55 -19.25 -17.24 -16.18 -15.24 -13.12 1322 1.00\rbeta[3] -20.60 0.04 1.52 -23.56 -21.58 -20.61 -19.61 -17.55 1430 1.00\rbeta[4] -0.48 0.00 0.05 -0.57 -0.51 -0.48 -0.45 -0.40 1068 1.01\rsigma 3.55 0.02 0.52 2.69 3.17 3.48 3.87 4.67 1175 1.00\rSamples were drawn using NUTS(diag_e) at Tue Feb 18 14:30:06 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(data.rstan, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 51.0 1.46 48.0 53.8 2 beta[2] -16.2 1.55 -19.3 -13.3 3 beta[3] -20.6 1.52 -23.4 -17.5 4 beta[4] -0.483 0.0460 -0.569 -0.392\r5 sigma 3.55 0.524 2.69 4.67 \rConclusions\n\rThe intercept of the first group (Group A) is \\(51\\).\n\rThe mean of the second group (Group B) is \\(-16.3\\) units greater than (A).\n\rThe mean of the third group (Group C) is \\(-20.7\\) units greater than (A).\n\rA one unit increase in B in Group A is associated with a \\(-0.484\\) units increase in \\(Y\\).\n\r\rThe \\(95\\)% confidence interval for the effects of Group B, Group C and the partial slope associated with B do not overlapp with 0 implying a significant difference between group A and groups B, C and a significant negative relationship with B. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, \u0026quot;beta[2]\u0026quot;]) # effect of (B-A = 0)\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, \u0026quot;beta[3]\u0026quot;]) # effect of (C-A = 0)\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, \u0026quot;beta[4]\u0026quot;]) # effect of (slope = 0)\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, 2:4]) # effect of (model)\r[1] 0\rThere is evidence that the reponse differs between the groups. There is evidence suggesting that the response of group D differs from that of group A. In a Bayesian context, we can compare models using the leave-one-out cross-validation statistics. Leave-one-out (LOO) cross-validation explores how well a series of models can predict withheld values Vehtari, Gelman, and Gabry (2017). The LOO Information Criterion (LOOIC) is analogous to the AIC except that the LOOIC takes priors into consideration, does not assume that the posterior distribution is drawn from a multivariate normal and integrates over parameter uncertainty so as to yield a distribution of looic rather than just a point estimate. The LOOIC does however assume that all observations are equally influential (it does not matter which observations are left out). This assumption can be examined via the Pareto \\(k\\) estimate (values greater than \\(0.5\\) or more conservatively \\(0.75\\) are considered overly influential). We can compute LOOIC if we store the loglikelihood from our STAN model, which can then be extracted to compute the information criterion using the package loo.\n\u0026gt; library(loo)\r\u0026gt; (full = loo(extract_log_lik(data.rstan)))\rComputed from 2000 by 30 log-likelihood matrix\rEstimate SE\relpd_loo -83.1 4.4\rp_loo 4.9 1.4\rlooic 166.2 8.9\r------\rMonte Carlo SE of elpd_loo is 0.1.\rPareto k diagnostic values:\rCount Pct. Min. n_eff\r(-Inf, 0.5] (good) 28 93.3% 269 (0.5, 0.7] (ok) 2 6.7% 371 (0.7, 1] (bad) 0 0.0% \u0026lt;NA\u0026gt; (1, Inf) (very bad) 0 0.0% \u0026lt;NA\u0026gt; All Pareto k estimates are ok (k \u0026lt; 0.7).\rSee help(\u0026#39;pareto-k-diagnostic\u0026#39;) for details.\r\u0026gt; \u0026gt; # now fit a model without main factor\r\u0026gt; modelString2 = \u0026quot;\r+ data {\r+ int\u0026lt;lower=1\u0026gt; n;\r+ int\u0026lt;lower=1\u0026gt; nX;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ }\r+ parameters {\r+ vector[nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ }\r+ transformed parameters {\r+ vector[n] mu;\r+ + mu = X*beta;\r+ }\r+ model {\r+ // Likelihood\r+ y~normal(mu,sigma);\r+ + // Priors\r+ beta ~ normal(0,1000);\r+ sigma~cauchy(0,5);\r+ }\r+ generated quantities {\r+ vector[n] log_lik;\r+ + for (i in 1:n) {\r+ log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); + }\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a stan file \u0026gt; writeLines(modelString2, con = \u0026quot;ancovaModel2.stan\u0026quot;)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~1, data)\r\u0026gt; data.list \u0026lt;- with(data, list(y = Y, X = Xmat, n = nrow(data), nX = ncol(Xmat)))\r\u0026gt; data.rstan.red \u0026lt;- stan(data = data.list, file = \u0026quot;ancovaModel2.stan\u0026quot;, chains = nChains,\r+ iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;ancovaModel2\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 1: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 1: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 1: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 1: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 1: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 1: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 1: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 1: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 1: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 1: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 1: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.018 seconds (Warm-up)\rChain 1: 0.025 seconds (Sampling)\rChain 1: 0.043 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;ancovaModel2\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 2: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 2: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 2: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 2: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 2: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 2: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 2: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 2: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 2: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 2: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 2: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.018 seconds (Warm-up)\rChain 2: 0.02 seconds (Sampling)\rChain 2: 0.038 seconds (Total)\rChain 2: \u0026gt; \u0026gt; (reduced = loo(extract_log_lik(data.rstan.red)))\rComputed from 2000 by 30 log-likelihood matrix\rEstimate SE\relpd_loo -116.3 3.1\rp_loo 1.6 0.3\rlooic 232.6 6.2\r------\rMonte Carlo SE of elpd_loo is 0.0.\rAll Pareto k estimates are good (k \u0026lt; 0.5).\rSee help(\u0026#39;pareto-k-diagnostic\u0026#39;) for details.\r\u0026gt; \u0026gt; par(mfrow = 1:2, mar = c(5, 3.8, 1, 0) + 0.1, las = 3)\r\u0026gt; plot(full, label_points = TRUE)\r\u0026gt; plot(reduced, label_points = TRUE)\rThe expected out-of-sample predictive accuracy is substantially lower for the model that includes \\(x\\). This might be used to suggest that the inferential evidence for a general effect of \\(x\\) on \\(y\\).\n\rGraphical summaries\rA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group.\n\u0026gt; mcmc = as.matrix(data.rstan)\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = expand.grid(A = levels(data$A), B = seq(min(data$B), max(data$B),\r+ len = 100))\r\u0026gt; Xmat = model.matrix(~A + B, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;, \u0026quot;beta[3]\u0026quot;, \u0026quot;beta[4]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = B, fill = A)) + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;B\u0026quot;) + theme_classic()\rAs this is simple single factor ANOVA, we can simple add the raw data to this figure. For more complex designs with additional predictors, it is necessary to plot partial residuals.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = data\r\u0026gt; fMat = rMat = model.matrix(~A + B, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$Y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = B, fill = A)) + geom_point(data = rdata,\r+ aes(y = partial.resid, x = B, color = A)) + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;B\u0026quot;) + theme_classic()\r\rPosteriors\rIn frequentist statistics, when we have more than two groups, we are typically not only interested in whether there is evidence for an overall “effect” of a factor - we are also interested in how various groups compare to one another. To explore these trends, we either compare each group to each other in a pairwise manner (controlling for family-wise Type I error rates) or we explore an independent subset of the possible comparisons. Although these alternate approaches can adequately address a specific research agenda, often they impose severe limitations and compromises on the scope and breadth of questions that can be asked of your data. The reason for these limitations is that in a frequentist framework, any single hypothesis carries with it a (nominally) \\(5\\)% chance of a false rejection (since it is based on long-run frequency). Thus, performing multiple tests are likely to compound this error rate. The point is, that each comparison is compared to its own probability distribution (and each carries a \\(5\\)% error rate). By contrast, in Bayesian statistics, all comparisons (contrasts) are drawn from the one (hopefully stable and convergent) posterior distribution and this posterior is invariant to the type and number of comparisons drawn. Hence, the theory clearly indicates that having generated our posterior distribution, we can then query this distribution in any way that we wish thereby allowing us to explore all of our research questions simultaneously.\nBayesian “contrasts” can be performed either:\n\rwithin the Bayesian sampling model or\n\rconstruct them from the returned MCMC samples (they are drawn from the posteriors)\n\r\rOnly the latter will be demonstrated as it provides a consistent approach across all routines. In order to allow direct comparison to the frequentist equivalents, I will explore the same set of planned and Tukey’s test comparisons described here. For the “planned comparison” we defined two contrasts: 1) group B vs group C; and 2) group A vs the average of groups B and C. Of course each of these could be explored at multiple values of B, however, since we fit an additive model (which assumes that the slopes are homogeneous), the contrasts will be constant throughout the domain of B.\nLets start by comparing each group to each other group in a pairwise manner. Arguably the most elegant way to do this is to generate a Tukey’s contrast matrix. This is a model matrix specific to comparing each group to each other group. Again, since the lines are parallel, it does not really matter what level of B we estimate these efffects at - so lets use the mean B.\n\u0026gt; mcmc = data.rstan\r\u0026gt; coefs \u0026lt;- as.matrix(mcmc)[, 1:4]\r\u0026gt; newdata \u0026lt;- data.frame(A = levels(data$A), B = mean(data$B))\r\u0026gt; # A Tukeys contrast matrix\r\u0026gt; library(multcomp)\r\u0026gt; tuk.mat \u0026lt;- contrMat(n = table(newdata$A), type = \u0026quot;Tukey\u0026quot;)\r\u0026gt; Xmat \u0026lt;- model.matrix(~A + B, data = newdata)\r\u0026gt; pairwise.mat \u0026lt;- tuk.mat %*% Xmat\r\u0026gt; pairwise.mat\r(Intercept) AGroup B AGroup C B\rGroup B - Group A 0 1 0 0\rGroup C - Group A 0 0 1 0\rGroup C - Group B 0 -1 1 0\r\u0026gt; \u0026gt; mcmc_areas(coefs %*% t(pairwise.mat))\r\u0026gt; \u0026gt; (comps = tidyMCMC(coefs %*% t(pairwise.mat), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 3 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 Group B - Group A -16.2 1.55 -19.3 -13.3 2 Group C - Group A -20.6 1.52 -23.4 -17.5 3 Group C - Group B -4.37 1.59 -7.30 -0.949\r\u0026gt; \u0026gt; ggplot(comps, aes(y = estimate, x = term)) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;) +\r+ scale_y_continuous(\u0026quot;Effect size\u0026quot;) + scale_x_discrete(\u0026quot;\u0026quot;) + coord_flip() +\r+ theme_classic()\rWith a couple of modifications, we could also express this as percentage changes. A percentage change represents the change (difference between groups) divided by one of the groups (determined by which group you want to express the percentage change to). Hence, we generate an additional mcmc matrix that represents the cell means for the divisor group (group we want to express change relative to). Since the tuk.mat defines comparisons as \\(-1\\) and \\(1\\) pairs, if we simply replace all the \\(-1\\) with \\(0\\), the eventual matrix multiplication will result in estimates of the divisor cell means instread of the difference. We can then divide the original mcmc matrix above with this new divisor mcmc matrix to yeild a mcmc matrix of percentage change.\n\u0026gt; # Modify the tuk.mat to replace -1 with 0. This will allow us to get a\r\u0026gt; # mcmc matrix of ..\r\u0026gt; tuk.mat[tuk.mat == -1] = 0\r\u0026gt; comp.mat \u0026lt;- tuk.mat %*% Xmat\r\u0026gt; comp.mat\r(Intercept) AGroup B AGroup C B\rGroup B - Group A 1 1 0 19.29344\rGroup C - Group A 1 0 1 19.29344\rGroup C - Group B 1 0 1 19.29344\r\u0026gt; \u0026gt; comp.mcmc = 100 * (coefs %*% t(pairwise.mat))/coefs %*% t(comp.mat)\r\u0026gt; (comps = tidyMCMC(comp.mcmc, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 3 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 Group B - Group A -64.2 8.46 -80.3 -47.3 2 Group C - Group A -98.5 11.7 -120. -74.6 3 Group C - Group B -21.1 8.48 -36.2 -3.13\r\u0026gt; \u0026gt; ggplot(comps, aes(y = estimate, x = term)) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;) +\r+ scale_y_continuous(\u0026quot;Effect size (%)\u0026quot;) + scale_x_discrete(\u0026quot;\u0026quot;) + coord_flip() +\r+ theme_classic()\rAnd now for the specific planned comparisons (Group B vs Group C as well as Group A vs the average of Groups B and C). This is achieved by generating our own contrast matrix (defining the contributions of each group to each contrast).\n\u0026gt; c.mat = rbind(c(0, 1, -1), c(1/2, -1/3, -1/3))\r\u0026gt; c.mat\r[,1] [,2] [,3]\r[1,] 0.0 1.0000000 -1.0000000\r[2,] 0.5 -0.3333333 -0.3333333\r\u0026gt; \u0026gt; mcmc = data.rstan\r\u0026gt; coefs \u0026lt;- as.matrix(mcmc)[, 1:4]\r\u0026gt; newdata \u0026lt;- data.frame(A = levels(data$A), B = mean(data$B))\r\u0026gt; Xmat \u0026lt;- model.matrix(~A + B, data = newdata)\r\u0026gt; c.mat = c.mat %*% Xmat\r\u0026gt; c.mat\r(Intercept) AGroup B AGroup C B\r[1,] 0.0000000 1.0000000 -1.0000000 0.000000\r[2,] -0.1666667 -0.3333333 -0.3333333 -3.215574\r\u0026gt; \u0026gt; (comps = tidyMCMC(as.mcmc(coefs %*% t(c.mat)), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 4.37 1.59 0.949 7.30\r2 var2 5.34 0.739 3.93 6.83\r\rFinite population standard deviations\rVariance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).\nFinite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (Gelman and others (2005)). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.\n parameters\riterations beta[1] beta[2] beta[3] beta[4] sigma log_lik[1]\r[1,] 50.10205 -15.67611 -17.41476 -0.4593792 2.882566 -2.008171\r[2,] 50.53300 -15.15556 -21.29190 -0.4213328 3.333128 -2.123994\r[3,] 50.82345 -16.88383 -18.55789 -0.4953391 3.187249 -2.086291\r[4,] 51.07765 -16.56728 -18.11673 -0.4914285 3.229001 -2.091676\r[5,] 50.86176 -17.23601 -22.42360 -0.4583002 3.709629 -2.230000\r[6,] 50.54948 -17.18733 -22.94231 -0.4507465 3.679292 -2.222665\rparameters\riterations log_lik[2] log_lik[3] log_lik[4] log_lik[5] log_lik[6] log_lik[7]\r[1,] -2.249891 -2.478578 -2.386901 -2.321355 -2.238304 -2.089910\r[2,] -2.580156 -2.151935 -2.208046 -2.179748 -2.123208 -2.123205\r[3,] -2.328045 -2.760643 -2.422131 -2.375369 -2.522340 -2.206070\r[4,] -2.408442 -2.613356 -2.346400 -2.306150 -2.408162 -2.166403\r[5,] -2.536428 -2.386884 -2.350958 -2.323661 -2.288689 -2.241980\r[6,] -2.494402 -2.378916 -2.366281 -2.335190 -2.278379 -2.239055\rparameters\riterations log_lik[8] log_lik[9] log_lik[10] log_lik[11] log_lik[12]\r[1,] -2.085684 -2.278190 -2.353424 -2.464663 -2.040152\r[2,] -2.270293 -2.537363 -2.654372 -3.366142 -2.511113\r[3,] -2.279861 -2.412778 -2.448060 -2.125289 -2.085211\r[4,] -2.341028 -2.494345 -2.536601 -2.229693 -2.095141\r[5,] -2.390241 -2.557991 -2.619126 -2.387999 -2.232080\r[6,] -2.340846 -2.504261 -2.567828 -2.386463 -2.222788\rparameters\riterations log_lik[13] log_lik[14] log_lik[15] log_lik[16] log_lik[17]\r[1,] -3.632110 -6.277306 -3.204443 -3.185018 -2.168245\r[2,] -4.446037 -4.129568 -2.560973 -4.463466 -2.764685\r[3,] -2.797030 -6.726618 -3.524519 -2.340742 -2.082699\r[4,] -3.059757 -6.029865 -3.198127 -2.541258 -2.137540\r[5,] -2.956316 -5.325142 -3.251598 -2.735297 -2.266883\r[6,] -2.938129 -5.436729 -3.330196 -2.760533 -2.255217\rparameters\riterations log_lik[18] log_lik[19] log_lik[20] log_lik[21] log_lik[22]\r[1,] -2.318947 -2.512054 -2.002140 -2.395950 -2.776903\r[2,] -2.520307 -2.156718 -2.317741 -2.142335 -2.154221\r[3,] -2.324384 -3.103347 -2.082892 -2.289348 -2.416272\r[4,] -2.459206 -2.810777 -2.095693 -2.462432 -2.639612\r[5,] -2.319568 -2.740113 -2.230665 -2.323836 -2.242369\r[6,] -2.278072 -2.749544 -2.225301 -2.436018 -2.283231\rparameters\riterations log_lik[23] log_lik[24] log_lik[25] log_lik[26] log_lik[27]\r[1,] -2.853655 -2.188530 -3.191131 -2.303986 -7.496570\r[2,] -2.136205 -3.236045 -2.217312 -3.730505 -4.567220\r[3,] -2.590975 -2.415939 -2.747167 -2.400663 -5.215582\r[4,] -2.836079 -2.262250 -3.029609 -2.261575 -5.810429\r[5,] -2.236545 -3.586947 -2.232197 -3.804767 -3.274838\r[6,] -2.277066 -3.966924 -2.230807 -4.274277 -3.064820\rparameters\riterations log_lik[28] log_lik[29] log_lik[30] lp__\r[1,] -2.055777 -2.580174 -2.341634 -54.29289\r[2,] -2.218528 -2.124156 -2.133648 -52.60671\r[3,] -2.078444 -2.420461 -2.098403 -51.48189\r[4,] -2.114514 -2.627423 -2.197494 -52.37989\r[5,] -2.575753 -2.272372 -2.340851 -50.99091\r[6,] -2.740488 -2.353656 -2.414602 -52.08341\r# A tibble: 3 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.A 3.10 1.12 0.681 5.16\r2 sd.B 7.11 0.677 5.77 8.38\r3 sd.resid 3.44 0.158 3.26 3.73\r# A tibble: 3 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.A 22.9 6.30 9.80 34.7\r2 sd.B 52.4 4.30 44.0 61.0\r3 sd.resid 24.8 3.04 20.9 31.2\rApproximately \\(22.86\\)% of the total finite population standard deviation is due to \\(x\\).\n\rR squared\rIn a frequentist context, the \\(R^2\\) value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, \\(R^2\\) is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an \\(R^2\\) greater than \\(100\\)%. Gelman et al. (2019) proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.\nSo in the standard regression model notation of:\n\\[ y_i \\sim \\text{Normal}(\\boldsymbol X \\boldsymbol \\beta, \\sigma),\\]\nthe \\(R^2\\) could be formulated as\n\\[ R^2 = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sigma^2_e},\\]\nwhere \\(\\sigma^2_f=\\text{var}(\\boldsymbol X \\boldsymbol \\beta)\\), and for normal models \\(\\sigma^2_e=\\text{var}(y-\\boldsymbol X \\boldsymbol \\beta)\\)\n\u0026gt; mcmc \u0026lt;- as.matrix(data.rstan)\r\u0026gt; Xmat = model.matrix(~A + B, data)\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = sweep(fit, 2, data$Y, \u0026quot;-\u0026quot;)\r\u0026gt; var_f = apply(fit, 1, var)\r\u0026gt; var_e = apply(resid, 1, var)\r\u0026gt; R2 = var_f/(var_f + var_e)\r\u0026gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.906 0.0136 0.879 0.922\r\u0026gt; \u0026gt; # for comparison with frequentist\r\u0026gt; summary(lm(Y ~ A + B, data))\rCall:\rlm(formula = Y ~ A + B, data = data)\rResiduals:\rMin 1Q Median 3Q Max -6.4381 -2.2244 -0.6829 2.1732 8.6607 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 51.00608 1.44814 35.22 \u0026lt; 2e-16 ***\rAGroup B -16.25472 1.54125 -10.55 6.92e-11 ***\rAGroup C -20.65596 1.57544 -13.11 5.74e-13 ***\rB -0.48399 0.04526 -10.69 5.14e-11 ***\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 3.44 on 26 degrees of freedom\rMultiple R-squared: 0.9149, Adjusted R-squared: 0.9051 F-statistic: 93.22 on 3 and 26 DF, p-value: 4.901e-14\r\rDealing with heterogeneous slopes\rGenerate the data with heterogeneous slope effects.\n\u0026gt; set.seed(123)\r\u0026gt; n \u0026lt;- 10\r\u0026gt; p \u0026lt;- 3\r\u0026gt; A.eff \u0026lt;- c(40, -15, -20)\r\u0026gt; beta \u0026lt;- c(-0.45, -0.1, 0.5)\r\u0026gt; sigma \u0026lt;- 4\r\u0026gt; B \u0026lt;- rnorm(n * p, 0, 15)\r\u0026gt; A \u0026lt;- gl(p, n, lab = paste(\u0026quot;Group\u0026quot;, LETTERS[1:3]))\r\u0026gt; mm \u0026lt;- model.matrix(~A * B)\r\u0026gt; data1 \u0026lt;- data.frame(A = A, B = B, Y = as.numeric(c(A.eff, beta) %*% t(mm)) + rnorm(n * p, 0, 4))\r\u0026gt; data1$B \u0026lt;- data1$B + 20\r\u0026gt; head(data1)\rA B Y\r1 Group A 11.59287 45.48907\r2 Group A 16.54734 40.37341\r3 Group A 43.38062 33.05922\r4 Group A 21.05763 43.03660\r5 Group A 21.93932 42.41363\r6 Group A 45.72597 31.17787\rExploratory data analysis\r\u0026gt; scatterplot(Y ~ B | A, data = data1)\r\u0026gt; \u0026gt; boxplot(Y ~ A, data1)\r\u0026gt; \u0026gt; # OR via ggplot\r\u0026gt; ggplot(data1, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;)\r\u0026gt; \u0026gt; ggplot(data1, aes(y = Y, x = A)) + geom_boxplot()\rThe slopes (\\(Y\\) vs B trends) do appear to differ between treatment groups - in particular, Group C seems to portray a different trend to Groups A and B.\n\u0026gt; anova(lm(Y ~ B * A, data = data1))\rAnalysis of Variance Table\rResponse: Y\rDf Sum Sq Mean Sq F value Pr(\u0026gt;F) B 1 442.02 442.02 41.380 1.187e-06 ***\rA 2 2760.60 1380.30 129.217 1.418e-13 ***\rB:A 2 285.75 142.87 13.375 0.0001251 ***\rResiduals 24 256.37 10.68 ---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThere is strong evidence to suggest that the assumption of equal slopes is violated.\n\rFitting the model\r\u0026gt; modelString2 = \u0026quot;\r+ data {\r+ int\u0026lt;lower=1\u0026gt; n;\r+ int\u0026lt;lower=1\u0026gt; nX;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ }\r+ parameters {\r+ vector[nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ }\r+ transformed parameters {\r+ vector[n] mu;\r+ + mu = X*beta;\r+ }\r+ model {\r+ // Likelihood\r+ y~normal(mu,sigma);\r+ + // Priors\r+ beta ~ normal(0,100);\r+ sigma~cauchy(0,5);\r+ }\r+ generated quantities {\r+ vector[n] log_lik;\r+ + for (i in 1:n) {\r+ log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); + }\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString2, con = \u0026quot;ancovaModel2.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; Xmat \u0026lt;- model.matrix(~A * B, data1)\r\u0026gt; data1.list \u0026lt;- with(data1, list(y = Y, X = Xmat, nX = ncol(Xmat), n = nrow(data1)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;log_lik\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1500\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model).\n\u0026gt; data1.rstan \u0026lt;- stan(data = data1.list, file = \u0026quot;ancovaModel2.stan\u0026quot;, chains = nChains, pars = params,\r+ iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;ancovaModel2\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 1: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 1: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 1: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 1: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 1: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 1: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 1: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 1: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 1: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 1: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 1: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.106 seconds (Warm-up)\rChain 1: 0.089 seconds (Sampling)\rChain 1: 0.195 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;ancovaModel2\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 2: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 2: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 2: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 2: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 2: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 2: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 2: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 2: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 2: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 2: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 2: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.128 seconds (Warm-up)\rChain 2: 0.1 seconds (Sampling)\rChain 2: 0.228 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data1.rstan, par = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: ancovaModel2.\r2 chains, each with iter=1500; warmup=500; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 48.14 0.09 2.00 44.21 46.88 48.13 49.45 52.06 541 1\rbeta[2] -10.58 0.11 2.83 -16.17 -12.34 -10.58 -8.77 -4.81 621 1\rbeta[3] -26.48 0.11 2.58 -31.66 -28.17 -26.48 -24.71 -21.48 551 1\rbeta[4] -0.35 0.00 0.08 -0.50 -0.40 -0.35 -0.30 -0.20 555 1\rbeta[5] -0.27 0.00 0.11 -0.48 -0.34 -0.27 -0.20 -0.05 597 1\rbeta[6] 0.27 0.00 0.12 0.04 0.20 0.27 0.35 0.50 552 1\rsigma 3.39 0.02 0.50 2.57 3.02 3.33 3.67 4.50 1108 1\rSamples were drawn using NUTS(diag_e) at Tue Feb 18 14:31:40 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\rMCMC diagnostics\r\u0026gt; mcmc \u0026lt;- As.mcmc.list(data1.rstan)\r\u0026gt; \u0026gt; denplot(mcmc, parms = c(\u0026quot;beta\u0026quot;))\r\u0026gt; traplot(mcmc, parms = c(\u0026quot;beta\u0026quot;))\rTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters (such as \\(\\beta\\)s).\n\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r[[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; autocorr.diag(mcmc)\rbeta[1] beta[2] beta[3] beta[4] beta[5]\rLag 0 1.000000000 1.000000000 1.00000000 1.000000000 1.00000000\rLag 1 0.487975871 0.425019403 0.44941737 0.460836797 0.46355010\rLag 5 0.088899283 0.062327893 0.08476067 0.087316054 0.05016421\rLag 10 0.007454517 -0.003124592 -0.00632013 -0.031363005 -0.02547379\rLag 50 -0.014702929 -0.011159634 0.01546605 0.007422907 -0.02987378\rbeta[6] sigma log_lik[1] log_lik[2] log_lik[3]\rLag 0 1.000000000 1.000000000 1.0000000000 1.000000000 1.000000000\rLag 1 0.407845502 0.229381985 0.3229947423 0.148911691 0.246151735\rLag 5 0.108990741 0.013015911 0.0359928135 0.007106644 -0.041606834\rLag 10 -0.002789907 -0.006160622 0.0074013124 0.034867394 0.005265643\rLag 50 0.021062271 -0.011828026 0.0004208875 -0.010961206 -0.036358518\rlog_lik[4] log_lik[5] log_lik[6] log_lik[7] log_lik[8]\rLag 0 1.0000000000 1.000000000 1.000000000 1.000000000 1.00000000\rLag 1 0.0851487042 0.097545282 0.204344885 0.269672389 0.38558262\rLag 5 -0.0003507043 0.002305382 -0.049386304 -0.000960398 0.02069575\rLag 10 -0.0166774245 -0.025515245 -0.001912747 -0.016722339 0.04889381\rLag 50 -0.0162443058 -0.014714579 -0.011135106 -0.014633639 0.01680510\rlog_lik[9] log_lik[10] log_lik[11] log_lik[12] log_lik[13]\rLag 0 1.000000000 1.000000000 1.00000000 1.000000000 1.000000000\rLag 1 0.313784783 0.237324163 0.09482508 0.227340224 -0.047089190\rLag 5 0.056460648 0.037850329 -0.03382433 0.001289457 0.011086069\rLag 10 0.035202184 0.034879805 -0.07137294 -0.040985200 -0.021897934\rLag 50 0.008974814 -0.006569175 0.04107530 0.028135748 0.006235697\rlog_lik[14] log_lik[15] log_lik[16] log_lik[17] log_lik[18]\rLag 0 1.000000000 1.000000000 1.00000000 1.000000000 1.00000000\rLag 1 0.066779275 0.012761718 0.05153404 0.139497451 0.06864146\rLag 5 0.021390228 0.036712340 -0.04509391 -0.007858839 0.03114302\rLag 10 -0.039601576 0.009368704 -0.06501412 -0.061861469 0.02566650\rLag 50 -0.009169908 -0.017729557 0.02175915 0.037622080 -0.04205178\rlog_lik[19] log_lik[20] log_lik[21] log_lik[22] log_lik[23]\rLag 0 1.00000000 1.000000000 1.000000000 1.000000000 1.000000000\rLag 1 -0.03208587 0.199707279 0.122441725 0.216102237 0.030364826\rLag 5 -0.01369268 0.017146307 0.056981177 0.031619771 0.038405137\rLag 10 -0.05184742 0.002266797 -0.002546269 -0.003615615 0.002827458\rLag 50 -0.01491998 -0.000185515 0.026181446 0.003742424 0.042867022\rlog_lik[24] log_lik[25] log_lik[26] log_lik[27] log_lik[28]\rLag 0 1.000000000 1.000000000 1.000000000 1.00000000 1.00000000\rLag 1 -0.043300952 0.008897055 0.055272052 -0.02494535 -0.03902829\rLag 5 -0.007623726 0.032328930 -0.003951948 0.04408482 -0.01410360\rLag 10 0.010511950 0.005615084 -0.005860687 0.01468160 0.01575607\rLag 50 0.051297631 0.036588261 0.018664972 0.03213627 0.04216138\rlog_lik[29] log_lik[30] lp__\rLag 0 1.000000000 1.00000000 1.0000000000\rLag 1 0.065026235 -0.01657044 0.5111573914\rLag 5 0.050138816 0.02213345 0.0005946888\rLag 10 0.002064092 0.02410877 0.0243211235\rLag 50 0.039550469 0.01788728 -0.0046540824\r\u0026gt; stan_rhat(data1.rstan)\r\u0026gt; stan_ess(data1.rstan)\rRhat and effective sample size. In this instance, most of the parameters have reasonably high effective samples and thus there is likely to be a good range of values from which to estimate paramter properties.\n\rModel validation\r\u0026gt; mcmc = as.data.frame(data1.rstan) %\u0026gt;% dplyr:::select(contains(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata1 = data1\r\u0026gt; Xmat = model.matrix(~A * B, newdata1)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:6], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data1$Y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rResiduals against predictors\n\u0026gt; mcmc = as.data.frame(data1.rstan) %\u0026gt;% dplyr:::select(contains(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata1 = newdata1\r\u0026gt; Xmat = model.matrix(~A * B, newdata1)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:6], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data1$Y - fit\r\u0026gt; newdata1 = newdata1 %\u0026gt;% cbind(fit, resid)\r\u0026gt; ggplot(newdata1) + geom_point(aes(y = resid, x = A)) + theme_classic()\r\u0026gt; \u0026gt; ggplot(newdata1) + geom_point(aes(y = resid, x = B)) + theme_classic()\rAnd now for studentised residuals\n\u0026gt; mcmc = as.data.frame(data1.rstan) %\u0026gt;% dplyr:::select(contains(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata1 = data1\r\u0026gt; Xmat = model.matrix(~A * B, newdata1)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:6], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data1$Y - fit\r\u0026gt; sresid = resid/sd(resid)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit)) + theme_classic()\rFor this simple model, the studentised residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = as.data.frame(data1.rstan) %\u0026gt;% dplyr:::select(contains(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~A * B, data1)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, 1:6]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data1), fit[i,\r+ ], mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; newdata1 = data.frame(A = data1$A, B = data1$B, yRep) %\u0026gt;% gather(key = Sample,\r+ value = Value, -A, -B)\r\u0026gt; ggplot(newdata1) + geom_violin(aes(y = Value, x = A, fill = \u0026quot;Model\u0026quot;),\r+ alpha = 0.5) + geom_violin(data = data1, aes(y = Y, x = A,\r+ fill = \u0026quot;Obs\u0026quot;), alpha = 0.5) + geom_point(data = data1, aes(y = Y,\r+ x = A), position = position_jitter(width = 0.1, height = 0),\r+ color = \u0026quot;black\u0026quot;) + theme_classic()\r\u0026gt; \u0026gt; ggplot(newdata1) + geom_violin(aes(y = Value, x = B, fill = \u0026quot;Model\u0026quot;,\r+ group = B, color = A), alpha = 0.5) + geom_point(data = data1,\r+ aes(y = Y, x = B, group = B, color = A)) + theme_classic()\rThe predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.\n\u0026gt; mcmc_intervals(as.matrix(data1.rstan), regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\u0026gt; mcmc_areas(as.matrix(data1.rstan), regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\rParameter estimates\rFirst, we look at the results from the additive model.\n\u0026gt; print(data1.rstan, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: ancovaModel2.\r2 chains, each with iter=1500; warmup=500; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 48.14 0.09 2.00 44.21 46.88 48.13 49.45 52.06 541 1\rbeta[2] -10.58 0.11 2.83 -16.17 -12.34 -10.58 -8.77 -4.81 621 1\rbeta[3] -26.48 0.11 2.58 -31.66 -28.17 -26.48 -24.71 -21.48 551 1\rbeta[4] -0.35 0.00 0.08 -0.50 -0.40 -0.35 -0.30 -0.20 555 1\rbeta[5] -0.27 0.00 0.11 -0.48 -0.34 -0.27 -0.20 -0.05 597 1\rbeta[6] 0.27 0.00 0.12 0.04 0.20 0.27 0.35 0.50 552 1\rsigma 3.39 0.02 0.50 2.57 3.02 3.33 3.67 4.50 1108 1\rSamples were drawn using NUTS(diag_e) at Tue Feb 18 14:31:40 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; # OR\r\u0026gt; tidyMCMC(data1.rstan, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\r# A tibble: 7 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 48.1 2.00 43.9 51.7 2 beta[2] -10.6 2.83 -16.0 -4.66 3 beta[3] -26.5 2.58 -31.8 -21.6 4 beta[4] -0.350 0.0778 -0.501 -0.204 5 beta[5] -0.270 0.106 -0.460 -0.0436\r6 beta[6] 0.269 0.115 0.0515 0.508 7 sigma 3.39 0.502 2.49 4.33 \rConclusions\n\rThe intercept of the first group (Group A) is \\(48.2\\).\n\rThe mean of the second group (Group B) is \\(-10.6\\) units greater than (A).\n\rThe mean of the third group (Group C) is \\(-26.5\\) units greater than (A).\n\rA one unit increase in B in Group A is associated with a \\(-0.351\\) units increase in \\(Y\\).\n\rdifference in slope between Group B and Group A \\(-0.270\\).\n\rdifference in slope between Group C and Group A \\(0.270\\).\n\r\rThe \\(95\\)% confidence interval for the effects of Group B, Group C and the partial slope associated with B do not overlapp with \\(0\\) implying a significant difference between group A and groups B, C (at the mean level of predictor B) and a significant negative relationship with B (for Group A). The slope associated with Group B was not found to be significantly different from that associated with Group A, however, the slope associated with Group C was found to be significantly less negative than the slope associated with Group A. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(as.matrix(data1.rstan)[, \u0026quot;beta[2]\u0026quot;]) # effect of (B-A = 0)\r[1] 5e-04\r\u0026gt; mcmcpvalue(as.matrix(data1.rstan)[, \u0026quot;beta[3]\u0026quot;]) # effect of (C-A = 0)\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data1.rstan)[, \u0026quot;beta[4]\u0026quot;]) # effect of (slope = 0)\r[1] 0.001\r\u0026gt; mcmcpvalue(as.matrix(data1.rstan)[, \u0026quot;beta[5]\u0026quot;]) # effect of (slopeB - slopeA = 0)\r[1] 0.0145\r\u0026gt; mcmcpvalue(as.matrix(data1.rstan)[, \u0026quot;beta[6]\u0026quot;]) # effect of (slopeC - slopeA = 0)\r[1] 0.0185\r\u0026gt; mcmcpvalue(as.matrix(data1.rstan)[, 2:6]) # effect of (model)\r[1] 0\rThere is evidence that the reponse differs between the groups.\n\u0026gt; (full = loo(extract_log_lik(data1.rstan)))\rComputed from 2000 by 30 log-likelihood matrix\rEstimate SE\relpd_loo -83.6 4.8\rp_loo 7.3 2.1\rlooic 167.1 9.5\r------\rMonte Carlo SE of elpd_loo is NA.\rPareto k diagnostic values:\rCount Pct. Min. n_eff\r(-Inf, 0.5] (good) 26 86.7% 465 (0.5, 0.7] (ok) 3 10.0% 233 (0.7, 1] (bad) 1 3.3% 34 (1, Inf) (very bad) 0 0.0% \u0026lt;NA\u0026gt; See help(\u0026#39;pareto-k-diagnostic\u0026#39;) for details.\r\u0026gt; \u0026gt; # now fit a model without main factor\r\u0026gt; modelString3 = \u0026quot;\r+ data {\r+ int\u0026lt;lower=1\u0026gt; n;\r+ int\u0026lt;lower=1\u0026gt; nX;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ }\r+ parameters {\r+ vector[nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ }\r+ transformed parameters {\r+ vector[n] mu;\r+ + mu = X*beta;\r+ }\r+ model {\r+ // Likelihood\r+ y~normal(mu,sigma);\r+ + // Priors\r+ beta ~ normal(0,1000);\r+ sigma~cauchy(0,5);\r+ }\r+ generated quantities {\r+ vector[n] log_lik;\r+ + for (i in 1:n) {\r+ log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); + }\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a stan file \u0026gt; writeLines(modelString3, con = \u0026quot;ancovaModel3.stan\u0026quot;)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~A + B, data1)\r\u0026gt; data1.list \u0026lt;- with(data1, list(y = Y, X = Xmat, n = nrow(data1), nX = ncol(Xmat)))\r\u0026gt; data1.rstan.red \u0026lt;- stan(data = data1.list, file = \u0026quot;ancovaModel3.stan\u0026quot;, chains = nChains,\r+ iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;ancovaModel2\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 1: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 1: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 1: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 1: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 1: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 1: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 1: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 1: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 1: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 1: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 1: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.077 seconds (Warm-up)\rChain 1: 0.062 seconds (Sampling)\rChain 1: 0.139 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;ancovaModel2\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 2: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 2: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 2: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 2: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 2: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 2: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 2: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 2: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 2: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 2: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 2: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.083 seconds (Warm-up)\rChain 2: 0.059 seconds (Sampling)\rChain 2: 0.142 seconds (Total)\rChain 2: \u0026gt; \u0026gt; (reduced = loo(extract_log_lik(data1.rstan.red)))\rComputed from 2000 by 30 log-likelihood matrix\rEstimate SE\relpd_loo -91.9 4.7\rp_loo 5.4 1.8\rlooic 183.8 9.4\r------\rMonte Carlo SE of elpd_loo is 0.1.\rPareto k diagnostic values:\rCount Pct. Min. n_eff\r(-Inf, 0.5] (good) 29 96.7% 379 (0.5, 0.7] (ok) 1 3.3% 166 (0.7, 1] (bad) 0 0.0% \u0026lt;NA\u0026gt; (1, Inf) (very bad) 0 0.0% \u0026lt;NA\u0026gt; All Pareto k estimates are ok (k \u0026lt; 0.7).\rSee help(\u0026#39;pareto-k-diagnostic\u0026#39;) for details.\r\u0026gt; \u0026gt; par(mfrow = 1:2, mar = c(5, 3.8, 1, 0) + 0.1, las = 3)\r\u0026gt; plot(full, label_points = TRUE)\r\u0026gt; plot(reduced, label_points = TRUE)\rThe expected out-of-sample predictive accuracy is substantially lower for the model that includes \\(x\\). This might be used to suggest that the inferential evidence for a general effect of \\(x\\) on \\(y\\).\n\rGraphical summaries\r\u0026gt; mcmc = as.matrix(data1.rstan)\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata1 = expand.grid(A = levels(data1$A), B = seq(min(data1$B), max(data1$B),\r+ len = 100))\r\u0026gt; Xmat = model.matrix(~A * B, newdata1)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;, \u0026quot;beta[3]\u0026quot;, \u0026quot;beta[4]\u0026quot;, \u0026quot;beta[5]\u0026quot;,\r+ \u0026quot;beta[6]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata1 = newdata1 %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; \u0026gt; ggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;B\u0026quot;) + theme_classic()\rAs this is simple single factor ANOVA, we can simple add the raw data to this figure. For more complex designs with additional predictors, it is necessary to plot partial residuals.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata1 = rdata1 = data1\r\u0026gt; fMat = rMat = model.matrix(~A * B, fdata1)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data1$Y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata1 = rdata1 %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; \u0026gt; ggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_point(data = rdata1,\r+ aes(y = partial.resid, x = B, color = A)) + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;B\u0026quot;) + theme_classic()\r\r\rReferences\rGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9.\n\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rGelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” The Annals of Statistics 33 (1): 1–53.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\rVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and Waic.” Statistics and Computing 27 (5): 1413–32.\n\r\r\r","date":1580955194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580955194,"objectID":"af27232471746bd6ad8d43dfaf4df000","permalink":"/stan/ancova-stan/ancova-stan/","publishdate":"2020-02-05T21:13:14-05:00","relpermalink":"/stan/ancova-stan/ancova-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","ancova","factor analysis"],"title":"Analysis of Covariance - STAN","type":"STAN"},{"authors":["Andrea Gabrio"],"categories":["R","anova","JAGS","factor analysis"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rSingle factor Analysis of Variance (ANOVA), also known as single factor classification, is used to investigate the effect of a single factor comprising two or more groups (treatment levels) from a completely randomised design. Completely randomised refers to the absence of restrictions on the random allocation of experimental or sampling units to factor levels.\nFor example, consider a situation in which three types of treatments (A, B and C) are applied to replicate sampling units across the sampling domain. Importantly, the treatments are applied at the scale of the sampling units and the treatments applied to each sampling unit do not extend to any other neighbouring sampling units. Another possible situation is where the scale of a treatment is far larger than that of a sampling unit. This design features two treatments, each replicated three times. Note that additional sampling units within each Site (the scale at which the treatment occurs) would NOT constitute additional replication. Rather, these would be sub-replicates. That is, they would be replicates of the Sites, not the treatments (since the treatments occur at the level of whole sites). In order to genuinely increase the number of replicates, it is necessary to have more Sites. The random allocation of sampling units within the sampling domain (such as population) is appropriate provided either the underlying response is reasonably homogenous throughout the domain, or else, there is a large number of sampling units. If the conditions are relatively hetrogenous, then the exact location of the sampling units is likely to be highly influential and may mask any detectable effects of treatments.\n\rFixed and random effects\rFrom a frequentist perspective, fixed factors are factors whose levels represent the specific populations of interest. For example, a factor that comprises “high”, “medium” and “low” temperature treatments is a fixed factor - we are only interested in comparing those three populations. Conclusions about the effects of a fixed factor are restricted to the specific treatment levels investigated and for any subsequent experiments to be comparable, the same specific treatments of the factor would need to be used. By contrast, random factors are factors whose levels are randomly chosen from all the possible levels of populations and are used as random representatives of the populations. For example, five random temperature treatments could be used to represent a full spectrum of temperature treatments. In this case, conclusions are extrapolated to all the possible treatment (temperature) levels and for subsequent experiments, a new random set of treatments of the factor would be selected.\nOther common examples of random factors include sites and subjects - factors for which we are attempting to generalise over. Furthermore, the nature of random factors means that we have no indication of how a new level of that factor (such as another subject or site) are likely to respond and thus it is not possible to predict new observations from random factors. These differences between fixed and random factors are reflected in the way their respective null hypotheses are formulated and interpreted. Whilst fixed factors contrast the effects of the different levels of the factor, random factors are modelled as the amount of additional variability they introduce. Random factors are modelled with a mean of \\(0\\) and their variance is estimated as the effect coefficient.\n\rLinear model\rThe linear model for single factor classification is similar to that of multiple linear regression. The linear model can thus be represented by either:\n\rMeans parameterisation - in which the regression slopes represent the means of each treatment group and the intercept is removed (to prevent over-parameterisation).\r\r\\[ y_{ij} = \\beta_1(\\text{level}_1)_{ij} + \\beta_2(\\text{level}_2)_{ij} + \\ldots + \\epsilon_{ij},\\]\nwhere \\(\\beta_1\\) and \\(\\beta_2\\) respectively represent the means response of treatment level \\(1\\) and \\(2\\). This is often simplified to \\(y_{ij}=\\alpha_i + \\epsilon_{ij}\\).\n\rEffects parameterisation - the intercept represents a property such as the mean of one of the treatment groups (treatment contrasts) or the overall mean (sum contrasts), and the slope parameters represent effects (differences between each other group and the reference mean for example).\r\r\\[ y_{ij} = \\mu + \\beta_2(\\text{level}_2)_{ij} + \\beta_3(\\text{level}_3)_{ij} + \\ldots + \\epsilon_{ij},\\]\nwhere \\(\\mu\\) is the mean of the first treatment group, \\(\\beta_2\\) and \\(\\beta_3\\) respectively represent the effects (change from level \\(1\\)) of level \\(2\\) and \\(3\\) on the mean response. This is often simplified to: \\(y_{ij}=\\mu + \\alpha_i + \\epsilon_{ij}\\), with \\(\\alpha_1=0\\).\nSince we are traditionally interested in investigating effects (differences) rather than treatment means, effects parameterisation is far more common (particularly when coupled with hypothesis testing). In a Bayesian framework, it does not really matter whether models are fit with means or effects parameterisation since the posterior likelihood can be querried in any way and repeatedly - thus enabling us to explore any specific effects after the model has been fit. Nevertheless, to ease comparisons with frequentist approaches, we will stick with effects paramterisation.\n\rNull hypothesis: fixed factor\rWe can associate a null hypothesis test with each estimated parameter. For example, in a cell for each estimated mean in a means model we could test a null hypothesis that the population mean is equal to zero (e.g. \\(H_0\\): \\(\\alpha_1=0\\), \\(H_0\\): \\(\\alpha_2=0\\), \\(\\ldots\\)). However, this rarely would be of much interest. By contrast, individual null hypotheses associated with each parameter of the effects model can be used to investigate the differences between each group and a reference group (for example). In addition to the individual null hypothesis tests, a single fixed factor ANOVA tests the collective \\(H_0\\) that there are no differences between the population group means:\n\r\\(H_0: \\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\) (the population group means are all equal). That is, that the mean of population \\(1\\) is equal to that of population \\(2\\) and so on, and thus all population means are equal to one another - no effect of the factor on the response. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the mean of the first group (\\(\\alpha_i=\\mu_i-\\mu_1\\)) then the \\(H_0\\) can alternatively be written as:\n\r\\(H_0 : \\alpha_2=\\alpha_3=\\ldots=\\alpha_i=0\\) (the effect of each group equals zero). If one or more of the \\(\\alpha_i\\) are different from zero (the response mean for this treatment differs from the overall response mean), there is evidence that the null hypothesis is not true indicating that the factor does affect the response variable.\n\r\r\rNull hypothesis: random factor\rThe collective \\(H_0\\) for a random factor is that the variance between all possible treatment groups equals zero:\n\r\\(H_0 : \\sigma^2_{\\alpha}=0\\) (added variance due to this factor equals zero).\r\rNote that whilst the null hypotheses for fixed and random factors are different (fixed: population group means all equal, random: variances between populations all equal zero), the linear model fitted for fixed and random factors in single factor ANOVA models is identical. For more complex multi-factor ANOVA models however, the distinction between fixed and random factors has important consequences for building and interpreting statistical models and null hypotheses.\n\rAnalysis of variance\rWhen the null hypothesis is true (and the populations are identical), the amount of variation among observations within groups should be similar to the amount of variation in observations between groups. However, when the null hypothesis is false (and some means are different from other means), the amount of variation among observations might be expected to be less than the amount of variation within groups. Analysis of variance, or ANOVA, partitions the total variance in the response (dependent) variable into a component of the variance that is explained by combinations of one or more categorical predictor variables (called factors) and a component of the variance that cannot be explained (residual). The variance ratio (F-ratio) from this partitioning can then be used to test the null hypothesis (\\(H_0\\)) that the population group or treatment means are all equal. Ttotal variation can be decomposed into components explained by the groups (\\(MS_{groups}\\)) and and unexplained (\\(MS_{residual}\\)) by the groups. The gray arrows in b) depict the relative amounts explained by the groups. The proposed groupings generally explain why the first few points are higher on the y-axis than the last three points. The probability of collecting our sample, and thus generating the sample ratio of explained to unexplained variation (or one more extreme), when the null hypothesis is true (and population means are equal) is the area under the F-distribution beyond our sample ratio (\\(\\text{F-ratio}=\\frac{MS_{groups}}{MS_{residual}}\\)).\nWhen the null hypothesis is true (and the test assumptions have not been violated), the ratio (F-ratio) of explained to unexplained variance follows a theoretical probability distribution (F-distribution). When the null hypothesis is true, and there is no effect of the treatment on the response variable, the ratio of explained variability to unexplained variability is expected to be \\(\\leq 1\\). Since the denominator should represent the expected numerator in the absence of an effect. Importantly, the denominator in an F-ratio calculation essentially represents what we would expect the numerator to be in the absence of a treatment effect. For simple analyses, identifying what these expected values are is relatively straightforward (equivalent to the degree of within group variability). However, in more complex designs (particularly involving random factors and hierarchical treatment levels), the logical “groups” can be more difficult (and in some cases impossible) to identify. In such cases, nominating the appropriate F-ratio denominator for estimating an specific effect requires careful consideration. The following table depicts the anatomy of the single factor ANOVA table\n\u0026gt; anova_table\rdf MS F-ratio Factor A \u0026quot;a-1\u0026quot; \u0026quot;MS A\u0026quot; \u0026quot;(MS A)/(MS res)\u0026quot;\rResidual \u0026quot;(n-1)a\u0026quot; \u0026quot;MS res\u0026quot; \u0026quot;\u0026quot; \rand corresponding R syntax.\n\u0026gt; anova(lm(DV ~ A, dataset))\r\u0026gt; # OR\r\u0026gt; anova(aov(DV ~ A, dataset))\rAn F-ratio substantially greater than \\(1\\) suggests that the model relating the response variable to the categorical variable explains substantially more variability than is left unexplained. In turn, this implies that the linear model does represent the data well and that differences between observations can be explained largely by differences in treatment levels rather than purely the result of random variation. If the probability of getting the observed (sample) F-ratio or one more extreme is less than some predefined critical value (typically \\(5\\)% or \\(0.05\\)), we conclude that it is highly unlikely that the observed samples could have been collected from populations in which the treatment has no effect and therefore we would reject the null hypothesis.\n\rAssumptions\rAn F-ratio from real data can only reliably relate to a theoretical F-distribution when the data conform to certain assumptions. Hypothesis testing for a single factor ANOVA model assumes that the residuals (and therefore the response variable for each of the treatment levels) are all:\n\rnormally distributed - although ANOVA is robust to non-normality provided sample sizes and variances are equal. Boxplots should be used to explore normality, skewness, bimodality and outliers. In the event of homogeneity of variance issues (see below), a Q-Q normal plot can also be useful for exploring normality (as this might be the cause of non-homogeneity). Scale transformations are often useful.\n\requally varied - provided sample sizes are equal and the largest to smallest variance ratio does not exceed 3:1 (9:1 for sd), ANOVA is reasonably robust to this assumption, however, relationships between variance and mean and/or sample size are of particular concern as they elevate the Type I error rate. Boxplots and plots of means against variance should be used to explore the spread of values. Residual plots should reveal no patterns. Since unequal variances are often the result of non-normality, transformations that improve normality will also improve variance homogeneity.\n\rindependent of one another - this assumption must be addressed at the design and collection stages and cannot be compensated for later (unless a model is used that specifically accounts for particular types of non-independent data, such as that introduced with hierarchical designs or autocorrelation)\n\r\rViolations of these assumptions reduce the reliability of the analysis.\n\r\rData generation\rLets say we had set up a natural experiment in which we measured a response from \\(10\\) sampling units (replicates) from each of \\(5\\) treatments. Hence, we have a single categorical factor with \\(5\\) levels - we might have five different locations, or five different habitat types or substrates etc. In statistical speak, we have sampled from \\(5\\) different populations. We have then randomly selected \\(10\\) independent and random (representative) units of each population to sample. That is, we have \\(10\\) samples (replicates) of each population. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(123)\r\u0026gt; ngroups \u0026lt;- 5 #number of populations\r\u0026gt; nsample \u0026lt;- 10 #number of reps in each\r\u0026gt; pop.means \u0026lt;- c(40, 45, 55, 40, 30) #population mean length\r\u0026gt; sigma \u0026lt;- 3 #residual standard deviation\r\u0026gt; n \u0026lt;- ngroups * nsample #total sample size\r\u0026gt; eps \u0026lt;- rnorm(n, 0, sigma) #residuals\r\u0026gt; x \u0026lt;- gl(ngroups, nsample, n, lab = LETTERS[1:5]) #factor\r\u0026gt; means \u0026lt;- rep(pop.means, rep(nsample, ngroups))\r\u0026gt; X \u0026lt;- model.matrix(~x - 1) #create a design matrix\r\u0026gt; y \u0026lt;- as.numeric(X %*% pop.means + eps)\r\u0026gt; data \u0026lt;- data.frame(y, x)\r\u0026gt; head(data) #print out the first six rows of the data set\ry x\r1 38.31857 A\r2 39.30947 A\r3 44.67612 A\r4 40.21153 A\r5 40.38786 A\r6 45.14519 A\r\u0026gt; \u0026gt; write.csv(data, \u0026quot;simpleAnova.csv\u0026quot;)\rWith these sort of data, we are primarily interested in investigating whether there is a relationship between the continuous response variable and the treatment type.\nExploratory data analysis\r\rNormality and Homogeneity of variance\r\r\u0026gt; boxplot(y ~ x, data)\r\u0026gt; \u0026gt; # OR via ggplot2\r\u0026gt; library(ggplot2)\r\u0026gt; ggplot(data, aes(y = y, x = x)) + geom_boxplot() +\r+ theme_classic()\rConclusions\nThere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical. There is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity. Obvious violations could be addressed either by, for example, transforming the scale of the response variables (to address normality etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\n\r\rModel fitting\rThe observed response (\\(y_i\\)) are assumed to be drawn from a normal distribution with a given mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The expected values (\\(\\mu\\)) are themselves determined by the linear predictor (\\(\\beta_0+\\boldsymbol \\beta \\boldsymbol X_i\\)). In this case, \\(\\beta_0\\) represents the mean of the first group and the set of \\(\\boldsymbol \\beta\\)’s represent the differences between each other group and the first group. MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying ‘uninformative’ priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (\\(100\\)) for both the intercept and the treatment effect and a wide half-cauchy (\\(\\text{scale}=5\\)) for the standard deviation.\n\\[y_i \\sim N(\\mu_i,\\sigma), \\]\nwhere \\(\\mu_i=\\beta_0 +\\boldsymbol \\beta \\boldsymbol X_i\\). The assumed priors are: \\(\\beta \\sim N(0,100)\\) and \\(\\sigma \\sim \\text{Cauchy}(0,5)\\). We proceed to code the model into JAGS (remember that in this software normal distribution are parameterised in terms of precisions \\(\\tau\\) rather than variances, where \\(\\tau=\\frac{1}{\\sigma^2}\\)). Note the following example as group means calculated as derived posteriors.\n\u0026gt; modelString = \u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mean[i],tau.res)\r+ mean[i] \u0026lt;- alpha+beta[x[i]]\r+ }\r+ + #Priors and derivatives\r+ alpha ~ dnorm(0,1.0E-6)\r+ beta[1] \u0026lt;- 0\r+ for (i in 2:ngroups) {\r+ beta[i] ~ dnorm(0, 1.0E-6) #prior\r+ }\r+ sigma.res ~ dunif(0, 100)\r+ tau.res \u0026lt;- 1 / (sigma.res * sigma.res)\r+ sigma.group \u0026lt;- sd(beta[])\r+ + #Group mean posteriors (derivatives)\r+ for (i in 1:ngroups) {\r+ Group.means[i] \u0026lt;- beta[i]+alpha\r+ }\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;anovaModel.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; data.list \u0026lt;- with(data, list(y = y, x = as.numeric(x), n = nrow(data),\r+ ngroups = length(levels(data$x))))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;alpha\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;sigma.res\u0026quot;, \u0026quot;Group.means\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10500\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\u0026gt; library(R2jags)\rNow run the JAGS code via the R2jags interface. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; data.r2jags \u0026lt;- jags(data = data.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;anovaModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 50\rUnobserved stochastic nodes: 6\rTotal graph size: 126\rInitializing model\r\u0026gt; \u0026gt; print(data.r2jags)\rInference for Bugs model at \u0026quot;anovaModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat\rGroup.means[1] 40.232 0.908 38.424 39.631 40.237 40.837 41.991 1.001\rGroup.means[2] 45.632 0.902 43.858 45.022 45.626 46.231 47.432 1.002\rGroup.means[3] 53.730 0.913 51.947 53.113 53.722 54.334 55.543 1.001\rGroup.means[4] 40.962 0.906 39.188 40.350 40.968 41.563 42.734 1.001\rGroup.means[5] 29.974 0.915 28.173 29.367 29.974 30.586 31.746 1.001\ralpha 40.232 0.908 38.424 39.631 40.237 40.837 41.991 1.001\rbeta[1] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000\rbeta[2] 5.400 1.278 2.889 4.551 5.395 6.244 7.896 1.001\rbeta[3] 13.498 1.286 11.017 12.639 13.485 14.354 16.049 1.001\rbeta[4] 0.730 1.283 -1.768 -0.122 0.722 1.582 3.261 1.001\rbeta[5] -10.258 1.294 -12.820 -11.110 -10.253 -9.412 -7.721 1.001\rsigma.res 2.864 0.320 2.313 2.638 2.832 3.056 3.578 1.001\rdeviance 245.540 3.787 240.323 242.761 244.832 247.511 254.843 1.001\rn.eff\rGroup.means[1] 15000\rGroup.means[2] 2200\rGroup.means[3] 3800\rGroup.means[4] 15000\rGroup.means[5] 15000\ralpha 15000\rbeta[1] 1\rbeta[2] 2900\rbeta[3] 15000\rbeta[4] 15000\rbeta[5] 15000\rsigma.res 15000\rdeviance 15000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 7.2 and DIC = 252.7\rDIC is an estimate of expected predictive error (lower deviance is better).\rModel matrix formulation\rFor very simple models such as this example, we can write the models as:\n\u0026gt; modelString2 = \u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mean[i],tau)\r+ mean[i] \u0026lt;- inprod(beta[],X[i,])\r+ }\r+ #Priors\r+ for (i in 1:ngroups) {\r+ beta[i] ~ dnorm(0, 1.0E-6) + }\r+ sigma ~ dunif(0, 100)\r+ tau \u0026lt;- 1 / (sigma * sigma)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString2, con = \u0026quot;anovaModel2.txt\u0026quot;)\rDefine the data to pass to R2jags.\n\u0026gt; X \u0026lt;- model.matrix(~x, data)\r\u0026gt; data.list \u0026lt;- with(data, list(y = y, X = X, n = nrow(data), ngroups = ncol(X)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10500\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Run the JAGS code via the R2jags interface. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; data.r2jags \u0026lt;- jags(data = data.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;anovaModel2.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 50\rUnobserved stochastic nodes: 6\rTotal graph size: 370\rInitializing model\r\u0026gt; \u0026gt; print(data.r2jags)\rInference for Bugs model at \u0026quot;anovaModel2.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 40.226 0.901 38.475 39.624 40.222 40.824 41.999 1.001 4800\rbeta[2] 5.401 1.272 2.906 4.552 5.397 6.242 7.900 1.001 15000\rbeta[3] 13.492 1.296 10.969 12.634 13.484 14.355 16.038 1.001 5100\rbeta[4] 0.734 1.279 -1.793 -0.114 0.740 1.582 3.263 1.001 15000\rbeta[5] -10.248 1.283 -12.785 -11.108 -10.242 -9.380 -7.731 1.001 9800\rsigma 2.863 0.315 2.321 2.642 2.838 3.053 3.558 1.001 6200\rdeviance 245.551 3.785 240.353 242.765 244.844 247.603 254.815 1.002 1800\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 7.2 and DIC = 252.7\rDIC is an estimate of expected predictive error (lower deviance is better).\r\r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots for the effects model as an example. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters, e.g. \\(\\boldsymbol \\beta\\).\n\u0026gt; library(mcmcplots)\r\u0026gt; denplot(data.r2jags, parms = c(\u0026quot;beta\u0026quot;))\r\u0026gt; traplot(data.r2jags, parms = c(\u0026quot;beta\u0026quot;))\rThese plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.\n\u0026gt; data.mcmc = as.mcmc(data.r2jags)\r\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(data.mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 2 3895 3746 1.040 beta[2] 2 3729 3746 0.995 beta[3] 2 3811 3746 1.020 beta[4] 3 4115 3746 1.100 beta[5] 2 3853 3746 1.030 deviance 2 3729 3746 0.995 sigma 5 5834 3746 1.560 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 2 3853 3746 1.03 beta[2] 2 3918 3746 1.05 beta[3] 2 3811 3746 1.02 beta[4] 2 3853 3746 1.03 beta[5] 2 3853 3746 1.03 deviance 2 3981 3746 1.06 sigma 4 5306 3746 1.42 \rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; autocorr.diag(data.mcmc)\rbeta[1] beta[2] beta[3] beta[4] beta[5]\rLag 0 1.0000000000 1.000000000 1.0000000000 1.0000000000 1.000000000\rLag 1 0.0015561854 0.001902670 -0.0023462263 0.0063854498 -0.008928813\rLag 5 -0.0006487164 0.003556616 -0.0008267107 -0.0003892349 0.004087306\rLag 10 0.0141414517 0.012308363 0.0064688638 -0.0029210457 0.009117446\rLag 50 -0.0019115790 0.005069522 0.0072096979 -0.0030858504 0.002938152\rdeviance sigma\rLag 0 1.000000000 1.000000000\rLag 1 0.198317688 0.334172270\rLag 5 -0.001425768 0.005514213\rLag 10 -0.000422188 -0.001600486\rLag 50 -0.008805916 0.007414425\rA lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. For more complex models (those that contain multiple effects), it is also advisable to plot the residuals against each of the individual predictors. For sampling designs that involve sample collection over space or time, it is also a good idea to explore whether there are any temporal or spatial patterns in the residuals.\nThere are numerous situations (e.g. when applying specific variance-covariance structures to a model) where raw residuals do not reflect the interior workings of the model. Typically, this is because they do not take into account the variance-covariance matrix or assume a very simple variance-covariance matrix. Since the purpose of exploring residuals is to evaluate the model, for these cases, it is arguably better to draw conclusions based on standardized (or studentised) residuals. Unfortunately the definitions of standardised and studentised residuals appears to vary and the two terms get used interchangeably. I will adopt the following definitions:\n\rStandardised residuals. The raw residuals divided by the true standard deviation of the residuals (which of course is rarely known).\n\rStudentised residuals. The raw residuals divided by the standard deviation of the residuals. Note that externally studentised residuals are calculated by dividing the raw residuals by a unique standard deviation for each observation that is calculated from regressions having left each successive observation out.\n\rPearson residuals. The raw residuals divided by the standard deviation of the response variable.\n\r\rhe mark of a good model is being able to predict well. In an ideal world, we would have sufficiently large sample size as to permit us to hold a fraction (such as \\(25\\)%) back thereby allowing us to train the model on \\(75\\)% of the data and then see how well the model can predict the withheld \\(25\\)%. Unfortunately, such a luxury is still rare. The next best option is to see how well the model can predict the observed data. Models tend to struggle most with the extremes of trends and have particular issues when the extremes approach logical boundaries (such as zero for count data and standard deviations). We can use the fitted model to generate random predicted observations and then explore some properties of these compared to the actual observed data.\nRather than dublicate this for both additive and multiplicative models, we will only explore the multiplicative model. Residuals are not computed directly within JAGS. However, we can calculate them manually form the posteriors.\n\u0026gt; library(dplyr)\r\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:5], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rResiduals against predictors\n\u0026gt; library(tidyr)\r\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = newdata\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:5], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; newdata = newdata %\u0026gt;% cbind(fit, resid)\r\u0026gt; ggplot(newdata) + geom_point(aes(y = resid, x = x)) + theme_classic()\rAnd now for studentised residuals\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:5], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; sresid = resid/sd(resid)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit)) + theme_classic()\rFor this simple model, the studentised residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~x, data)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, 1:5]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i,\r+ ], mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; newdata = data.frame(x = data$x, yRep) %\u0026gt;% gather(key = Sample,\r+ value = Value, -x)\r\u0026gt; ggplot(newdata) + geom_violin(aes(y = Value, x = x, fill = \u0026quot;Model\u0026quot;),\r+ alpha = 0.5) + geom_violin(data = data, aes(y = y, x = x,\r+ fill = \u0026quot;Obs\u0026quot;), alpha = 0.5) + geom_point(data = data, aes(y = y,\r+ x = x), position = position_jitter(width = 0.1, height = 0),\r+ color = \u0026quot;black\u0026quot;) + theme_classic()\rThe predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.\n\u0026gt; library(bayesplot)\r\u0026gt; mcmc_intervals(data.r2jags$BUGSoutput$sims.matrix, regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\u0026gt; mcmc_areas(data.r2jags$BUGSoutput$sims.matrix, regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\rParameter estimates\rAlthough all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and \\(95\\)% credibility intervals.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\rFirst, we look at the results from the additive model.\n\u0026gt; print(data.r2jags)\rInference for Bugs model at \u0026quot;anovaModel2.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 40.226 0.901 38.475 39.624 40.222 40.824 41.999 1.001 4800\rbeta[2] 5.401 1.272 2.906 4.552 5.397 6.242 7.900 1.001 15000\rbeta[3] 13.492 1.296 10.969 12.634 13.484 14.355 16.038 1.001 5100\rbeta[4] 0.734 1.279 -1.793 -0.114 0.740 1.582 3.263 1.001 15000\rbeta[5] -10.248 1.283 -12.785 -11.108 -10.242 -9.380 -7.731 1.001 9800\rsigma 2.863 0.315 2.321 2.642 2.838 3.053 3.558 1.001 6200\rdeviance 245.551 3.785 240.353 242.765 244.844 247.603 254.815 1.002 1800\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 7.2 and DIC = 252.7\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(as.mcmc(data.r2jags), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 7 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 40.2 0.901 38.5 42.0 2 beta[2] 5.40 1.27 2.90 7.89\r3 beta[3] 13.5 1.30 11.0 16.1 4 beta[4] 0.734 1.28 -1.82 3.21\r5 beta[5] -10.2 1.28 -12.7 -7.68\r6 deviance 246. 3.79 240. 253. 7 sigma 2.86 0.315 2.26 3.48\rConclusions\n\rthe mean of the first group (A) is \\(40.2\\)\rthe mean of the second group (B) is \\(5.4\\) units greater than (A)\rthe mean of the third group (C) is \\(13.5\\) units greater than (A)\rthe mean of the forth group (D) is \\(0.74\\) units greater than (A)\rthe mean of the fifth group (E) is \\(-10.2\\) units greater (i.e. less) than (A)\r\rThe \\(95\\)% confidence interval for the effects of B, C and E do not overlap with \\(0\\) implying a significant difference between group A and groups B, C and E. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[2]\u0026quot;]) # effect of (B-A)\r[1] 6.666667e-05\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[3]\u0026quot;]) # effect of (C-A)\r[1] 0\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[4]\u0026quot;]) # effect of (D-A)\r[1] 0.5576\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, \u0026quot;beta[5]\u0026quot;]) # effect of (E-A)\r[1] 0\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, 2:5]) # effect of (all groups)\r[1] 0\rThere is evidence that the reponse differs between the groups. There is evidence suggesting that the response of group D differs from that of group A.\n\rGraphical summaries\rA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group.\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = rbind(data.frame(x = levels(data$x)))\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;, \u0026quot;beta[3]\u0026quot;, \u0026quot;beta[4]\u0026quot;, \u0026quot;beta[5]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_linerange(aes(ymin = conf.low,\r+ ymax = conf.high)) + geom_point() + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) +\r+ theme_classic()\rAs this is simple single factor ANOVA, we can simple add the raw data to this figure. For more complex designs with additional predictors, it is necessary to plot partial residuals.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = data\r\u0026gt; fMat = rMat = model.matrix(~x, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = as.numeric(x) - 0.1)) + geom_blank(aes(x = x)) +\r+ geom_point(data = rdata, aes(y = partial.resid, x = as.numeric(x) +\r+ 0.1), color = \u0026quot;gray\u0026quot;) + geom_linerange(aes(ymin = conf.low, ymax = conf.high)) +\r+ geom_point() + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;\u0026quot;) + theme_classic()\r\rPosteriors\rIn frequentist statistics, when we have more than two groups, we are typically not only interested in whether there is evidence for an overall “effect” of a factor - we are also interested in how various groups compare to one another. To explore these trends, we either compare each group to each other in a pairwise manner (controlling for family-wise Type I error rates) or we explore an independent subset of the possible comparisons. Although these alternate approaches can adequately address a specific research agenda, often they impose severe limitations and compromises on the scope and breadth of questions that can be asked of your data. The reason for these limitations is that in a frequentist framework, any single hypothesis carries with it a (nominally) \\(5\\)% chance of a false rejection (since it is based on long-run frequency). Thus, performing multiple tests are likely to compound this error rate. The point is, that each comparison is compared to its own probability distribution (and each carries a \\(5\\)% error rate). By contrast, in Bayesian statistics, all comparisons (contrasts) are drawn from the one (hopefully stable and convergent) posterior distribution and this posterior is invariant to the type and number of comparisons drawn. Hence, the theory clearly indicates that having generated our posterior distribution, we can then query this distribution in any way that we wish thereby allowing us to explore all of our research questions simultaneously.\nBayesian “contrasts” can be performed either:\n\rwithin the Bayesian sampling model or\n\rconstruct them from the returned MCMC samples (they are drawn from the posteriors)\n\r\rOnly the latter will be demonstrated as it povides a consistent approach across all routines. In order to allow direct comparison to the frequentist equivalents, I will explore the same set of planned and Tukey’s test comparisons described here. For the “planned comparison” we defined two contrasts: 1) group 3 vs group 5; and 2) the average of groups 1 and 2 vs the average of groups 3, 4 and 5.\nLets start by comparing each group to each other group in a pairwise manner. Arguably the most elegant way to do this is to generate a Tukey’s contrast matrix. This is a model matrix specific to comparing each group to each other group.\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; coefs \u0026lt;- as.matrix(mcmc)[, 1:5]\r\u0026gt; newdata \u0026lt;- data.frame(x = levels(data$x))\r\u0026gt; # A Tukeys contrast matrix\r\u0026gt; library(multcomp)\r\u0026gt; # table(newdata$x) - gets the number of replicates of each level\r\u0026gt; tuk.mat \u0026lt;- contrMat(n = table(newdata$x), type = \u0026quot;Tukey\u0026quot;)\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data = newdata)\r\u0026gt; pairwise.mat \u0026lt;- tuk.mat %*% Xmat\r\u0026gt; pairwise.mat\r(Intercept) xB xC xD xE\rB - A 0 1 0 0 0\rC - A 0 0 1 0 0\rD - A 0 0 0 1 0\rE - A 0 0 0 0 1\rC - B 0 -1 1 0 0\rD - B 0 -1 0 1 0\rE - B 0 -1 0 0 1\rD - C 0 0 -1 1 0\rE - C 0 0 -1 0 1\rE - D 0 0 0 -1 1\r\u0026gt; \u0026gt; mcmc_areas(coefs %*% t(pairwise.mat))\r\u0026gt; \u0026gt; (comps = tidyMCMC(coefs %*% t(pairwise.mat), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 10 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 B - A 5.40 1.27 2.90 7.89\r2 C - A 13.5 1.30 11.0 16.1 3 D - A 0.734 1.28 -1.82 3.21\r4 E - A -10.2 1.28 -12.7 -7.68\r5 C - B 8.09 1.29 5.58 10.7 6 D - B -4.67 1.30 -7.19 -2.02\r7 E - B -15.6 1.28 -18.1 -13.1 8 D - C -12.8 1.31 -15.3 -10.2 9 E - C -23.7 1.29 -26.2 -21.2 10 E - D -11.0 1.29 -13.5 -8.46\r\u0026gt; \u0026gt; ggplot(comps, aes(y = estimate, x = term)) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;) +\r+ scale_y_continuous(\u0026quot;Effect size\u0026quot;) + scale_x_discrete(\u0026quot;\u0026quot;) + coord_flip() +\r+ theme_classic()\rWith a couple of modifications, we could also express this as percentage changes. A percentage change represents the change (difference between groups) divided by one of the groups (determined by which group you want to express the percentage change to). Hence, we generate an additional mcmc matrix that represents the cell means for the divisor group (group we want to express change relative to). Since the tuk.mat defines comparisons as \\(-1\\) and \\(1\\) pairs, if we simply replace all the \\(-1\\) with \\(0\\), the eventual matrix multiplication will result in estimates of the divisor cell means instread of the difference. We can then divide the original mcmc matrix above with this new divisor mcmc matrix to yeild a mcmc matrix of percentage change.\n\u0026gt; # Modify the tuk.mat to replace -1 with 0. This will allow us to get a\r\u0026gt; # mcmc matrix of ..\r\u0026gt; tuk.mat[tuk.mat == -1] = 0\r\u0026gt; comp.mat \u0026lt;- tuk.mat %*% Xmat\r\u0026gt; comp.mat\r(Intercept) xB xC xD xE\rB - A 1 1 0 0 0\rC - A 1 0 1 0 0\rD - A 1 0 0 1 0\rE - A 1 0 0 0 1\rC - B 1 0 1 0 0\rD - B 1 0 0 1 0\rE - B 1 0 0 0 1\rD - C 1 0 0 1 0\rE - C 1 0 0 0 1\rE - D 1 0 0 0 1\r\u0026gt; \u0026gt; comp.mcmc = 100 * (coefs %*% t(pairwise.mat))/coefs %*% t(comp.mat)\r\u0026gt; (comps = tidyMCMC(comp.mcmc, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 10 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 B - A 11.8 2.63 6.52 16.8 2 C - A 25.1 2.13 21.0 29.4 3 D - A 1.74 3.10 -4.30 7.88\r4 E - A -34.3 5.09 -44.3 -24.4 5 C - B 15.0 2.24 10.4 19.2 6 D - B -11.5 3.38 -18.1 -4.70\r7 E - B -52.3 5.53 -63.2 -41.6 8 D - C -31.2 3.73 -38.5 -23.9 9 E - C -79.4 6.26 -91.9 -67.5 10 E - D -36.8 5.15 -47.1 -27.0 \u0026gt; \u0026gt; ggplot(comps, aes(y = estimate, x = term)) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;) +\r+ scale_y_continuous(\u0026quot;Effect size (%)\u0026quot;) + scale_x_discrete(\u0026quot;\u0026quot;) + coord_flip() +\r+ theme_classic()\rAnd now for the specific planned comparisons (Group 3 vs Group 5 and the average of Groups 1 and 2 vs the average of Groups 3, 4 and 5). This is achieved by generating our own contrast matrix (defining the contributions of each group to each contrast).\n\u0026gt; c.mat = rbind(c(0, 0, -1, 0, 1), c(-1/2, -1/2, 1/3, 1/3, 1/3))\r\u0026gt; c.mat\r[,1] [,2] [,3] [,4] [,5]\r[1,] 0.0 0.0 -1.0000000 0.0000000 1.0000000\r[2,] -0.5 -0.5 0.3333333 0.3333333 0.3333333\r\u0026gt; \u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; coefs \u0026lt;- as.matrix(mcmc)[, 1:5]\r\u0026gt; newdata \u0026lt;- data.frame(x = levels(data$x))\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data = newdata)\r\u0026gt; c.mat = c.mat %*% Xmat\r\u0026gt; c.mat\r(Intercept) xB xC xD xE\r[1,] 0.000000e+00 0.0 -1.0000000 0.0000000 1.0000000\r[2,] -1.110223e-16 -0.5 0.3333333 0.3333333 0.3333333\r\u0026gt; \u0026gt; (comps = tidyMCMC(as.mcmc(coefs %*% t(c.mat)), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 -23.7 1.29 -26.2 -21.2 2 var2 -1.37 0.836 -3.01 0.273\r\rFinite population standard deviations\rVariance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).\nFinite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (Gelman and others (2005)). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.\n beta[1] beta[2] beta[3] beta[4] beta[5] deviance sigma\r[1,] 41.14988 5.425974 13.10634 0.5423808 -12.004913 245.9651 2.374957\r[2,] 41.77436 3.165155 12.08478 -2.5284367 -11.070257 251.2837 3.546706\r[3,] 39.87873 5.074910 13.46806 0.7805140 -7.932663 245.7947 3.020465\r[4,] 41.15168 3.079048 10.80976 -0.5505218 -10.396170 249.3934 2.547300\r[5,] 39.93263 4.548017 13.82126 1.2192389 -9.549601 242.2442 2.449639\r[6,] 40.41198 4.705732 12.87972 2.3548628 -8.868949 250.1582 2.432338\r# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x 9.94 0.528 8.86 10.9 2 sd.resid 2.79 0.0903 2.67 2.96\r# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x 78.3 1.07 76.0 79.7\r2 sd.resid 21.7 1.07 20.3 24.0\rApproximately \\(78.3\\)% of the total finite population standard deviation is due to \\(x\\).\n\rR squared\rIn a frequentist context, the \\(R^2\\) value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, \\(R^2\\) is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an \\(R^2\\) greater than \\(100\\)%. Gelman et al. (2019) proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.\nSo in the standard regression model notation of:\n\\[ y_i \\sim \\text{Normal}(\\boldsymbol X \\boldsymbol \\beta, \\sigma),\\]\nthe \\(R^2\\) could be formulated as\n\\[ R^2 = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sigma^2_e},\\]\nwhere \\(\\sigma^2_f=\\text{var}(\\boldsymbol X \\boldsymbol \\beta)\\), and for normal models \\(\\sigma^2_e=\\text{var}(y-\\boldsymbol X \\boldsymbol \\beta)\\)\n\u0026gt; mcmc \u0026lt;- data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; Xmat = model.matrix(~x, data)\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; var_f = apply(fit, 1, var)\r\u0026gt; var_e = apply(resid, 1, var)\r\u0026gt; R2 = var_f/(var_f + var_e)\r\u0026gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.887 0.0127 0.862 0.905\r\u0026gt; \u0026gt; # for comparison with frequentist\r\u0026gt; summary(lm(y ~ x, data))\rCall:\rlm(formula = y ~ x, data = data)\rResiduals:\rMin 1Q Median 3Q Max -6.5257 -1.9000 -0.2589 1.4935 6.5330 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 40.2239 0.8801 45.702 \u0026lt; 2e-16 ***\rxB 5.4020 1.2447 4.340 7.97e-05 ***\rxC 13.5024 1.2447 10.848 3.82e-14 ***\rxD 0.7423 1.2447 0.596 0.554 xE -10.2500 1.2447 -8.235 1.57e-10 ***\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 2.783 on 45 degrees of freedom\rMultiple R-squared: 0.8957, Adjusted R-squared: 0.8865 F-statistic: 96.64 on 4 and 45 DF, p-value: \u0026lt; 2.2e-16\r\rBayesian model selection\rA statistical model is by definition a low-dimensional (over simplification) representation of what is really likely to be a very complex system. As a result, no model is right. Some models however can provide useful insights into some of the processes operating on the system. Frequentist statistics have various methods (model selection, dredging, lasso, cross validation) for selecting parsimonious models. These are models that provide a good comprimise between minimizing unexplained patterns and minimizing model complexity. The basic premise is that since no model can hope to capture the full complexity of a system with all its subtleties, only the very major patterns can be estimated. Overly complex models are likely to be representing artificial complexity present only in the specific observed data (not the general population). The Bayesian approach is to apply priors to the non-variance parameters such that parameters close to zero are further shrunk towards zero whilst priors on parameters further away from zero are less effected. The most popular form of prior for sparsity is the horseshoe prior, so called because the shape of a component of this prior resembles a horseshoe (with most of the mass either close to \\(0\\) or close to \\(1\\)).\nRather than apply weakly informative Gaussian priors on parameters as:\n\\[ \\beta_j \\sim N(0,\\sigma^2),\\]\nthe horseshoe prior is defined as\n\\[ \\beta_j \\sim N(0,\\tau^2\\lambda_j^2),\\]\nwhere \\(\\tau \\sim \\text{Cauchy}(0,1)\\) and \\(\\lambda_j \\sim \\text{Cauchy}(0,1)\\), for \\(j=1,\\ldots,D\\). Using this prior, \\(D\\) is the number of (non-intercept or variance) parameters, \\(\\tau\\) represents the global scale that weights or shrinks all parameters towards zero and \\(\\lambda_j\\) are thick tailed local scales that allow some of the \\(j\\) parameters to escape shrinkage. More recently, Piironen, Vehtari, and others (2017) have argued that whilst the above horseshoe priors do guarantee that strong effects (parameters) will not be over-shrunk, there is the potential for weekly identified effects (those based on relatively little data) to be misrepresented in the posteriors. As an alternative they advocated the use of regularised horseshoe priors in which the amount of shrinkage applied to the largest effects can be controlled. The prior is defined as:\n\\[ \\beta_j \\sim N(0,\\tau^2 \\tilde{\\lambda}_j^2),\\]\nwhere \\(\\tilde{\\lambda}_j^2 = \\frac{c^2\\lambda^2_j}{c^2+\\tau^2 \\lambda^2_j}\\) and \\(c\\) is (slab width, actually variance) is a constant. For small effects (when \\(\\tau^2 \\lambda^2_j \u0026lt; c^2\\)) the prior approaches a regular prior. However, for large effects (when \\(\\tau^2 \\lambda^2_j \u0026gt; c^2\\)) the prior approaches \\(N(0,c^2)\\). Finally, they recommend applying a inverse-gamma prior on \\(c^2\\):\n\\[ c^2 \\sim \\text{Inv-Gamma}(\\alpha,\\beta),\\]\nwhere \\(\\alpha=v/2\\) and \\(\\beta=vs^2/2\\), which translates to a \\(\\text{Student-t}_ν(0, s^2)\\) slab for the coefficients far from zero and is typically a good default choice for a weakly informative prior.\n\rReferences\rGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9.\n\rGelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” The Annals of Statistics 33 (1): 1–53.\n\rPiironen, Juho, Aki Vehtari, and others. 2017. “Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors.” Electronic Journal of Statistics 11 (2): 5018–51.\n\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1580868794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580868794,"objectID":"26e718557fc6afe39e30b8e0083bc57f","permalink":"/jags/single-factor-anova-jags/single-factor-anova-jags/","publishdate":"2020-02-04T21:13:14-05:00","relpermalink":"/jags/single-factor-anova-jags/single-factor-anova-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","anova","factor analysis"],"title":"Single Factor Anova - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","anova","STAN","factor analysis"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rSingle factor Analysis of Variance (ANOVA), also known as single factor classification, is used to investigate the effect of a single factor comprising two or more groups (treatment levels) from a completely randomised design. Completely randomised refers to the absence of restrictions on the random allocation of experimental or sampling units to factor levels.\nFor example, consider a situation in which three types of treatments (A, B and C) are applied to replicate sampling units across the sampling domain. Importantly, the treatments are applied at the scale of the sampling units and the treatments applied to each sampling unit do not extend to any other neighbouring sampling units. Another possible situation is where the scale of a treatment is far larger than that of a sampling unit. This design features two treatments, each replicated three times. Note that additional sampling units within each Site (the scale at which the treatment occurs) would NOT constitute additional replication. Rather, these would be sub-replicates. That is, they would be replicates of the Sites, not the treatments (since the treatments occur at the level of whole sites). In order to genuinely increase the number of replicates, it is necessary to have more Sites. The random allocation of sampling units within the sampling domain (such as population) is appropriate provided either the underlying response is reasonably homogenous throughout the domain, or else, there is a large number of sampling units. If the conditions are relatively hetrogenous, then the exact location of the sampling units is likely to be highly influential and may mask any detectable effects of treatments.\n\rFixed and random effects\rFrom a frequentist perspective, fixed factors are factors whose levels represent the specific populations of interest. For example, a factor that comprises “high”, “medium” and “low” temperature treatments is a fixed factor - we are only interested in comparing those three populations. Conclusions about the effects of a fixed factor are restricted to the specific treatment levels investigated and for any subsequent experiments to be comparable, the same specific treatments of the factor would need to be used. By contrast, random factors are factors whose levels are randomly chosen from all the possible levels of populations and are used as random representatives of the populations. For example, five random temperature treatments could be used to represent a full spectrum of temperature treatments. In this case, conclusions are extrapolated to all the possible treatment (temperature) levels and for subsequent experiments, a new random set of treatments of the factor would be selected.\nOther common examples of random factors include sites and subjects - factors for which we are attempting to generalise over. Furthermore, the nature of random factors means that we have no indication of how a new level of that factor (such as another subject or site) are likely to respond and thus it is not possible to predict new observations from random factors. These differences between fixed and random factors are reflected in the way their respective null hypotheses are formulated and interpreted. Whilst fixed factors contrast the effects of the different levels of the factor, random factors are modelled as the amount of additional variability they introduce. Random factors are modelled with a mean of \\(0\\) and their variance is estimated as the effect coefficient.\n\rLinear model\rThe linear model for single factor classification is similar to that of multiple linear regression. The linear model can thus be represented by either:\n\rMeans parameterisation - in which the regression slopes represent the means of each treatment group and the intercept is removed (to prevent over-parameterisation).\r\r\\[ y_{ij} = \\beta_1(\\text{level}_1)_{ij} + \\beta_2(\\text{level}_2)_{ij} + \\ldots + \\epsilon_{ij},\\]\nwhere \\(\\beta_1\\) and \\(\\beta_2\\) respectively represent the means response of treatment level \\(1\\) and \\(2\\). This is often simplified to \\(y_{ij}=\\alpha_i + \\epsilon_{ij}\\).\n\rEffects parameterisation - the intercept represents a property such as the mean of one of the treatment groups (treatment contrasts) or the overall mean (sum contrasts), and the slope parameters represent effects (differences between each other group and the reference mean for example).\r\r\\[ y_{ij} = \\mu + \\beta_2(\\text{level}_2)_{ij} + \\beta_3(\\text{level}_3)_{ij} + \\ldots + \\epsilon_{ij},\\]\nwhere \\(\\mu\\) is the mean of the first treatment group, \\(\\beta_2\\) and \\(\\beta_3\\) respectively represent the effects (change from level \\(1\\)) of level \\(2\\) and \\(3\\) on the mean response. This is often simplified to: \\(y_{ij}=\\mu + \\alpha_i + \\epsilon_{ij}\\), with \\(\\alpha_1=0\\).\nSince we are traditionally interested in investigating effects (differences) rather than treatment means, effects parameterisation is far more common (particularly when coupled with hypothesis testing). In a Bayesian framework, it does not really matter whether models are fit with means or effects parameterisation since the posterior likelihood can be querried in any way and repeatedly - thus enabling us to explore any specific effects after the model has been fit. Nevertheless, to ease comparisons with frequentist approaches, we will stick with effects paramterisation.\n\rNull hypothesis: fixed factor\rWe can associate a null hypothesis test with each estimated parameter. For example, in a cell for each estimated mean in a means model we could test a null hypothesis that the population mean is equal to zero (e.g. \\(H_0\\): \\(\\alpha_1=0\\), \\(H_0\\): \\(\\alpha_2=0\\), \\(\\ldots\\)). However, this rarely would be of much interest. By contrast, individual null hypotheses associated with each parameter of the effects model can be used to investigate the differences between each group and a reference group (for example). In addition to the individual null hypothesis tests, a single fixed factor ANOVA tests the collective \\(H_0\\) that there are no differences between the population group means:\n\r\\(H_0: \\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\) (the population group means are all equal). That is, that the mean of population \\(1\\) is equal to that of population \\(2\\) and so on, and thus all population means are equal to one another - no effect of the factor on the response. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the mean of the first group (\\(\\alpha_i=\\mu_i-\\mu_1\\)) then the \\(H_0\\) can alternatively be written as:\n\r\\(H_0 : \\alpha_2=\\alpha_3=\\ldots=\\alpha_i=0\\) (the effect of each group equals zero). If one or more of the \\(\\alpha_i\\) are different from zero (the response mean for this treatment differs from the overall response mean), there is evidence that the null hypothesis is not true indicating that the factor does affect the response variable.\n\r\r\rNull hypothesis: random factor\rThe collective \\(H_0\\) for a random factor is that the variance between all possible treatment groups equals zero:\n\r\\(H_0 : \\sigma^2_{\\alpha}=0\\) (added variance due to this factor equals zero).\r\rNote that whilst the null hypotheses for fixed and random factors are different (fixed: population group means all equal, random: variances between populations all equal zero), the linear model fitted for fixed and random factors in single factor ANOVA models is identical. For more complex multi-factor ANOVA models however, the distinction between fixed and random factors has important consequences for building and interpreting statistical models and null hypotheses.\n\rAnalysis of variance\rWhen the null hypothesis is true (and the populations are identical), the amount of variation among observations within groups should be similar to the amount of variation in observations between groups. However, when the null hypothesis is false (and some means are different from other means), the amount of variation among observations might be expected to be less than the amount of variation within groups. Analysis of variance, or ANOVA, partitions the total variance in the response (dependent) variable into a component of the variance that is explained by combinations of one or more categorical predictor variables (called factors) and a component of the variance that cannot be explained (residual). The variance ratio (F-ratio) from this partitioning can then be used to test the null hypothesis (\\(H_0\\)) that the population group or treatment means are all equal. Ttotal variation can be decomposed into components explained by the groups (\\(MS_{groups}\\)) and and unexplained (\\(MS_{residual}\\)) by the groups. The gray arrows in b) depict the relative amounts explained by the groups. The proposed groupings generally explain why the first few points are higher on the y-axis than the last three points. The probability of collecting our sample, and thus generating the sample ratio of explained to unexplained variation (or one more extreme), when the null hypothesis is true (and population means are equal) is the area under the F-distribution beyond our sample ratio (\\(\\text{F-ratio}=\\frac{MS_{groups}}{MS_{residual}}\\)).\nWhen the null hypothesis is true (and the test assumptions have not been violated), the ratio (F-ratio) of explained to unexplained variance follows a theoretical probability distribution (F-distribution). When the null hypothesis is true, and there is no effect of the treatment on the response variable, the ratio of explained variability to unexplained variability is expected to be \\(\\leq 1\\). Since the denominator should represent the expected numerator in the absence of an effect. Importantly, the denominator in an F-ratio calculation essentially represents what we would expect the numerator to be in the absence of a treatment effect. For simple analyses, identifying what these expected values are is relatively straightforward (equivalent to the degree of within group variability). However, in more complex designs (particularly involving random factors and hierarchical treatment levels), the logical “groups” can be more difficult (and in some cases impossible) to identify. In such cases, nominating the appropriate F-ratio denominator for estimating an specific effect requires careful consideration. The following table depicts the anatomy of the single factor ANOVA table\n\u0026gt; anova_table\rdf MS F-ratio Factor A \u0026quot;a-1\u0026quot; \u0026quot;MS A\u0026quot; \u0026quot;(MS A)/(MS res)\u0026quot;\rResidual \u0026quot;(n-1)a\u0026quot; \u0026quot;MS res\u0026quot; \u0026quot;\u0026quot; \rand corresponding R syntax.\n\u0026gt; anova(lm(DV ~ A, dataset))\r\u0026gt; # OR\r\u0026gt; anova(aov(DV ~ A, dataset))\rAn F-ratio substantially greater than \\(1\\) suggests that the model relating the response variable to the categorical variable explains substantially more variability than is left unexplained. In turn, this implies that the linear model does represent the data well and that differences between observations can be explained largely by differences in treatment levels rather than purely the result of random variation. If the probability of getting the observed (sample) F-ratio or one more extreme is less than some predefined critical value (typically \\(5\\)% or \\(0.05\\)), we conclude that it is highly unlikely that the observed samples could have been collected from populations in which the treatment has no effect and therefore we would reject the null hypothesis.\n\rAssumptions\rAn F-ratio from real data can only reliably relate to a theoretical F-distribution when the data conform to certain assumptions. Hypothesis testing for a single factor ANOVA model assumes that the residuals (and therefore the response variable for each of the treatment levels) are all:\n\rnormally distributed - although ANOVA is robust to non-normality provided sample sizes and variances are equal. Boxplots should be used to explore normality, skewness, bimodality and outliers. In the event of homogeneity of variance issues (see below), a Q-Q normal plot can also be useful for exploring normality (as this might be the cause of non-homogeneity). Scale transformations are often useful.\n\requally varied - provided sample sizes are equal and the largest to smallest variance ratio does not exceed 3:1 (9:1 for sd), ANOVA is reasonably robust to this assumption, however, relationships between variance and mean and/or sample size are of particular concern as they elevate the Type I error rate. Boxplots and plots of means against variance should be used to explore the spread of values. Residual plots should reveal no patterns. Since unequal variances are often the result of non-normality, transformations that improve normality will also improve variance homogeneity.\n\rindependent of one another - this assumption must be addressed at the design and collection stages and cannot be compensated for later (unless a model is used that specifically accounts for particular types of non-independent data, such as that introduced with hierarchical designs or autocorrelation)\n\r\rViolations of these assumptions reduce the reliability of the analysis.\n\r\rData generation\rLets say we had set up a natural experiment in which we measured a response from \\(10\\) sampling units (replicates) from each of \\(5\\) treatments. Hence, we have a single categorical factor with \\(5\\) levels - we might have five different locations, or five different habitat types or substrates etc. In statistical speak, we have sampled from \\(5\\) different populations. We have then randomly selected \\(10\\) independent and random (representative) units of each population to sample. That is, we have \\(10\\) samples (replicates) of each population. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(123)\r\u0026gt; ngroups \u0026lt;- 5 #number of populations\r\u0026gt; nsample \u0026lt;- 10 #number of reps in each\r\u0026gt; pop.means \u0026lt;- c(40, 45, 55, 40, 30) #population mean length\r\u0026gt; sigma \u0026lt;- 3 #residual standard deviation\r\u0026gt; n \u0026lt;- ngroups * nsample #total sample size\r\u0026gt; eps \u0026lt;- rnorm(n, 0, sigma) #residuals\r\u0026gt; x \u0026lt;- gl(ngroups, nsample, n, lab = LETTERS[1:5]) #factor\r\u0026gt; means \u0026lt;- rep(pop.means, rep(nsample, ngroups))\r\u0026gt; X \u0026lt;- model.matrix(~x - 1) #create a design matrix\r\u0026gt; y \u0026lt;- as.numeric(X %*% pop.means + eps)\r\u0026gt; data \u0026lt;- data.frame(y, x)\r\u0026gt; head(data) #print out the first six rows of the data set\ry x\r1 38.31857 A\r2 39.30947 A\r3 44.67612 A\r4 40.21153 A\r5 40.38786 A\r6 45.14519 A\r\u0026gt; \u0026gt; write.csv(data, \u0026quot;simpleAnova.csv\u0026quot;)\rWith these sort of data, we are primarily interested in investigating whether there is a relationship between the continuous response variable and the treatment type.\nExploratory data analysis\r\rNormality and Homogeneity of variance\r\r\u0026gt; boxplot(y ~ x, data)\r\u0026gt; \u0026gt; # OR via ggplot2\r\u0026gt; library(ggplot2)\r\u0026gt; ggplot(data, aes(y = y, x = x)) + geom_boxplot() +\r+ theme_classic()\rConclusions\nThere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical. There is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity. Obvious violations could be addressed either by, for example, transforming the scale of the response variables (to address normality etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\n\r\rModel fitting\rThe observed response (\\(y_i\\)) are assumed to be drawn from a normal distribution with a given mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The expected values (\\(\\mu\\)) are themselves determined by the linear predictor (\\(\\beta_0+\\boldsymbol \\beta \\boldsymbol X_i\\)). In this case, \\(\\beta_0\\) represents the mean of the first group and the set of \\(\\boldsymbol \\beta\\)’s represent the differences between each other group and the first group. MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying ‘uninformative’ priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (\\(100\\)) for both the intercept and the treatment effect and a wide half-cauchy (\\(\\text{scale}=5\\)) for the standard deviation.\n\\[y_i \\sim N(\\mu_i,\\sigma), \\]\nwhere \\(\\mu_i=\\beta_0 +\\boldsymbol \\beta \\boldsymbol X_i\\). The assumed priors are: \\(\\beta \\sim N(0,100)\\) and \\(\\sigma \\sim \\text{Cauchy}(0,5)\\). We proceed to code the model into STAN.\n\u0026gt; modelString = \u0026quot;\r+ data {\r+ int\u0026lt;lower=1\u0026gt; n;\r+ int\u0026lt;lower=1\u0026gt; nX;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ }\r+ parameters {\r+ vector[nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ }\r+ transformed parameters {\r+ vector[n] mu;\r+ + mu = X*beta;\r+ }\r+ model {\r+ //Likelihood\r+ y~normal(mu,sigma);\r+ + //Priors\r+ beta ~ normal(0,1000);\r+ sigma~cauchy(0,5);\r+ }\r+ generated quantities {\r+ vector[n] log_lik;\r+ + for (i in 1:n) {\r+ log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); + }\r+ }\r+ + \u0026quot;\r\u0026gt; ## write the model to a stan file \u0026gt; writeLines(modelString, con = \u0026quot;anovaModel.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; Xmat \u0026lt;- model.matrix(~x, data)\r\u0026gt; data.list \u0026lt;- with(data, list(y = y, X = Xmat, nX = ncol(Xmat), n = nrow(data)))\rDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;log_lik\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 500\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 2000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 1500\rNow compile and run the Stan code via the rstan interface. Note that the first time stan is run after the rstan package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; library(rstan)\rDuring the warmup stage, the No-U-Turn sampler (NUTS) attempts to determine the optimum stepsize - the stepsize that achieves the target acceptance rate (\\(0.8\\) or \\(80\\)% by default) without divergence (occurs when the stepsize is too large relative to the curvature of the log posterior and results in approximations that are likely to diverge and be biased) - and without hitting the maximum treedepth (\\(10\\)). At each iteration of the NUTS algorithm, the number of leapfrog steps doubles (as it increases the treedepth) and only terminates when either the NUTS criterion are satisfied or the tree depth reaches the maximum (\\(10\\) by default).\n\u0026gt; data.rstan \u0026lt;- stan(data = data.list, file = \u0026quot;anovaModel.stan\u0026quot;, chains = nChains, pars = params,\r+ iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;anovaModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 1: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 1: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 1: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 1: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 1: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 1: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 1: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 1: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 1: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 1: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 1: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.05 seconds (Warm-up)\rChain 1: 0.055 seconds (Sampling)\rChain 1: 0.105 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;anovaModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 2: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 2: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 2: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 2: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 2: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 2: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 2: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 2: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 2: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 2: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 2: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.049 seconds (Warm-up)\rChain 2: 0.063 seconds (Sampling)\rChain 2: 0.112 seconds (Total)\rChain 2: \u0026gt; \u0026gt; print(data.rstan, par = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: anovaModel.\r2 chains, each with iter=1500; warmup=500; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 40.24 0.04 0.89 38.45 39.69 40.24 40.87 41.93 634 1.01\rbeta[2] 5.38 0.04 1.25 2.97 4.54 5.36 6.18 7.86 833 1.00\rbeta[3] 13.48 0.04 1.29 10.89 12.64 13.41 14.34 15.97 888 1.00\rbeta[4] 0.70 0.04 1.25 -1.81 -0.13 0.69 1.56 3.06 949 1.01\rbeta[5] -10.27 0.04 1.25 -12.57 -11.13 -10.30 -9.40 -7.85 817 1.00\rsigma 2.85 0.01 0.31 2.34 2.64 2.83 3.04 3.53 1108 1.00\rSamples were drawn using NUTS(diag_e) at Mon Feb 17 11:28:36 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots for the effects model as an example.\n\u0026gt; library(mcmcplots)\r\u0026gt; s = as.array(data.rstan)\r\u0026gt; mcmc \u0026lt;- do.call(mcmc.list, plyr:::alply(s[, , -(length(s[1, 1, ]))], 2, as.mcmc))\r\u0026gt; denplot(mcmc, parms = c(\u0026quot;beta\u0026quot;,\u0026quot;sigma\u0026quot;))\r\u0026gt; traplot(mcmc, parms = c(\u0026quot;beta\u0026quot;,\u0026quot;sigma\u0026quot;))\rThese plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.\n\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(mcmc)\r$`1`\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r$`2`\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; stan_ac(data.rstan, pars = c(\u0026quot;beta\u0026quot;))\rA lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).\n\u0026gt; stan_rhat(data.rstan, pars = c(\u0026quot;beta\u0026quot;))\r\u0026gt; stan_ess(data.rstan, pars = c(\u0026quot;beta\u0026quot;))\rRhat and effective sample size. In this instance, most of the parameters have reasonably high effective samples and thus there is likely to be a good range of values from which to estimate paramter properties.\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. Ideally, a good model should also be able to predict the data used to fit the model. Residuals are not computed directly within rstan However, we can calculate them manually form the posteriors.\n\u0026gt; library(dplyr)\r\u0026gt; mcmc = as.data.frame(data.rstan) %\u0026gt;% dplyr:::select(contains(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:5], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\rResiduals against predictors\n\u0026gt; library(tidyr)\r\u0026gt; mcmc = as.data.frame(data.rstan) %\u0026gt;% dplyr:::select(contains(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = newdata\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:5], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; newdata = newdata %\u0026gt;% cbind(fit, resid)\r\u0026gt; ggplot(newdata) + geom_point(aes(y = resid, x = x)) + theme_classic()\rAnd now for studentised residuals\n\u0026gt; mcmc = as.data.frame(data.rstan) %\u0026gt;% dplyr:::select(contains(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:5], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; sresid = resid/sd(resid)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit)) + theme_classic()\rFor this simple model, the studentized residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = as.data.frame(data.rstan) %\u0026gt;% dplyr:::select(contains(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~x, data)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, 1:5]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i,\r+ ], mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; newdata = data.frame(x = data$x, yRep) %\u0026gt;% gather(key = Sample,\r+ value = Value, -x)\r\u0026gt; ggplot(newdata) + geom_violin(aes(y = Value, x = x, fill = \u0026quot;Model\u0026quot;),\r+ alpha = 0.5) + geom_violin(data = data, aes(y = y, x = x,\r+ fill = \u0026quot;Obs\u0026quot;), alpha = 0.5) + geom_point(data = data, aes(y = y,\r+ x = x), position = position_jitter(width = 0.1, height = 0),\r+ color = \u0026quot;black\u0026quot;) + theme_classic()\rThe predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.\n\u0026gt; library(bayesplot)\r\u0026gt; mcmc_intervals(as.matrix(data.rstan), regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\u0026gt; mcmc_areas(as.matrix(data.rstan), regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\rParameter estimates\rAlthough all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and \\(95\\)% credibility intervals.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\rFirst, we look at the results from the additive model.\n\u0026gt; print(data.rstan, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: anovaModel.\r2 chains, each with iter=1500; warmup=500; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 40.24 0.04 0.89 38.45 39.69 40.24 40.87 41.93 634 1.01\rbeta[2] 5.38 0.04 1.25 2.97 4.54 5.36 6.18 7.86 833 1.00\rbeta[3] 13.48 0.04 1.29 10.89 12.64 13.41 14.34 15.97 888 1.00\rbeta[4] 0.70 0.04 1.25 -1.81 -0.13 0.69 1.56 3.06 949 1.01\rbeta[5] -10.27 0.04 1.25 -12.57 -11.13 -10.30 -9.40 -7.85 817 1.00\rsigma 2.85 0.01 0.31 2.34 2.64 2.83 3.04 3.53 1108 1.00\rSamples were drawn using NUTS(diag_e) at Mon Feb 17 11:28:36 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(data.rstan, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\r# A tibble: 6 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 40.2 0.895 38.4 41.9 2 beta[2] 5.38 1.25 3.09 7.96\r3 beta[3] 13.5 1.29 11.0 16.1 4 beta[4] 0.703 1.25 -1.94 2.90\r5 beta[5] -10.3 1.25 -12.6 -7.85\r6 sigma 2.85 0.306 2.33 3.50\rConclusions\n\rthe mean of the first group (A) is \\(40.2\\)\rthe mean of the second group (B) is \\(5.4\\) units greater than (A)\rthe mean of the third group (C) is \\(13.5\\) units greater than (A)\rthe mean of the forth group (D) is \\(0.74\\) units greater than (A)\rthe mean of the fifth group (E) is \\(-10.2\\) units greater (i.e. less) than (A)\r\rThe \\(95\\)% confidence interval for the effects of B, C and E do not overlap with \\(0\\) implying a significant difference between group A and groups B, C and E. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, \u0026quot;beta[2]\u0026quot;]) # effect of (B-A)\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, \u0026quot;beta[3]\u0026quot;]) # effect of (C-A)\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, \u0026quot;beta[4]\u0026quot;]) # effect of (D-A)\r[1] 0.5805\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, \u0026quot;beta[5]\u0026quot;]) # effect of (E-A)\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, 2:5]) # effect of (all groups)\r[1] 0\rThere is evidence that the reponse differs between the groups. There is evidence suggesting that the response of group D differs from that of group A. In a Bayesian context, we can compare models using the leave-one-out cross-validation statistics. Leave-one-out (LOO) cross-validation explores how well a series of models can predict withheld values Vehtari, Gelman, and Gabry (2017). The LOO Information Criterion (LOOIC) is analogous to the AIC except that the LOOIC takes priors into consideration, does not assume that the posterior distribution is drawn from a multivariate normal and integrates over parameter uncertainty so as to yield a distribution of looic rather than just a point estimate. The LOOIC does however assume that all observations are equally influential (it does not matter which observations are left out). This assumption can be examined via the Pareto \\(k\\) estimate (values greater than \\(0.5\\) or more conservatively \\(0.75\\) are considered overly influential). We can compute LOOIC if we store the loglikelihood from our STAN model, which can then be extracted to compute the information criterion using the package loo.\n\u0026gt; library(loo)\r\u0026gt; (full = loo(extract_log_lik(data.rstan)))\rComputed from 2000 by 50 log-likelihood matrix\rEstimate SE\relpd_loo -125.8 5.1\rp_loo 5.6 1.1\rlooic 251.6 10.2\r------\rMonte Carlo SE of elpd_loo is 0.1.\rAll Pareto k estimates are good (k \u0026lt; 0.5).\rSee help(\u0026#39;pareto-k-diagnostic\u0026#39;) for details.\r\u0026gt; \u0026gt; # now fit a model without main factor\r\u0026gt; modelString2 = \u0026quot;\r+ data {\r+ int\u0026lt;lower=1\u0026gt; n;\r+ int\u0026lt;lower=1\u0026gt; nX;\r+ vector [n] y;\r+ matrix [n,nX] X;\r+ }\r+ parameters {\r+ vector[nX] beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ }\r+ transformed parameters {\r+ vector[n] mu;\r+ + mu = X*beta;\r+ }\r+ model {\r+ //Likelihood\r+ y~normal(mu,sigma);\r+ + //Priors\r+ beta ~ normal(0,1000);\r+ sigma~cauchy(0,5);\r+ }\r+ generated quantities {\r+ vector[n] log_lik;\r+ + for (i in 1:n) {\r+ log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); + }\r+ }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a stan file \u0026gt; writeLines(modelString2, con = \u0026quot;anovaModel2.stan\u0026quot;)\r\u0026gt; \u0026gt; Xmat \u0026lt;- model.matrix(~1, data)\r\u0026gt; data.list \u0026lt;- with(data, list(y = y, X = Xmat, n = nrow(data), nX = ncol(Xmat)))\r\u0026gt; data.rstan.red \u0026lt;- stan(data = data.list, file = \u0026quot;anovaModel2.stan\u0026quot;, chains = nChains,\r+ iter = nIter, warmup = burnInSteps, thin = thinSteps)\rSAMPLING FOR MODEL \u0026#39;anovaModel\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 1: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 1: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 1: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 1: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 1: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 1: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 1: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 1: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 1: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 1: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 1: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.019 seconds (Warm-up)\rChain 1: 0.042 seconds (Sampling)\rChain 1: 0.061 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;anovaModel\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 1500 [ 0%] (Warmup)\rChain 2: Iteration: 150 / 1500 [ 10%] (Warmup)\rChain 2: Iteration: 300 / 1500 [ 20%] (Warmup)\rChain 2: Iteration: 450 / 1500 [ 30%] (Warmup)\rChain 2: Iteration: 501 / 1500 [ 33%] (Sampling)\rChain 2: Iteration: 650 / 1500 [ 43%] (Sampling)\rChain 2: Iteration: 800 / 1500 [ 53%] (Sampling)\rChain 2: Iteration: 950 / 1500 [ 63%] (Sampling)\rChain 2: Iteration: 1100 / 1500 [ 73%] (Sampling)\rChain 2: Iteration: 1250 / 1500 [ 83%] (Sampling)\rChain 2: Iteration: 1400 / 1500 [ 93%] (Sampling)\rChain 2: Iteration: 1500 / 1500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.021 seconds (Warm-up)\rChain 2: 0.094 seconds (Sampling)\rChain 2: 0.115 seconds (Total)\rChain 2: \u0026gt; \u0026gt; (reduced = loo(extract_log_lik(data.rstan.red)))\rComputed from 2000 by 50 log-likelihood matrix\rEstimate SE\relpd_loo -177.8 4.4\rp_loo 1.6 0.3\rlooic 355.6 8.7\r------\rMonte Carlo SE of elpd_loo is 0.0.\rAll Pareto k estimates are good (k \u0026lt; 0.5).\rSee help(\u0026#39;pareto-k-diagnostic\u0026#39;) for details.\r\u0026gt; \u0026gt; par(mfrow = 1:2, mar = c(5, 3.8, 1, 0) + 0.1, las = 3)\r\u0026gt; plot(full, label_points = TRUE)\r\u0026gt; plot(reduced, label_points = TRUE)\rThe expected out-of-sample predictive accuracy is substantially lower for the model that includes \\(x\\). This might be used to suggest that the inferential evidence for a general effect of \\(x\\) on \\(y\\).\n\rGraphical summaries\rWith appropriate use of model matrices and data wrangling, it is possible to produce a single prediction data set along with ggplot syntax to produce a multi-panel figure. First we look at the additive model.\n\u0026gt; mcmc = as.matrix(data.rstan)\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = rbind(data.frame(x = levels(data$x)))\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;, \u0026quot;beta[3]\u0026quot;, \u0026quot;beta[4]\u0026quot;, \u0026quot;beta[5]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_linerange(aes(ymin = conf.low,\r+ ymax = conf.high)) + geom_point() + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) +\r+ theme_classic()\rAs this is simple single factor ANOVA, we can simple add the raw data to this figure. For more complex designs with additional predictors, it is necessary to plot partial residuals.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = data\r\u0026gt; fMat = rMat = model.matrix(~x, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = as.numeric(x) - 0.1)) + geom_blank(aes(x = x)) +\r+ geom_point(data = rdata, aes(y = partial.resid, x = as.numeric(x) +\r+ 0.1), color = \u0026quot;gray\u0026quot;) + geom_linerange(aes(ymin = conf.low, ymax = conf.high)) +\r+ geom_point() + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;\u0026quot;) + theme_classic()\r\rPosteriors\rIn frequentist statistics, when we have more than two groups, we are typically not only interested in whether there is evidence for an overall “effect” of a factor - we are also interested in how various groups compare to one another. To explore these trends, we either compare each group to each other in a pairwise manner (controlling for family-wise Type I error rates) or we explore an independent subset of the possible comparisons. Although these alternate approaches can adequately address a specific research agenda, often they impose severe limitations and compromises on the scope and breadth of questions that can be asked of your data. The reason for these limitations is that in a frequentist framework, any single hypothesis carries with it a (nominally) \\(5\\)% chance of a false rejection (since it is based on long-run frequency). Thus, performing multiple tests are likely to compound this error rate. The point is, that each comparison is compared to its own probability distribution (and each carries a \\(5\\)% error rate). By contrast, in Bayesian statistics, all comparisons (contrasts) are drawn from the one (hopefully stable and convergent) posterior distribution and this posterior is invariant to the type and number of comparisons drawn. Hence, the theory clearly indicates that having generated our posterior distribution, we can then query this distribution in any way that we wish thereby allowing us to explore all of our research questions simultaneously.\nBayesian “contrasts” can be performed either:\n\rwithin the Bayesian sampling model or\n\rconstruct them from the returned MCMC samples (they are drawn from the posteriors)\n\r\rOnly the latter will be demonstrated as it povides a consistent approach across all routines. In order to allow direct comparison to the frequentist equivalents, I will explore the same set of planned and Tukey’s test comparisons described here. For the “planned comparison” we defined two contrasts: 1) group 3 vs group 5; and 2) the average of groups 1 and 2 vs the average of groups 3, 4 and 5.\nLets start by comparing each group to each other group in a pairwise manner. Arguably the most elegant way to do this is to generate a Tukey’s contrast matrix. This is a model matrix specific to comparing each group to each other group.\n\u0026gt; mcmc = data.rstan\r\u0026gt; coefs \u0026lt;- as.matrix(mcmc)[, 1:5]\r\u0026gt; newdata \u0026lt;- data.frame(x = levels(data$x))\r\u0026gt; # A Tukeys contrast matrix\r\u0026gt; library(multcomp)\r\u0026gt; # table(newdata$x) - gets the number of replicates of each level\r\u0026gt; tuk.mat \u0026lt;- contrMat(n = table(newdata$x), type = \u0026quot;Tukey\u0026quot;)\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data = newdata)\r\u0026gt; pairwise.mat \u0026lt;- tuk.mat %*% Xmat\r\u0026gt; pairwise.mat\r(Intercept) xB xC xD xE\rB - A 0 1 0 0 0\rC - A 0 0 1 0 0\rD - A 0 0 0 1 0\rE - A 0 0 0 0 1\rC - B 0 -1 1 0 0\rD - B 0 -1 0 1 0\rE - B 0 -1 0 0 1\rD - C 0 0 -1 1 0\rE - C 0 0 -1 0 1\rE - D 0 0 0 -1 1\r\u0026gt; \u0026gt; mcmc_areas(coefs %*% t(pairwise.mat))\r\u0026gt; \u0026gt; (comps = tidyMCMC(coefs %*% t(pairwise.mat), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 10 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 B - A 5.38 1.25 3.09 7.96\r2 C - A 13.5 1.29 11.0 16.1 3 D - A 0.703 1.25 -1.94 2.90\r4 E - A -10.3 1.25 -12.6 -7.85\r5 C - B 8.10 1.28 5.71 10.8 6 D - B -4.68 1.26 -7.06 -2.21\r7 E - B -15.6 1.25 -18.3 -13.4 8 D - C -12.8 1.29 -15.3 -10.1 9 E - C -23.7 1.31 -26.2 -21.0 10 E - D -11.0 1.27 -13.5 -8.63\r\u0026gt; \u0026gt; ggplot(comps, aes(y = estimate, x = term)) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;) +\r+ scale_y_continuous(\u0026quot;Effect size\u0026quot;) + scale_x_discrete(\u0026quot;\u0026quot;) + coord_flip() +\r+ theme_classic()\rWith a couple of modifications, we could also express this as percentage changes. A percentage change represents the change (difference between groups) divided by one of the groups (determined by which group you want to express the percentage change to). Hence, we generate an additional mcmc matrix that represents the cell means for the divisor group (group we want to express change relative to). Since the tuk.mat defines comparisons as \\(-1\\) and \\(1\\) pairs, if we simply replace all the \\(-1\\) with \\(0\\), the eventual matrix multiplication will result in estimates of the divisor cell means instread of the difference. We can then divide the original mcmc matrix above with this new divisor mcmc matrix to yeild a mcmc matrix of percentage change.\n\u0026gt; # Modify the tuk.mat to replace -1 with 0. This will allow us to get a\r\u0026gt; # mcmc matrix of ..\r\u0026gt; tuk.mat[tuk.mat == -1] = 0\r\u0026gt; comp.mat \u0026lt;- tuk.mat %*% Xmat\r\u0026gt; comp.mat\r(Intercept) xB xC xD xE\rB - A 1 1 0 0 0\rC - A 1 0 1 0 0\rD - A 1 0 0 1 0\rE - A 1 0 0 0 1\rC - B 1 0 1 0 0\rD - B 1 0 0 1 0\rE - B 1 0 0 0 1\rD - C 1 0 0 1 0\rE - C 1 0 0 0 1\rE - D 1 0 0 0 1\r\u0026gt; \u0026gt; comp.mcmc = 100 * (coefs %*% t(pairwise.mat))/coefs %*% t(comp.mat)\r\u0026gt; (comps = tidyMCMC(comp.mcmc, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 10 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 B - A 11.8 2.59 6.40 16.5 2 C - A 25.1 2.11 20.9 29.1 3 D - A 1.67 3.02 -4.53 7.24\r4 E - A -34.4 4.95 -43.7 -24.9 5 C - B 15.1 2.20 10.9 19.6 6 D - B -11.5 3.27 -17.7 -5.09\r7 E - B -52.3 5.36 -62.5 -41.7 8 D - C -31.3 3.64 -38.4 -23.7 9 E - C -79.4 6.27 -90.6 -66.2 10 E - D -36.7 5.06 -46.9 -27.4 \u0026gt; \u0026gt; ggplot(comps, aes(y = estimate, x = term)) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;) +\r+ scale_y_continuous(\u0026quot;Effect size (%)\u0026quot;) + scale_x_discrete(\u0026quot;\u0026quot;) + coord_flip() +\r+ theme_classic()\rAnd now for the specific planned comparisons (Group 3 vs Group 5 and the average of Groups 1 and 2 vs the average of Groups 3, 4 and 5). This is achieved by generating our own contrast matrix (defining the contributions of each group to each contrast).\n\u0026gt; c.mat = rbind(c(0, 0, -1, 0, 1), c(-1/2, -1/2, 1/3, 1/3, 1/3))\r\u0026gt; c.mat\r[,1] [,2] [,3] [,4] [,5]\r[1,] 0.0 0.0 -1.0000000 0.0000000 1.0000000\r[2,] -0.5 -0.5 0.3333333 0.3333333 0.3333333\r\u0026gt; \u0026gt; mcmc = data.rstan\r\u0026gt; coefs \u0026lt;- as.matrix(mcmc)[, 1:5]\r\u0026gt; newdata \u0026lt;- data.frame(x = levels(data$x))\r\u0026gt; Xmat \u0026lt;- model.matrix(~x, data = newdata)\r\u0026gt; c.mat = c.mat %*% Xmat\r\u0026gt; c.mat\r(Intercept) xB xC xD xE\r[1,] 0.000000e+00 0.0 -1.0000000 0.0000000 1.0000000\r[2,] -1.110223e-16 -0.5 0.3333333 0.3333333 0.3333333\r\u0026gt; \u0026gt; (comps = tidyMCMC(as.mcmc(coefs %*% t(c.mat)), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 -23.7 1.31 -26.2 -21.0 2 var2 -1.38 0.806 -2.92 0.186\r\rFinite population standard deviations\rVariance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).\nFinite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (Gelman and others (2005)). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.\n# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x 9.94 0.532 8.89 11.0 2 sd.resid 2.79 0.0888 2.67 2.96\r# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x 78.3 1.06 76.0 79.7\r2 sd.resid 21.7 1.06 20.3 24.0\rApproximately \\(78.3\\)% of the total finite population standard deviation is due to \\(x\\).\n\rR squared\rIn a frequentist context, the \\(R^2\\) value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, \\(R^2\\) is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an \\(R^2\\) greater than \\(100\\)%. Gelman et al. (2019) proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.\nSo in the standard regression model notation of:\n\\[ y_i \\sim \\text{Normal}(\\boldsymbol X \\boldsymbol \\beta, \\sigma),\\]\nthe \\(R^2\\) could be formulated as\n\\[ R^2 = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sigma^2_e},\\]\nwhere \\(\\sigma^2_f=\\text{var}(\\boldsymbol X \\boldsymbol \\beta)\\), and for normal models \\(\\sigma^2_e=\\text{var}(y-\\boldsymbol X \\boldsymbol \\beta)\\)\n\u0026gt; mcmc \u0026lt;- as.matrix(data.rstan)\r\u0026gt; Xmat = model.matrix(~x, data)\r\u0026gt; wch = grep(\u0026quot;beta\u0026quot;, colnames(mcmc))\r\u0026gt; coefs = mcmc[, wch]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = sweep(fit, 2, data$y, \u0026quot;-\u0026quot;)\r\u0026gt; var_f = apply(fit, 1, var)\r\u0026gt; var_e = apply(resid, 1, var)\r\u0026gt; R2 = var_f/(var_f + var_e)\r\u0026gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.887 0.0126 0.863 0.905\r\u0026gt; \u0026gt; # for comparison with frequentist\r\u0026gt; summary(lm(y ~ x, data))\rCall:\rlm(formula = y ~ x, data = data)\rResiduals:\rMin 1Q Median 3Q Max -6.5257 -1.9000 -0.2589 1.4935 6.5330 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 40.2239 0.8801 45.702 \u0026lt; 2e-16 ***\rxB 5.4020 1.2447 4.340 7.97e-05 ***\rxC 13.5024 1.2447 10.848 3.82e-14 ***\rxD 0.7423 1.2447 0.596 0.554 xE -10.2500 1.2447 -8.235 1.57e-10 ***\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 2.783 on 45 degrees of freedom\rMultiple R-squared: 0.8957, Adjusted R-squared: 0.8865 F-statistic: 96.64 on 4 and 45 DF, p-value: \u0026lt; 2.2e-16\r\rReferences\rGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9.\n\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rGelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” The Annals of Statistics 33 (1): 1–53.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\rVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and Waic.” Statistics and Computing 27 (5): 1413–32.\n\r\r\r","date":1580868794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580868794,"objectID":"dc919318a49b8d196424845dcf1791a5","permalink":"/stan/single-factor-anova-stan/single-factor-anova-stan/","publishdate":"2020-02-04T21:13:14-05:00","relpermalink":"/stan/single-factor-anova-stan/single-factor-anova-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","anova","factor analysis"],"title":"Single Factor Anova - STAN","type":"STAN"},{"authors":["Andrea Gabrio"],"categories":["R","linear regression","JAGS"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rMultiple regression is an extension of simple linear regression whereby a response variable is modelled against a linear combination of two or more simultaneously measured predictor variables. There are two main purposes of multiple linear regression:\nTo develop a better predictive model (equation) than is possible from models based on single independent variables.\n\rTo investigate the relative individual effects of each of the multiple independent variables above and beyond (standardised across) the effects of the other variables.\n\r\rAlthough the relationship between response variable and the additive effect of all the predictor variables is represented overall by a single multidimensional plane (surface), the individual effects of each of the predictor variables on the response variable (standardised across the other variables) can be depicted by single partial regression lines. The slope of any single partial regression line (partial regression slope) thereby represents the rate of change or effect of that specific predictor variable (holding all the other predictor variables constant to their respective mean values) on the response variable. In essence, it is the effect of one predictor variable at one specific level (the means) of all the other predictor variables (i.e. when each of the other predictors are set to their averages).\nMultiple regression models can be constructed additively (containing only the predictor variables themselves) or in a multiplicative design (which incorporate interactions between predictor variables in addition to the predictor variables themselves). Multiplicative models are used primarily for testing inferences about the effects of various predictor variables and their interactions on the response variable. Additive models by contrast are used for generating predictive models and estimating the relative importance of individual predictor variables more so than hypothesis testing.\n\rAdditive Model\r\\[ y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\ldots + \\beta_Jx_{iJ} + \\epsilon_i,\\]\nwhere \\(\\beta_0\\) is the population \\(y\\)-intercept (value of \\(y\\) when all partial slopes equal zero), \\(\\beta_1,\\beta_2,\\ldots,\\beta_{J}\\) are the partial population slopes of \\(Y\\) on \\(X_1,X_2,\\ldots,X_J\\) respectively holding the other \\(X\\) constant. \\(\\epsilon_i\\) is the random unexplained error or residual component. The additive model assumes that the effect of one predictor variable (partial slope) is independent of the levels of the other predictor variables.\n\rMultiplicative Model\r\\[ y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i1}x_{i2} + \\ldots + \\beta_Jx_{iJ} + \\epsilon_i,\\]\nwhere \\(\\beta_3x_{i1}x_{i2}\\) is the interactive effect of \\(X_1\\) and \\(X_2\\) on \\(Y\\) and it examines the degree to which the effect of one of the predictor variables depends on the levels of the other predictor variable(s).\n\rData generation\rLets say we had set up a natural experiment in which we measured a response (\\(y\\)) from each of \\(20\\) sampling units (\\(n=20\\)) across a landscape. At the same time, we also measured two other continuous covariates (\\(x_1\\) and \\(x_2\\)) from each of the sampling units. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(123)\r\u0026gt; n = 100\r\u0026gt; intercept = 5\r\u0026gt; temp = runif(n)\r\u0026gt; nitro = runif(n) + 0.8 * temp\r\u0026gt; int.eff = 2\r\u0026gt; temp.eff \u0026lt;- 0.85\r\u0026gt; nitro.eff \u0026lt;- 0.5\r\u0026gt; res = rnorm(n, 0, 1)\r\u0026gt; coef \u0026lt;- c(int.eff, temp.eff, nitro.eff, int.eff)\r\u0026gt; mm \u0026lt;- model.matrix(~temp * nitro)\r\u0026gt; \u0026gt; y \u0026lt;- t(coef %*% t(mm)) + res\r\u0026gt; data \u0026lt;- data.frame(y, x1 = temp, x2 = nitro, cx1 = scale(temp,\r+ scale = F), cx2 = scale(nitro, scale = F))\r\u0026gt; head(data)\ry x1 x2 cx1 cx2\r1 2.426468 0.2875775 0.8300510 -0.21098147 -0.08302110\r2 4.927690 0.7883051 0.9634676 0.28974614 0.05039557\r3 3.176118 0.4089769 0.8157946 -0.08958207 -0.09727750\r4 6.166652 0.8830174 1.6608878 0.38445841 0.74781568\r5 4.788890 0.9404673 1.2352762 0.44190829 0.32220415\r6 2.541536 0.0455565 0.9267954 -0.45300249 0.01372335\rWith these sort of data, we are primarily interested in investigating whether there is a relationship between the continuous response variable and the components linear predictor (continuous predictors). We could model the relationship via either:\n\rAn additive model in which the effects of each predictor contribute in an additive way to the response - we do not allow for an interaction as we consider an interaction either not of great importance or likely to be absent.\n\rA multiplicative model in which the effects of each predictor and their interaction contribute to the response - we allow for the impact of one predictor to vary across the range of the other predictor.\n\r\r\rCentering the data\rWhen a linear model contains a covariate (continuous predictor variable) in addition to another predictor (continuous or categorical), it is nearly always advisable that the continuous predictor variables are centered prior to the analysis. Centering is a process by which the mean of a variable is subtracted from each of the values such that the scale of the variable is shifted so as to be centered around \\(0\\). Hence the mean of the new centered variable will be \\(0\\), yet it will retain the same variance.\nThere are multiple reasons for this:\nIt provides some clinical meaning to the \\(y\\)-intercept. Recall that the \\(y\\)-intercept is the value of \\(Y\\) when \\(X\\) is equal to zero. If \\(X\\) is centered, then the \\(y\\)-intercept represents the value of \\(Y\\) at the mid-point of the \\(X\\) range. The \\(y\\)-intercept of an uncentered \\(X\\) typically represents a unreal value of \\(Y\\) (as an \\(X\\) of \\(0\\) is often beyond the reasonable range of values).\n\rIn multiplicative models (in which predictors and their interactions are included), main effects and interaction terms built from centered predictors will not be correlated to one another.\n\rFor more complex models, centering the covariates can increase the likelihood that the modelling engine converges (arrives at a numerically stable and reliable outcome).\n\r\rNote, centering will not effect the slope estimates. In R, centering is easily achieved with the scale function, which centers and scales (divides by standard deviation) the data. We only really need to center the data, so we provide the argument scale=FALSE. Also note that the scale function attaches the pre-centered mean (and standard deviation if scaling is performed) as attributes to the scaled data in order to facilitate back-scaling to the original scale. While these attributes are often convenient, they do cause issues for some of the Bayesian routines and so we will strip these attributes using the as.numeric function. Instead, we will create separate scalar variables to store the pre-scaled means.\n\u0026gt; data \u0026lt;- within(data, {\r+ cx1 \u0026lt;- as.numeric(scale(x1, scale = FALSE))\r+ cx2 \u0026lt;- as.numeric(scale(x2, scale = FALSE))\r+ })\r\u0026gt; head(data)\ry x1 x2 cx1 cx2\r1 2.426468 0.2875775 0.8300510 -0.21098147 -0.08302110\r2 4.927690 0.7883051 0.9634676 0.28974614 0.05039557\r3 3.176118 0.4089769 0.8157946 -0.08958207 -0.09727750\r4 6.166652 0.8830174 1.6608878 0.38445841 0.74781568\r5 4.788890 0.9404673 1.2352762 0.44190829 0.32220415\r6 2.541536 0.0455565 0.9267954 -0.45300249 0.01372335\r\u0026gt; \u0026gt; mean.x1 = mean(data$x1)\r\u0026gt; mean.x2 = mean(data$x2)\r\r\rAssumptions\rThe assumptions of the model are:\n\rAll of the observations are independent - this must be addressed at the design and collection stages.\n\rThe response variable (and thus the residuals) should be normally distributed. A boxplot of the entire variable is usually useful for diagnosing major issues with normality.\n\rThe response variable should be equally varied (variance should not be related to mean as these are supposed to be estimated separately). Scatterplots with linear smoothers can be useful for exploring the spread of observations around the trendline. The spread of observations around the trendline should not increase (or decrease) along its length.\n\rThe predictor variables should be uniformly or normally distributed. Again, boxplots can be useful.\n\rThe relationships between the linear predictors (right hand side of the regression formula) and the response variable should be linear. Scatterplots with smoothers can be useful for identifying possible non-linearity.\n\r(Multi)collinearity. The number of predictor variables must be less than the number of observations otherwise the linear model will be over-parameterized (more parameters to estimate than there are independent data from which estimates are calculated).\n\r\r(Multi)collinearity breaks the assumption that a predictor variable must not be correlated to the combination of other predictor variables (known collectively as the linear predictor). Multicollinearity has major detrimental effects on model fitting:\n\rInstability of the estimated partial regression slopes (small changes in the data or variable inclusion can cause dramatic changes in parameter estimates).\n\rInflated standard errors and confidence intervals of model parameters, thereby increasing the type II error rate (reducing power) of parameter hypothesis tests.\n\r\rMulticollinearity can be diagnosed with the following situatons:\n\rInvestigate pairwise correlations between all the predictor variables either by a correlation matrix or a scatterplot matrix\n\rCalculate the tolerance \\((1−r^2)\\) of the relationship between a predictor variable and all the other predictor variables for each of the predictor variables. Tolerance is a measure of the degree of collinearity and values less than \\(0.2\\) should be considered and values less than \\(0.1\\) should be given serious attention. Variance inflation factor (VIF) is the inverse of tolerance and thus values greater than \\(5\\), or worse, \\(10\\) indicate collinearity.\n\rPCA (principle components analysis) eigenvalues (from a correlation matrix for all the predictor variables) close to zero indicate collinearity and component loadings may be useful in determining which predictor variables cause collinearity.\n\r\rThere are several approaches to dealing with collinearity (however the first two of these are likely to result in biased parameter estimates):\nRemove the highly correlated predictor variable(s), starting with the least most clinically interesting variable(s)\n\rPCA (principle components analysis) regression - regress the response variable against the principal components resulting from a correlation matrix for all the predictor variables. Each of these principal components by definition are completely independent, but the resulting parameter estimates must be back-calculated in order to have any clinical meaning.\n\rApply a regression tree - regression trees recursively partitioning (subsetting) the data in accordance to individual variables that explain the greatest remaining variance. Since at each iteration, each predictor variable is effectively evaluated in isolation, (multi)collinearity is not an issue.\n\r\r\rModel fitting\rMultiple linear regression models can include predictors (terms) that are incorporated additively (no interactions) or multiplicatively (with interactions). As such we will explore these separately for each modelling tool. The observed responses (\\(y_i\\)) are assumed to be drawn from a normal distribution with a given mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The expected values are themselves determined by the linear predictor. In this case, \\(\\beta_0\\) represents the \\(y\\)-intercept (value of \\(y\\) when all of the \\(x\\)’s are equal to zero) and the set of \\(\\beta\\)’s represent the rates of change in y for every unit change in each \\(x\\) (the effect) holding each other \\(x\\) constant. Note that since we should always center all predictors (by subtracting the mean of each \\(x\\) from the repective values of each \\(x\\)), the \\(y\\)-intercept represents the value of \\(y\\) at the average value of each \\(x\\).\nMCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying “uninformative” priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (\\(100\\)) for both the intercept and the treatment effect and a wide half-cauchy (\\(\\text{scale}=5\\)) for the standard deviation:\n\\[ y_i \\sim \\text{Normal}(\\mu_i, \\sigma),\\]\nwhere \\(\\mu_i=\\beta_0 + \\boldsymbol \\beta \\boldsymbol X_i\\). Priors are specified as: \\(\\boldsymbol \\beta \\sim \\text{Normal}(0,1000)\\) and \\(\\sigma \\sim \\text{Cauchy}(0,5)\\). We will explore Bayesian modelling of multiple linear regression using JAGS. Remember that in this software normal distributions are specified in terms of precision \\(\\tau\\) rather than standard deviation \\(\\sigma\\), where \\(\\tau=\\frac{1}{\\sigma^2}\\).\n\u0026gt; modelString = \u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- beta0 + inprod(beta[],X[i,])\r+ }\r+ #Priors\r+ beta0 ~ dnorm(0.01,1.0E-6)\r+ for (j in 1:nX) {\r+ beta[j] ~ dnorm(0.01,1.0E-6)\r+ }\r+ tau \u0026lt;- 1 / (sigma * sigma)\r+ sigma~dunif(0,100)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;ttestModel.txt\u0026quot;)\rAdditive Model\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; X = model.matrix(~cx1 + cx2, data = data)\r\u0026gt; data.list \u0026lt;- with(data, list(y = y, X = X[, -1], nX = ncol(X) -\r+ 1, n = nrow(data)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta0\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10500\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\u0026gt; library(R2jags)\rNow run the JAGS code via the R2jags interface. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; data.r2jags.add \u0026lt;- jags(data = data.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;ttestModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 100\rUnobserved stochastic nodes: 4\rTotal graph size: 614\rInitializing model\r\u0026gt; \u0026gt; print(data.r2jags.add)\rInference for Bugs model at \u0026quot;ttestModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 2.830 0.442 1.964 2.538 2.830 3.125 3.694 1.001 7400\rbeta[2] 1.582 0.380 0.833 1.327 1.581 1.834 2.319 1.001 14000\rbeta0 3.799 0.100 3.603 3.733 3.797 3.865 3.997 1.001 15000\rsigma 0.996 0.074 0.864 0.944 0.992 1.043 1.154 1.001 15000\rdeviance 281.420 2.961 277.779 279.260 280.727 282.888 288.827 1.001 15000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 4.4 and DIC = 285.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMultiplicative Model\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; X = model.matrix(~cx1 * cx2, data = data)\r\u0026gt; data.list \u0026lt;- with(data, list(y = y, X = X[, -1], nX = ncol(X) - 1, n = nrow(data)))\rDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta0\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10500\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Run the JAGS code via the R2jags interface. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; data.r2jags.mult \u0026lt;- jags(data = data.list, inits = NULL, parameters.to.save = params,\r+ model.file = \u0026quot;ttestModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter,\r+ n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 100\rUnobserved stochastic nodes: 5\rTotal graph size: 715\rInitializing model\r\u0026gt; \u0026gt; print(data.r2jags.mult)\rInference for Bugs model at \u0026quot;ttestModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 2.800 0.451 1.914 2.500 2.801 3.104 3.680 1.001 15000\rbeta[2] 1.504 0.389 0.744 1.237 1.505 1.766 2.267 1.001 15000\rbeta[3] 1.451 1.210 -0.933 0.643 1.456 2.238 3.849 1.001 15000\rbeta0 3.715 0.122 3.475 3.633 3.715 3.797 3.957 1.001 6000\rsigma 0.994 0.073 0.863 0.944 0.989 1.039 1.151 1.001 15000\rdeviance 280.964 3.307 276.617 278.541 280.281 282.649 289.157 1.001 15000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 5.5 and DIC = 286.4\rDIC is an estimate of expected predictive error (lower deviance is better).\r\r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots for the effects model as an example. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters, e.g. \\(\\boldsymbol \\beta\\). Rather than dublicate this for both additive and multiplicative models, we will only explore the multiplicative model.\n\u0026gt; library(mcmcplots)\r\u0026gt; denplot(data.r2jags.mult, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta\u0026quot;))\r\u0026gt; traplot(data.r2jags.mult, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta\u0026quot;))\rThese plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.\n\u0026gt; data.mcmc = as.mcmc(data.r2jags.mult)\r\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(data.mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 2 3609 3746 0.963 beta[2] 2 3811 3746 1.020 beta[3] 2 3811 3746 1.020 beta0 2 3770 3746 1.010 deviance 2 3729 3746 0.995 sigma 4 4989 3746 1.330 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta[1] 2 3729 3746 0.995 beta[2] 2 3730 3746 0.996 beta[3] 2 3811 3746 1.020 beta0 2 3729 3746 0.995 deviance 2 3751 3746 1.000 sigma 4 5306 3746 1.420 \rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; autocorr.diag(data.mcmc)\rbeta[1] beta[2] beta[3] beta0 deviance\rLag 0 1.000000000 1.0000000000 1.000000000 1.000000000 1.000000e+00\rLag 1 -0.007495093 -0.0002601039 -0.004404658 -0.016267523 1.340676e-01\rLag 5 0.004013980 -0.0121560194 0.004193180 0.006361847 7.319664e-05\rLag 10 -0.009167511 -0.0004423631 0.007960201 0.005194172 -5.183038e-03\rLag 50 0.001459434 0.0077668977 -0.006551273 -0.003063066 -5.021565e-03\rsigma\rLag 0 1.000000000\rLag 1 0.262166680\rLag 5 -0.020700390\rLag 10 -0.006918124\rLag 50 0.001501713\rA lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. For more complex models (those that contain multiple effects), it is also advisable to plot the residuals against each of the individual predictors. For sampling designs that involve sample collection over space or time, it is also a good idea to explore whether there are any temporal or spatial patterns in the residuals.\nThere are numerous situations (e.g. when applying specific variance-covariance structures to a model) where raw residuals do not reflect the interior workings of the model. Typically, this is because they do not take into account the variance-covariance matrix or assume a very simple variance-covariance matrix. Since the purpose of exploring residuals is to evaluate the model, for these cases, it is arguably better to draw conclusions based on standardized (or studentised) residuals. Unfortunately the definitions of standardised and studentised residuals appears to vary and the two terms get used interchangeably. I will adopt the following definitions:\n\rStandardised residuals. The raw residuals divided by the true standard deviation of the residuals (which of course is rarely known).\n\rStudentised residuals. The raw residuals divided by the standard deviation of the residuals. Note that externally studentised residuals are calculated by dividing the raw residuals by a unique standard deviation for each observation that is calculated from regressions having left each successive observation out.\n\rPearson residuals. The raw residuals divided by the standard deviation of the response variable.\n\r\rhe mark of a good model is being able to predict well. In an ideal world, we would have sufficiently large sample size as to permit us to hold a fraction (such as \\(25\\)%) back thereby allowing us to train the model on \\(75\\)% of the data and then see how well the model can predict the withheld \\(25\\)%. Unfortunately, such a luxury is still rare. The next best option is to see how well the model can predict the observed data. Models tend to struggle most with the extremes of trends and have particular issues when the extremes approach logical boundaries (such as zero for count data and standard deviations). We can use the fitted model to generate random predicted observations and then explore some properties of these compared to the actual observed data.\nRather than dublicate this for both additive and multiplicative models, we will only explore the multiplicative model. Residuals are not computed directly within JAGS. However, we can calculate them manually form the posteriors.\n\u0026gt; library(ggplot2)\r\u0026gt; library(dplyr)\r\u0026gt; mcmc = data.r2jags.mult$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;% dplyr:::select(beta0,\r+ contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:4], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit))\rResiduals against predictors\n\u0026gt; library(tidyr)\r\u0026gt; mcmc = data.r2jags.mult$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;% dplyr:::select(beta0,\r+ contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = newdata\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:4], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; newdata = data %\u0026gt;% cbind(fit, resid)\r\u0026gt; newdata.melt = newdata %\u0026gt;% gather(key = Pred, value = Value, cx1:cx2)\r\u0026gt; ggplot(newdata.melt) + geom_point(aes(y = resid, x = Value)) + facet_wrap(~Pred)\rAnd now for studentised residuals\n\u0026gt; mcmc = data.r2jags.mult$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;% dplyr:::select(beta0, + contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:4], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; sresid = resid/sd(resid)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit))\rFor this simple model, the studentised residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = data.r2jags.mult$BUGSoutput$sims.matrix %\u0026gt;% as.data.frame %\u0026gt;%\r+ dplyr:::select(beta0, contains(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, data)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, 1:4]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i,\r+ ], mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; ggplot() + geom_density(data = NULL, aes(x = as.vector(yRep),\r+ fill = \u0026quot;Model\u0026quot;), alpha = 0.5) + geom_density(data = data,\r+ aes(x = y, fill = \u0026quot;Obs\u0026quot;), alpha = 0.5)\rWe can also explore the posteriors of each parameter.\n\u0026gt; library(bayesplot)\r\u0026gt; mcmc_intervals(data.r2jags.mult$BUGSoutput$sims.matrix, regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\u0026gt; mcmc_areas(data.r2jags.mult$BUGSoutput$sims.matrix, regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\rParameter estimates\rAlthough all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and \\(95\\)% credibility intervals.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\rFirst, we look at the results from the additive model.\n\u0026gt; print(data.r2jags.add)\rInference for Bugs model at \u0026quot;ttestModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 2.830 0.442 1.964 2.538 2.830 3.125 3.694 1.001 7400\rbeta[2] 1.582 0.380 0.833 1.327 1.581 1.834 2.319 1.001 14000\rbeta0 3.799 0.100 3.603 3.733 3.797 3.865 3.997 1.001 15000\rsigma 0.996 0.074 0.864 0.944 0.992 1.043 1.154 1.001 15000\rdeviance 281.420 2.961 277.779 279.260 280.727 282.888 288.827 1.001 15000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 4.4 and DIC = 285.8\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(as.mcmc(data.r2jags.add), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 2.83 0.442 1.96 3.69\r2 beta[2] 1.58 0.380 0.844 2.33\r3 beta0 3.80 0.1000 3.60 3.99\r4 deviance 281. 2.96 277. 287. 5 sigma 0.996 0.0742 0.857 1.14\rConclusions\n\rWhen cx2 is held constant, a one unit increase in cx1 is associated with a \\(2.83\\) change in \\(y\\). That is, \\(y\\) increases at a rate of \\(2.83\\) per unit increase in cx1 when standardised for cx2.\n\rWhen cx1 is held constant, a one unit increase in cx2 is associated with a \\(1.58\\) change in \\(y\\). That is, \\(y\\) increases at a rate of \\(1.58\\) per unit increase in cx2 when standardised for cx1.\n\r\rNote, as this is an additive model, the rates associated with cx1 are assumed to be constant throughtout the range of cx2 and vice versa. The \\(95\\)% confidence interval for each partial slope does not overlap with \\(0\\) implying a significant effects of cx1 and cx2 on \\(y\\). While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(data.r2jags.add$BUGSoutput$sims.matrix[, \u0026quot;beta[1]\u0026quot;])\r[1] 0\r\u0026gt; mcmcpvalue(data.r2jags.add$BUGSoutput$sims.matrix[, \u0026quot;beta[2]\u0026quot;])\r[1] 0.0001333333\rWith a p-value of essentially \\(0\\), we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship. Next, we look at the results from the multiplicative model.\n\u0026gt; print(data.r2jags.mult)\rInference for Bugs model at \u0026quot;ttestModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 2.800 0.451 1.914 2.500 2.801 3.104 3.680 1.001 15000\rbeta[2] 1.504 0.389 0.744 1.237 1.505 1.766 2.267 1.001 15000\rbeta[3] 1.451 1.210 -0.933 0.643 1.456 2.238 3.849 1.001 15000\rbeta0 3.715 0.122 3.475 3.633 3.715 3.797 3.957 1.001 6000\rsigma 0.994 0.073 0.863 0.944 0.989 1.039 1.151 1.001 15000\rdeviance 280.964 3.307 276.617 278.541 280.281 282.649 289.157 1.001 15000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 5.5 and DIC = 286.4\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(as.mcmc(data.r2jags.mult), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 6 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta[1] 2.80 0.451 1.91 3.67\r2 beta[2] 1.50 0.389 0.746 2.27\r3 beta[3] 1.45 1.21 -0.976 3.79\r4 beta0 3.71 0.122 3.47 3.95\r5 deviance 281. 3.31 276. 287. 6 sigma 0.994 0.0729 0.856 1.14\rConclusions\n\rAt the average level of cx2 (=0), a one unit increase in cx1 is associated with a \\(2.80\\) change in y. That is, y increases at a rate of \\(2.80\\) per unit increase in cx1 when standardised for cx2.\n\rAt the average level of cx1 (=0), a one unit increase in cx2 is associated with a \\(1.50\\) change in \\(y\\). That is, \\(y\\) increases at a rate of \\(1.50\\) per unit increase in cx2 when standardised for cx1.\n\rThe degree to which the rate of change in response associated with a one unit change in cx1 changes over the range of cx2 (and vice versa) is \\(1.45\\).\n\r\rThe \\(95\\)% confidence intervals for the interaction partial slope does not overlap with \\(0\\) implying a significant interaction between cx1 and cx2. This suggests that the nature of the relationship between \\(y\\) and cx1 depends on the level of cx2 (and vice versa). The estimates of the effect of cx1 are only appropriate when cx2 = 0 etc. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(data.r2jags.mult$BUGSoutput$sims.matrix[, \u0026quot;beta[1]\u0026quot;])\r[1] 0\r\u0026gt; mcmcpvalue(data.r2jags.mult$BUGSoutput$sims.matrix[, \u0026quot;beta[2]\u0026quot;])\r[1] 6.666667e-05\r\u0026gt; mcmcpvalue(data.r2jags.mult$BUGSoutput$sims.matrix[, \u0026quot;beta[3]\u0026quot;])\r[1] 0.2236\rWith a p-value of essentially \\(0\\), we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship.\n\rGraphical summaries\rA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group.\nWith appropriate use of model matrices and data wrangling, it is possible to produce a single prediction data set along with ggplot syntax to produce a multi-panel figure. First we look at the additive model.\n\u0026gt; mcmc = data.r2jags.add$BUGSoutput$sims.matrix\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = rbind(data.frame(cx1 = seq(min(data$cx1, na.rm = TRUE), max(data$cx1,\r+ na.rm = TRUE), len = 100), cx2 = 0, Pred = 1), data.frame(cx1 = 0,\r+ cx2 = seq(min(data$cx2, na.rm = TRUE), max(data$cx2, na.rm = TRUE),\r+ len = 100), Pred = 2))\r\u0026gt; Xmat = model.matrix(~cx1 + cx2, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% mutate(x1 = cx1 + mean.x1, x2 = cx2 + mean.x2) %\u0026gt;%\r+ cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)) %\u0026gt;%\r+ mutate(x = dplyr:::recode(Pred, x1, x2))\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X\u0026quot;) + theme_classic() + facet_wrap(~Pred)\rWe cannot simply add the raw data to this figure. The reason for this is that the trends represent the effect of one predictor holding the other variable constant. Therefore, the observations we represent on the figure must likewise be standardised. We can achieve this by adding the partial residuals to the figure. Partial residuals are the fitted values plus the residuals.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = rbind(data.frame(cx1 = data$cx1, cx2 = 0, Pred = 1), data.frame(cx1 = 0,\r+ cx2 = data$cx2, Pred = 2))\r\u0026gt; fMat = rMat = model.matrix(~cx1 + cx2, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit) %\u0026gt;% mutate(x1 = cx1 +\r+ mean.x1, x2 = cx2 + mean.x2) %\u0026gt;% mutate(x = dplyr:::recode(Pred, x1,\r+ x2))\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\r+ color = \u0026quot;gray\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low, ymax = conf.high),\r+ fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) + theme_classic() +\r+ facet_wrap(~Pred, strip.position = \u0026quot;bottom\u0026quot;, labeller = label_bquote(\u0026quot;x\u0026quot; *\r+ .(Pred))) + theme(axis.title.x = element_blank(), strip.background = element_blank(),\r+ strip.placement = \u0026quot;outside\u0026quot;)\rHowever, this method (whist partially elegant) does become overly opaque if we need more extensive axes labels since the x-axes labels are actually strip labels (which must largely be defined outside of the ggplot structure). The alternative is to simply produce each partial plot separately before arranging them together in the one figure using the package gridExtra.\n\u0026gt; library(gridExtra)\r\u0026gt; mcmc = data.r2jags.add$BUGSoutput$sims.matrix\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = data.frame(cx1 = seq(min(data$cx1, na.rm = TRUE), max(data$cx1,\r+ na.rm = TRUE), len = 100), cx2 = 0)\r\u0026gt; Xmat = model.matrix(~cx1 + cx2, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% mutate(x1 = cx1 + mean.x1, x2 = cx2 + mean.x2) %\u0026gt;%\r+ cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; ## Now the partial residuals\r\u0026gt; fdata = rdata = data.frame(cx1 = data$cx1, cx2 = 0)\r\u0026gt; fMat = rMat = model.matrix(~cx1 + cx2, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit) %\u0026gt;% mutate(x1 = cx1 +\r+ mean.x1, x2 = cx2 + mean.x2)\r\u0026gt; g1 = ggplot(newdata, aes(y = estimate, x = x1)) + geom_point(data = rdata,\r+ aes(y = partial.resid), color = \u0026quot;grey\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X1\u0026quot;) + theme_classic()\r\u0026gt; \u0026gt; newdata = data.frame(cx2 = seq(min(data$cx2, na.rm = TRUE), max(data$cx2,\r+ na.rm = TRUE), len = 100), cx1 = 0)\r\u0026gt; Xmat = model.matrix(~cx1 + cx2, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% mutate(x1 = cx1 + mean.x1, x2 = cx2 + mean.x2) %\u0026gt;%\r+ cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; ## Now the partial residuals\r\u0026gt; fdata = rdata = data.frame(cx1 = 0, cx2 = data$cx2)\r\u0026gt; fMat = rMat = model.matrix(~cx1 + cx2, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit) %\u0026gt;% mutate(x1 = cx1 +\r+ mean.x1, x2 = cx2 + mean.x2)\r\u0026gt; g2 = ggplot(newdata, aes(y = estimate, x = x2)) + geom_point(data = rdata,\r+ aes(y = partial.resid), color = \u0026quot;grey\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X2\u0026quot;) + theme_classic()\r\u0026gt; \u0026gt; grid.arrange(g1, g2, ncol = 2)\rFor the multiplicative model, we could elect to split the trends up so as to explore the effects of one predictor at several set levels of another predictor. In this example, we will explore the effects of \\(x_1\\) when \\(x_2\\) is equal to its mean in the original data as well as one and two standard deviations below and above this mean.\n\u0026gt; library(fields)\r\u0026gt; mcmc = data.r2jags.mult$BUGSoutput$sims.matrix\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = expand.grid(cx1 = seq(min(data$cx1, na.rm = TRUE), max(data$cx1,\r+ na.rm = TRUE), len = 100), cx2 = mean(data$cx2) + sd(data$cx2) %*%\r+ -2:2)\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;, \u0026quot;beta[3]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% mutate(x1 = cx1 + mean.x1, x2 = cx2 + mean.x2) %\u0026gt;%\r+ cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)) %\u0026gt;%\r+ mutate(x2 = factor(x2, labels = paste(\u0026quot;X2:~\u0026quot;, c(-2, -1, 0, 1, 2), \u0026quot;*sigma\u0026quot;)))\r\u0026gt; ## Partial residuals\r\u0026gt; fdata = rdata = expand.grid(cx1 = data$cx1, cx2 = mean(data$cx2) + sd(data$cx2) *\r+ -2:2)\r\u0026gt; fMat = rMat = model.matrix(~cx1 * cx2, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit) %\u0026gt;% mutate(x1 = cx1 +\r+ mean.x1, x2 = cx2 + mean.x2)\r\u0026gt; ## Partition the partial residuals such that each x1 trend only includes\r\u0026gt; ## x2 data that is within that range in the observed data\r\u0026gt; findNearest = function(x, y) {\r+ ff = fields:::rdist(x, y)\r+ apply(ff, 1, function(x) which(x == min(x)))\r+ }\r\u0026gt; fn = findNearest(x = data[, c(\u0026quot;x1\u0026quot;, \u0026quot;x2\u0026quot;)], y = rdata[, c(\u0026quot;x1\u0026quot;, \u0026quot;x2\u0026quot;)])\r\u0026gt; rdata = rdata[fn, ] %\u0026gt;% mutate(x2 = factor(x2, labels = paste(\u0026quot;X2:~\u0026quot;, c(-2,\r+ -1, 0, 1, 2), \u0026quot;*sigma\u0026quot;)))\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x1)) + geom_line() + geom_blank(aes(y = 9)) +\r+ geom_point(data = rdata, aes(y = partial.resid), color = \u0026quot;grey\u0026quot;) +\r+ geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \u0026quot;blue\u0026quot;,\r+ alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_continuous(\u0026quot;X1\u0026quot;) +\r+ facet_wrap(~x2, labeller = label_parsed, nrow = 1, scales = \u0026quot;free_y\u0026quot;) +\r+ theme_classic() + theme(strip.background = element_blank())\rAlternatively, we could explore the interaction by plotting a two dimensional surface as a heat map.\n\rEffect sizes\rIn addition to deriving the distribution means for the slope parameter, we could make use of the Bayesian framework to derive the distribution of the effect size. In so doing, effect size could be considered as either the rate of change or alternatively, the difference between pairs of values along the predictor gradient. For the latter case, there are multiple ways of calculating an effect size, but the two most common are:\n\rRaw effect size. The difference between two groups (as already calculated)\n\rCohen’s D. The effect size standardized by division with the pooled standard deviation: \\(D=\\frac{(\\mu_A-\\mu_B)}{\\sigma}\\)\n\rPercentage change. Express the effect size as a percent of one of the pairs. That is, whether you expressing a percentage increase or a percentage decline depends on which of the pairs of values are considered a reference value. Care must be exercised to ensure no division by zeros occur.\n\r\rFor simple linear models, effect size based on a rate is essentially the same as above except that it is expressed per unit of the predictor. Of course in many instances, one unit change in the predictor represents too subtle a shift in the underlying gradient to likely yield any clinically meaningful or appreciable change in response.\nProbability that a change in \\(x_1\\) is associated with greater than a \\(50\\)% increase in \\(y\\) at various levels of \\(x_2\\). Clearly, in order to explore this inference, we must first express the change in \\(y\\) as a percentage. This in turn requires us to calculate start and end points from which to calculate the magnitude of the effect (amount of increase in \\(y\\)) as well as the percentage decline. Hence, we start by predicting the distribution of \\(y\\) at the lowest and highest values of \\(x_1\\) at five levels of \\(x_2\\) (representing two standard deviations below the cx2 mean, one standard deviation below the cx2 mean, the cx2 mean, one standard deviation above the cx2 mean and \\(2\\) standard deviations above the cx2 mean. For this exercise we will only use the multiplicative model. Needless to say, the process would be very similar for the additive model.\n\u0026gt; mcmc = data.r2jags.mult$BUGSoutput$sims.matrix\r\u0026gt; newdata = expand.grid(cx1 = c(min(data$cx1), max(data$cx1)), cx2 = (-2:2) *\r+ sd(data$cx2))\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;, \u0026quot;beta[3]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; s1 = seq(1, 9, b = 2)\r\u0026gt; s2 = seq(2, 10, b = 2)\r\u0026gt; ## Raw effect size\r\u0026gt; (RES = tidyMCMC(as.mcmc(fit[, s2] - fit[, s1]), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 2 1.82 0.938 -0.0378 3.64\r2 4 2.30 0.616 1.13 3.54\r3 6 2.78 0.448 1.90 3.65\r4 8 3.26 0.586 2.12 4.42\r5 10 3.74 0.899 2.02 5.55\r\u0026gt; ## Cohen\u0026#39;s D\r\u0026gt; cohenD = (fit[, s2] - fit[, s1])/sqrt(mcmc[, \u0026quot;sigma\u0026quot;])\r\u0026gt; (cohenDES = tidyMCMC(as.mcmc(cohenD), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 2 1.83 0.940 0.0489 3.74\r2 4 2.31 0.622 1.11 3.57\r3 6 2.80 0.461 1.89 3.68\r4 8 3.28 0.599 2.10 4.45\r5 10 3.76 0.910 1.98 5.54\r\u0026gt; # Percentage change (relative to Group A)\r\u0026gt; ESp = 100 * (fit[, s2] - fit[, s1])/fit[, s1]\r\u0026gt; (PES = tidyMCMC(as.mcmc(ESp), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 2 124. 142. -19.5 318.\r2 4 117. 45.2 33.1 205.\r3 6 123. 32.9 62.1 187.\r4 8 135. 50.0 48.0 230.\r5 10 150. 89.1 29.4 308.\r\u0026gt; # Probability that the effect is greater than 50% (an increase of \u0026gt;50%)\r\u0026gt; (p50 = apply(ESp, 2, function(x) sum(x \u0026gt; 50)/length(x)))\r2 4 6 8 10 0.7996667 0.9576667 0.9978667 0.9925333 0.9723333 \u0026gt; ## fractional change\r\u0026gt; (FES = tidyMCMC(as.mcmc(fit[, s2]/fit[, s1]), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 2 2.24 1.42 0.805 4.18\r2 4 2.17 0.452 1.33 3.05\r3 6 2.23 0.329 1.62 2.87\r4 8 2.35 0.500 1.48 3.30\r5 10 2.50 0.891 1.29 4.08\rConclusions\n\rOn average, when \\(x_2\\) is equal to its mean, \\(Y\\) increases by \\(2.79\\) over the observed range of \\(x_1\\). We are \\(95\\)% confident that the increase is between \\(1.91\\) and \\(3.66\\).\n\rThe Cohen’s D associated change over the observed range of \\(x_1\\) is \\(2.80\\).\n\rOn average, \\(Y\\) increases by \\(124\\)% over the observed range of \\(x_1\\) (at average \\(x_2\\)). We are \\(95\\)% confident that the increase is between \\(65\\)% and \\(190\\)%.\n\rThe probability that \\(Y\\) increases by more than \\(50\\)% over the observed range of \\(x_1\\) (average \\(x_2\\)) is \\(0.998\\).\n\rOn average, \\(Y\\) increases by a factor of \\(2.24\\)% over the observed range of \\(x_1\\) (average \\(x_2\\)). We are \\(95\\)% confident that the decline is between a factor of \\(1.65\\)% and \\(2.90\\)%.\n\r\r\rFinite population standard deviations\rVariance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).\nFinite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (Gelman and others (2005)). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.\n# A tibble: 4 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x1 0.798 0.129 0.544 1.05 2 sd.x2 0.501 0.130 0.249 0.756\r3 sd.x1x2 0.136 0.0877 0.00000784 0.296\r4 sd.resid 0.981 0.0128 0.965 1.01 # A tibble: 4 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x1 33.1 4.97 23.4 42.7\r2 sd.x2 20.8 5.14 10.4 30.3\r3 sd.x1x2 5.27 3.46 0.000322 11.7\r4 sd.resid 40.5 2.15 36.7 44.9\rApproximately \\(59\\)% of the total finite population standard deviation is due to \\(x_1\\), \\(x_2\\) and their interaction.\n\rR squared\rIn a frequentist context, the \\(R^2\\) value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, \\(R^2\\) is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an \\(R^2\\) greater than \\(100\\)%. Gelman et al. (2019) proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.\nSo in the standard regression model notation of:\n\\[ y_i \\sim \\text{Normal}(\\boldsymbol X \\boldsymbol \\beta, \\sigma),\\]\nthe \\(R^2\\) could be formulated as\n\\[ R^2 = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sigma^2_e},\\]\nwhere \\(\\sigma^2_f=\\text{var}(\\boldsymbol X \\boldsymbol \\beta)\\), and for normal models \\(\\sigma^2_e=\\text{var}(y-\\boldsymbol X \\boldsymbol \\beta)\\)\n\u0026gt; mcmc \u0026lt;- data.r2jags.mult$BUGSoutput$sims.matrix\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, data)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;, \u0026quot;beta[3]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = sweep(fit, 2, data$y, \u0026quot;-\u0026quot;)\r\u0026gt; var_f = apply(fit, 1, var)\r\u0026gt; var_e = apply(resid, 1, var)\r\u0026gt; R2 = var_f/(var_f + var_e)\r\u0026gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.605 0.0400 0.526 0.676\r\u0026gt; \u0026gt; # for comparison with frequentist\r\u0026gt; summary(lm(y ~ cx1 * cx2, data))\rCall:\rlm(formula = y ~ cx1 * cx2, data = data)\rResiduals:\rMin 1Q Median 3Q Max -1.8173 -0.7167 -0.1092 0.5890 3.3861 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 3.7152 0.1199 30.987 \u0026lt; 2e-16 ***\rcx1 2.8072 0.4390 6.394 5.84e-09 ***\rcx2 1.4988 0.3810 3.934 0.000158 ***\rcx1:cx2 1.4464 1.1934 1.212 0.228476 ---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 0.9804 on 96 degrees of freedom\rMultiple R-squared: 0.6115, Adjusted R-squared: 0.5994 F-statistic: 50.37 on 3 and 96 DF, p-value: \u0026lt; 2.2e-16\r\rBayesian model selection\rA statistical model is by definition a low-dimensional (over simplification) representation of what is really likely to be a very complex system. As a result, no model is right. Some models however can provide useful insights into some of the processes operating on the system. Frequentist statistics have various methods (model selection, dredging, lasso, cross validation) for selecting parsimonious models. These are models that provide a good comprimise between minimizing unexplained patterns and minimizing model complexity. The basic premise is that since no model can hope to capture the full complexity of a system with all its subtleties, only the very major patterns can be estimated. Overly complex models are likely to be representing artificial complexity present only in the specific observed data (not the general population). The Bayesian approach is to apply priors to the non-variance parameters such that parameters close to zero are further shrunk towards zero whilst priors on parameters further away from zero are less effected. The most popular form of prior for sparsity is the horseshoe prior, so called because the shape of a component of this prior resembles a horseshoe (with most of the mass either close to \\(0\\) or close to \\(1\\)).\nRather than apply weakly informative Gaussian priors on parameters as:\n\\[ \\beta_j \\sim N(0,\\sigma^2),\\]\nthe horseshoe prior is defined as\n\\[ \\beta_j \\sim N(0,\\tau^2\\lambda_j^2),\\]\nwhere \\(\\tau \\sim \\text{Cauchy}(0,1)\\) and \\(\\lambda_j \\sim \\text{Cauchy}(0,1)\\), for \\(j=1,\\ldots,D\\). Using this prior, \\(D\\) is the number of (non-intercept or variance) parameters, \\(\\tau\\) represents the global scale that weights or shrinks all parameters towards zero and \\(\\lambda_j\\) are thick tailed local scales that allow some of the \\(j\\) parameters to escape shrinkage. More recently, Piironen, Vehtari, and others (2017) have argued that whilst the above horseshoe priors do guarantee that strong effects (parameters) will not be over-shrunk, there is the potential for weekly identified effects (those based on relatively little data) to be misrepresented in the posteriors. As an alternative they advocated the use of regularised horseshoe priors in which the amount of shrinkage applied to the largest effects can be controlled. The prior is defined as:\n\\[ \\beta_j \\sim N(0,\\tau^2 \\tilde{\\lambda}_j^2),\\]\nwhere \\(\\tilde{\\lambda}_j^2 = \\frac{c^2\\lambda^2_j}{c^2+\\tau^2 \\lambda^2_j}\\) and \\(c\\) is (slab width, actually variance) is a constant. For small effects (when \\(\\tau^2 \\lambda^2_j \u0026lt; c^2\\)) the prior approaches a regular prior. However, for large effects (when \\(\\tau^2 \\lambda^2_j \u0026gt; c^2\\)) the prior approaches \\(N(0,c^2)\\). Finally, they recommend applying a inverse-gamma prior on \\(c^2\\):\n\\[ c^2 \\sim \\text{Inv-Gamma}(\\alpha,\\beta),\\]\nwhere \\(\\alpha=v/2\\) and \\(\\beta=vs^2/2\\), which translates to a \\(\\text{Student-t}_ν(0, s^2)\\) slab for the coefficients far from zero and is typically a good default choice for a weakly informative prior.\n\rReferences\rGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9.\n\rGelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” The Annals of Statistics 33 (1): 1–53.\n\rPiironen, Juho, Aki Vehtari, and others. 2017. “Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors.” Electronic Journal of Statistics 11 (2): 5018–51.\n\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1580782394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580782394,"objectID":"dba2bbc8ce72062999fccab6926289d0","permalink":"/jags/multiple-linear-regression-jags/multiple-linear-regression-jags/","publishdate":"2020-02-03T21:13:14-05:00","relpermalink":"/jags/multiple-linear-regression-jags/multiple-linear-regression-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","linear regression"],"title":"Multiple Linear Regression - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","linear regression","STAN"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rMultiple regression is an extension of simple linear regression whereby a response variable is modelled against a linear combination of two or more simultaneously measured predictor variables. There are two main purposes of multiple linear regression:\nTo develop a better predictive model (equation) than is possible from models based on single independent variables.\n\rTo investigate the relative individual effects of each of the multiple independent variables above and beyond (standardised across) the effects of the other variables.\n\r\rAlthough the relationship between response variable and the additive effect of all the predictor variables is represented overall by a single multidimensional plane (surface), the individual effects of each of the predictor variables on the response variable (standardised across the other variables) can be depicted by single partial regression lines. The slope of any single partial regression line (partial regression slope) thereby represents the rate of change or effect of that specific predictor variable (holding all the other predictor variables constant to their respective mean values) on the response variable. In essence, it is the effect of one predictor variable at one specific level (the means) of all the other predictor variables (i.e. when each of the other predictors are set to their averages).\nMultiple regression models can be constructed additively (containing only the predictor variables themselves) or in a multiplicative design (which incorporate interactions between predictor variables in addition to the predictor variables themselves). Multiplicative models are used primarily for testing inferences about the effects of various predictor variables and their interactions on the response variable. Additive models by contrast are used for generating predictive models and estimating the relative importance of individual predictor variables more so than hypothesis testing.\n\rAdditive Model\r\\[ y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\ldots + \\beta_Jx_{iJ} + \\epsilon_i,\\]\nwhere \\(\\beta_0\\) is the population \\(y\\)-intercept (value of \\(y\\) when all partial slopes equal zero), \\(\\beta_1,\\beta_2,\\ldots,\\beta_{J}\\) are the partial population slopes of \\(Y\\) on \\(X_1,X_2,\\ldots,X_J\\) respectively holding the other \\(X\\) constant. \\(\\epsilon_i\\) is the random unexplained error or residual component. The additive model assumes that the effect of one predictor variable (partial slope) is independent of the levels of the other predictor variables.\n\rMultiplicative Model\r\\[ y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i1}x_{i2} + \\ldots + \\beta_Jx_{iJ} + \\epsilon_i,\\]\nwhere \\(\\beta_3x_{i1}x_{i2}\\) is the interactive effect of \\(X_1\\) and \\(X_2\\) on \\(Y\\) and it examines the degree to which the effect of one of the predictor variables depends on the levels of the other predictor variable(s).\n\rData generation\rLets say we had set up a natural experiment in which we measured a response (\\(y\\)) from each of \\(20\\) sampling units (\\(n=20\\)) across a landscape. At the same time, we also measured two other continuous covariates (\\(x_1\\) and \\(x_2\\)) from each of the sampling units. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(123)\r\u0026gt; n = 100\r\u0026gt; intercept = 5\r\u0026gt; temp = runif(n)\r\u0026gt; nitro = runif(n) + 0.8 * temp\r\u0026gt; int.eff = 2\r\u0026gt; temp.eff \u0026lt;- 0.85\r\u0026gt; nitro.eff \u0026lt;- 0.5\r\u0026gt; res = rnorm(n, 0, 1)\r\u0026gt; coef \u0026lt;- c(int.eff, temp.eff, nitro.eff, int.eff)\r\u0026gt; mm \u0026lt;- model.matrix(~temp * nitro)\r\u0026gt; \u0026gt; y \u0026lt;- t(coef %*% t(mm)) + res\r\u0026gt; data \u0026lt;- data.frame(y, x1 = temp, x2 = nitro, cx1 = scale(temp,\r+ scale = F), cx2 = scale(nitro, scale = F))\r\u0026gt; head(data)\ry x1 x2 cx1 cx2\r1 2.426468 0.2875775 0.8300510 -0.21098147 -0.08302110\r2 4.927690 0.7883051 0.9634676 0.28974614 0.05039557\r3 3.176118 0.4089769 0.8157946 -0.08958207 -0.09727750\r4 6.166652 0.8830174 1.6608878 0.38445841 0.74781568\r5 4.788890 0.9404673 1.2352762 0.44190829 0.32220415\r6 2.541536 0.0455565 0.9267954 -0.45300249 0.01372335\rWith these sort of data, we are primarily interested in investigating whether there is a relationship between the continuous response variable and the components linear predictor (continuous predictors). We could model the relationship via either:\n\rAn additive model in which the effects of each predictor contribute in an additive way to the response - we do not allow for an interaction as we consider an interaction either not of great importance or likely to be absent.\n\rA multiplicative model in which the effects of each predictor and their interaction contribute to the response - we allow for the impact of one predictor to vary across the range of the other predictor.\n\r\r\rCentering the data\rWhen a linear model contains a covariate (continuous predictor variable) in addition to another predictor (continuous or categorical), it is nearly always advisable that the continuous predictor variables are centered prior to the analysis. Centering is a process by which the mean of a variable is subtracted from each of the values such that the scale of the variable is shifted so as to be centered around \\(0\\). Hence the mean of the new centered variable will be \\(0\\), yet it will retain the same variance.\nThere are multiple reasons for this:\nIt provides some clinical meaning to the \\(y\\)-intercept. Recall that the \\(y\\)-intercept is the value of \\(Y\\) when \\(X\\) is equal to zero. If \\(X\\) is centered, then the \\(y\\)-intercept represents the value of \\(Y\\) at the mid-point of the \\(X\\) range. The \\(y\\)-intercept of an uncentered \\(X\\) typically represents a unreal value of \\(Y\\) (as an \\(X\\) of \\(0\\) is often beyond the reasonable range of values).\n\rIn multiplicative models (in which predictors and their interactions are included), main effects and interaction terms built from centered predictors will not be correlated to one another.\n\rFor more complex models, centering the covariates can increase the likelihood that the modelling engine converges (arrives at a numerically stable and reliable outcome).\n\r\rNote, centering will not effect the slope estimates. In R, centering is easily achieved with the scale function, which centers and scales (divides by standard deviation) the data. We only really need to center the data, so we provide the argument scale=FALSE. Also note that the scale function attaches the pre-centered mean (and standard deviation if scaling is performed) as attributes to the scaled data in order to facilitate back-scaling to the original scale. While these attributes are often convenient, they do cause issues for some of the Bayesian routines and so we will strip these attributes using the as.numeric function. Instead, we will create separate scalar variables to store the pre-scaled means.\n\u0026gt; data \u0026lt;- within(data, {\r+ cx1 \u0026lt;- as.numeric(scale(x1, scale = FALSE))\r+ cx2 \u0026lt;- as.numeric(scale(x2, scale = FALSE))\r+ })\r\u0026gt; head(data)\ry x1 x2 cx1 cx2\r1 2.426468 0.2875775 0.8300510 -0.21098147 -0.08302110\r2 4.927690 0.7883051 0.9634676 0.28974614 0.05039557\r3 3.176118 0.4089769 0.8157946 -0.08958207 -0.09727750\r4 6.166652 0.8830174 1.6608878 0.38445841 0.74781568\r5 4.788890 0.9404673 1.2352762 0.44190829 0.32220415\r6 2.541536 0.0455565 0.9267954 -0.45300249 0.01372335\r\u0026gt; \u0026gt; mean.x1 = mean(data$x1)\r\u0026gt; mean.x2 = mean(data$x2)\r\r\rAssumptions\rThe assumptions of the model are:\n\rAll of the observations are independent - this must be addressed at the design and collection stages.\n\rThe response variable (and thus the residuals) should be normally distributed. A boxplot of the entire variable is usually useful for diagnosing major issues with normality.\n\rThe response variable should be equally varied (variance should not be related to mean as these are supposed to be estimated separately). Scatterplots with linear smoothers can be useful for exploring the spread of observations around the trendline. The spread of observations around the trendline should not increase (or decrease) along its length.\n\rThe predictor variables should be uniformly or normally distributed. Again, boxplots can be useful.\n\rThe relationships between the linear predictors (right hand side of the regression formula) and the response variable should be linear. Scatterplots with smoothers can be useful for identifying possible non-linearity.\n\r(Multi)collinearity. The number of predictor variables must be less than the number of observations otherwise the linear model will be over-parameterized (more parameters to estimate than there are independent data from which estimates are calculated).\n\r\r(Multi)collinearity breaks the assumption that a predictor variable must not be correlated to the combination of other predictor variables (known collectively as the linear predictor). Multicollinearity has major detrimental effects on model fitting:\n\rInstability of the estimated partial regression slopes (small changes in the data or variable inclusion can cause dramatic changes in parameter estimates).\n\rInflated standard errors and confidence intervals of model parameters, thereby increasing the type II error rate (reducing power) of parameter hypothesis tests.\n\r\rMulticollinearity can be diagnosed with the following situatons:\n\rInvestigate pairwise correlations between all the predictor variables either by a correlation matrix or a scatterplot matrix\n\rCalculate the tolerance \\((1−r^2)\\) of the relationship between a predictor variable and all the other predictor variables for each of the predictor variables. Tolerance is a measure of the degree of collinearity and values less than \\(0.2\\) should be considered and values less than \\(0.1\\) should be given serious attention. Variance inflation factor (VIF) is the inverse of tolerance and thus values greater than \\(5\\), or worse, \\(10\\) indicate collinearity.\n\rPCA (principle components analysis) eigenvalues (from a correlation matrix for all the predictor variables) close to zero indicate collinearity and component loadings may be useful in determining which predictor variables cause collinearity.\n\r\rThere are several approaches to dealing with collinearity (however the first two of these are likely to result in biased parameter estimates):\nRemove the highly correlated predictor variable(s), starting with the least most clinically interesting variable(s)\n\rPCA (principle components analysis) regression - regress the response variable against the principal components resulting from a correlation matrix for all the predictor variables. Each of these principal components by definition are completely independent, but the resulting parameter estimates must be back-calculated in order to have any clinical meaning.\n\rApply a regression tree - regression trees recursively partitioning (subsetting) the data in accordance to individual variables that explain the greatest remaining variance. Since at each iteration, each predictor variable is effectively evaluated in isolation, (multi)collinearity is not an issue.\n\r\r\rModel fitting\rMultiple linear regression models can include predictors (terms) that are incorporated additively (no interactions) or multiplicatively (with interactions). As such we will explore these separately for each modelling tool. The observed responses (\\(y_i\\)) are assumed to be drawn from a normal distribution with a given mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The expected values are themselves determined by the linear predictor. In this case, \\(\\beta_0\\) represents the \\(y\\)-intercept (value of \\(y\\) when all of the \\(x\\)’s are equal to zero) and the set of \\(\\beta\\)’s represent the rates of change in y for every unit change in each \\(x\\) (the effect) holding each other \\(x\\) constant. Note that since we should always center all predictors (by subtracting the mean of each \\(x\\) from the repective values of each \\(x\\)), the \\(y\\)-intercept represents the value of \\(y\\) at the average value of each \\(x\\).\nMCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying “uninformative” priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (\\(100\\)) for both the intercept and the treatment effect and a wide half-cauchy (\\(\\text{scale}=5\\)) for the standard deviation:\n\\[ y_i \\sim \\text{Normal}(\\mu_i, \\sigma),\\]\nwhere \\(\\mu_i=\\beta_0 + \\boldsymbol \\beta \\boldsymbol X_i\\). Priors are specified as: \\(\\boldsymbol \\beta \\sim \\text{Normal}(0,1000)\\) and \\(\\sigma \\sim \\text{Cauchy}(0,5)\\). We will explore Bayesian modelling of multiple linear regression using STAN. The minimum model in STAN required to fit the above simple regression follows. Note the following modifications from the model defined in JAGS:\n\rThe normal distribution is defined by standard deviation rather than precision\n\rRather than using a uniform prior for \\(\\sigma\\), I am using a half-Cauchy\n\r\rAdditive model\rWe now translate the likelihood for the additive model into STAN code.\n\u0026gt; modelString = \u0026quot;\r+ data { + int\u0026lt;lower=1\u0026gt; n; // total number of observations + vector[n] Y; // response variable + int\u0026lt;lower=1\u0026gt; nX; // number of effects + matrix[n, nX] X; // model matrix + } + transformed data { + matrix[n, nX - 1] Xc; // centered version of X + vector[nX - 1] means_X; // column means of X before centering + + for (i in 2:nX) { + means_X[i - 1] = mean(X[, i]); + Xc[, i - 1] = X[, i] - means_X[i - 1]; + } + } + parameters { + vector[nX-1] beta; // population-level effects + real cbeta0; // center-scale intercept + real\u0026lt;lower=0\u0026gt; sigma; // residual SD + } + transformed parameters { + } + model { + vector[n] mu; + mu = Xc * beta + cbeta0; + // prior specifications + beta ~ normal(0, 100); + cbeta0 ~ normal(0, 100); + sigma ~ cauchy(0, 5); + // likelihood contribution + Y ~ normal(mu, sigma); + } + generated quantities {\r+ real beta0; // population-level intercept + vector[n] log_lik;\r+ beta0 = cbeta0 - dot_product(means_X, beta);\r+ for (i in 1:n) {\r+ log_lik[i] = normal_lpdf(Y[i] | Xc[i] * beta + cbeta0, sigma);\r+ } + }\r+ + \u0026quot;\r\u0026gt; ## write the model to a stan file \u0026gt; writeLines(modelString, con = \u0026quot;linregModeladd.stan\u0026quot;)\r\u0026gt; writeLines(modelString, con = \u0026quot;linregModelmult.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; X = model.matrix(~cx1 + cx2, data = data)\r\u0026gt; data.list \u0026lt;- with(data, list(Y = y, X = X, nX = ncol(X), n = nrow(data)))\rDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026quot;beta0\u0026quot;, \u0026quot;cbeta0\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;log_lik\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 1000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 3000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 2500\rNow compile and run the Stan code via the rstan interface. Note that the first time stan is run after the rstan package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\u0026gt; library(rstan)\rDuring the warmup stage, the No-U-Turn sampler (NUTS) attempts to determine the optimum stepsize - the stepsize that achieves the target acceptance rate (\\(0.8\\) or \\(80\\)% by default) without divergence (occurs when the stepsize is too large relative to the curvature of the log posterior and results in approximations that are likely to diverge and be biased) - and without hitting the maximum treedepth (\\(10\\)). At each iteration of the NUTS algorithm, the number of leapfrog steps doubles (as it increases the treedepth) and only terminates when either the NUTS criterion are satisfied or the tree depth reaches the maximum (\\(10\\) by default).\n\u0026gt; data.rstan.add \u0026lt;- stan(data = data.list, file = \u0026quot;linregModeladd.stan\u0026quot;, chains = nChains, pars = params,\r+ iter = nIter, warmup = burnInSteps, thin = thinSteps, save_dso = TRUE)\rSAMPLING FOR MODEL \u0026#39;linregModeladd\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 2500 [ 0%] (Warmup)\rChain 1: Iteration: 250 / 2500 [ 10%] (Warmup)\rChain 1: Iteration: 500 / 2500 [ 20%] (Warmup)\rChain 1: Iteration: 750 / 2500 [ 30%] (Warmup)\rChain 1: Iteration: 1000 / 2500 [ 40%] (Warmup)\rChain 1: Iteration: 1001 / 2500 [ 40%] (Sampling)\rChain 1: Iteration: 1250 / 2500 [ 50%] (Sampling)\rChain 1: Iteration: 1500 / 2500 [ 60%] (Sampling)\rChain 1: Iteration: 1750 / 2500 [ 70%] (Sampling)\rChain 1: Iteration: 2000 / 2500 [ 80%] (Sampling)\rChain 1: Iteration: 2250 / 2500 [ 90%] (Sampling)\rChain 1: Iteration: 2500 / 2500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.069 seconds (Warm-up)\rChain 1: 0.095 seconds (Sampling)\rChain 1: 0.164 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;linregModeladd\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 2500 [ 0%] (Warmup)\rChain 2: Iteration: 250 / 2500 [ 10%] (Warmup)\rChain 2: Iteration: 500 / 2500 [ 20%] (Warmup)\rChain 2: Iteration: 750 / 2500 [ 30%] (Warmup)\rChain 2: Iteration: 1000 / 2500 [ 40%] (Warmup)\rChain 2: Iteration: 1001 / 2500 [ 40%] (Sampling)\rChain 2: Iteration: 1250 / 2500 [ 50%] (Sampling)\rChain 2: Iteration: 1500 / 2500 [ 60%] (Sampling)\rChain 2: Iteration: 1750 / 2500 [ 70%] (Sampling)\rChain 2: Iteration: 2000 / 2500 [ 80%] (Sampling)\rChain 2: Iteration: 2250 / 2500 [ 90%] (Sampling)\rChain 2: Iteration: 2500 / 2500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.068 seconds (Warm-up)\rChain 2: 0.094 seconds (Sampling)\rChain 2: 0.162 seconds (Total)\rChain 2: \u0026gt; \u0026gt; data.rstan.add\rInference for Stan model: linregModeladd.\r2 chains, each with iter=2500; warmup=1000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 2.83 0.01 0.45 1.95 2.52 2.82 3.13 3.72 2562 1\rbeta[2] 1.58 0.01 0.38 0.84 1.33 1.58 1.85 2.32 2623 1\rbeta0 3.80 0.00 0.10 3.60 3.73 3.80 3.87 4.00 2672 1\rcbeta0 3.80 0.00 0.10 3.60 3.73 3.80 3.87 4.00 2672 1\rsigma 0.99 0.00 0.07 0.86 0.94 0.99 1.04 1.15 3017 1\rlog_lik[1] -1.13 0.00 0.09 -1.33 -1.19 -1.13 -1.07 -0.96 2825 1\rlog_lik[2] -0.95 0.00 0.08 -1.11 -1.00 -0.95 -0.89 -0.80 2813 1\rlog_lik[3] -0.94 0.00 0.07 -1.09 -0.99 -0.94 -0.89 -0.80 2983 1\rlog_lik[4] -0.94 0.00 0.09 -1.13 -1.00 -0.94 -0.88 -0.78 2666 1\rlog_lik[5] -1.23 0.00 0.15 -1.56 -1.32 -1.22 -1.13 -0.98 3319 1\rlog_lik[6] -0.94 0.00 0.08 -1.11 -0.99 -0.93 -0.88 -0.78 2591 1\rlog_lik[7] -1.26 0.00 0.15 -1.60 -1.36 -1.25 -1.15 -1.00 2637 1\rlog_lik[8] -2.00 0.00 0.28 -2.59 -2.18 -1.99 -1.81 -1.54 3642 1\rlog_lik[9] -1.00 0.00 0.08 -1.16 -1.05 -0.99 -0.94 -0.86 2885 1\rlog_lik[10] -1.43 0.00 0.17 -1.81 -1.53 -1.41 -1.30 -1.13 2549 1\rlog_lik[11] -0.94 0.00 0.09 -1.12 -0.99 -0.94 -0.88 -0.78 2568 1\rlog_lik[12] -1.14 0.00 0.10 -1.35 -1.20 -1.13 -1.07 -0.97 2419 1\rlog_lik[13] -2.48 0.01 0.39 -3.32 -2.73 -2.44 -2.21 -1.82 2526 1\rlog_lik[14] -0.93 0.00 0.08 -1.10 -0.98 -0.93 -0.88 -0.78 2702 1\rlog_lik[15] -1.16 0.00 0.14 -1.46 -1.24 -1.14 -1.06 -0.93 2584 1\rlog_lik[16] -0.95 0.00 0.09 -1.14 -1.01 -0.95 -0.89 -0.79 2460 1\rlog_lik[17] -0.95 0.00 0.08 -1.11 -1.00 -0.94 -0.89 -0.80 2913 1\rlog_lik[18] -1.16 0.00 0.17 -1.55 -1.26 -1.14 -1.04 -0.89 2488 1\rlog_lik[19] -1.25 0.00 0.10 -1.46 -1.32 -1.25 -1.18 -1.06 2670 1\rlog_lik[20] -1.34 0.00 0.17 -1.73 -1.44 -1.32 -1.21 -1.04 3156 1\rlog_lik[21] -0.99 0.00 0.10 -1.20 -1.05 -0.99 -0.93 -0.82 3217 1\rlog_lik[22] -1.43 0.00 0.14 -1.74 -1.52 -1.42 -1.33 -1.18 2520 1\rlog_lik[23] -1.07 0.00 0.09 -1.26 -1.13 -1.06 -1.01 -0.90 2655 1\rlog_lik[24] -0.97 0.00 0.10 -1.18 -1.02 -0.96 -0.90 -0.80 2490 1\rlog_lik[25] -2.60 0.01 0.29 -3.21 -2.78 -2.59 -2.40 -2.08 2818 1\rlog_lik[26] -1.05 0.00 0.12 -1.33 -1.12 -1.04 -0.96 -0.85 2885 1\rlog_lik[27] -0.95 0.00 0.08 -1.12 -1.00 -0.95 -0.89 -0.80 2646 1\rlog_lik[28] -0.93 0.00 0.08 -1.09 -0.98 -0.93 -0.87 -0.78 2601 1\rlog_lik[29] -1.15 0.00 0.14 -1.46 -1.24 -1.14 -1.05 -0.92 3221 1\rlog_lik[30] -0.93 0.00 0.08 -1.09 -0.97 -0.92 -0.87 -0.78 2844 1\rlog_lik[31] -2.54 0.01 0.39 -3.37 -2.78 -2.52 -2.27 -1.86 3492 1\rlog_lik[32] -1.34 0.00 0.21 -1.82 -1.47 -1.32 -1.19 -1.00 3489 1\rlog_lik[33] -0.92 0.00 0.08 -1.08 -0.97 -0.92 -0.87 -0.78 2945 1\rlog_lik[34] -0.95 0.00 0.08 -1.13 -1.01 -0.95 -0.90 -0.81 2937 1\rlog_lik[35] -2.25 0.01 0.34 -2.96 -2.48 -2.23 -2.01 -1.66 3180 1\rlog_lik[36] -1.55 0.00 0.13 -1.83 -1.64 -1.54 -1.46 -1.32 2570 1\rlog_lik[37] -1.78 0.00 0.25 -2.32 -1.93 -1.76 -1.60 -1.35 3392 1\rlog_lik[38] -1.21 0.00 0.14 -1.50 -1.30 -1.20 -1.11 -0.98 2419 1\rlog_lik[39] -2.57 0.01 0.42 -3.49 -2.83 -2.54 -2.28 -1.86 2458 1\rlog_lik[40] -1.70 0.00 0.18 -2.08 -1.82 -1.69 -1.58 -1.37 3038 1\rlog_lik[41] -1.59 0.00 0.21 -2.06 -1.73 -1.57 -1.44 -1.23 3270 1\rlog_lik[42] -0.94 0.00 0.08 -1.09 -0.99 -0.94 -0.89 -0.80 2984 1\rlog_lik[43] -1.97 0.01 0.32 -2.67 -2.18 -1.94 -1.74 -1.42 2786 1\rlog_lik[44] -1.86 0.00 0.24 -2.37 -2.02 -1.85 -1.70 -1.45 2785 1\rlog_lik[45] -2.24 0.01 0.35 -2.98 -2.47 -2.22 -1.99 -1.65 2523 1\rlog_lik[46] -0.93 0.00 0.08 -1.10 -0.98 -0.93 -0.88 -0.78 2849 1\rlog_lik[47] -1.58 0.00 0.20 -2.01 -1.71 -1.56 -1.43 -1.22 3202 1\rlog_lik[48] -1.22 0.00 0.15 -1.56 -1.31 -1.21 -1.11 -0.97 2573 1\rlog_lik[49] -3.84 0.01 0.54 -5.01 -4.18 -3.82 -3.46 -2.87 3218 1\rlog_lik[50] -1.47 0.00 0.20 -1.90 -1.59 -1.45 -1.34 -1.14 3648 1\rlog_lik[51] -1.33 0.00 0.20 -1.78 -1.46 -1.31 -1.18 -1.01 2469 1\rlog_lik[52] -1.23 0.00 0.09 -1.42 -1.29 -1.23 -1.17 -1.07 2508 1\rlog_lik[53] -0.98 0.00 0.08 -1.15 -1.03 -0.98 -0.92 -0.82 2892 1\rlog_lik[54] -1.05 0.00 0.12 -1.31 -1.12 -1.03 -0.97 -0.85 3408 1\rlog_lik[55] -0.94 0.00 0.08 -1.11 -0.99 -0.93 -0.88 -0.79 2682 1\rlog_lik[56] -0.92 0.00 0.08 -1.08 -0.97 -0.92 -0.87 -0.78 2941 1\rlog_lik[57] -1.26 0.00 0.14 -1.57 -1.35 -1.25 -1.16 -1.03 2851 1\rlog_lik[58] -1.03 0.00 0.10 -1.25 -1.09 -1.02 -0.96 -0.85 2528 1\rlog_lik[59] -1.53 0.00 0.19 -1.94 -1.64 -1.51 -1.40 -1.20 3250 1\rlog_lik[60] -0.95 0.00 0.08 -1.12 -1.00 -0.95 -0.89 -0.80 2944 1\rlog_lik[61] -1.48 0.00 0.12 -1.75 -1.56 -1.48 -1.40 -1.26 2941 1\rlog_lik[62] -1.09 0.00 0.12 -1.36 -1.16 -1.08 -1.01 -0.89 3504 1\rlog_lik[63] -1.74 0.00 0.16 -2.08 -1.85 -1.73 -1.62 -1.45 2551 1\rlog_lik[64] -7.01 0.02 0.96 -9.02 -7.60 -6.96 -6.33 -5.26 3101 1\rlog_lik[65] -1.01 0.00 0.09 -1.22 -1.07 -1.01 -0.95 -0.85 2752 1\rlog_lik[66] -0.96 0.00 0.08 -1.11 -1.01 -0.96 -0.91 -0.82 2946 1\rlog_lik[67] -1.29 0.00 0.15 -1.62 -1.38 -1.27 -1.18 -1.03 3487 1\rlog_lik[68] -1.09 0.00 0.12 -1.35 -1.16 -1.08 -1.01 -0.89 2517 1\rlog_lik[69] -1.07 0.00 0.10 -1.27 -1.13 -1.06 -1.00 -0.89 2958 1\rlog_lik[70] -1.02 0.00 0.09 -1.20 -1.07 -1.01 -0.96 -0.85 2673 1\rlog_lik[71] -0.93 0.00 0.08 -1.08 -0.98 -0.93 -0.88 -0.79 2896 1\rlog_lik[72] -0.92 0.00 0.08 -1.08 -0.97 -0.92 -0.87 -0.78 2738 1\rlog_lik[73] -0.93 0.00 0.08 -1.10 -0.98 -0.93 -0.88 -0.78 2813 1\rlog_lik[74] -3.84 0.01 0.63 -5.17 -4.23 -3.80 -3.39 -2.75 2911 1\rlog_lik[75] -1.22 0.00 0.10 -1.41 -1.28 -1.21 -1.15 -1.04 2633 1\rlog_lik[76] -1.42 0.00 0.15 -1.73 -1.52 -1.41 -1.31 -1.16 2747 1\rlog_lik[77] -0.93 0.00 0.08 -1.08 -0.97 -0.92 -0.87 -0.78 2978 1\rlog_lik[78] -0.96 0.00 0.08 -1.11 -1.01 -0.96 -0.91 -0.81 3039 1\rlog_lik[79] -0.99 0.00 0.10 -1.20 -1.05 -0.98 -0.93 -0.82 2575 1\rlog_lik[80] -0.94 0.00 0.08 -1.11 -1.00 -0.94 -0.89 -0.80 2971 1\rlog_lik[81] -1.56 0.00 0.21 -1.99 -1.69 -1.54 -1.40 -1.21 2434 1\rlog_lik[82] -1.68 0.00 0.17 -2.06 -1.78 -1.67 -1.56 -1.37 2568 1\rlog_lik[83] -0.99 0.00 0.08 -1.15 -1.04 -0.99 -0.94 -0.84 2818 1\rlog_lik[84] -1.36 0.00 0.16 -1.72 -1.46 -1.35 -1.25 -1.09 2593 1\rlog_lik[85] -0.93 0.00 0.08 -1.08 -0.97 -0.92 -0.87 -0.78 2878 1\rlog_lik[86] -0.93 0.00 0.07 -1.08 -0.98 -0.93 -0.88 -0.79 2977 1\rlog_lik[87] -1.62 0.00 0.25 -2.18 -1.77 -1.59 -1.44 -1.19 2899 1\rlog_lik[88] -0.96 0.00 0.09 -1.15 -1.02 -0.96 -0.90 -0.80 3100 1\rlog_lik[89] -1.65 0.00 0.28 -2.28 -1.82 -1.62 -1.44 -1.18 3480 1\rlog_lik[90] -1.09 0.00 0.13 -1.38 -1.17 -1.07 -1.00 -0.88 2482 1\rlog_lik[91] -1.18 0.00 0.14 -1.51 -1.27 -1.17 -1.08 -0.95 3154 1\rlog_lik[92] -0.99 0.00 0.08 -1.17 -1.04 -0.98 -0.93 -0.84 2766 1\rlog_lik[93] -0.93 0.00 0.08 -1.10 -0.98 -0.93 -0.88 -0.78 2556 1\rlog_lik[94] -1.31 0.00 0.11 -1.55 -1.38 -1.31 -1.24 -1.11 3091 1\rlog_lik[95] -1.96 0.01 0.30 -2.60 -2.15 -1.94 -1.74 -1.47 2459 1\rlog_lik[96] -3.52 0.01 0.47 -4.52 -3.81 -3.50 -3.19 -2.69 3235 1\rlog_lik[97] -1.11 0.00 0.10 -1.32 -1.18 -1.10 -1.04 -0.93 2932 1\rlog_lik[98] -1.48 0.00 0.19 -1.90 -1.61 -1.47 -1.34 -1.15 2845 1\rlog_lik[99] -1.08 0.00 0.11 -1.33 -1.15 -1.07 -1.00 -0.89 2761 1\rlog_lik[100] -1.66 0.00 0.13 -1.94 -1.74 -1.65 -1.56 -1.42 2616 1\rlp__ -48.86 0.04 1.42 -52.37 -49.58 -48.52 -47.80 -47.06 1447 1\rSamples were drawn using NUTS(diag_e) at Thu Feb 13 15:27:59 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\rMultiplicative model\rWe now translate the likelihood for the multiplicative model into STAN code. Arrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; X = model.matrix(~cx1 * cx2, data = data)\r\u0026gt; data.list \u0026lt;- with(data, list(Y = y, X = X, nX = ncol(X), n = nrow(data)))\rDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026quot;beta0\u0026quot;, \u0026quot;cbeta0\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;log_lik\u0026quot;)\r\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 1000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 3000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 2500\rNow compile and run the Stan code via the rstan interface.\n\u0026gt; data.rstan.mult \u0026lt;- stan(data = data.list, file = \u0026quot;linregModelmult.stan\u0026quot;, chains = nChains, pars = params,\r+ iter = nIter, warmup = burnInSteps, thin = thinSteps, save_dso = TRUE)\rSAMPLING FOR MODEL \u0026#39;linregModeladd\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 2500 [ 0%] (Warmup)\rChain 1: Iteration: 250 / 2500 [ 10%] (Warmup)\rChain 1: Iteration: 500 / 2500 [ 20%] (Warmup)\rChain 1: Iteration: 750 / 2500 [ 30%] (Warmup)\rChain 1: Iteration: 1000 / 2500 [ 40%] (Warmup)\rChain 1: Iteration: 1001 / 2500 [ 40%] (Sampling)\rChain 1: Iteration: 1250 / 2500 [ 50%] (Sampling)\rChain 1: Iteration: 1500 / 2500 [ 60%] (Sampling)\rChain 1: Iteration: 1750 / 2500 [ 70%] (Sampling)\rChain 1: Iteration: 2000 / 2500 [ 80%] (Sampling)\rChain 1: Iteration: 2250 / 2500 [ 90%] (Sampling)\rChain 1: Iteration: 2500 / 2500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.079 seconds (Warm-up)\rChain 1: 0.098 seconds (Sampling)\rChain 1: 0.177 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;linregModeladd\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 2500 [ 0%] (Warmup)\rChain 2: Iteration: 250 / 2500 [ 10%] (Warmup)\rChain 2: Iteration: 500 / 2500 [ 20%] (Warmup)\rChain 2: Iteration: 750 / 2500 [ 30%] (Warmup)\rChain 2: Iteration: 1000 / 2500 [ 40%] (Warmup)\rChain 2: Iteration: 1001 / 2500 [ 40%] (Sampling)\rChain 2: Iteration: 1250 / 2500 [ 50%] (Sampling)\rChain 2: Iteration: 1500 / 2500 [ 60%] (Sampling)\rChain 2: Iteration: 1750 / 2500 [ 70%] (Sampling)\rChain 2: Iteration: 2000 / 2500 [ 80%] (Sampling)\rChain 2: Iteration: 2250 / 2500 [ 90%] (Sampling)\rChain 2: Iteration: 2500 / 2500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.079 seconds (Warm-up)\rChain 2: 0.096 seconds (Sampling)\rChain 2: 0.175 seconds (Total)\rChain 2: \u0026gt; \u0026gt; data.rstan.mult\rInference for Stan model: linregModeladd.\r2 chains, each with iter=2500; warmup=1000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta[1] 2.81 0.01 0.45 1.90 2.51 2.81 3.11 3.69 3050 1\rbeta[2] 1.50 0.01 0.38 0.77 1.24 1.50 1.77 2.26 2954 1\rbeta[3] 1.41 0.02 1.22 -0.98 0.58 1.44 2.24 3.76 3328 1\rbeta0 3.72 0.00 0.12 3.48 3.63 3.71 3.80 3.96 3353 1\rcbeta0 3.80 0.00 0.10 3.61 3.73 3.80 3.87 4.00 3449 1\rsigma 0.99 0.00 0.07 0.86 0.94 0.99 1.04 1.15 3271 1\rlog_lik[1] -1.10 0.00 0.09 -1.29 -1.17 -1.10 -1.04 -0.93 3407 1\rlog_lik[2] -0.97 0.00 0.09 -1.15 -1.02 -0.97 -0.91 -0.82 3196 1\rlog_lik[3] -0.93 0.00 0.07 -1.08 -0.98 -0.93 -0.88 -0.79 3279 1\rlog_lik[4] -0.98 0.00 0.11 -1.25 -1.03 -0.96 -0.90 -0.80 2510 1\rlog_lik[5] -1.31 0.00 0.18 -1.72 -1.41 -1.29 -1.18 -1.01 3885 1\rlog_lik[6] -0.94 0.00 0.08 -1.12 -0.99 -0.94 -0.88 -0.79 2606 1\rlog_lik[7] -1.19 0.00 0.15 -1.55 -1.28 -1.17 -1.08 -0.93 3240 1\rlog_lik[8] -2.17 0.01 0.34 -2.93 -2.38 -2.14 -1.93 -1.60 4441 1\rlog_lik[9] -0.97 0.00 0.08 -1.13 -1.02 -0.97 -0.92 -0.82 3225 1\rlog_lik[10] -1.45 0.00 0.18 -1.85 -1.56 -1.43 -1.32 -1.14 3432 1\rlog_lik[11] -1.05 0.00 0.19 -1.55 -1.13 -1.01 -0.93 -0.82 2965 1\rlog_lik[12] -1.17 0.00 0.10 -1.39 -1.23 -1.16 -1.09 -0.98 3567 1\rlog_lik[13] -2.26 0.01 0.41 -3.15 -2.52 -2.23 -1.96 -1.54 3444 1\rlog_lik[14] -0.93 0.00 0.08 -1.09 -0.98 -0.93 -0.88 -0.79 2841 1\rlog_lik[15] -1.16 0.00 0.13 -1.45 -1.24 -1.14 -1.06 -0.93 3297 1\rlog_lik[16] -0.99 0.00 0.11 -1.26 -1.05 -0.98 -0.91 -0.81 2781 1\rlog_lik[17] -0.95 0.00 0.08 -1.10 -1.00 -0.95 -0.89 -0.80 3330 1\rlog_lik[18] -1.09 0.00 0.16 -1.46 -1.18 -1.06 -0.97 -0.84 2960 1\rlog_lik[19] -1.21 0.00 0.10 -1.42 -1.27 -1.20 -1.13 -1.02 3358 1\rlog_lik[20] -1.39 0.00 0.19 -1.84 -1.50 -1.37 -1.25 -1.07 3724 1\rlog_lik[21] -0.96 0.00 0.09 -1.16 -1.02 -0.96 -0.90 -0.80 3449 1\rlog_lik[22] -1.34 0.00 0.15 -1.69 -1.44 -1.33 -1.23 -1.08 3373 1\rlog_lik[23] -1.02 0.00 0.10 -1.24 -1.08 -1.02 -0.96 -0.85 3117 1\rlog_lik[24] -0.96 0.00 0.09 -1.17 -1.02 -0.95 -0.90 -0.80 2550 1\rlog_lik[25] -2.77 0.01 0.35 -3.52 -3.00 -2.74 -2.53 -2.13 3713 1\rlog_lik[26] -1.08 0.00 0.14 -1.40 -1.16 -1.06 -0.99 -0.86 3525 1\rlog_lik[27] -0.97 0.00 0.09 -1.17 -1.02 -0.97 -0.91 -0.81 2981 1\rlog_lik[28] -0.94 0.00 0.08 -1.11 -0.99 -0.94 -0.88 -0.79 2570 1\rlog_lik[29] -1.26 0.00 0.18 -1.67 -1.36 -1.23 -1.13 -0.97 3476 1\rlog_lik[30] -0.93 0.00 0.08 -1.08 -0.98 -0.92 -0.87 -0.78 3170 1\rlog_lik[31] -2.23 0.01 0.42 -3.14 -2.51 -2.19 -1.92 -1.51 4024 1\rlog_lik[32] -1.17 0.00 0.22 -1.69 -1.28 -1.12 -1.00 -0.86 3923 1\rlog_lik[33] -0.93 0.00 0.07 -1.08 -0.98 -0.93 -0.87 -0.79 3259 1\rlog_lik[34] -0.98 0.00 0.09 -1.16 -1.03 -0.97 -0.92 -0.81 3436 1\rlog_lik[35] -2.63 0.01 0.52 -3.77 -2.96 -2.60 -2.27 -1.76 3962 1\rlog_lik[36] -1.67 0.00 0.18 -2.05 -1.78 -1.66 -1.54 -1.35 3477 1\rlog_lik[37] -1.86 0.00 0.27 -2.44 -2.03 -1.83 -1.67 -1.41 4335 1\rlog_lik[38] -1.29 0.00 0.17 -1.67 -1.40 -1.28 -1.17 -1.01 3135 1\rlog_lik[39] -2.94 0.01 0.58 -4.21 -3.31 -2.90 -2.52 -1.98 3294 1\rlog_lik[40] -1.78 0.00 0.20 -2.20 -1.91 -1.76 -1.63 -1.42 3848 1\rlog_lik[41] -1.38 0.00 0.24 -1.96 -1.52 -1.35 -1.21 -1.00 3613 1\rlog_lik[42] -0.93 0.00 0.07 -1.08 -0.98 -0.93 -0.88 -0.79 3311 1\rlog_lik[43] -2.03 0.01 0.34 -2.78 -2.24 -2.00 -1.78 -1.44 3238 1\rlog_lik[44] -1.92 0.00 0.25 -2.47 -2.08 -1.90 -1.73 -1.47 3460 1\rlog_lik[45] -2.08 0.01 0.34 -2.82 -2.30 -2.05 -1.84 -1.50 3103 1\rlog_lik[46] -1.00 0.00 0.13 -1.31 -1.06 -0.98 -0.91 -0.81 2724 1\rlog_lik[47] -1.77 0.00 0.28 -2.40 -1.95 -1.74 -1.57 -1.29 3795 1\rlog_lik[48] -1.24 0.00 0.16 -1.60 -1.34 -1.22 -1.13 -0.98 3253 1\rlog_lik[49] -3.58 0.01 0.54 -4.72 -3.92 -3.54 -3.20 -2.60 3539 1\rlog_lik[50] -1.62 0.00 0.26 -2.21 -1.78 -1.60 -1.44 -1.20 4473 1\rlog_lik[51] -1.38 0.00 0.22 -1.87 -1.51 -1.35 -1.22 -1.03 3141 1\rlog_lik[52] -1.29 0.00 0.11 -1.51 -1.36 -1.28 -1.22 -1.10 3466 1\rlog_lik[53] -1.00 0.00 0.09 -1.19 -1.05 -0.99 -0.94 -0.83 3475 1\rlog_lik[54] -1.25 0.00 0.25 -1.85 -1.39 -1.21 -1.07 -0.90 3457 1\rlog_lik[55] -0.93 0.00 0.08 -1.10 -0.98 -0.93 -0.88 -0.79 2537 1\rlog_lik[56] -0.93 0.00 0.08 -1.09 -0.98 -0.93 -0.88 -0.78 3031 1\rlog_lik[57] -1.21 0.00 0.14 -1.51 -1.29 -1.19 -1.11 -0.98 3535 1\rlog_lik[58] -0.99 0.00 0.10 -1.22 -1.05 -0.98 -0.92 -0.82 2718 1\rlog_lik[59] -1.50 0.00 0.19 -1.92 -1.61 -1.48 -1.36 -1.17 3926 1\rlog_lik[60] -0.96 0.00 0.08 -1.13 -1.01 -0.95 -0.90 -0.81 3188 1\rlog_lik[61] -1.56 0.00 0.15 -1.88 -1.66 -1.55 -1.45 -1.29 3824 1\rlog_lik[62] -1.28 0.00 0.24 -1.84 -1.42 -1.24 -1.11 -0.93 3575 1\rlog_lik[63] -1.63 0.00 0.17 -1.99 -1.75 -1.62 -1.51 -1.33 3227 1\rlog_lik[64] -6.83 0.02 0.94 -8.75 -7.44 -6.78 -6.16 -5.13 3520 1\rlog_lik[65] -0.99 0.00 0.09 -1.20 -1.05 -0.99 -0.93 -0.83 3065 1\rlog_lik[66] -0.99 0.00 0.08 -1.15 -1.04 -0.99 -0.94 -0.85 3279 1\rlog_lik[67] -1.22 0.00 0.15 -1.54 -1.31 -1.21 -1.11 -0.97 4342 1\rlog_lik[68] -1.04 0.00 0.12 -1.31 -1.11 -1.03 -0.96 -0.85 2870 1\rlog_lik[69] -1.09 0.00 0.10 -1.32 -1.16 -1.09 -1.02 -0.91 3711 1\rlog_lik[70] -1.03 0.00 0.09 -1.22 -1.09 -1.02 -0.97 -0.87 3430 1\rlog_lik[71] -0.93 0.00 0.08 -1.08 -0.98 -0.93 -0.88 -0.79 3117 1\rlog_lik[72] -0.93 0.00 0.08 -1.09 -0.98 -0.93 -0.88 -0.79 2675 1\rlog_lik[73] -0.93 0.00 0.08 -1.09 -0.98 -0.93 -0.87 -0.79 3199 1\rlog_lik[74] -3.70 0.01 0.62 -5.03 -4.11 -3.66 -3.25 -2.63 3528 1\rlog_lik[75] -1.15 0.00 0.11 -1.37 -1.22 -1.14 -1.07 -0.96 3241 1\rlog_lik[76] -1.40 0.00 0.14 -1.70 -1.49 -1.39 -1.30 -1.15 3445 1\rlog_lik[77] -0.93 0.00 0.07 -1.08 -0.98 -0.93 -0.88 -0.79 3260 1\rlog_lik[78] -0.99 0.00 0.08 -1.15 -1.04 -0.99 -0.93 -0.84 3340 1\rlog_lik[79] -1.07 0.00 0.13 -1.36 -1.14 -1.05 -0.97 -0.86 3060 1\rlog_lik[80] -0.97 0.00 0.09 -1.15 -1.02 -0.96 -0.91 -0.81 3250 1\rlog_lik[81] -1.42 0.00 0.21 -1.88 -1.55 -1.40 -1.27 -1.07 3112 1\rlog_lik[82] -1.80 0.00 0.23 -2.30 -1.95 -1.79 -1.64 -1.40 3734 1\rlog_lik[83] -0.96 0.00 0.08 -1.13 -1.01 -0.96 -0.90 -0.81 3208 1\rlog_lik[84] -1.29 0.00 0.17 -1.66 -1.38 -1.27 -1.17 -1.01 3325 1\rlog_lik[85] -0.93 0.00 0.08 -1.08 -0.98 -0.93 -0.87 -0.78 3030 1\rlog_lik[86] -0.92 0.00 0.07 -1.07 -0.97 -0.92 -0.87 -0.78 3172 1\rlog_lik[87] -1.62 0.00 0.26 -2.22 -1.78 -1.60 -1.43 -1.19 3500 1\rlog_lik[88] -0.95 0.00 0.08 -1.13 -1.00 -0.94 -0.89 -0.80 3265 1\rlog_lik[89] -1.41 0.00 0.30 -2.11 -1.58 -1.36 -1.18 -0.96 4135 1\rlog_lik[90] -1.02 0.00 0.12 -1.31 -1.10 -1.01 -0.94 -0.82 2936 1\rlog_lik[91] -1.06 0.00 0.15 -1.44 -1.13 -1.03 -0.95 -0.83 3111 1\rlog_lik[92] -0.96 0.00 0.08 -1.14 -1.01 -0.96 -0.91 -0.81 2933 1\rlog_lik[93] -0.96 0.00 0.09 -1.17 -1.01 -0.95 -0.89 -0.80 2348 1\rlog_lik[94] -1.27 0.00 0.11 -1.52 -1.34 -1.26 -1.18 -1.07 3617 1\rlog_lik[95] -1.74 0.01 0.32 -2.44 -1.93 -1.70 -1.51 -1.21 3087 1\rlog_lik[96] -3.34 0.01 0.46 -4.30 -3.63 -3.31 -3.02 -2.54 3551 1\rlog_lik[97] -1.14 0.00 0.11 -1.39 -1.21 -1.14 -1.06 -0.95 3744 1\rlog_lik[98] -1.53 0.00 0.20 -1.97 -1.65 -1.51 -1.38 -1.18 3594 1\rlog_lik[99] -1.07 0.00 0.11 -1.31 -1.13 -1.06 -0.99 -0.87 3345 1\rlog_lik[100] -1.56 0.00 0.15 -1.87 -1.65 -1.54 -1.45 -1.30 3334 1\rlp__ -48.64 0.04 1.58 -52.44 -49.52 -48.28 -47.44 -46.52 1457 1\rSamples were drawn using NUTS(diag_e) at Thu Feb 13 15:28:01 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots for the effects model as an example.\n\u0026gt; library(mcmcplots)\r\u0026gt; s = as.array(data.rstan.mult)\r\u0026gt; mcmc \u0026lt;- do.call(mcmc.list, plyr:::alply(s[, , -(length(s[1, 1, ]))], 2, as.mcmc))\r\u0026gt; denplot(mcmc, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta\u0026quot;,\u0026quot;cbeta0\u0026quot;,\u0026quot;sigma\u0026quot;))\r\u0026gt; traplot(mcmc, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta\u0026quot;,\u0026quot;cbeta0\u0026quot;,\u0026quot;sigma\u0026quot;))\rThese plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.\n\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(mcmc)\r$`1`\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\r$`2`\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 You need a sample size of at least 3746 with these values of q, r and s\rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; stan_ac(data.rstan.mult, pars = c(\u0026quot;beta\u0026quot;,\u0026quot;beta0\u0026quot;))\rA lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).\n\u0026gt; stan_ac(data.rstan.mult, pars = c(\u0026quot;beta\u0026quot;,\u0026quot;beta0\u0026quot;))\r\u0026gt; stan_ess(data.rstan.mult, pars = c(\u0026quot;beta\u0026quot;,\u0026quot;beta0\u0026quot;))\rRhat and effective sample size. In this instance, most of the parameters have reasonably high effective samples and thus there is likely to be a good range of values from which to estimate paramter properties.\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. Ideally, a good model should also be able to predict the data used to fit the model. Residuals are not computed directly within rstan However, we can calculate them manually form the posteriors.\n\u0026gt; library(ggplot2)\r\u0026gt; library(dplyr)\r\u0026gt; mcmc = as.data.frame(data.rstan.mult) %\u0026gt;% dplyr:::select(beta0, starts_with(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:4], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit))\rResiduals against predictors\n\u0026gt; library(tidyr)\r\u0026gt; mcmc = as.data.frame(data.rstan.mult) %\u0026gt;% dplyr:::select(beta0, starts_with(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = newdata\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:4], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; newdata = data %\u0026gt;% cbind(fit, resid)\r\u0026gt; newdata.melt = newdata %\u0026gt;% gather(key = Pred, value = Value, cx1:cx2)\r\u0026gt; ggplot(newdata.melt) + geom_point(aes(y = resid, x = Value)) + facet_wrap(~Pred)\rAnd now for studentised residuals\n\u0026gt; mcmc = as.data.frame(data.rstan.mult) %\u0026gt;% dplyr:::select(beta0, starts_with(\u0026quot;beta\u0026quot;),\r+ sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc[, 1:4], 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; sresid = resid/sd(resid)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit))\rFor this simple model, the studentized residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = as.data.frame(data.rstan.mult) %\u0026gt;% dplyr:::select(beta0,\r+ starts_with(\u0026quot;beta\u0026quot;), sigma) %\u0026gt;% as.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, data)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, 1:4]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i,\r+ ], mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; ggplot() + geom_density(data = NULL, aes(x = as.vector(yRep),\r+ fill = \u0026quot;Model\u0026quot;), alpha = 0.5) + geom_density(data = data,\r+ aes(x = y, fill = \u0026quot;Obs\u0026quot;), alpha = 0.5)\rWe can also explore the posteriors of each parameter.\n\u0026gt; library(bayesplot)\r\u0026gt; mcmc_intervals(as.matrix(data.rstan.mult), regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\u0026gt; mcmc_areas(as.matrix(data.rstan.mult), regex_pars = \u0026quot;beta|sigma\u0026quot;)\r\rParameter estimates\rAlthough all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and \\(95\\)% credibility intervals.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\rFirst, we look at the results from the additive model.\n\u0026gt; print(data.rstan.add, pars = c(\u0026quot;beta0\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: linregModeladd.\r2 chains, each with iter=2500; warmup=1000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta0 3.80 0.00 0.10 3.60 3.73 3.80 3.87 4.00 2672 1\rbeta[1] 2.83 0.01 0.45 1.95 2.52 2.82 3.13 3.72 2562 1\rbeta[2] 1.58 0.01 0.38 0.84 1.33 1.58 1.85 2.32 2623 1\rsigma 0.99 0.00 0.07 0.86 0.94 0.99 1.04 1.15 3017 1\rSamples were drawn using NUTS(diag_e) at Thu Feb 13 15:27:59 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(data.rstan.add, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;,\r+ pars = c(\u0026quot;beta0\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\r# A tibble: 4 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta0 3.80 0.101 3.59 3.99\r2 beta[1] 2.83 0.445 1.93 3.69\r3 beta[2] 1.58 0.377 0.823 2.31\r4 sigma 0.994 0.0740 0.856 1.15\rConclusions\n\rWhen cx2 is held constant, a one unit increase in cx1 is associated with a \\(2.83\\) change in \\(y\\). That is, \\(y\\) increases at a rate of \\(2.83\\) per unit increase in cx1 when standardised for cx2.\n\rWhen cx1 is held constant, a one unit increase in cx2 is associated with a \\(1.58\\) change in \\(y\\). That is, \\(y\\) increases at a rate of \\(1.58\\) per unit increase in cx2 when standardised for cx1.\n\r\rNote, as this is an additive model, the rates associated with cx1 are assumed to be constant throughtout the range of cx2 and vice versa. The \\(95\\)% confidence interval for each partial slope does not overlap with \\(0\\) implying a significant effects of cx1 and cx2 on \\(y\\). While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; mcmcpvalue(as.matrix(data.rstan.add)[, \u0026quot;beta[1]\u0026quot;])\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan.add)[, \u0026quot;beta[2]\u0026quot;])\r[1] 0\rWith a p-value of essentially \\(0\\), we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship. Next, we look at the results from the multiplicative model.\n\u0026gt; print(data.rstan.mult, pars = c(\u0026quot;beta0\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\rInference for Stan model: linregModeladd.\r2 chains, each with iter=2500; warmup=1000; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat\rbeta0 3.72 0.00 0.12 3.48 3.63 3.71 3.80 3.96 3353 1\rbeta[1] 2.81 0.01 0.45 1.90 2.51 2.81 3.11 3.69 3050 1\rbeta[2] 1.50 0.01 0.38 0.77 1.24 1.50 1.77 2.26 2954 1\rbeta[3] 1.41 0.02 1.22 -0.98 0.58 1.44 2.24 3.76 3328 1\rsigma 0.99 0.00 0.07 0.86 0.94 0.99 1.04 1.15 3271 1\rSamples were drawn using NUTS(diag_e) at Thu Feb 13 15:28:01 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(data.rstan.mult, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;,\r+ pars = c(\u0026quot;beta0\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;))\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta0 3.72 0.124 3.48 3.96\r2 beta[1] 2.81 0.455 1.96 3.73\r3 beta[2] 1.50 0.385 0.769 2.26\r4 beta[3] 1.41 1.22 -0.985 3.76\r5 sigma 0.993 0.0725 0.850 1.13\rConclusions\n\rAt the average level of cx2 (=0), a one unit increase in cx1 is associated with a \\(2.80\\) change in y. That is, y increases at a rate of \\(2.80\\) per unit increase in cx1 when standardised for cx2.\n\rAt the average level of cx1 (=0), a one unit increase in cx2 is associated with a \\(1.50\\) change in \\(y\\). That is, \\(y\\) increases at a rate of \\(1.50\\) per unit increase in cx2 when standardised for cx1.\n\rThe degree to which the rate of change in response associated with a one unit change in cx1 changes over the range of cx2 (and vice versa) is \\(1.45\\).\n\r\rThe \\(95\\)% confidence intervals for the interaction partial slope does not overlap with \\(0\\) implying a significant interaction between cx1 and cx2. This suggests that the nature of the relationship between \\(y\\) and cx1 depends on the level of cx2 (and vice versa). The estimates of the effect of cx1 are only appropriate when cx2 = 0 etc. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(as.matrix(data.rstan.mult)[, \u0026quot;beta[1]\u0026quot;])\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan.mult)[, \u0026quot;beta[2]\u0026quot;])\r[1] 0\r\u0026gt; mcmcpvalue(as.matrix(data.rstan.mult)[, \u0026quot;beta[3]\u0026quot;])\r[1] 0.2476667\rWith a p-value of essentially \\(0\\), we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship. An alternative way of quantifying the impact of an interaction is to compare models with and without the interactions. In a Bayesian context, this can be achieved by comparing the leave-one-out cross-validation statistics. Leave-one-out (LOO) cross-validation explores how well a series of models can predict withheld values Vehtari, Gelman, and Gabry (2017). The LOO Information Criterion (LOOIC) is analogous to the AIC except that the LOOIC takes priors into consideration, does not assume that the posterior distribution is drawn from a multivariate normal and integrates over parameter uncertainty so as to yield a distribution of looic rather than just a point estimate. The LOOIC does however assume that all observations are equally influential (it does not matter which observations are left out). This assumption can be examined via the Pareto \\(k\\) estimate (values greater than \\(0.5\\) or more conservatively \\(0.75\\) are considered overly influential). We can compute LOOIC if we store the loglikelihood from our STAN model, which can then be extracted to compute the information criterion using the package loo.\n\u0026gt; ## since values are less than zero\r\u0026gt; library(loo)\r\u0026gt; (full = loo(extract_log_lik(data.rstan.mult)))\rComputed from 3000 by 100 log-likelihood matrix\rEstimate SE\relpd_loo -143.3 8.5\rp_loo 5.2 1.1\rlooic 286.5 17.0\r------\rMonte Carlo SE of elpd_loo is 0.1.\rAll Pareto k estimates are good (k \u0026lt; 0.5).\rSee help(\u0026#39;pareto-k-diagnostic\u0026#39;) for details.\r\u0026gt; \u0026gt; (reduced = loo(extract_log_lik(data.rstan.add)))\rComputed from 3000 by 100 log-likelihood matrix\rEstimate SE\relpd_loo -143.1 8.7\rp_loo 4.4 1.1\rlooic 286.1 17.4\r------\rMonte Carlo SE of elpd_loo is 0.0.\rAll Pareto k estimates are good (k \u0026lt; 0.5).\rSee help(\u0026#39;pareto-k-diagnostic\u0026#39;) for details.\r\u0026gt; \u0026gt; par(mfrow = 1:2, mar = c(5, 3.8, 1, 0) + 0.1, las = 3)\r\u0026gt; plot(full, label_points = TRUE)\r\u0026gt; plot(reduced, label_points = TRUE)\rThe expected out-of-sample predictive accuracy is very similar (slightly lower) for the additive model compared to the multiplicative model (model containing the interaction). This might be used to suggest that the inferential evidence for an interaction is low.\n\rGraphical summaries\rWith appropriate use of model matrices and data wrangling, it is possible to produce a single prediction data set along with ggplot syntax to produce a multi-panel figure. First we look at the additive model.\n\u0026gt; mcmc = as.matrix(data.rstan.add)\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = rbind(data.frame(cx1 = seq(min(data$cx1, na.rm = TRUE), max(data$cx1,\r+ na.rm = TRUE), len = 100), cx2 = 0, Pred = 1), data.frame(cx1 = 0,\r+ cx2 = seq(min(data$cx2, na.rm = TRUE), max(data$cx2, na.rm = TRUE),\r+ len = 100), Pred = 2))\r\u0026gt; Xmat = model.matrix(~cx1 + cx2, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% mutate(x1 = cx1 + mean.x1, x2 = cx2 + mean.x2) %\u0026gt;%\r+ cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)) %\u0026gt;%\r+ mutate(x = dplyr:::recode(Pred, x1, x2))\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X\u0026quot;) + theme_classic() + facet_wrap(~Pred)\rWe cannot simply add the raw data to this figure. The reason for this is that the trends represent the effect of one predictor holding the other variable constant. Therefore, the observations we represent on the figure must likewise be standardised. We can achieve this by adding the partial residuals to the figure. Partial residuals are the fitted values plus the residuals.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = rbind(data.frame(cx1 = data$cx1, cx2 = 0, Pred = 1), data.frame(cx1 = 0,\r+ cx2 = data$cx2, Pred = 2))\r\u0026gt; fMat = rMat = model.matrix(~cx1 + cx2, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit) %\u0026gt;% mutate(x1 = cx1 +\r+ mean.x1, x2 = cx2 + mean.x2) %\u0026gt;% mutate(x = dplyr:::recode(Pred, x1,\r+ x2))\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\r+ color = \u0026quot;gray\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low, ymax = conf.high),\r+ fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) + theme_classic() +\r+ facet_wrap(~Pred, strip.position = \u0026quot;bottom\u0026quot;, labeller = label_bquote(\u0026quot;x\u0026quot; *\r+ .(Pred))) + theme(axis.title.x = element_blank(), strip.background = element_blank(),\r+ strip.placement = \u0026quot;outside\u0026quot;)\rHowever, this method (whist partially elegant) does become overly opaque if we need more extensive axes labels since the x-axes labels are actually strip labels (which must largely be defined outside of the ggplot structure). The alternative is to simply produce each partial plot separately before arranging them together in the one figure using the package gridExtra.\n\u0026gt; library(gridExtra)\r\u0026gt; mcmc = as.matrix(data.rstan.add)\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = data.frame(cx1 = seq(min(data$cx1, na.rm = TRUE), max(data$cx1,\r+ na.rm = TRUE), len = 100), cx2 = 0)\r\u0026gt; Xmat = model.matrix(~cx1 + cx2, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% mutate(x1 = cx1 + mean.x1, x2 = cx2 + mean.x2) %\u0026gt;%\r+ cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; ## Now the partial residuals\r\u0026gt; fdata = rdata = data.frame(cx1 = data$cx1, cx2 = 0)\r\u0026gt; fMat = rMat = model.matrix(~cx1 + cx2, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit) %\u0026gt;% mutate(x1 = cx1 +\r+ mean.x1, x2 = cx2 + mean.x2)\r\u0026gt; g1 = ggplot(newdata, aes(y = estimate, x = x1)) + geom_point(data = rdata,\r+ aes(y = partial.resid), color = \u0026quot;grey\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X1\u0026quot;) + theme_classic()\r\u0026gt; \u0026gt; newdata = data.frame(cx2 = seq(min(data$cx2, na.rm = TRUE), max(data$cx2,\r+ na.rm = TRUE), len = 100), cx1 = 0)\r\u0026gt; Xmat = model.matrix(~cx1 + cx2, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% mutate(x1 = cx1 + mean.x1, x2 = cx2 + mean.x2) %\u0026gt;%\r+ cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; ## Now the partial residuals\r\u0026gt; fdata = rdata = data.frame(cx1 = 0, cx2 = data$cx2)\r\u0026gt; fMat = rMat = model.matrix(~cx1 + cx2, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit) %\u0026gt;% mutate(x1 = cx1 +\r+ mean.x1, x2 = cx2 + mean.x2)\r\u0026gt; g2 = ggplot(newdata, aes(y = estimate, x = x2)) + geom_point(data = rdata,\r+ aes(y = partial.resid), color = \u0026quot;grey\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X2\u0026quot;) + theme_classic()\r\u0026gt; \u0026gt; grid.arrange(g1, g2, ncol = 2)\rFor the multiplicative model, we could elect to split the trends up so as to explore the effects of one predictor at several set levels of another predictor. In this example, we will explore the effects of \\(x_1\\) when \\(x_2\\) is equal to its mean in the original data as well as one and two standard deviations below and above this mean.\n\u0026gt; library(fields)\r\u0026gt; mcmc = as.matrix(data.rstan.mult)\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = expand.grid(cx1 = seq(min(data$cx1, na.rm = TRUE), max(data$cx1,\r+ na.rm = TRUE), len = 100), cx2 = mean(data$cx2) + sd(data$cx2) %*%\r+ -2:2)\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;, \u0026quot;beta[3]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% mutate(x1 = cx1 + mean.x1, x2 = cx2 + mean.x2) %\u0026gt;%\r+ cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)) %\u0026gt;%\r+ mutate(x2 = factor(x2, labels = paste(\u0026quot;X2:~\u0026quot;, c(-2, -1, 0, 1, 2), \u0026quot;*sigma\u0026quot;)))\r\u0026gt; ## Partial residuals\r\u0026gt; fdata = rdata = expand.grid(cx1 = data$cx1, cx2 = mean(data$cx2) + sd(data$cx2) *\r+ -2:2)\r\u0026gt; fMat = rMat = model.matrix(~cx1 * cx2, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit) %\u0026gt;% mutate(x1 = cx1 +\r+ mean.x1, x2 = cx2 + mean.x2)\r\u0026gt; ## Partition the partial residuals such that each x1 trend only includes\r\u0026gt; ## x2 data that is within that range in the observed data\r\u0026gt; findNearest = function(x, y) {\r+ ff = fields:::rdist(x, y)\r+ apply(ff, 1, function(x) which(x == min(x)))\r+ }\r\u0026gt; fn = findNearest(x = data[, c(\u0026quot;x1\u0026quot;, \u0026quot;x2\u0026quot;)], y = rdata[, c(\u0026quot;x1\u0026quot;, \u0026quot;x2\u0026quot;)])\r\u0026gt; rdata = rdata[fn, ] %\u0026gt;% mutate(x2 = factor(x2, labels = paste(\u0026quot;X2:~\u0026quot;, c(-2,\r+ -1, 0, 1, 2), \u0026quot;*sigma\u0026quot;)))\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x1)) + geom_line() + geom_blank(aes(y = 9)) +\r+ geom_point(data = rdata, aes(y = partial.resid), color = \u0026quot;grey\u0026quot;) +\r+ geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \u0026quot;blue\u0026quot;,\r+ alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_continuous(\u0026quot;X1\u0026quot;) +\r+ facet_wrap(~x2, labeller = label_parsed, nrow = 1, scales = \u0026quot;free_y\u0026quot;) +\r+ theme_classic() + theme(strip.background = element_blank())\rAlternatively, we could explore the interaction by plotting a two dimensional surface as a heat map.\n\rEffect sizes\rIn addition to deriving the distribution means for the slope parameter, we could make use of the Bayesian framework to derive the distribution of the effect size. In so doing, effect size could be considered as either the rate of change or alternatively, the difference between pairs of values along the predictor gradient. For the latter case, there are multiple ways of calculating an effect size, but the two most common are:\n\rRaw effect size. The difference between two groups (as already calculated)\n\rCohen’s D. The effect size standardized by division with the pooled standard deviation: \\(D=\\frac{(\\mu_A-\\mu_B)}{\\sigma}\\)\n\rPercentage change. Express the effect size as a percent of one of the pairs. That is, whether you expressing a percentage increase or a percentage decline depends on which of the pairs of values are considered a reference value. Care must be exercised to ensure no division by zeros occur.\n\r\rFor simple linear models, effect size based on a rate is essentially the same as above except that it is expressed per unit of the predictor. Of course in many instances, one unit change in the predictor represents too subtle a shift in the underlying gradient to likely yield any clinically meaningful or appreciable change in response.\nProbability that a change in \\(x_1\\) is associated with greater than a \\(50\\)% increase in \\(y\\) at various levels of \\(x_2\\). Clearly, in order to explore this inference, we must first express the change in \\(y\\) as a percentage. This in turn requires us to calculate start and end points from which to calculate the magnitude of the effect (amount of increase in \\(y\\)) as well as the percentage decline. Hence, we start by predicting the distribution of \\(y\\) at the lowest and highest values of \\(x_1\\) at five levels of \\(x_2\\) (representing two standard deviations below the cx2 mean, one standard deviation below the cx2 mean, the cx2 mean, one standard deviation above the cx2 mean and \\(2\\) standard deviations above the cx2 mean. For this exercise we will only use the multiplicative model. Needless to say, the process would be very similar for the additive model.\n\u0026gt; mcmc = as.matrix(data.rstan.mult)\r\u0026gt; newdata = expand.grid(cx1 = c(min(data$cx1), max(data$cx1)), cx2 = (-2:2) *\r+ sd(data$cx1))\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;, \u0026quot;beta[3]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; s1 = seq(1, 9, b = 2)\r\u0026gt; s2 = seq(2, 10, b = 2)\r\u0026gt; ## Raw effect size\r\u0026gt; (RES = tidyMCMC(as.mcmc(fit[, s2] - fit[, s1]), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 2 1.99 0.846 0.278 3.59\r2 4 2.39 0.583 1.29 3.58\r3 6 2.79 0.452 1.94 3.71\r4 8 3.19 0.554 2.11 4.21\r5 10 3.59 0.806 2.09 5.18\r\u0026gt; ## Cohen\u0026#39;s D\r\u0026gt; cohenD = (fit[, s2] - fit[, s1])/sqrt(mcmc[, \u0026quot;sigma\u0026quot;])\r\u0026gt; (cohenDES = tidyMCMC(as.mcmc(cohenD), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 2 2.00 0.849 0.283 3.60\r2 4 2.40 0.590 1.26 3.56\r3 6 2.81 0.465 1.91 3.73\r4 8 3.21 0.570 2.14 4.33\r5 10 3.61 0.820 1.90 5.10\r\u0026gt; # Percentage change (relative to Group A)\r\u0026gt; ESp = 100 * (fit[, s2] - fit[, s1])/fit[, s1]\r\u0026gt; (PES = tidyMCMC(as.mcmc(ESp), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 2 121. 80.0 -1.23 277.\r2 4 119. 41.4 38.8 200.\r3 6 124. 33.1 64.6 190.\r4 8 132. 46.6 54.8 225.\r5 10 144. 73.2 37.8 282.\r\u0026gt; # Probability that the effect is greater than 50% (an increase of \u0026gt;50%)\r\u0026gt; (p50 = apply(ESp, 2, function(x) sum(x \u0026gt; 50)/length(x)))\r2 4 6 8 10 0.8586667 0.9740000 0.9983333 0.9940000 0.9793333 \u0026gt; ## fractional change\r\u0026gt; (FES = tidyMCMC(as.mcmc(fit[, s2]/fit[, s1]), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 5 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 2 2.21 0.800 0.988 3.77\r2 4 2.19 0.414 1.39 3.00\r3 6 2.24 0.331 1.65 2.90\r4 8 2.32 0.466 1.55 3.25\r5 10 2.44 0.732 1.38 3.82\rConclusions\n\rOn average, when \\(x_2\\) is equal to its mean, \\(Y\\) increases by \\(2.79\\) over the observed range of \\(x_1\\). We are \\(95\\)% confident that the increase is between \\(1.91\\) and \\(3.66\\).\n\rThe Cohen’s D associated change over the observed range of \\(x_1\\) is \\(2.80\\).\n\rOn average, \\(Y\\) increases by \\(124\\)% over the observed range of \\(x_1\\) (at average \\(x_2\\)). We are \\(95\\)% confident that the increase is between \\(65\\)% and \\(190\\)%.\n\rThe probability that \\(Y\\) increases by more than \\(50\\)% over the observed range of \\(x_1\\) (average \\(x_2\\)) is \\(0.998\\).\n\rOn average, \\(Y\\) increases by a factor of \\(2.24\\)% over the observed range of \\(x_1\\) (average \\(x_2\\)). We are \\(95\\)% confident that the decline is between a factor of \\(1.65\\)% and \\(2.90\\)%.\n\r\r\rFinite population standard deviations\rVariance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).\nFinite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (Gelman and others (2005)). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.\n# A tibble: 4 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x1 0.800 0.130 0.558 1.06 2 sd.x2 0.501 0.128 0.256 0.754\r3 sd.x1x2 0.134 0.0873 0.000182 0.291\r4 sd.resid 0.981 0.0125 0.966 1.01 # A tibble: 4 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x1 33.2 5.00 22.5 42.1\r2 sd.x2 20.8 5.11 11.2 30.9\r3 sd.x1x2 5.16 3.44 0.00805 11.5\r4 sd.resid 40.5 2.13 36.7 45.0\rApproximately \\(59\\)% of the total finite population standard deviation is due to \\(x_1\\), \\(x_2\\) and their interaction.\n\rR squared\rIn a frequentist context, the \\(R^2\\) value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, \\(R^2\\) is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an \\(R^2\\) greater than \\(100\\)%. Gelman et al. (2019) proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.\nSo in the standard regression model notation of:\n\\[ y_i \\sim \\text{Normal}(\\boldsymbol X \\boldsymbol \\beta, \\sigma),\\]\nthe \\(R^2\\) could be formulated as\n\\[ R^2 = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sigma^2_e},\\]\nwhere \\(\\sigma^2_f=\\text{var}(\\boldsymbol X \\boldsymbol \\beta)\\), and for normal models \\(\\sigma^2_e=\\text{var}(y-\\boldsymbol X \\boldsymbol \\beta)\\)\n\u0026gt; mcmc \u0026lt;- as.matrix(data.rstan.mult)\r\u0026gt; Xmat = model.matrix(~cx1 * cx2, data)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;, \u0026quot;beta[3]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = sweep(fit, 2, data$y, \u0026quot;-\u0026quot;)\r\u0026gt; var_f = apply(fit, 1, var)\r\u0026gt; var_e = apply(resid, 1, var)\r\u0026gt; R2 = var_f/(var_f + var_e)\r\u0026gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.605 0.0390 0.531 0.678\r\u0026gt; \u0026gt; # for comparison with frequentist\r\u0026gt; summary(lm(y ~ cx1 * cx2, data))\rCall:\rlm(formula = y ~ cx1 * cx2, data = data)\rResiduals:\rMin 1Q Median 3Q Max -1.8173 -0.7167 -0.1092 0.5890 3.3861 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 3.7152 0.1199 30.987 \u0026lt; 2e-16 ***\rcx1 2.8072 0.4390 6.394 5.84e-09 ***\rcx2 1.4988 0.3810 3.934 0.000158 ***\rcx1:cx2 1.4464 1.1934 1.212 0.228476 ---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 0.9804 on 96 degrees of freedom\rMultiple R-squared: 0.6115, Adjusted R-squared: 0.5994 F-statistic: 50.37 on 3 and 96 DF, p-value: \u0026lt; 2.2e-16\r\rBayesian model selection\rA statistical model is by definition a low-dimensional (over simplification) representation of what is really likely to be a very complex system. As a result, no model is right. Some models however can provide useful insights into some of the processes operating on the system. Frequentist statistics have various methods (model selection, dredging, lasso, cross validation) for selecting parsimonious models. These are models that provide a good comprimise between minimizing unexplained patterns and minimizing model complexity. The basic premise is that since no model can hope to capture the full complexity of a system with all its subtleties, only the very major patterns can be estimated. Overly complex models are likely to be representing artificial complexity present only in the specific observed data (not the general population). The Bayesian approach is to apply priors to the non-variance parameters such that parameters close to zero are further shrunk towards zero whilst priors on parameters further away from zero are less effected. The most popular form of prior for sparsity is the horseshoe prior, so called because the shape of a component of this prior resembles a horseshoe (with most of the mass either close to \\(0\\) or close to \\(1\\)).\nRather than apply weakly informative Gaussian priors on parameters as:\n\\[ \\beta_j \\sim N(0,\\sigma^2),\\]\nthe horseshoe prior is defined as\n\\[ \\beta_j \\sim N(0,\\tau^2\\lambda_j^2),\\]\nwhere \\(\\tau \\sim \\text{Cauchy}(0,1)\\) and \\(\\lambda_j \\sim \\text{Cauchy}(0,1)\\), for \\(j=1,\\ldots,D\\). Using this prior, \\(D\\) is the number of (non-intercept or variance) parameters, \\(\\tau\\) represents the global scale that weights or shrinks all parameters towards zero and \\(\\lambda_j\\) are thick tailed local scales that allow some of the \\(j\\) parameters to escape shrinkage. More recently, Piironen, Vehtari, and others (2017) have argued that whilst the above horseshoe priors do guarantee that strong effects (parameters) will not be over-shrunk, there is the potential for weekly identified effects (those based on relatively little data) to be misrepresented in the posteriors. As an alternative they advocated the use of regularised horseshoe priors in which the amount of shrinkage applied to the largest effects can be controlled. The prior is defined as:\n\\[ \\beta_j \\sim N(0,\\tau^2 \\tilde{\\lambda}_j^2),\\]\nwhere \\(\\tilde{\\lambda}_j^2 = \\frac{c^2\\lambda^2_j}{c^2+\\tau^2 \\lambda^2_j}\\) and \\(c\\) is (slab width, actually variance) is a constant. For small effects (when \\(\\tau^2 \\lambda^2_j \u0026lt; c^2\\)) the prior approaches a regular prior. However, for large effects (when \\(\\tau^2 \\lambda^2_j \u0026gt; c^2\\)) the prior approaches \\(N(0,c^2)\\). Finally, they recommend applying a inverse-gamma prior on \\(c^2\\):\n\\[ c^2 \\sim \\text{Inv-Gamma}(\\alpha,\\beta),\\]\nwhere \\(\\alpha=v/2\\) and \\(\\beta=vs^2/2\\), which translates to a \\(\\text{Student-t}_ν(0, s^2)\\) slab for the coefficients far from zero and is typically a good default choice for a weakly informative prior. This prior can be encoded into STAN using the following code.\n\u0026gt; modelStringHP = \u0026quot;\r+ data {\r+ int \u0026lt; lower =0 \u0026gt; n; // number of observations\r+ int \u0026lt; lower =0 \u0026gt; nX; // number of predictors\r+ vector [ n] Y; // outputs\r+ matrix [n ,nX] X; // inputs\r+ real \u0026lt; lower =0 \u0026gt; scale_icept ; // prior std for the intercept\r+ real \u0026lt; lower =0 \u0026gt; scale_global ; // scale for the half -t prior for tau\r+ real \u0026lt; lower =1 \u0026gt; nu_global ; // degrees of freedom for the half -t priors for tau\r+ real \u0026lt; lower =1 \u0026gt; nu_local ; // degrees of freedom for the half - t priors for lambdas\r+ real \u0026lt; lower =0 \u0026gt; slab_scale ; // slab scale for the regularized horseshoe\r+ real \u0026lt; lower =0 \u0026gt; slab_df ; // slab degrees of freedom for the regularized horseshoe\r+ }\r+ transformed data {\r+ matrix[n, nX - 1] Xc; // centered version of X + vector[nX - 1] means_X; // column means of X before centering + for (i in 2:nX) { + means_X[i - 1] = mean(X[, i]); + Xc[, i - 1] = X[, i] - means_X[i - 1]; + } + }\r+ parameters {\r+ real logsigma ;\r+ real cbeta0 ;\r+ vector [ nX-1] z;\r+ real \u0026lt; lower =0 \u0026gt; tau ; // global shrinkage parameter\r+ vector \u0026lt; lower =0 \u0026gt;[ nX-1] lambda ; // local shrinkage parameter\r+ real \u0026lt; lower =0 \u0026gt; caux ;\r+ }\r+ transformed parameters {\r+ real \u0026lt; lower =0 \u0026gt; sigma ; // noise std\r+ vector \u0026lt; lower =0 \u0026gt;[ nX-1] lambda_tilde ; // truncated local shrinkage parameter\r+ real \u0026lt; lower =0 \u0026gt; c; // slab scale\r+ vector [ nX-1] beta ; // regression coefficients\r+ vector [ n] mu; // latent function values\r+ sigma = exp ( logsigma );\r+ c = slab_scale * sqrt ( caux );\r+ lambda_tilde = sqrt ( c ^2 * square ( lambda ) ./ (c ^2 + tau ^2* square ( lambda )) );\r+ beta = z .* lambda_tilde * tau ;\r+ mu = cbeta0 + Xc* beta ;\r+ }\r+ model {\r+ // half -t priors for lambdas and tau , and inverse - gamma for c ^2\r+ z ~ normal (0 , 1);\r+ lambda ~ student_t ( nu_local , 0, 1);\r+ tau ~ student_t ( nu_global , 0 , scale_global * sigma );\r+ caux ~ inv_gamma (0.5* slab_df , 0.5* slab_df );\r+ cbeta0 ~ normal (0 , scale_icept );\r+ Y ~ normal (mu , sigma );\r+ }\r+ generated quantities { + real beta0; // population-level intercept + vector[n] log_lik;\r+ beta0 = cbeta0 - dot_product(means_X, beta);\r+ for (i in 1:n) {\r+ log_lik[i] = normal_lpdf(Y[i] | Xc[i] * beta + cbeta0, sigma);\r+ }\r+ }\r+ + \u0026quot;\r\u0026gt; ## write the model to a stan file \u0026gt; writeLines(modelStringHP, con = \u0026quot;linregModelHP.stan\u0026quot;)\rWe can now try to refit the model (additive) using this new specification.\n\u0026gt; X = model.matrix(~cx1 + cx2, data = data)\r\u0026gt; data.list \u0026lt;- with(data, list(Y = y, X = X, nX = ncol(X), n = nrow(data),\r+ scale_icept = 100, scale_global = 1, nu_global = 1, nu_local = 1, slab_scale = 2,\r+ slab_df = 4))\r\u0026gt; \u0026gt; data.rstan.sparsity \u0026lt;- stan(data = data.list, file = \u0026quot;linregModelHP.stan\u0026quot;, pars = params,\r+ chains = nChains, iter = nIter, warmup = burnInSteps, thin = thinSteps, save_dso = TRUE)\rSAMPLING FOR MODEL \u0026#39;linregModelHP\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 2500 [ 0%] (Warmup)\rChain 1: Iteration: 250 / 2500 [ 10%] (Warmup)\rChain 1: Iteration: 500 / 2500 [ 20%] (Warmup)\rChain 1: Iteration: 750 / 2500 [ 30%] (Warmup)\rChain 1: Iteration: 1000 / 2500 [ 40%] (Warmup)\rChain 1: Iteration: 1001 / 2500 [ 40%] (Sampling)\rChain 1: Iteration: 1250 / 2500 [ 50%] (Sampling)\rChain 1: Iteration: 1500 / 2500 [ 60%] (Sampling)\rChain 1: Iteration: 1750 / 2500 [ 70%] (Sampling)\rChain 1: Iteration: 2000 / 2500 [ 80%] (Sampling)\rChain 1: Iteration: 2250 / 2500 [ 90%] (Sampling)\rChain 1: Iteration: 2500 / 2500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.364 seconds (Warm-up)\rChain 1: 0.484 seconds (Sampling)\rChain 1: 0.848 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;linregModelHP\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 2500 [ 0%] (Warmup)\rChain 2: Iteration: 250 / 2500 [ 10%] (Warmup)\rChain 2: Iteration: 500 / 2500 [ 20%] (Warmup)\rChain 2: Iteration: 750 / 2500 [ 30%] (Warmup)\rChain 2: Iteration: 1000 / 2500 [ 40%] (Warmup)\rChain 2: Iteration: 1001 / 2500 [ 40%] (Sampling)\rChain 2: Iteration: 1250 / 2500 [ 50%] (Sampling)\rChain 2: Iteration: 1500 / 2500 [ 60%] (Sampling)\rChain 2: Iteration: 1750 / 2500 [ 70%] (Sampling)\rChain 2: Iteration: 2000 / 2500 [ 80%] (Sampling)\rChain 2: Iteration: 2250 / 2500 [ 90%] (Sampling)\rChain 2: Iteration: 2500 / 2500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.328 seconds (Warm-up)\rChain 2: 0.444 seconds (Sampling)\rChain 2: 0.772 seconds (Total)\rChain 2: \u0026gt; \u0026gt; tidyMCMC(data.rstan.sparsity, pars = c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;), conf.int = TRUE,\r+ conf.type = \u0026quot;HPDinterval\u0026quot;, rhat = TRUE, ess = TRUE)\r# A tibble: 2 x 7\rterm estimate std.error conf.low conf.high rhat ess\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r1 beta[1] 2.75 0.446 1.86 3.61 0.999 3138\r2 beta[2] 1.56 0.379 0.816 2.30 1.000 2782\r\u0026gt; \u0026gt; mcmc_areas(as.matrix(data.rstan.sparsity), regex_par = \u0026quot;beta\u0026quot;)\rObviously, these data are not really appropriate for model selection as there are only two predictors. Both predictors have substantial effects mass larger than zero.\n\rReferences\rGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9.\n\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rGelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” The Annals of Statistics 33 (1): 1–53.\n\rPiironen, Juho, Aki Vehtari, and others. 2017. “Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors.” Electronic Journal of Statistics 11 (2): 5018–51.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\rVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and Waic.” Statistics and Computing 27 (5): 1413–32.\n\r\r\r","date":1580782394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580782394,"objectID":"cf2ab91daa7cea269cd29aed1fb73155","permalink":"/stan/multiple-linear-regression-stan/multiple-linear-regression-stan/","publishdate":"2020-02-03T21:13:14-05:00","relpermalink":"/stan/multiple-linear-regression-stan/multiple-linear-regression-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","linear regression"],"title":"Multiple Linear Regression - STAN","type":"STAN"},{"authors":["Andrea Gabrio"],"categories":["R","linear regression","JAGS"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rMany clinicians get a little twitchy and nervous around mathematical and statistical formulae and nomenclature. Whilst it is possible to perform basic statistics without too much regard for the actual equation (model) being employed, as the complexity of the analysis increases, the need to understand the underlying model becomes increasingly important. Moreover, model specification in BUGS/JAGS/STAN (the language used to program Bayesian modelling) aligns very closely to the underlying formulae. Hence a good understanding of the underlying model is vital to be able to create a sensible Bayesian model. Consequently, I will always present the linear model formulae along with the analysis.\nTo introduce the philosophical and mathematical differences between classical (frequentist) and Bayesian statistics, based on previous works, we present a provocative yet compelling trend analysis of two hypothetical populations (A vs B). The temporal trend of population A shows very little variability from a very subtle linear decline (\\(n=10\\), \\(\\text{slope}=-0.10\\), \\(\\text{p-value}=0.048\\)). By contrast, the B population appears to decline more dramatically, yet has substantially more variability (\\(n=10\\), \\(\\text{slope}=-10.23\\), \\(\\text{p-value}=0.058\\)). From a traditional frequentist perspective, we would conclude that there is a “significant” relationship in Population A (\\(p\u0026lt;0.05\\)), yet not in Population B (\\(p\u0026gt;0.05\\)). However, if we consider a third population C which is exactly the same as populstion B but with a higher number of observations, then we may end up with a completely different conclusion compared with that based on population B (\\(n=100\\), \\(\\text{slope}=-10.47\\), \\(\\text{p-value}\u0026lt;0.001\\)).\nThe above illustrates a couple of things:\n\rstatistical significance does not necessarily translate into clinical importance. Indeed, population B is declining at nearly \\(10\\) times the rate of population A. That sounds rather important, yet on the basis of the hypothesis test, we would dismiss the decline in population B.\n\rthat a p-value is just the probability of detecting an effect or relationship - what is the probability that the sample size is large enough to pick up a difference.\n\r\rLet us now look at it from a Bayesian perspective, with a focus on population A and B. We would conclude that:\n\rthe mean (plus or minus CI) slopes for Population A and B are \\(-0.1 (-0.21,0)\\) and \\(-10.08 (-20.32,0.57)\\) respectively\n\rthe Bayesian approach allows us to query the posterior distribution is many other ways in order to ask sensible clinical questions. For example, we might consider that a rate of change of \\(5\\)% or greater represents an important biological impact. For population A and B, the probability that the rate is \\(5\\)% or greater is \\(0\\) and \\(0.85\\) respectively.\n\r\r\rLinear regression\rSimple linear regression is a linear modelling process that models a continuous response against a single continuous predictor. The linear model is expressed as:\n\\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma),\\]\nwhere \\(y_i\\) is the response variable for each of the \\(i=1\\ldots,n\\) observations, \\(\\beta_0\\) is the intercept (value when \\(x=0\\)), \\(\\beta_1\\) is the slope (rate of change in \\(y\\) per unit change in \\(x\\)), \\(x_i\\) is the predictor variable, \\(\\epsilon_i\\) is the residual value (difference between the observed value and the value expected by the model). The parameters of the trendline \\(\\boldsymbol \\beta=(\\beta_0,\\beta_1)\\) are determined by Ordinary Least Squares (OLS) in which the sum of the squared residuals is minimized. A non-zero population slope is indicative of a relationship.\n\r\rData generation\rLets say we had set up an experiment in which we applied a continuous treatment (\\(x\\)) ranging in magnitude from \\(0\\) to \\(16\\) to a total of \\(16\\) sampling units (\\(n=16\\)) and then measured a response (\\(y\\)) from each unit. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(123)\r\u0026gt; n \u0026lt;- 16\r\u0026gt; a \u0026lt;- 40 #intercept\r\u0026gt; b \u0026lt;- -1.5 #slope\r\u0026gt; sigma2 \u0026lt;- 25 #residual variance (sd=5)\r\u0026gt; x \u0026lt;- 1:n #values of the year covariate\r\u0026gt; eps \u0026lt;- rnorm(n, mean = 0, sd = sqrt(sigma2)) #residuals\r\u0026gt; y \u0026lt;- a + b * x + eps #response variable\r\u0026gt; # OR\r\u0026gt; y \u0026lt;- (model.matrix(~x) %*% c(a, b)) + eps\r\u0026gt; data \u0026lt;- data.frame(y, x) #dataset\r\u0026gt; head(data) #print out the first six rows of the data set\ry x\r1 35.69762 1\r2 35.84911 2\r3 43.29354 3\r4 34.35254 4\r5 33.14644 5\r6 39.57532 6\rWith these sort of data, we are primarily interested in investigating whether there is a relationship between the continuous response variable and the linear predictor (single continuous predictor).\nCentering the data\rWhen a linear model contains a covariate (continuous predictor variable) in addition to another predictor (continuous or categorical), it is nearly always advisable that the continuous predictor variables are centered prior to the analysis. Centering is a process by which the mean of a variable is subtracted from each of the values such that the scale of the variable is shifted so as to be centered around \\(0\\). Hence the mean of the new centered variable will be \\(0\\), yet it will retain the same variance.\nThere are multiple reasons for this:\nIt provides some clinical meaning to the \\(y\\)-intercept. Recall that the \\(y\\)-intercept is the value of \\(Y\\) when \\(X\\) is equal to zero. If \\(X\\) is centered, then the \\(y\\)-intercept represents the value of \\(Y\\) at the mid-point of the \\(X\\) range. The \\(y\\)-intercept of an uncentered \\(X\\) typically represents a unreal value of \\(Y\\) (as an \\(X\\) of \\(0\\) is often beyond the reasonable range of values).\n\rIn multiplicative models (in which predictors and their interactions are included), main effects and interaction terms built from centered predictors will not be correlated to one another.\n\rFor more complex models, centering the covariates can increase the likelihood that the modelling engine converges (arrives at a numerically stable and reliable outcome).\n\r\rNote, centering will not effect the slope estimates. In R, centering is easily achieved with the scale function.\n\u0026gt; data \u0026lt;- within(data, {\r+ cx \u0026lt;- as.numeric(scale(x, scale = FALSE))\r+ })\r\u0026gt; head(data)\ry x cx\r1 35.69762 1 -7.5\r2 35.84911 2 -6.5\r3 43.29354 3 -5.5\r4 34.35254 4 -4.5\r5 33.14644 5 -3.5\r6 39.57532 6 -2.5\r\r\rExploratory data analysis\rNormality\rEstimation and inference testing in linear regression assumes that the response is normally distributed in each of the populations. In this case, the populations are all possible measurements that could be collected at each level of \\(x\\) - hence there are \\(16\\) populations. Typically however, we only collect a single observation from each population (as is also the case here). How then can be evaluate whether each of these populations are likely to have been normal? For a given response, the population distributions should follow much the same distribution shapes. Therefore provided the single samples from each population are unbiased representations of those populations, a boxplot of all observations should reflect the population distributions.\n\rHomogeneity of variance\rSimple linear regression also assumes that each of the populations are equally varied. Actually, it is prospect of a relationship between the mean and variance of \\(y\\)-values across x-values that is of the greatest concern. Strictly the assumption is that the distribution of \\(y\\) values at each \\(x\\) value are equally varied and that there is no relationship between mean and variance. However, as we only have a single \\(y\\)-value for each \\(x\\)-value, it is difficult to directly determine whether the assumption of homogeneity of variance is likely to have been violated (mean of one value is meaningless and variability can’t be assessed from a single value). If we then plot the residuals (difference between observed values and those predicted by the trendline) against the predict values and observe a definite presence of a pattern, then it is indicative of issues with the assumption of homogeneity of variance.\nHence looking at the spread of values around a trendline on a scatterplot of \\(y\\) against \\(x\\) is a useful way of identifying gross violations of homogeneity of variance. Residual plots provide an even better diagnostic. The presence of a wedge shape is indicative that the population mean and variance are related.\n\rLinearity\rLinear regression fits a straight (linear) line through the data. Therefore, prior to fitting such a model, it is necessary to establish whether this really is the most sensible way of describing the relationship. That is, does the relationship appear to be linearly related or could some other non-linear function describe the relationship better. Scatterplots and residual plots are useful diagnostics.\n\rModel assumptions\rThe typical assumptions which need to be checked when fitting a standard linear regression model are:\n\rAll of the observations are independent - this must be addressed at the design and collection stages\n\rThe response variable (and thus the residuals) should be normally distributed\n\rThe response variable should be equally varied (variance should not be related to mean as these are supposed to be estimated separately)\n\rThe relationship between the linear predictor (right hand side of the regression formula) and the link function should be linear. A scatterplot with smoother can be useful for identifying possible non-linearity.\n\r\rSo lets explore normality, homogeneity of variances and linearity by constructing a scatterplot of the relationship between the response (\\(y\\)) and the predictor (\\(x\\)). We will also include a range of smoothers (linear and lowess) and marginal boxplots on the scatterplot to assist in exploring linearity and normality respectively.\n\u0026gt; # scatterplot\r\u0026gt; library(car)\r\u0026gt; scatterplot(y ~ x, data)\rConclusions:\nThere is no evidence that the response variable is non-normal. The spread of values around the trendline seems fairly even (hence it there is no evidence of non-homogeneity). The data seems well represented by the linear trendline. Furthermore, the lowess smoother does not appear to have a consistent shift trajectory. Obvious violations could be addressed either by:\n\rConsider a non-linear linear predictor (such as a polynomial, spline or other non-linear function)\n\rTransform the scale of the response variables (e.g. to address normality)\n\r\r\r\rModel fitting\rThe purpose of fitting a model in this case is to explore the relationship between \\(y\\) and \\(x\\). Since both \\(y\\) and \\(x\\) are continuous, a simple regression line is a good start. The observed response (\\(y_i\\)) are assumed to be drawn from a normal distribution with a given mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The expected values (\\(\\mu\\)) are themselves determined by the linear predictor (\\(\\beta_0+\\beta_1\\)). In this case, \\(\\beta_0\\) represents the \\(y\\)-intercept (value of \\(y\\) when \\(x\\) is equal to zero) and \\(\\beta_1\\) represents the rate of change in \\(y\\) for every unit change in \\(x\\) (the effect).\nNote that in this form, the \\(y\\)-intercept is of little interest. Indeed for many applications, a value of x would be outside the domain of the collected data, outside the logical bounds of the actual variable or else outside the domain of interest. If however, we center the predictor variable (by subtracting the mean of \\(x\\) from each \\(x\\), then the \\(y\\)-intercept represents the value of \\(y\\) at the average value of \\(x\\). This certainly has more meaning. Note that centering the predictor does not effect the estimate of slope. MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying “uninformative” priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important.\nFor this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (1000) for both the intercept and the treatment effect and a wide half-cauchy (\\(\\text{scale}=25\\)) for the standard deviation.\n\\[ y_i \\sim \\text{Normal}(\\mu_i, \\sigma),\\]\nwhere \\(\\mu_i=\\beta_0+\\beta_1x_i\\). Priors are specified as: \\(\\boldsymbol \\beta \\sim \\text{Normal}(0,1000)\\) and \\(\\sigma \\sim \\text{Cauchy}(0,25)\\). We will explore Bayesian modelling of simple linear regression using JAGS. Remember that in this software normal distributions are specified in terms of precision \\(\\tau\\) rather than standard deviation \\(\\sigma\\), where \\(\\tau=\\frac{1}{\\sigma^2}\\). In addition, we will derive the following quantities.\n\rThe percentage decline \\(\\left(100 \\times \\frac{(\\text{max}(x) - \\text{min}(x))\\beta_1 + \\text{min}(y)}{\\text{min}(y)} \\right)\\)\n\rThe probability that \\(y\\) decline by more than \\(25\\)%\n\rThe finite-population variance components\n\r\r\u0026gt; modelString = \u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- beta0+beta1*x[i]\r+ y.err[i] \u0026lt;- y[i] - mu[i]\r+ }\r+ + #Priors\r+ beta0 ~ dnorm(0.01,1.0E-6)\r+ beta1 ~ dnorm(0,1.0E-6)\r+ tau \u0026lt;- 1 / (sigma * sigma)\r+ sigma~dunif(0,100)\r+ + #Other Derived parameters + p.decline \u0026lt;- 1-step(beta1)\r+ ymin\u0026lt;-beta0+beta1*min(x) + xrange \u0026lt;- max(x) - min(x) + decline \u0026lt;- 100*((xrange*beta1)+ymin)/ymin + p.decline25 \u0026lt;- step(decline-25)\r+ + #finite-population variance components\r+ sd.x \u0026lt;- abs(beta1)*sd(x[])\r+ sd.resid \u0026lt;- sd(y.err)\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;ttestModel.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; data.list \u0026lt;- with(data, list(y = y, x = x, n = nrow(data)))\r\u0026gt; data.list\r$y\r[1] 35.69762 35.84911 43.29354 34.35254 33.14644 39.57532 31.80458 21.67469\r[9] 23.06574 22.77169 29.62041 23.79907 22.50386 19.55341 14.72079 24.93457\r$x\r[1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\r$n\r[1] 16\rDefine the initial values for the chain. Reasonable starting points can be gleaned from the data themselves.\n\u0026gt; inits \u0026lt;- rep(list(list(beta0 = mean(data$y), beta1 = diff(tapply(data$y,\r+ data$x, mean)), sigma = sd(data$y))), 2)\rDefine the nodes (parameters and derivatives) to monitor.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;, \u0026quot;sigma\u0026quot;)\rDefine the chain parameters.\n\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 3000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 15000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 10500\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\u0026gt; library(R2jags)\rWhen using the jags function (R2jags package), it is not necessary to provide initial values. However, if they are to be supplied, the inital values must be provided as a list of the same length as the number of chains. Then print the results.\n\u0026gt; data.r2jags \u0026lt;- jags(data = data.list, inits = NULL, parameters.to.save = params, model.file = \u0026quot;ttestModel.txt\u0026quot;, n.chains = nChains, n.iter = nIter, n.burnin = burnInSteps, n.thin = thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 16\rUnobserved stochastic nodes: 3\rTotal graph size: 109\rInitializing model\r\u0026gt; \u0026gt; print(data.r2jags)\rInference for Bugs model at \u0026quot;ttestModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 40.332 2.779 34.814 38.547 40.341 42.142 45.768 1.001 15000\rbeta1 -1.390 0.283 -1.957 -1.571 -1.390 -1.209 -0.822 1.001 15000\rsigma 5.187 1.125 3.549 4.399 5.009 5.772 7.848 1.001 14000\rdeviance 96.319 2.919 93.005 94.208 95.578 97.595 103.875 1.001 15000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 4.3 and DIC = 100.6\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots for the effects model as an example. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters, e.g. \\(\\boldsymbol \\beta\\).\n\u0026gt; library(mcmcplots)\r\u0026gt; denplot(data.r2jags, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;))\r\u0026gt; traplot(data.r2jags, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;))\rThese plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.\n\u0026gt; data.mcmc = as.mcmc(data.r2jags)\r\u0026gt; #Raftery diagnostic\r\u0026gt; raftery.diag(data.mcmc)\r[[1]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 3 4115 3746 1.10 beta1 2 3855 3746 1.03 deviance 3 4026 3746 1.07 sigma 4 4907 3746 1.31 [[2]]\rQuantile (q) = 0.025\rAccuracy (r) = +/- 0.005\rProbability (s) = 0.95 Burn-in Total Lower bound Dependence\r(M) (N) (Nmin) factor (I)\rbeta0 2 3938 3746 1.05 beta1 2 3770 3746 1.01 deviance 2 3811 3746 1.02 sigma 4 4853 3746 1.30 \rThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\u0026gt; #Autocorrelation diagnostic\r\u0026gt; autocorr.diag(data.mcmc)\rbeta0 beta1 deviance sigma\rLag 0 1.000000000 1.0000000000 1.000000000 1.00000000\rLag 1 -0.007010696 0.0009369893 0.397147648 0.46491253\rLag 5 0.002086800 0.0011849092 0.049133264 0.05413994\rLag 10 0.005430778 0.0054667236 0.008226042 0.01218053\rLag 50 -0.011848951 -0.0054465800 -0.014357351 -0.01271746\rA lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. Ideally, a good model should also be able to predict the data used to fit the model.\nAlthough residuals can be computed directly within R2jags, we can calculate them manually from the posteriors to be consistent across other approaches.\n\u0026gt; library(ggplot2)\r\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;)]\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc, 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit))\rResiduals against predictors\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;)]\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc, 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data$x))\rAnd now for studentized residuals\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;)]\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc, 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; sresid = resid/sd(resid)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit))\rFor this simple model, the studentized residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~x, data)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i,\r+ ], mcmc[i, \u0026quot;sigma\u0026quot;]))\r\u0026gt; ggplot() + geom_density(data = NULL, aes(x = as.vector(yRep),\r+ fill = \u0026quot;Model\u0026quot;), alpha = 0.5) + geom_density(data = data,\r+ aes(x = y, fill = \u0026quot;Obs\u0026quot;), alpha = 0.5)\r\rParameter estimates\rAlthough all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and \\(95\\)% credibility intervals.\n\u0026gt; print(data.r2jags)\rInference for Bugs model at \u0026quot;ttestModel.txt\u0026quot;, fit using jags,\r2 chains, each with 10500 iterations (first 3000 discarded)\rn.sims = 15000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 40.332 2.779 34.814 38.547 40.341 42.142 45.768 1.001 15000\rbeta1 -1.390 0.283 -1.957 -1.571 -1.390 -1.209 -0.822 1.001 15000\rsigma 5.187 1.125 3.549 4.399 5.009 5.772 7.848 1.001 14000\rdeviance 96.319 2.919 93.005 94.208 95.578 97.595 103.875 1.001 15000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 4.3 and DIC = 100.6\rDIC is an estimate of expected predictive error (lower deviance is better).\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(as.mcmc(data.r2jags), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 4 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 beta0 40.3 2.78 34.9 45.8 2 beta1 -1.39 0.283 -1.94 -0.812\r3 deviance 96.3 2.92 92.8 102. 4 sigma 5.19 1.13 3.31 7.38 \rA one unit increase in \\(x\\) is associated with a \\(-1.39\\) change in \\(y\\). That is, \\(y\\) declines at a rate of \\(-1.39\\) per unit increase in \\(x\\). The \\(95\\)% confidence interval for the slope does not overlap with \\(0\\) implying a significant effect of \\(x\\) on \\(y\\). While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\r\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(data.r2jags$BUGSoutput$sims.matrix[, c(\u0026quot;beta1\u0026quot;)])\r[1] 0\rWith a p-value of essentially \\(0\\), we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship.\n\rGraphical summaries\rA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group. We do this by loading functions in the package dplyr.\n\u0026gt; library(dplyr)\r\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = data.frame(x = seq(min(data$x, na.rm = TRUE), max(data$x, na.rm = TRUE),\r+ len = 1000))\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X\u0026quot;) + theme_classic()\rIf you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.\n\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data, aes(y = y,\r+ x = x), color = \u0026quot;gray\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X\u0026quot;) + theme_classic()\rA more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since \\(\\text{resid}=\\text{obs}−\\text{fitted}\\) and the fitted values depend only on the single predictor we are interested in.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = data\r\u0026gt; fMat = rMat = model.matrix(~x, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\r+ color = \u0026quot;gray\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low, ymax = conf.high),\r+ fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_continuous(\u0026quot;X\u0026quot;) +\r+ theme_classic()\r\rEffect sizes\rLets explore a range of effect sizes:\n\rRaw effect size between the largest and smallest \\(x\\)\n\rCohen’s D\n\rPercentage change between the largest and smallest \\(x\\)\n\rFractional change between the largest and smallest \\(x\\)\n\rProbability that a change in \\(x\\) is associated with greater than a \\(25\\)% decline in \\(y\\).\n\r\rClearly, in order to explore this inference, we must first express the change in \\(y\\) as a percentage. This in turn requires us to calculate start and end points from which to calculate the magnitude of the effect (amount of decline in \\(y\\)) as well as the percentage decline. Hence, we start by predicting the distribution of \\(y\\) at the lowest and highest values of \\(x\\).\n\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; newdata = data.frame(x = c(min(data$x, na.rm = TRUE), max(data$x, na.rm = TRUE)))\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## Raw effect size\r\u0026gt; (RES = tidyMCMC(as.mcmc(fit[, 2] - fit[, 1]), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 -20.9 4.24 -29.2 -12.2\r\u0026gt; ## Cohen\u0026#39;s D\r\u0026gt; cohenD = (fit[, 2] - fit[, 1])/mcmc[, \u0026quot;sigma\u0026quot;]\r\u0026gt; (cohenDES = tidyMCMC(as.mcmc(cohenD), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 -4.19 1.14 -6.40 -1.94\r\u0026gt; # Percentage change (relative to Group A)\r\u0026gt; ESp = 100 * (fit[, 2] - fit[, 1])/fit[, 1]\r\u0026gt; (PES = tidyMCMC(as.mcmc(ESp), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 -53.2 8.25 -69.4 -36.5\r\u0026gt; # Probability that the effect is greater than 25% (a decline of \u0026gt;25%)\r\u0026gt; sum(-1 * ESp \u0026gt; 25)/length(ESp)\r[1] 0.9964667\r\u0026gt; ## fractional change\r\u0026gt; fit = fit[fit[, 2] \u0026gt; 0, ]\r\u0026gt; (FES = tidyMCMC(as.mcmc(fit[, 2]/fit[, 1]), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.468 0.0825 0.306 0.635\rConclusions\n\rOn average, \\(Y\\) declines by \\(-20.9\\) over the observed range of \\(x\\). We are \\(95\\)% confident that the decline is between \\(-29.2\\) and \\(-12.2\\).\n\rThe Cohen’s D associated with a change over the observed range of \\(x\\) is \\(-4.19\\).\n\rOn average, \\(Y\\) declines by \\(-53.2\\)% over the observed range of \\(x\\). We are \\(95\\)% confident that the decline is between \\(-69.4\\)% and \\(-36.5\\)%.\n\rThe probability that \\(Y\\) declines by more than \\(25\\)% over the observed range of \\(x\\) is \\(0.996\\).\n\rOn average, \\(Y\\) declines by a factor of \\(0.468\\)% over the observed range of \\(x\\). We are \\(95\\)% confident that the decline is between a factor of \\(0.306\\)% and \\(0.635\\)%.\n\r\r\rFinite population standard deviations\rVariance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).\nFinite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (Gelman and others (2005)). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.\n# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x 6.62 1.35 3.87 9.26\r2 sd.resid 4.72 0.279 4.54 5.28\r# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x 59.3 5.71 46.6 63.9\r2 sd.resid 40.7 5.71 36.1 53.4\rApproximately \\(59.3\\)% of the total finite population standard deviation is due to \\(x\\).\n\rR squared\rIn a frequentist context, the \\(R^2\\) value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, \\(R^2\\) is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an \\(R^2\\) greater than \\(100\\)%. Gelman et al. (2019) proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.\nSo in the standard regression model notation of:\n\\[ y_i \\sim \\text{Normal}(\\boldsymbol X \\boldsymbol \\beta, \\sigma),\\]\nthe \\(R^2\\) could be formulated as\n\\[ R^2 = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sigma^2_e},\\]\nwhere \\(\\sigma^2_f=\\text{var}(\\boldsymbol X \\boldsymbol \\beta)\\), and for normal models \\(\\sigma^2_e=\\text{var}(y-\\boldsymbol X \\boldsymbol \\beta)\\)\n\u0026gt; mcmc \u0026lt;- data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; Xmat = model.matrix(~x, data)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = sweep(fit, 2, data$y, \u0026quot;-\u0026quot;)\r\u0026gt; var_f = apply(fit, 1, var)\r\u0026gt; var_e = apply(resid, 1, var)\r\u0026gt; R2 = var_f/(var_f + var_e)\r\u0026gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.649 0.106 0.433 0.758\r\u0026gt; \u0026gt; # for comparison with frequentist\r\u0026gt; summary(lm(y ~ x, data))\rCall:\rlm(formula = y ~ x, data = data)\rResiduals:\rMin 1Q Median 3Q Max -7.5427 -3.3510 -0.3309 2.0411 7.5791 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 40.3328 2.4619 16.382 1.58e-10 ***\rx -1.3894 0.2546 -5.457 8.45e-05 ***\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 4.695 on 14 degrees of freedom\rMultiple R-squared: 0.6802, Adjusted R-squared: 0.6574 F-statistic: 29.78 on 1 and 14 DF, p-value: 8.448e-05\r\rReferences\rGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9.\n\rGelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” The Annals of Statistics 33 (1): 1–53.\n\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1580695994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580695994,"objectID":"6a51dbc0c0120e11a262bc317c7fdc6e","permalink":"/jags/simple-linear-regression-jags/simple-linear-regression-jags/","publishdate":"2020-02-02T21:13:14-05:00","relpermalink":"/jags/simple-linear-regression-jags/simple-linear-regression-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","linear regression"],"title":"Simple Linear Regression - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","linear regression","STAN"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nOverview\rIntroduction\rMany clinicians get a little twitchy and nervous around mathematical and statistical formulae and nomenclature. Whilst it is possible to perform basic statistics without too much regard for the actual equation (model) being employed, as the complexity of the analysis increases, the need to understand the underlying model becomes increasingly important. Moreover, model specification in BUGS/JAGS/STAN (the language used to program Bayesian modelling) aligns very closely to the underlying formulae. Hence a good understanding of the underlying model is vital to be able to create a sensible Bayesian model. Consequently, I will always present the linear model formulae along with the analysis.\nTo introduce the philosophical and mathematical differences between classical (frequentist) and Bayesian statistics, based on previous works, we present a provocative yet compelling trend analysis of two hypothetical populations (A vs B). The temporal trend of population A shows very little variability from a very subtle linear decline (\\(n=10\\), \\(\\text{slope}=-0.10\\), \\(\\text{p-value}=0.048\\)). By contrast, the B population appears to decline more dramatically, yet has substantially more variability (\\(n=10\\), \\(\\text{slope}=-10.23\\), \\(\\text{p-value}=0.058\\)). From a traditional frequentist perspective, we would conclude that there is a “significant” relationship in Population A (\\(p\u0026lt;0.05\\)), yet not in Population B (\\(p\u0026gt;0.05\\)). However, if we consider a third population C which is exactly the same as populstion B but with a higher number of observations, then we may end up with a completely different conclusion compared with that based on population B (\\(n=100\\), \\(\\text{slope}=-10.47\\), \\(\\text{p-value}\u0026lt;0.001\\)).\nThe above illustrates a couple of things:\n\rstatistical significance does not necessarily translate into clinical importance. Indeed, population B is declining at nearly \\(10\\) times the rate of population A. That sounds rather important, yet on the basis of the hypothesis test, we would dismiss the decline in population B.\n\rthat a p-value is just the probability of detecting an effect or relationship - what is the probability that the sample size is large enough to pick up a difference.\n\r\rLet us now look at it from a Bayesian perspective, with a focus on population A and B. We would conclude that:\n\rthe mean (plus or minus CI) slopes for Population A and B are \\(-0.1 (-0.21,0)\\) and \\(-10.08 (-20.32,0.57)\\) respectively\n\rthe Bayesian approach allows us to query the posterior distribution is many other ways in order to ask sensible clinical questions. For example, we might consider that a rate of change of \\(5\\)% or greater represents an important biological impact. For population A and B, the probability that the rate is \\(5\\)% or greater is \\(0\\) and \\(0.85\\) respectively.\n\r\r\rLinear regression\rSimple linear regression is a linear modelling process that models a continuous response against a single continuous predictor. The linear model is expressed as:\n\\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma),\\]\nwhere \\(y_i\\) is the response variable for each of the \\(i=1\\ldots,n\\) observations, \\(\\beta_0\\) is the intercept (value when \\(x=0\\)), \\(\\beta_1\\) is the slope (rate of change in \\(y\\) per unit change in \\(x\\)), \\(x_i\\) is the predictor variable, \\(\\epsilon_i\\) is the residual value (difference between the observed value and the value expected by the model). The parameters of the trendline \\(\\boldsymbol \\beta=(\\beta_0,\\beta_1)\\) are determined by Ordinary Least Squares (OLS) in which the sum of the squared residuals is minimized. A non-zero population slope is indicative of a relationship.\n\r\rData generation\rLets say we had set up an experiment in which we applied a continuous treatment (\\(x\\)) ranging in magnitude from \\(0\\) to \\(16\\) to a total of \\(16\\) sampling units (\\(n=16\\)) and then measured a response (\\(y\\)) from each unit. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\u0026gt; set.seed(123)\r\u0026gt; n \u0026lt;- 16\r\u0026gt; a \u0026lt;- 40 #intercept\r\u0026gt; b \u0026lt;- -1.5 #slope\r\u0026gt; sigma2 \u0026lt;- 25 #residual variance (sd=5)\r\u0026gt; x \u0026lt;- 1:n #values of the year covariate\r\u0026gt; eps \u0026lt;- rnorm(n, mean = 0, sd = sqrt(sigma2)) #residuals\r\u0026gt; y \u0026lt;- a + b * x + eps #response variable\r\u0026gt; # OR\r\u0026gt; y \u0026lt;- (model.matrix(~x) %*% c(a, b)) + eps\r\u0026gt; data \u0026lt;- data.frame(y, x) #dataset\r\u0026gt; head(data) #print out the first six rows of the data set\ry x\r1 35.69762 1\r2 35.84911 2\r3 43.29354 3\r4 34.35254 4\r5 33.14644 5\r6 39.57532 6\rWith these sort of data, we are primarily interested in investigating whether there is a relationship between the continuous response variable and the linear predictor (single continuous predictor).\nCentering the data\rWhen a linear model contains a covariate (continuous predictor variable) in addition to another predictor (continuous or categorical), it is nearly always advisable that the continuous predictor variables are centered prior to the analysis. Centering is a process by which the mean of a variable is subtracted from each of the values such that the scale of the variable is shifted so as to be centered around \\(0\\). Hence the mean of the new centered variable will be \\(0\\), yet it will retain the same variance.\nThere are multiple reasons for this:\nIt provides some clinical meaning to the \\(y\\)-intercept. Recall that the \\(y\\)-intercept is the value of \\(Y\\) when \\(X\\) is equal to zero. If \\(X\\) is centered, then the \\(y\\)-intercept represents the value of \\(Y\\) at the mid-point of the \\(X\\) range. The \\(y\\)-intercept of an uncentered \\(X\\) typically represents a unreal value of \\(Y\\) (as an \\(X\\) of \\(0\\) is often beyond the reasonable range of values).\n\rIn multiplicative models (in which predictors and their interactions are included), main effects and interaction terms built from centered predictors will not be correlated to one another.\n\rFor more complex models, centering the covariates can increase the likelihood that the modelling engine converges (arrives at a numerically stable and reliable outcome).\n\r\rNote, centering will not effect the slope estimates. In R, centering is easily achieved with the scale function.\n\u0026gt; data \u0026lt;- within(data, {\r+ cx \u0026lt;- as.numeric(scale(x, scale = FALSE))\r+ })\r\u0026gt; head(data)\ry x cx\r1 35.69762 1 -7.5\r2 35.84911 2 -6.5\r3 43.29354 3 -5.5\r4 34.35254 4 -4.5\r5 33.14644 5 -3.5\r6 39.57532 6 -2.5\r\r\rExploratory data analysis\rNormality\rEstimation and inference testing in linear regression assumes that the response is normally distributed in each of the populations. In this case, the populations are all possible measurements that could be collected at each level of \\(x\\) - hence there are \\(16\\) populations. Typically however, we only collect a single observation from each population (as is also the case here). How then can be evaluate whether each of these populations are likely to have been normal? For a given response, the population distributions should follow much the same distribution shapes. Therefore provided the single samples from each population are unbiased representations of those populations, a boxplot of all observations should reflect the population distributions.\n\rHomogeneity of variance\rSimple linear regression also assumes that each of the populations are equally varied. Actually, it is prospect of a relationship between the mean and variance of \\(y\\)-values across x-values that is of the greatest concern. Strictly the assumption is that the distribution of \\(y\\) values at each \\(x\\) value are equally varied and that there is no relationship between mean and variance. However, as we only have a single \\(y\\)-value for each \\(x\\)-value, it is difficult to directly determine whether the assumption of homogeneity of variance is likely to have been violated (mean of one value is meaningless and variability can’t be assessed from a single value). If we then plot the residuals (difference between observed values and those predicted by the trendline) against the predict values and observe a definite presence of a pattern, then it is indicative of issues with the assumption of homogeneity of variance.\nHence looking at the spread of values around a trendline on a scatterplot of \\(y\\) against \\(x\\) is a useful way of identifying gross violations of homogeneity of variance. Residual plots provide an even better diagnostic. The presence of a wedge shape is indicative that the population mean and variance are related.\n\rLinearity\rLinear regression fits a straight (linear) line through the data. Therefore, prior to fitting such a model, it is necessary to establish whether this really is the most sensible way of describing the relationship. That is, does the relationship appear to be linearly related or could some other non-linear function describe the relationship better. Scatterplots and residual plots are useful diagnostics.\n\rModel assumptions\rThe typical assumptions which need to be checked when fitting a standard linear regression model are:\n\rAll of the observations are independent - this must be addressed at the design and collection stages\n\rThe response variable (and thus the residuals) should be normally distributed\n\rThe response variable should be equally varied (variance should not be related to mean as these are supposed to be estimated separately)\n\rThe relationship between the linear predictor (right hand side of the regression formula) and the link function should be linear. A scatterplot with smoother can be useful for identifying possible non-linearity.\n\r\rSo lets explore normality, homogeneity of variances and linearity by constructing a scatterplot of the relationship between the response (\\(y\\)) and the predictor (\\(x\\)). We will also include a range of smoothers (linear and lowess) and marginal boxplots on the scatterplot to assist in exploring linearity and normality respectively.\n\u0026gt; # scatterplot\r\u0026gt; library(car)\r\u0026gt; scatterplot(y ~ x, data)\rConclusions:\nThere is no evidence that the response variable is non-normal. The spread of values around the trendline seems fairly even (hence it there is no evidence of non-homogeneity). The data seems well represented by the linear trendline. Furthermore, the lowess smoother does not appear to have a consistent shift trajectory. Obvious violations could be addressed either by:\n\rConsider a non-linear linear predictor (such as a polynomial, spline or other non-linear function)\n\rTransform the scale of the response variables (e.g. to address normality)\n\r\r\r\rModel fitting\rWhilst Gibbs sampling provides an elegantly simple MCMC sampling routine, very complex hierarchical models can take enormous numbers of iterations (often prohibitory large) to converge on a stable posterior distribution. To address this, Andrew Gelman (and other collaborators) have implemented a variation on Hamiltonian Monte Carlo (HMC): a sampler that selects subsequent samples in a way that reduces the correlation between samples, thereby speeding up convergence) called the No-U-Turn (NUTS) sampler. All of these developments are brought together into a tool called Stan. By design (to appeal to the vast BUGS/JAGS users), STAN models are defined in a manner reminiscent of BUGS/JAGS. STAN first converts these models into C++ code which is then compiled to allow very rapid computation. Consistent with this, the model must be accompanied by variable declarations for all inputs and parameters.\nNote the following important characteristics of a STAN code:\n\rA STAN model file comprises a number of blocks (not all of which are compulsory).\n\rThe STAN language is an intermediary between (R/BUGS and c++) and requires all types (integers, vectors, matrices etc) to be declared prior to use and it uses c++ commenting (// and /* */)\n\rCode order is important, objects must be declared before they are used. When a type is declared in one block, it is available in subsequent blocks.\n\r\r\rdata {\r// declare the input data / parameters\r}\rtransformed data {\r// optional - for transforming/scaling input data\r}\rparameters {\r// define model parameters\r}\rtransformed parameters {\r// optional - for deriving additional non-model parameters\r// note however, as they are part of the sampling chain\r// transformed parameters slow sampling down.\r}\rmodel {\r// specifying priors and likelihood as well as the linear predictor\r}\rgenerated quantities {\r// optional - derivatives (posteriors) of the samples\r}\r\rThe minimum model in STAN required to fit the above simple regression follows. Note the following modifications from the model defined in JAGS:\n\rThe normal distribution is defined by standard deviation rather than precision\n\rRather than using a uniform prior for \\(\\sigma\\), I am using a half-Cauchy\n\r\rWe now translate the likelihood model into STAN code.\n\u0026gt; modelString = \u0026quot;\r+ data {\r+ int\u0026lt;lower=0\u0026gt; n;\r+ vector [n] y;\r+ vector [n] x;\r+ }\r+ parameters {\r+ real beta0;\r+ real beta;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ }\r+ model {\r+ vector [n] mu;\r+ #Priors\r+ beta0 ~ normal(0,10000);\r+ beta ~ normal(0,10000);\r+ sigma ~ cauchy(0,5);\r+ + mu = beta0+beta*x;\r+ + #Likelihood\r+ y~normal(mu,sigma);\r+ }\r+ + \u0026quot;\r\u0026gt; ## write the model to a stan file \u0026gt; writeLines(modelString, con = \u0026quot;linregModel.stan\u0026quot;)\rThe No-U-Turn sampler operates much more efficiently if all predictors are centered. Although it is possible to pre-center all predictors that are passed to STAN, it is then often necessary to later convert back to the original scale for graphing and further analyses. Since centering is a routine procedure, arguably it should be built into the STAN we generate. Furthermore, we should also include the back-scaling as well. In this version, the data are to be supplied as a model matrix (so as to leverage various vectorized and matrix multiplier routines). The transformed data block is used to center the non-intercept columns of the predictor model matrix. The model is fit on centered data thereby generating a slope and intercept. This intercept parameter is also expressed back on the non-centered scale (generated properties block).\n\u0026gt; modelStringv2 = \u0026quot;\r+ data { + int\u0026lt;lower=1\u0026gt; n; // total number of observations + vector[n] Y; // response variable + int\u0026lt;lower=1\u0026gt; nX; // number of effects + matrix[n, nX] X; // model matrix + } + transformed data { + matrix[n, nX - 1] Xc; // centered version of X + vector[nX - 1] means_X; // column means of X before centering + + for (i in 2:nX) { + means_X[i - 1] = mean(X[, i]); + Xc[, i - 1] = X[, i] - means_X[i - 1]; + } + } + parameters { + vector[nX-1] beta; // population-level effects + real cbeta0; // center-scale intercept + real\u0026lt;lower=0\u0026gt; sigma; // residual SD + } + transformed parameters { + } + model { + vector[n] mu; + mu = Xc * beta + cbeta0; + // prior specifications + beta ~ normal(0, 100); + cbeta0 ~ normal(0, 100); + sigma ~ cauchy(0, 5); + // likelihood contribution + Y ~ normal(mu, sigma); + } + generated quantities { + real beta0; // population-level intercept + beta0 = cbeta0 - dot_product(means_X, beta); + }\r+ + \u0026quot;\r\u0026gt; ## write the model to a stan file \u0026gt; writeLines(modelStringv2, con = \u0026quot;linregModelv2.stan\u0026quot;)\rArrange the data as a list (as required by STAN). As input, STAN will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\u0026gt; Xmat \u0026lt;- model.matrix(~x, data = data)\r\u0026gt; data.list \u0026lt;- with(data, list(Y = y, X = Xmat, nX = ncol(Xmat), n = nrow(data)))\r\u0026gt; data.list\r$Y\r[1] 35.69762 35.84911 43.29354 34.35254 33.14644 39.57532 31.80458 21.67469\r[9] 23.06574 22.77169 29.62041 23.79907 22.50386 19.55341 14.72079 24.93457\r$X\r(Intercept) x\r1 1 1\r2 1 2\r3 1 3\r4 1 4\r5 1 5\r6 1 6\r7 1 7\r8 1 8\r9 1 9\r10 1 10\r11 1 11\r12 1 12\r13 1 13\r14 1 14\r15 1 15\r16 1 16\rattr(,\u0026quot;assign\u0026quot;)\r[1] 0 1\r$nX\r[1] 2\r$n\r[1] 16\rDefine the initial values for the chain. Reasonable starting points can be gleaned from the data themselves.\n\u0026gt; inits \u0026lt;- rep(list(list(beta0 = mean(data$y), beta1 = diff(tapply(data$y,\r+ data$x, mean)), sigma = sd(data$y))), 2)\rDefine the nodes (parameters and derivatives) to monitor.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta\u0026quot;,\u0026quot;beta0\u0026quot;, \u0026quot;cbeta0\u0026quot;, \u0026quot;sigma\u0026quot;)\rDefine the chain parameters.\n\u0026gt; nChains = 2\r\u0026gt; burnInSteps = 1000\r\u0026gt; thinSteps = 1\r\u0026gt; numSavedSteps = 3000 #across all chains\r\u0026gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\r\u0026gt; nIter\r[1] 2500\rStart the STAN model (check the model, load data into the model, specify the number of chains and compile the model). Load the rstan package.\n\u0026gt; library(rstan)\rWhen using the stan function (rstan package), it is not necessary to provide initial values. However, if they are to be supplied, the inital values must be provided as a list of the same length as the number of chains.\n\u0026gt; data.rstan \u0026lt;- stan(data = data.list, file = \u0026quot;linregModelv2.stan\u0026quot;, + chains = nChains, iter = nIter, warmup = burnInSteps,\r+ thin = thinSteps, save_dso = TRUE)\rSAMPLING FOR MODEL \u0026#39;linregModelv2\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 2500 [ 0%] (Warmup)\rChain 1: Iteration: 250 / 2500 [ 10%] (Warmup)\rChain 1: Iteration: 500 / 2500 [ 20%] (Warmup)\rChain 1: Iteration: 750 / 2500 [ 30%] (Warmup)\rChain 1: Iteration: 1000 / 2500 [ 40%] (Warmup)\rChain 1: Iteration: 1001 / 2500 [ 40%] (Sampling)\rChain 1: Iteration: 1250 / 2500 [ 50%] (Sampling)\rChain 1: Iteration: 1500 / 2500 [ 60%] (Sampling)\rChain 1: Iteration: 1750 / 2500 [ 70%] (Sampling)\rChain 1: Iteration: 2000 / 2500 [ 80%] (Sampling)\rChain 1: Iteration: 2250 / 2500 [ 90%] (Sampling)\rChain 1: Iteration: 2500 / 2500 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.027 seconds (Warm-up)\rChain 1: 0.035 seconds (Sampling)\rChain 1: 0.062 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;linregModelv2\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 2500 [ 0%] (Warmup)\rChain 2: Iteration: 250 / 2500 [ 10%] (Warmup)\rChain 2: Iteration: 500 / 2500 [ 20%] (Warmup)\rChain 2: Iteration: 750 / 2500 [ 30%] (Warmup)\rChain 2: Iteration: 1000 / 2500 [ 40%] (Warmup)\rChain 2: Iteration: 1001 / 2500 [ 40%] (Sampling)\rChain 2: Iteration: 1250 / 2500 [ 50%] (Sampling)\rChain 2: Iteration: 1500 / 2500 [ 60%] (Sampling)\rChain 2: Iteration: 1750 / 2500 [ 70%] (Sampling)\rChain 2: Iteration: 2000 / 2500 [ 80%] (Sampling)\rChain 2: Iteration: 2250 / 2500 [ 90%] (Sampling)\rChain 2: Iteration: 2500 / 2500 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.028 seconds (Warm-up)\rChain 2: 0.034 seconds (Sampling)\rChain 2: 0.062 seconds (Total)\rChain 2: \r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots for the effects model as an example.\n\u0026gt; library(mcmcplots)\r\u0026gt; s = as.array(data.rstan)\r\u0026gt; mcmc \u0026lt;- do.call(mcmc.list, plyr:::alply(s[, , -(length(s[1, 1, ]))], 2, as.mcmc))\r\u0026gt; denplot(mcmc, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta\u0026quot;,\u0026quot;cbeta0\u0026quot;,\u0026quot;sigma\u0026quot;))\r\u0026gt; traplot(mcmc, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta\u0026quot;,\u0026quot;cbeta0\u0026quot;,\u0026quot;sigma\u0026quot;))\rThese plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. We can also look at just the density plot computed from the bayesplot package.\n\u0026gt; library(bayesplot)\r\u0026gt; mcmc_dens(as.array(data.rstan))\rDensity plots sugggest mean or median would be appropriate to describe the fixed posteriors and median is appropriate for the \\(\\sigma\\) posterior.\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. Ideally, a good model should also be able to predict the data used to fit the model.\nAlthough residuals can be computed directly within rstan, we can calculate them manually from the posteriors to be consistent across other approaches.\n\u0026gt; library(ggplot2)\r\u0026gt; mcmc = as.matrix(data.rstan)[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;)]\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc, 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit))\rResiduals against predictors\n\u0026gt; mcmc = as.matrix(data.rstan)[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;)]\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc, 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = data$x))\rAnd now for studentized residuals\n\u0026gt; mcmc = as.matrix(data.rstan)[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;)]\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc, 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; sresid = resid/sd(resid)\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit))\rFor this simple model, the studentized residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\u0026gt; mcmc = as.matrix(data.rstan)\r\u0026gt; # generate a model matrix\r\u0026gt; Xmat = model.matrix(~x, data)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## draw samples from this model\r\u0026gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i, ], mcmc[i,\r+ \u0026quot;sigma\u0026quot;]))\r\u0026gt; ggplot() + geom_density(data = NULL, aes(x = as.vector(yRep), fill = \u0026quot;Model\u0026quot;),\r+ alpha = 0.5) + geom_density(data = data, aes(x = y, fill = \u0026quot;Obs\u0026quot;),\r+ alpha = 0.5)\r\rParameter estimates\rAlthough all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and \\(95\\)% credibility intervals.\n\u0026gt; summary(data.rstan)\r$summary\rmean se_mean sd 2.5% 25% 50%\rbeta[1] -1.386241 0.005409925 0.2628701 -1.928032 -1.556143 -1.390692\rcbeta0 28.526455 0.025241285 1.2563021 26.053580 27.679148 28.517154\rsigma 4.938850 0.022620333 0.9747690 3.433777 4.255653 4.800261\rbeta0 40.309502 0.051054981 2.5104697 35.305689 38.660487 40.321813\rlp__ -32.392865 0.038016453 1.2930877 -35.617366 -32.976932 -32.063282\r75% 97.5% n_eff Rhat\rbeta[1] -1.216324 -0.8695824 2361.021 1.0010917\rcbeta0 29.358476 31.0250363 2477.224 1.0000293\rsigma 5.472441 7.1969472 1856.972 1.0020160\rbeta0 41.901624 45.2722391 2417.874 0.9999759\rlp__ -31.431890 -30.9164925 1156.945 1.0014468\r$c_summary\r, , chains = chain:1\rstats\rparameter mean sd 2.5% 25% 50% 75%\rbeta[1] -1.395354 0.2663176 -1.938168 -1.570786 -1.397259 -1.224694\rcbeta0 28.499907 1.2671111 25.965301 27.665662 28.499904 29.339459\rsigma 4.974297 1.0354718 3.403207 4.248213 4.814835 5.545663\rbeta0 40.360414 2.4961956 35.294178 38.744797 40.418903 41.921210\rlp__ -32.448925 1.3633114 -35.893989 -33.121738 -32.073215 -31.438407\rstats\rparameter 97.5%\rbeta[1] -0.8701744\rcbeta0 31.0552336\rsigma 7.3827875\rbeta0 45.2489270\rlp__ -30.9157752\r, , chains = chain:2\rstats\rparameter mean sd 2.5% 25% 50% 75%\rbeta[1] -1.377128 0.2591452 -1.911379 -1.544377 -1.385973 -1.203797\rcbeta0 28.553004 1.2452556 26.163699 27.717229 28.540569 29.378535\rsigma 4.903404 0.9089921 3.457094 4.261745 4.785809 5.423194\rbeta0 40.258590 2.5244684 35.319104 38.595264 40.250020 41.864964\rlp__ -32.336805 1.2167003 -35.405972 -32.883087 -32.042686 -31.424106\rstats\rparameter 97.5%\rbeta[1] -0.8699495\rcbeta0 31.0050930\rsigma 7.0778361\rbeta0 45.2803661\rlp__ -30.9173256\r\u0026gt; \u0026gt; # OR\r\u0026gt; library(broom)\r\u0026gt; tidyMCMC(data.rstan, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;, rhat = TRUE, ess = TRUE)\r# A tibble: 4 x 7\rterm estimate std.error conf.low conf.high rhat ess\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r1 beta[1] -1.39 0.263 -1.90 -0.856 1.00 2361\r2 cbeta0 28.5 1.26 26.2 31.1 1.00 2477\r3 sigma 4.94 0.975 3.31 6.96 1.00 1857\r4 beta0 40.3 2.51 35.3 45.2 1.000 2418\rA one unit increase in \\(x\\) is associated with a \\(-1.39\\) change in \\(y\\). That is, \\(y\\) declines at a rate of \\(-1.39\\) per unit increase in \\(x\\). The \\(95\\)% confidence interval for the slope does not overlap with \\(0\\) implying a significant effect of \\(x\\) on \\(y\\). While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\nAlso note that since our STAN model incorporated predictor centering, we have estimates of the intercept based on both centered (cbeta0) and uncentered data (beta0). Since the intercept from uncentered data is beyond the domain of our sampling data it has very little interpretability. However, the intercept based on centered data can be interpreted as the estimate of the response at the mean predictor (in this case \\(28.5\\)).\n\u0026gt; mcmcpvalue \u0026lt;- function(samp) {\r+ ## elementary version that creates an empirical p-value for the\r+ ## hypothesis that the columns of samp have mean zero versus a general\r+ ## multivariate distribution with elliptical contours.\r+ + ## differences from the mean standardized by the observed\r+ ## variance-covariance factor\r+ + ## Note, I put in the bit for single terms\r+ if (length(dim(samp)) == 0) {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/length(samp)\r+ } else {\r+ std \u0026lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\r+ transpose = TRUE)\r+ sqdist \u0026lt;- colSums(std * std)\r+ sum(sqdist[-1] \u0026gt; sqdist[1])/nrow(samp)\r+ }\r+ + }\r\u0026gt; ## since values are less than zero\r\u0026gt; mcmcpvalue(as.matrix(data.rstan)[, c(\u0026quot;beta[1]\u0026quot;)])\r[1] 0\rWith a p-value of essentially \\(0\\), we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship.\n\rGraphical summaries\rA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group. We do this by loading functions in the package dplyr.\n\u0026gt; library(dplyr)\r\u0026gt; mcmc = as.matrix(data.rstan)\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = data.frame(x = seq(min(data$x, na.rm = TRUE), max(data$x, na.rm = TRUE),\r+ len = 1000))\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X\u0026quot;) + theme_classic()\rIf you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.\n\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data, aes(y = y,\r+ x = x), color = \u0026quot;gray\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low,\r+ ymax = conf.high), fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) +\r+ scale_x_continuous(\u0026quot;X\u0026quot;) + theme_classic()\rA more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since \\(\\text{resid}=\\text{obs}−\\text{fitted}\\) and the fitted values depend only on the single predictor we are interested in.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = data\r\u0026gt; fMat = rMat = model.matrix(~x, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\r+ color = \u0026quot;gray\u0026quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low, ymax = conf.high),\r+ fill = \u0026quot;blue\u0026quot;, alpha = 0.3) + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_continuous(\u0026quot;X\u0026quot;) +\r+ theme_classic()\r\rEffect sizes\rLets explore a range of effect sizes:\n\rRaw effect size between the largest and smallest \\(x\\)\n\rCohen’s D\n\rPercentage change between the largest and smallest \\(x\\)\n\rFractional change between the largest and smallest \\(x\\)\n\rProbability that a change in \\(x\\) is associated with greater than a \\(25\\)% decline in \\(y\\).\n\r\rClearly, in order to explore this inference, we must first express the change in \\(y\\) as a percentage. This in turn requires us to calculate start and end points from which to calculate the magnitude of the effect (amount of decline in \\(y\\)) as well as the percentage decline. Hence, we start by predicting the distribution of \\(y\\) at the lowest and highest values of \\(x\\).\n\u0026gt; mcmc = as.matrix(data.rstan)\r\u0026gt; newdata = data.frame(x = c(min(data$x, na.rm = TRUE), max(data$x, na.rm = TRUE)))\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; ## Raw effect size\r\u0026gt; (RES = tidyMCMC(as.mcmc(fit[, 2] - fit[, 1]), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 -20.8 3.94 -28.5 -12.8\r\u0026gt; ## Cohen\u0026#39;s D\r\u0026gt; cohenD = (fit[, 2] - fit[, 1])/mcmc[, \u0026quot;sigma\u0026quot;]\r\u0026gt; (cohenDES = tidyMCMC(as.mcmc(cohenD), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 -4.37 1.14 -6.68 -2.27\r\u0026gt; # Percentage change (relative to Group A)\r\u0026gt; ESp = 100 * (fit[, 2] - fit[, 1])/fit[, 1]\r\u0026gt; (PES = tidyMCMC(as.mcmc(ESp), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 -53.1 7.81 -68.5 -37.5\r\u0026gt; # Probability that the effect is greater than 25% (a decline of \u0026gt;25%)\r\u0026gt; sum(-1 * ESp \u0026gt; 25)/length(ESp)\r[1] 0.998\r\u0026gt; ## fractional change\r\u0026gt; fit = fit[fit[, 2] \u0026gt; 0, ]\r\u0026gt; (FES = tidyMCMC(as.mcmc(fit[, 2]/fit[, 1]), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.469 0.0781 0.315 0.625\rConclusions\n\rOn average, \\(Y\\) declines by \\(-20.8\\) over the observed range of \\(x\\). We are \\(95\\)% confident that the decline is between \\(-28.5\\) and \\(-12.8\\).\n\rThe Cohen’s D associated with a change over the observed range of \\(x\\) is \\(-4.37\\).\n\rOn average, \\(Y\\) declines by \\(-53.1\\)% over the observed range of \\(x\\). We are \\(95\\)% confident that the decline is between \\(-68.5\\)% and \\(-37.5\\)%.\n\rThe probability that \\(Y\\) declines by more than \\(25\\)% over the observed range of \\(x\\) is \\(0.998\\).\n\rOn average, \\(Y\\) declines by a factor of \\(0.469\\)% over the observed range of \\(x\\). We are \\(95\\)% confident that the decline is between a factor of \\(0.315\\)% and \\(0.625\\)%.\n\r\r\rFinite population standard deviations\rVariance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).\nFinite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (Gelman and others (2005)). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.\n# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x 6.60 1.25 4.07 9.04\r2 sd.resid 4.70 0.243 4.54 5.18\r# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x 59.3 5.30 47.8 63.9\r2 sd.resid 40.7 5.30 36.1 52.2\rApproximately \\(59.3\\)% of the total finite population standard deviation is due to \\(x\\).\n\rR squared\rIn a frequentist context, the \\(R^2\\) value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, \\(R^2\\) is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an \\(R^2\\) greater than \\(100\\)%. Gelman et al. (2019) proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.\nSo in the standard regression model notation of:\n\\[ y_i \\sim \\text{Normal}(\\boldsymbol X \\boldsymbol \\beta, \\sigma),\\]\nthe \\(R^2\\) could be formulated as\n\\[ R^2 = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sigma^2_e},\\]\nwhere \\(\\sigma^2_f=\\text{var}(\\boldsymbol X \\boldsymbol \\beta)\\), and for normal models \\(\\sigma^2_e=\\text{var}(y-\\boldsymbol X \\boldsymbol \\beta)\\)\n\u0026gt; mcmc \u0026lt;- as.matrix(data.rstan)\r\u0026gt; Xmat = model.matrix(~x, data)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[1]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; resid = sweep(fit, 2, data$y, \u0026quot;-\u0026quot;)\r\u0026gt; var_f = apply(fit, 1, var)\r\u0026gt; var_e = apply(resid, 1, var)\r\u0026gt; R2 = var_f/(var_f + var_e)\r\u0026gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 0.652 0.0982 0.456 0.758\r\u0026gt; \u0026gt; # for comparison with frequentist\r\u0026gt; summary(lm(y ~ x, data))\rCall:\rlm(formula = y ~ x, data = data)\rResiduals:\rMin 1Q Median 3Q Max -7.5427 -3.3510 -0.3309 2.0411 7.5791 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 40.3328 2.4619 16.382 1.58e-10 ***\rx -1.3894 0.2546 -5.457 8.45e-05 ***\r---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 4.695 on 14 degrees of freedom\rMultiple R-squared: 0.6802, Adjusted R-squared: 0.6574 F-statistic: 29.78 on 1 and 14 DF, p-value: 8.448e-05\r\rReferences\rGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9.\n\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rGelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” The Annals of Statistics 33 (1): 1–53.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\r\r\r","date":1580695994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580695994,"objectID":"666c1c78dfb81a8f95a5900a901bc33b","permalink":"/stan/simple-linear-regression-stan/simple-linear-regression-stan/","publishdate":"2020-02-02T21:13:14-05:00","relpermalink":"/stan/simple-linear-regression-stan/simple-linear-regression-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","linear regression"],"title":"Simple Linear Regression - STAN","type":"STAN"},{"authors":["Andrea Gabrio"],"categories":["R","one sample t-test","JAGS"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to explore differences between two populations. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThe BUGS/JAGS/STAN languages and algorithms are very powerful and flexible. However, the cost of this power and flexibility is complexity and the need for a firm understanding of the model you wish to fit as well as the priors to be used. The algorithms requires the following inputs.\n\rWithin the model:\nThe likelihood function relating the response to the predictors.\n\rThe definition of the priors.\n\r\rChain properties:\nThe number of chains.\n\rThe length of chains (number of iterations).\n\rThe burn-in length (number of initial iterations to ignore).\n\rThe thinning rate (number of iterations to count on before storing a sample).\n\r\rThe initial estimates to start an MCMC chain. If there are multiple chains, these starting values can differ between chains.\n\rThe list of model parameters and derivatives to monitor (and return the posterior distributions of)\n\r\rThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages.\nData generation\rWe will start by generating a random data set. Note, I am creating two versions of the predictor variable (a numeric version and a factorial version).\n\u0026gt; set.seed(123)\r\u0026gt; nA \u0026lt;- 60 #sample size from Population A\r\u0026gt; nB \u0026lt;- 40 #sample size from Population B\r\u0026gt; muA \u0026lt;- 105 #population mean of Population A\r\u0026gt; muB \u0026lt;- 77.5 #population mean of Population B\r\u0026gt; sigma \u0026lt;- 3 #standard deviation of both populations (equally varied)\r\u0026gt; yA \u0026lt;- rnorm(nA, muA, sigma) #Population A sample\r\u0026gt; yB \u0026lt;- rnorm(nB, muB, sigma) #Population B sample\r\u0026gt; y \u0026lt;- c(yA, yB)\r\u0026gt; x \u0026lt;- factor(rep(c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;), c(nA, nB))) #categorical listing of the populations\r\u0026gt; xn \u0026lt;- as.numeric(x) #numerical version of the population category for means parameterization. # Should not start at 0.\r\u0026gt; data \u0026lt;- data.frame(y, x, xn) # dataset\rLet inspect the first few rows of the dataset using the command head\n\u0026gt; head(data)\ry x xn\r1 103.3186 A 1\r2 104.3095 A 1\r3 109.6761 A 1\r4 105.2115 A 1\r5 105.3879 A 1\r6 110.1452 A 1\rWe can also perform some exploratory data analysis - in this case, a boxplot of the response for each level of the predictor.\n\u0026gt; boxplot(y ~ x, data)\r\rThe One Sample t-test\rA t-test is essentially just a simple regression model in which the categorical predictor is represented by a binary variable in which one level is coded as \\(0\\) and the other \\(1\\). For the model itself, the observed response \\(y_i\\) are assumed to be drawn from a normal distribution with a given mean \\(\\mu\\) and standard deviation \\(\\sigma\\). The expected values are themselves determined by the linear predictor \\(\\mu_i=\\beta_0+\\beta_1x_i\\), where \\(\\beta_0\\) represents the mean of the first treatment group and \\(\\beta_1\\) represents the difference between the mean of the first group and the mean of the second group (the effect of interest).\nMCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying “uninformative” priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (\\(1000\\)) for both the intercept and the treatment effect and a wide half-cauchy (scale=\\(25\\)) for the standard deviation (Gelman and others (2006)).\n\\[y_i \\sim \\text{Normal}(\\mu_i, \\sigma), \\]\nwhere \\(\\mu_i=\\beta_0+\\beta_1x_i\\).\nPriors are defined as:\n\\[ \\beta_j \\sim \\text{Normal}(0,1000), \\;\\;\\; \\text{and} \\;\\;\\; \\sigma \\sim \\text{Cauchy}(0,25), \\]\nfor \\(j=0,1\\).\nFitting the model in JAGS\rBroadly, there are two ways of parameterising (expressing the unknown (to be estimated) components of a model) a model. Either we can estimate the means of each group (Means parameterisation) or we can estimate the mean of one group and the difference between this group and the other group(s) (Effects parameterisation). The latter is commonly used for frequentist null hypothesis testing as its parameters are more consistent with the null hypothesis of interest (that the difference between the two groups equals zero).\nEffects parameterisation\r\r\\[ y_i = \\beta_0 + \\beta_{j}x_i + \\epsilon_i, \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma). \\]\nEach \\(y_i\\) is modelled by an intercept \\(\\beta_0\\) (mean of group A) plus a difference parameter \\(\\beta_j\\) (difference between mean of group A and group B) multiplied by an indicator of which group the observation came from (\\(x_i\\)), plus a residual drawn from a normal distribution with mean \\(0\\) and standard deviation \\(\\sigma\\). Actually, there are as many \\(\\beta_j\\) parameters as there are groups but one of them (typically the first) is set to be equal to zero (to avoid over-parameterization). Expected values of \\(y\\) are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of effect parameters and whose variance is defined by the degree of variability in this mean. The parameters are: \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\).\nMeans parameterisation\r\r\\[ y_i = \\beta_{j} + \\epsilon_i, \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma). \\]\nEach \\(y_i\\) is modelled as the mean \\(\\beta_j\\) of each group (\\(j=1,2\\)) plus a residual drawn from a normal distribution with a mean of zero and a standard deviation of \\(\\sigma\\). Actually, \\(\\boldsymbol \\beta\\) is a set of \\(j\\) coefficients corresponding to the \\(j\\) dummy coded factor levels. Expected values of \\(y\\) are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of means parameters and whose variance is defined by the degree of variability in this mean. The parameters are: \\(\\beta_1\\), \\(\\beta_2\\) and \\(\\sigma\\).\nIn JAGS, distributions are defined by their precision \\(\\tau\\) rather than their standard deviation \\(\\sigma\\). Precision is just the inverse of variance (\\(\\tau=\\frac{1}{\\sigma^2}\\)) and are chosen as they permit the gamma distribution to be used as the conjugate prior of the variance of a normal distribution. Bayesian analyses require that priors are specified for all the parameters. We will define vague (non-informative) priors for each of the parameters such that the posterior distributions are almost entirely influenced by the likelihood (and thus the data). Hence, appropriate (conjugate) priors for the effects parameterisation could be:\n\r\\(\\boldsymbol \\beta \\sim \\text{Normal}(0,1.0\\text{E-}6)\\) - a very flat normal distribution centered around zero. Note, \\(1.0\\text{E-}6\\) is scientific notation for \\(0.000001\\).\n\r\\(\\tau \\sim \\text{Gamma}(0.1,0.1)\\) a vague gamma distribution with a shape parameter close to zero (must be greater than \\(0\\)).\n\r\rThe JAGS language very closely matches the above model and prior definitions - hence the importance on understanding the model you wish to fit. The JAGS language resembles R in many respects. It basically consists of:\n\rstochastic nodes - those that appear on the left hand side of \\(\\sim\\)\n\rdeterministic nodes - those that appear on the left hand side of \u0026lt;-\n\r\\(R\\)-like for loops and functions to transform and summarise the data\n\r\rThat said, JAGS is based on a declarative language, which means: the order with which statements appear in the model definition are not important; nodes should not be defined more than once (you cannot change a value).We are now in a good position to define the model (Likelihood function and prior distributions).\nEffects Parameterisation\n\u0026gt; modelString = \u0026quot; + model {\r+ #Likelihood\r+ for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- beta0+beta[x[i]]\r+ }\r+ + #Priors\r+ beta0 ~ dnorm(0,1.0E-06)\r+ beta[1] \u0026lt;- 0\r+ beta[2] ~ dnorm(0,1.0E-06)\r+ tau ~ dgamma(0.1,0.1)\r+ sigma\u0026lt;-1/sqrt(tau)\r+ + #Other Derived parameters + # Group means (note, beta is a vector)\r+ Group.means \u0026lt;-beta0+beta + }\r+ \u0026quot;\r\u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString, con = \u0026quot;ttestModel.txt\u0026quot;)\rMeans Parameterisation\n\u0026gt; modelString.means = \u0026quot; + model {\r+ #Likelihood + for (i in 1:n) {\r+ y[i]~dnorm(mu[i],tau)\r+ mu[i] \u0026lt;- beta[x[i]]\r+ }\r+ + #Priors\r+ for (j in min(x):max(x)) {\r+ beta[j] ~ dnorm(0,0.001)\r+ }\r+ + tau~dgamma(0.1,0.1)\r+ sigma\u0026lt;-1/sqrt(tau)\r+ + #Other Derived parameters + effect \u0026lt;-beta[2]-beta[1]\r+ }\r+ \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file\r\u0026gt; writeLines(modelString.means, con = \u0026quot;ttestModelMeans.txt\u0026quot;)\rArrange the data as a list (as required by JAGS). Note, all variables must be numeric, therefore we use the numeric version of \\(x\\). Furthermore, the first level must be \\(1\\).\n\u0026gt; data.list \u0026lt;- with(data, list(y = y, x = xn, n = nrow(data)))\r\u0026gt; data.list.means \u0026lt;- with(data, list(y = y, x = xn, n = nrow(data)))\rDefine the initial values for the chain. Reasonable starting points can be gleaned from the data themselves.\n\u0026gt; inits \u0026lt;- list(beta0 = mean(data$y), beta = c(NA, diff(tapply(data$y,\r+ data$x, mean))), sigma = sd(data$y/2))\r\u0026gt; inits.means \u0026lt;- list(beta = tapply(data$y, data$x, mean), sigma = sd(data$y/2))\rDefine the nodes (parameters and derivatives) to monitor.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta0\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;Group.means\u0026quot;)\r\u0026gt; params.means \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;effect\u0026quot;, \u0026quot;sigma\u0026quot;)\rDefine the chain parameters.\n\u0026gt; adaptSteps = 1000 # the number of steps over which to establish a good stepping distance\r\u0026gt; burnInSteps = 2000 # the number of initial samples to discard\r\u0026gt; nChains = 2 # the number of independed sampling chains to perform \u0026gt; numSavedSteps = 50000 # the total number of samples to store\r\u0026gt; thinSteps = 1 # the thinning rate\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\rStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\u0026gt; library(R2jags)\rWhen using the jags function (R2jags package), it is not necessary to provide initial values. However, if they are to be supplied, the inital values must be provided as a list of the same length as the number of chains.\nEffects Parameterisation\n\u0026gt; data.r2jags \u0026lt;- jags(data=data.list,\r+ inits=NULL, #or inits=list(inits,inits) # since there are two chains\r+ parameters.to.save=params,\r+ model.file=\u0026quot;ttestModel.txt\u0026quot;,\r+ n.chains=nChains,\r+ n.iter=nIter,\r+ n.burnin=burnInSteps,\r+ n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 100\rUnobserved stochastic nodes: 3\rTotal graph size: 214\rInitializing model\r\u0026gt; \u0026gt; #print results\r\u0026gt; print(data.r2jags)\rInference for Bugs model at \u0026quot;ttestModel.txt\u0026quot;, fit using jags,\r2 chains, each with 25000 iterations (first 2000 discarded)\rn.sims = 46000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat\rGroup.means[1] 105.200 0.357 104.497 104.959 105.201 105.441 105.900 1.001\rGroup.means[2] 77.882 0.438 77.018 77.589 77.882 78.174 78.746 1.001\rbeta[1] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000\rbeta[2] -27.318 0.563 -28.426 -27.696 -27.315 -26.943 -26.212 1.001\rbeta0 105.200 0.357 104.497 104.959 105.201 105.441 105.900 1.001\rsigma 2.771 0.202 2.408 2.630 2.759 2.900 3.198 1.001\rdeviance 487.192 2.485 484.376 485.370 486.547 488.331 493.506 1.001\rn.eff\rGroup.means[1] 46000\rGroup.means[2] 15000\rbeta[1] 1\rbeta[2] 35000\rbeta0 46000\rsigma 46000\rdeviance 46000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.1 and DIC = 490.3\rDIC is an estimate of expected predictive error (lower deviance is better).\rMeans Parameterisation\n\u0026gt; data.r2jags.means \u0026lt;- jags(data=data.list.means,\r+ inits=NULL, #or inits=list(inits.means,inits.means) # since there are two chains\r+ parameters.to.save=params.means,\r+ model.file=\u0026quot;ttestModelMeans.txt\u0026quot;,\r+ n.chains=nChains,\r+ n.iter=nIter,\r+ n.burnin=burnInSteps,\r+ n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 100\rUnobserved stochastic nodes: 3\rTotal graph size: 211\rInitializing model\r\u0026gt; \u0026gt; #print results\r\u0026gt; print(data.r2jags.means)\rInference for Bugs model at \u0026quot;ttestModelMeans.txt\u0026quot;, fit using jags,\r2 chains, each with 25000 iterations (first 2000 discarded)\rn.sims = 46000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta[1] 105.184 0.357 104.481 104.947 105.184 105.423 105.884 1.001 46000\rbeta[2] 77.867 0.439 77.001 77.575 77.866 78.160 78.736 1.001 39000\reffect -27.317 0.566 -28.433 -27.696 -27.317 -26.940 -26.197 1.001 46000\rsigma 2.768 0.201 2.408 2.626 2.755 2.897 3.192 1.001 34000\rdeviance 487.195 2.498 484.360 485.377 486.540 488.323 493.721 1.001 46000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.1 and DIC = 490.3\rDIC is an estimate of expected predictive error (lower deviance is better).\rNotes\n\rIf inits=NULL the jags function will generate vaguely sensible initial values for each chain based on the data.\n\rIn addition to the mean and quantiles of each of the sample nodes, the jags function will calculate.\nThe effective sample size for each sample - if n.eff for a node is substantially less than the number of iterations, then it suggests poor mixing.\n\rThe Potential scale reduction factor or Rhat values for each sample - these are a convergence diagnostic (values of \\(1\\) indicate full convergence, values greater than \\(1.01\\) are indicative of non-convergence.\n\rAn information criteria (DIC) for model selection.\n\r\r\rThe total number samples collected is \\(46000\\). That is, there are \\(46000\\) samples collected from the multidimensional posterior distribution and thus, \\(46000\\) samples collected from the posterior distributions of each parameter. The effective number of samples column indicates the number of independent samples represented in the total. It is clear that for all parameters the chains were well mixed.\n\r\rMCMC diagnostics\rIn addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.\n\rTraceplots for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.\n\rAutocorrelation plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of \\(0\\) represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of \\(1\\)). A lag of \\(1\\) represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).\n\rPotential scale reduction factor (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of \\(1.05\\) or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\r\rPrior to examining the summaries, we should have explored the convergence diagnostics. We use the package mcmcplots to obtain density and trace plots for the effects model as an example.\n\u0026gt; library(mcmcplots)\r\u0026gt; denplot(data.r2jags, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta[2]\u0026quot;,\u0026quot;sigma\u0026quot;))\r\u0026gt; traplot(data.r2jags, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta[2]\u0026quot;,\u0026quot;sigma\u0026quot;))\rThese plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.\n\rModel validation\rModel validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. Ideally, a good model should also be able to predict the data used to fit the model.\nResiduals are not computed directly within R2jags. However, we can calculate them manually form the posteriors and plot them using the package ggplot2.\n\u0026gt; library(ggplot2)\r\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc, 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit))\rThere is no evidence that the mcmc chain did not converge on a stable posterior distribution. We are now in a position to examine the summaries of the parameters.\n\rParameter estimates\rAlthough all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and \\(95\\)% credibility intervals.\n\u0026gt; library(broom)\r\u0026gt; tidyMCMC(as.mcmc(data.r2jags), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 7 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 Group.means[1] 105. 0.357 105. 106. 2 Group.means[2] 77.9 0.438 77.0 78.7 3 beta[1] 0 0 0 0 4 beta[2] -27.3 0.563 -28.4 -26.2 5 beta0 105. 0.357 105. 106. 6 deviance 487. 2.49 484. 492. 7 sigma 2.77 0.202 2.39 3.17\rThe Group A is typically \\(27.3\\) units greater than Group B. The \\(95\\)% confidence interval for the difference between Group A and B does not overlap with \\(0\\) implying a significant difference between the two groups.\n\rGraphical summaries\rA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group. We do this by loading functions in the package dplyr.\n\u0026gt; library(dplyr)\r\u0026gt; mcmc = data.r2jags$BUGSoutput$sims.matrix\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = data.frame(x = levels(data$x))\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta0\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) +\r+ theme_classic()\rIf you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.\n\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data, aes(y = y,\r+ x = x), color = \u0026quot;gray\u0026quot;) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\r+ scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) + theme_classic()\rA more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since \\(\\text{resid}=\\text{obs}−\\text{fitted}\\) and the fitted values depend only on the single predictor we are interested in.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = data\r\u0026gt; fMat = rMat = model.matrix(~x, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\r+ color = \u0026quot;gray\u0026quot;) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\r+ scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) + theme_classic()\r\rEffect sizes\rIn addition to deriving the distribution means for the second group, we could make use of the Bayesian framework to derive the distribution of the effect size. There are multiple ways of calculating an effect size, but the most common are:\n\rRaw effect size - the difference between two groups (as already calculated)\n\rCohen’s D - the effect size standardised by division with the pooled standard deviation\n\rPercent - the effect size expressed as a percent of the reference group mean\n\r\rCalculating the percent effect size involves division by an estimate of \\(\\beta_0\\). The very first sample collected of each parameter (including \\(\\beta_0\\)) is based on the initial values supplied. If inits=NULL the jags function appears to generate initial values from the priors. Recall that in the previous model definition, \\(\\beta_0\\) was deemed to be distributed as a normal distribution with a mean of \\(0\\). Hence, \\(\\beta_0\\) would initially be assigned a value of \\(0\\). Division by zero is of course illegal and thus an error would be thrown. There are two ways to overcome this:\n\rModify the prior such that it has a mean close to zero (and thus the first \\(\\beta_0\\) sample is not zero), yet not actually zero (such as \\(0.0001\\)). This is the method used here.\n\rDefine initial values that are based on the observed data (and not zero).\n\r\r\u0026gt; paramsv2 \u0026lt;- c(\u0026quot;beta0\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;Group.means\u0026quot;, \u0026quot;cohenD\u0026quot;, \u0026quot;ES\u0026quot;, \u0026quot;p10\u0026quot;)\r\u0026gt; data.r2jagsv2 \u0026lt;- jags(data=data.list,\r+ inits=NULL, #or inits=list(inits,inits) # since there are two chains\r+ parameters.to.save=paramsv2,\r+ model.file=\u0026quot;ttestModelv2.txt\u0026quot;,\r+ n.chains=nChains,\r+ n.iter=nIter,\r+ n.burnin=burnInSteps,\r+ n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 100\rUnobserved stochastic nodes: 3\rTotal graph size: 224\rInitializing model\r\u0026gt; \u0026gt; #print results\r\u0026gt; print(data.r2jagsv2)\rInference for Bugs model at \u0026quot;ttestModelv2.txt\u0026quot;, fit using jags,\r2 chains, each with 25000 iterations (first 2000 discarded)\rn.sims = 46000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat\rES -25.965 0.488 -26.918 -26.294 -25.967 -25.637 -24.992 1.001\rGroup.means[1] 105.197 0.358 104.495 104.957 105.199 105.437 105.900 1.001\rGroup.means[2] 77.881 0.439 77.020 77.586 77.882 78.174 78.748 1.001\rbeta[1] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000\rbeta[2] -27.316 0.567 -28.428 -27.696 -27.317 -26.934 -26.191 1.001\rbeta0 105.197 0.358 104.495 104.957 105.199 105.437 105.900 1.001\rcohenD -9.914 0.736 -11.390 -10.402 -9.905 -9.413 -8.503 1.001\rp10 1.000 0.000 1.000 1.000 1.000 1.000 1.000 1.000\rsigma 2.770 0.199 2.413 2.631 2.758 2.897 3.190 1.001\rdeviance 487.184 2.473 484.372 485.370 486.546 488.317 493.572 1.001\rn.eff\rES 46000\rGroup.means[1] 46000\rGroup.means[2] 46000\rbeta[1] 1\rbeta[2] 46000\rbeta0 46000\rcohenD 46000\rp10 1\rsigma 46000\rdeviance 46000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.1 and DIC = 490.2\rDIC is an estimate of expected predictive error (lower deviance is better).\rThe Cohen’s D value is \\(-9.91\\). This value is far greater than the nominal “large effect” guidelines outlined by Cohen and thus we might proclaim the treatment as having a large negative effect. The effect size expressed as a percentage of the Group A mean is \\(-27.3\\). Hence the treatment was associated with a \\(27.3\\)% reduction.\n\rProbability statements\rBayesian statistics provide a natural means to generate probability statements. For example, we could calculate the probability that there is an effect of the treatment. Moreover, we could calculate the probability that the treatment effect exceeds some threshold (which might be based on a measure of clinically important difference or other compliance guidelines for example).\n\u0026gt; mcmc = data.r2jagsv2$BUGSoutput$sims.matrix\r\u0026gt; # Percentage change (relative to Group A)\r\u0026gt; ES = 100 * mcmc[, \u0026quot;beta[2]\u0026quot;]/mcmc[, \u0026quot;beta0\u0026quot;]\r\u0026gt; hist(ES)\r\u0026gt; \u0026gt; # Probability that the effect is greater than 10% (a decline of \u0026gt;10%)\r\u0026gt; sum(-1 * ES \u0026gt; 10)/length(ES)\r[1] 1\r\u0026gt; # Probability that the effect is greater than 25% (a decline of \u0026gt;25%)\r\u0026gt; sum(-1 * ES \u0026gt; 25)/length(ES)\r[1] 0.9741304\rWe have defined two additional probability derivatives, both of which utilize the step function (which generates a binary vector based on whether values evaluate less than zero or not).\n\rP0 - the probability (mean of 1-step()) that the raw effect is greater than zero.\rP25 - the probability (mean of 1-step()) that the percent effect size is greater than \\(25\\)%.\r\r\u0026gt; paramsv3 \u0026lt;- c(\u0026quot;beta0\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;Group.means\u0026quot;, \u0026quot;cohenD\u0026quot;, \u0026quot;ES\u0026quot;, \u0026quot;P0\u0026quot;, \u0026quot;P25\u0026quot;)\r\u0026gt; data.r2jagsv3 \u0026lt;- jags(data=data.list,\r+ inits=NULL, #or inits=list(inits,inits) # since there are two chains\r+ parameters.to.save=paramsv3,\r+ model.file=\u0026quot;ttestModelv3.txt\u0026quot;,\r+ n.chains=nChains,\r+ n.iter=nIter,\r+ n.burnin=burnInSteps,\r+ n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 100\rUnobserved stochastic nodes: 3\rTotal graph size: 225\rInitializing model\r\u0026gt; \u0026gt; #print results\r\u0026gt; print(data.r2jagsv3)\rInference for Bugs model at \u0026quot;ttestModelv3.txt\u0026quot;, fit using jags,\r2 chains, each with 25000 iterations (first 2000 discarded)\rn.sims = 46000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat\rES -25.964 0.489 -26.920 -26.293 -25.965 -25.637 -24.999 1.001\rGroup.means[1] 105.197 0.359 104.485 104.959 105.196 105.435 105.897 1.001\rGroup.means[2] 77.882 0.441 77.022 77.585 77.881 78.178 78.748 1.001\rP0 1.000 0.000 1.000 1.000 1.000 1.000 1.000 1.000\rP25 0.975 0.156 0.000 1.000 1.000 1.000 1.000 1.001\rbeta[1] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000\rbeta[2] -27.315 0.568 -28.427 -27.696 -27.314 -26.935 -26.195 1.001\rbeta0 105.197 0.359 104.485 104.959 105.196 105.435 105.897 1.001\rcohenD -9.912 0.740 -11.385 -10.405 -9.903 -9.412 -8.477 1.001\rsigma 2.770 0.200 2.411 2.631 2.758 2.896 3.198 1.001\rdeviance 487.202 2.492 484.364 485.378 486.557 488.334 493.696 1.001\rn.eff\rES 46000\rGroup.means[1] 46000\rGroup.means[2] 46000\rP0 1\rP25 46000\rbeta[1] 1\rbeta[2] 46000\rbeta0 46000\rcohenD 37000\rsigma 27000\rdeviance 46000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.1 and DIC = 490.3\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rFinite population standard deviations\rIt is often useful to be able to estimate the relative amount of variability associated with each predictor (or term) in a model. This can provide a sort of relative importance measure for each predictor.\nIn frequentist statistics, such measures are only available for so called random factors (factors whose observational levels are randomly selected to represent all possible levels rather than to represent specific treatment levels). For such random factors, the collective variances (or standard deviation) of each factor are known as the variance components. Each component can also be expressed as a percentage of the total so as to provide a percentage breakdown of the relative contributions of each scale of sampling. Frequentist approaches model random factors according to the variance they add to the model, whereas fixed factors are modelled according to their effects (deviations from reference means). The model does not seek to generalise beyond the observed levels of a given fixed factor (such as control vs treatment) and thus it apparently does not make sense to estimate the population variability between levels (which is what variance components estimate).\nThe notion of “fixed” and “random” factors is somewhat arbitrary and does not really have any meaning within a Bayesian context (as all parameters and thus factors are considered random). Instead, the spirit of what many consider is that the difference between fixed and random factors can be captured by conceptualising whether the levels of a factor are drawn from a finite population (from which the observed factor levels are the only ones possible) or a superpopulation (from which the observed factor levels are just a random selection of the infinite possible levels possible). Hence, variance components could be defined in terms of either finite population or superpopulation standard deviations. Superpopulation standard deviations have traditionally been used to describe the relative scale of sampling variation (e.g. where is the greatest source of variability; plots, subplots within plots, individual quadrats within subplots, …. or years, months within years, weeks within months, days within weeks, …) and are most logically applicable to factors that have a relatively large number of levels (such as spatial or temporal sampling units). On the other hand, finite population standard deviations can be used to explore the relative impact or effect of a set of (fixed) treatments.\nCalculate the amount of unexplained (residual) variance absorbed by the factor. This is generated by fitting a model with (full model) and without (reduced model) the term and subtracting the standard deviations of the residuals one another.\n\\[ \\sigma_A = \\sigma_{reduced} - \\sigma_{full} \\]\nThis approach works fine for models that only include fixed factors (indeed it is somewhat analogous to the partitioning of variance employed by an ANOVA table), but cannot be used when the model includes random factors.\n\u0026gt; data.lmFull \u0026lt;- lm(y ~ x, data)\r\u0026gt; data.lmRed \u0026lt;- lm(y ~ 1, data)\r\u0026gt; sd.a \u0026lt;- sd(data.lmRed$resid) - sd(data.lmFull$resid)\r\u0026gt; sd.resid \u0026lt;- sd(data.lmFull$resid)\r\u0026gt; sds \u0026lt;- c(sd.a, sd.resid)\r\u0026gt; 100 * sds/sum(sds)\r[1] 80.05772 19.94228\rHowever, options are somewhat limiting if we want to estimate the relative impacts of a mixture of “fixed” and “random” terms. For example, we may wish to explore the relative importance of a treatment compared to the spatial and/or temporal sampling heterogeneity. The Bayesian framework provides a relatively simple way to generate both finite population and superpopulation standard deviation estimates for all factors.\n\rFinite populations. The standard deviations of the MCMC samples across each of the parameters associated with a factor (eg, \\(\\beta_1\\) and \\(\\beta_2\\) in the effects parameterisation model) provide natural estimates of the variability between group levels (and thus the finite population standard deviation).\n\rSuperpopulation. The mechanism of defining priors also provides a mechanism for calculating infinite population standard deviations. Recall that in the means model, the prior for \\(\\beta_0\\) specifies that each of the \\(\\beta_0\\) values are drawn from a normal distribution with a particular mean and a certain level of precision (reciprocal of variability). We could further parameterise this prior into an estimatable mean and precision via hyperpriors \\(\\beta_0 \\sim \\text{Normal}(\\mu,\\tau)\\), with \\(\\mu \\sim \\text{Normal}(0,1.0\\text{E}-6)\\) and \\(\\tau \\sim \\text{Gamma}(0.1,0.1)\\). Since the normal distribution in line one above represents the distribution from which the (infinite) population means are drawn, \\(\\tau\\) provides a direct measure of the variability of the population from which the means are drawn.\n\r\rWhen the number of levels of a factor are large, the finite population and superpopulation standard deviation point estimates will be very similar. However, when the number of factor levels is small (such as two levels), the finite population estimate will be very precise whereas the superpopulation standard deviation estimate will be very imprecise (highly varied). For this reason, if the purpose of estimating standard deviations is to compare relative contributions of various predictors (some of which have small numbers of levels and others large), then it is best to use finite population standard deviation estimates.\n\u0026gt; paramsv4 \u0026lt;- c(\u0026quot;beta0\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;sd.a\u0026quot;, \u0026quot;sd.resid\u0026quot;, \u0026quot;sigma.a\u0026quot;)\r\u0026gt; data.r2jagsv4 \u0026lt;- jags(data=data.list,\r+ inits=NULL, #or inits=list(inits,inits) # since there are two chains\r+ parameters.to.save=paramsv4,\r+ model.file=\u0026quot;ttestModelv4.txt\u0026quot;,\r+ n.chains=nChains,\r+ n.iter=nIter,\r+ n.burnin=burnInSteps,\r+ n.thin=thinSteps)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 100\rUnobserved stochastic nodes: 4\rTotal graph size: 319\rInitializing model\r\u0026gt; \u0026gt; #print results\r\u0026gt; print(data.r2jagsv4)\rInference for Bugs model at \u0026quot;ttestModelv4.txt\u0026quot;, fit using jags,\r2 chains, each with 25000 iterations (first 2000 discarded)\rn.sims = 46000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75%\rbeta[1] 0.000000e+00 0.000000e+00 0.000 0.000 0.000 0.000\rbeta[2] -2.731400e+01 5.670000e-01 -28.417 -27.694 -27.314 -26.937\rbeta0 1.051970e+02 3.590000e-01 104.490 104.955 105.198 105.440\rsd.a 1.931400e+01 4.010000e-01 18.521 19.047 19.314 19.583\rsd.resid 2.751000e+00 2.000000e-02 2.737 2.738 2.743 2.755\rsigma 2.769000e+00 1.990000e-01 2.411 2.629 2.757 2.895\rsigma.a 1.095446e+22 1.956638e+24 0.323 1.712 13.394 440.403\rdeviance 4.871890e+02 2.480000e+00 484.365 485.386 486.550 488.303\r97.5% Rhat n.eff\rbeta[1] 0.000 1.000 1\rbeta[2] -26.193 1.001 46000\rbeta0 105.899 1.001 46000\rsd.a 20.094 1.001 46000\rsd.resid 2.808 1.001 46000\rsigma 3.187 1.001 46000\rsigma.a 43469187.743 1.001 46000\rdeviance 493.637 1.001 46000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 3.1 and DIC = 490.3\rDIC is an estimate of expected predictive error (lower deviance is better).\rThe between group (finite population) standard deviation is \\(20.1\\) whereas the within group standard deviation is \\(2.81\\). These equate to respectively. Compared to the finite population standard deviation, the superpopulation between group standard deviation estimate (\\(\\sigma_a\\)) is both very large and highly variable. This is to be expected, whilst the finite population standard deviation represents the degree of variation between the observed levels, the superpopulation standard deviation seeks to estimate the variability of the population from which the group means of the observed levels AND all other possible levels are drawn. There are only two levels from which to estimate this standard deviation and therefore, its value and variability are going to be higher than those pertaining only to the scope of the current data.\nExamination of the quantiles for \\(\\sigma_a\\) suggest that its samples are not distributed normally. Consequently, the mean is not an appropriate measure of its location. We will instead characterise the superpopulation between group and within group standard deviations via their respective medians and as percent medians. The contrast between finite population and superpopulation standard deviations is also emphasised by the respective estimates for the residuals. The residuals are of course a “random” factor with a large number of observed levels. It is therefore not surprising that the point estimates for the residuals variance components are very similar. However, also notice that the precision of the finite population standard deviation estimate is substantially higher (lower standard deviation of the standard deviation estimate) than that of the superpopulation estimate.\n\rUnequally varied populations\rWe can also generate data assuming two populations with different variances, e.g. between male and female subgroups.\n\u0026gt; set.seed(123)\r\u0026gt; n1 \u0026lt;- 60 #sample size from population 1\r\u0026gt; n2 \u0026lt;- 40 #sample size from population 2\r\u0026gt; mu1 \u0026lt;- 105 #population mean of population 1\r\u0026gt; mu2 \u0026lt;- 77.5 #population mean of population 2\r\u0026gt; sigma1 \u0026lt;- 3 #standard deviation of population 1\r\u0026gt; sigma2 \u0026lt;- 2 #standard deviation of population 2\r\u0026gt; n \u0026lt;- n1 + n2 #total sample size\r\u0026gt; y1 \u0026lt;- rnorm(n1, mu1, sigma1) #population 1 sample\r\u0026gt; y2 \u0026lt;- rnorm(n2, mu2, sigma2) #population 2 sample\r\u0026gt; y \u0026lt;- c(y1, y2)\r\u0026gt; x \u0026lt;- factor(rep(c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;), c(n1, n2))) #categorical listing of the populations\r\u0026gt; xn \u0026lt;- rep(c(0, 1), c(n1, n2)) #numerical version of the population category\r\u0026gt; data2 \u0026lt;- data.frame(y, x, xn) # dataset\r\u0026gt; head(data2) #print out the first six rows of the data set\ry x xn\r1 103.3186 A 0\r2 104.3095 A 0\r3 109.6761 A 0\r4 105.2115 A 0\r5 105.3879 A 0\r6 110.1452 A 0\rStart by defining the model\n\\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon, \\]\nwhere \\(\\epsilon_1 \\sim \\text{Normal}(0,\\sigma_1)\\) for \\(x_1=0\\) (females), and \\(\\epsilon_2 \\sim \\text{Normal}(0,\\sigma_2)\\) for \\(x_2=1\\) (males). In JAGS code, the model becomes:\n\u0026gt; modelStringv5=\u0026quot;\r+ model {\r+ #Likelihood\r+ for (i in 1:n1) {\r+ y1[i]~dnorm(mu1,tau1)\r+ }\r+ for (i in 1:n2) {\r+ y2[i]~dnorm(mu2,tau2)\r+ }\r+ + #Priors\r+ mu1 ~ dnorm (0,0.001)\r+ mu2 ~ dnorm(0,0.001)\r+ tau1 \u0026lt;- 1 / (sigma1 * sigma1)\r+ sigma1~dunif(0,100)\r+ tau2 \u0026lt;- 1 / (sigma2 * sigma2)\r+ sigma2~dunif(0,100)\r+ + #Other Derived parameters + delta \u0026lt;- mu2 - mu1\r+ }\r+ \u0026quot;\r\u0026gt; ## write the model to a text file \u0026gt; writeLines(modelStringv5,con=\u0026quot;ttestModelv5.txt\u0026quot;)\rWe specify priors directly on \\(\\sigma_1\\) and \\(\\sigma_2\\) using Uniform distributions between \\(0\\) and \\(100\\), and then express \\(\\tau\\) as a deterministic function of \\(\\sigma\\). Next, arrange the data as a list (as required by JAGS) and define the MCMC parameters. Note, all variables must be numeric, therefore we use the numeric version of \\(x\\). Define the initial values for two chains so that the initial values list must include two elements (if provided).\n\u0026gt; data2.list \u0026lt;- with(data2,list(y1=y[xn==0], y2=y[xn==1], + n1=length(y[xn==0]), n2=length(y[xn==1])))\r\u0026gt; inits \u0026lt;- list(list(mu1=rnorm(1), mu2=rnorm(1), sigma1=rlnorm(1), sigma2=rlnorm(1)),\r+ list(mu1=rnorm(1), mu2=rnorm(1), sigma1=rlnorm(1), sigma2=rlnorm(1)))\r\u0026gt; paramsv5 \u0026lt;- c(\u0026quot;mu1\u0026quot;,\u0026quot;mu2\u0026quot;,\u0026quot;delta\u0026quot;,\u0026quot;sigma1\u0026quot;,\u0026quot;sigma2\u0026quot;)\r\u0026gt; adaptSteps = 1000\r\u0026gt; burnInSteps = 2000\r\u0026gt; nChains = 2\r\u0026gt; numSavedSteps = 50000\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = ceiling((numSavedSteps * thinSteps)/nChains)\rFinally, fit the model in JAGS and print the results.\n\u0026gt; data2.r2jagsv5 \u0026lt;- jags(data=data2.list,\r+ inits=NULL, #or inits=list(inits,inits) # since there are two chains\r+ parameters.to.save=paramsv5,\r+ model.file=\u0026quot;ttestModelv5.txt\u0026quot;,\r+ n.chains=nChains,\r+ n.iter=nIter,\r+ n.burnin=burnInSteps,\r+ n.thin=1)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 100\rUnobserved stochastic nodes: 4\rTotal graph size: 115\rInitializing model\r\u0026gt; \u0026gt; print(data2.r2jagsv5)\rInference for Bugs model at \u0026quot;ttestModelv5.txt\u0026quot;, fit using jags,\r2 chains, each with 25000 iterations (first 2000 discarded)\rn.sims = 46000 iterations saved\rmu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff\rdelta -27.435 0.473 -28.367 -27.755 -27.433 -27.116 -26.508 1.001 27000\rmu1 105.181 0.360 104.478 104.937 105.181 105.422 105.891 1.001 44000\rmu2 77.746 0.306 77.142 77.543 77.748 77.948 78.347 1.001 46000\rsigma1 2.787 0.265 2.328 2.602 2.767 2.951 3.361 1.001 16000\rsigma2 1.913 0.225 1.534 1.753 1.893 2.049 2.414 1.001 21000\rdeviance 455.879 2.945 452.217 453.714 455.215 457.354 463.257 1.001 46000\rFor each parameter, n.eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\rDIC info (using the rule, pD = var(deviance)/2)\rpD = 4.3 and DIC = 460.2\rDIC is an estimate of expected predictive error (lower deviance is better).\r\rReferences\rGelman, Andrew, and others. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).” Bayesian Analysis 1 (3): 515–34.\n\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1580609594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580609594,"objectID":"00731120d746a637f0fe73b289c5a893","permalink":"/jags/comparing-two-populations-jags/comparing-two-populations-jags/","publishdate":"2020-02-01T21:13:14-05:00","relpermalink":"/jags/comparing-two-populations-jags/comparing-two-populations-jags/","section":"JAGS","summary":"This tutorial will focus on the use of Bayesian estimation to explore differences between two populations. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","JAGS","population differences"],"title":"Comparing Two Populations - JAGS","type":"JAGS"},{"authors":["Andrea Gabrio"],"categories":["R","one sample t-test","STAN"],"content":"\r\rThis tutorial will focus on the use of Bayesian estimation to explore differences between two populations. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.\n\rSTAN - a dedicated Bayesian modelling framework written in C++ and implementing Hamiltonian MCMC samplers.\n\r\rWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\rR2OpenBUGS - interfaces with OpenBUGS\n\rR2jags - interfaces with JAGS\n\rrstan - interfaces with STAN\n\r\rThe BUGS/JAGS/STAN languages and algorithms are very powerful and flexible. However, the cost of this power and flexibility is complexity and the need for a firm understanding of the model you wish to fit as well as the priors to be used. The algorithms requires the following inputs.\n\rWithin the model:\nThe likelihood function relating the response to the predictors.\n\rThe definition of the priors.\n\r\rChain properties:\nThe number of chains.\n\rThe length of chains (number of iterations).\n\rThe burn-in length (number of initial iterations to ignore).\n\rThe thinning rate (number of iterations to count on before storing a sample).\n\r\rThe initial estimates to start an MCMC chain. If there are multiple chains, these starting values can differ between chains.\n\rThe list of model parameters and derivatives to monitor (and return the posterior distributions of)\n\r\rThis tutorial will demonstrate how to fit models in STAN (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages.\nData generation\rWe will start by generating a random data set. Note, I am creating two versions of the predictor variable (a numeric version and a factorial version).\n\u0026gt; set.seed(123)\r\u0026gt; nA \u0026lt;- 60 #sample size from Population A\r\u0026gt; nB \u0026lt;- 40 #sample size from Population B\r\u0026gt; muA \u0026lt;- 105 #population mean of Population A\r\u0026gt; muB \u0026lt;- 77.5 #population mean of Population B\r\u0026gt; sigma \u0026lt;- 3 #standard deviation of both populations (equally varied)\r\u0026gt; yA \u0026lt;- rnorm(nA, muA, sigma) #Population A sample\r\u0026gt; yB \u0026lt;- rnorm(nB, muB, sigma) #Population B sample\r\u0026gt; y \u0026lt;- c(yA, yB)\r\u0026gt; x \u0026lt;- factor(rep(c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;), c(nA, nB))) #categorical listing of the populations\r\u0026gt; xn \u0026lt;- as.numeric(x) #numerical version of the population category for means parameterization. # Should not start at 0.\r\u0026gt; data \u0026lt;- data.frame(y, x, xn) # dataset\rLet inspect the first few rows of the dataset using the command head\n\u0026gt; head(data)\ry x xn\r1 103.3186 A 1\r2 104.3095 A 1\r3 109.6761 A 1\r4 105.2115 A 1\r5 105.3879 A 1\r6 110.1452 A 1\rWe can also perform some exploratory data analysis - in this case, a boxplot of the response for each level of the predictor.\n\u0026gt; boxplot(y ~ x, data)\r\rThe One Sample t-test\rA t-test is essentially just a simple regression model in which the categorical predictor is represented by a binary variable in which one level is coded as \\(0\\) and the other \\(1\\). For the model itself, the observed response \\(y_i\\) are assumed to be drawn from a normal distribution with a given mean \\(\\mu\\) and standard deviation \\(\\sigma\\). The expected values are themselves determined by the linear predictor \\(\\mu_i=\\beta_0+\\beta_1x_i\\), where \\(\\beta_0\\) represents the mean of the first treatment group and \\(\\beta_1\\) represents the difference between the mean of the first group and the mean of the second group (the effect of interest).\nMCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying “uninformative” priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (\\(1000\\)) for both the intercept and the treatment effect and a wide half-cauchy (scale=\\(25\\)) for the standard deviation (Gelman and others (2006)).\n\\[y_i \\sim \\text{Normal}(\\mu_i, \\sigma), \\]\nwhere \\(\\mu_i=\\beta_0+\\beta_1x_i\\).\nPriors are defined as:\n\\[ \\beta_j \\sim \\text{Normal}(0,1000), \\;\\;\\; \\text{and} \\;\\;\\; \\sigma \\sim \\text{Cauchy}(0,25), \\]\nfor \\(j=0,1\\).\nFitting the model in STAN\rBroadly, there are two ways of parameterising (expressing the unknown (to be estimated) components of a model) a model. Either we can estimate the means of each group (Means parameterisation) or we can estimate the mean of one group and the difference between this group and the other group(s) (Effects parameterisation). The latter is commonly used for frequentist null hypothesis testing as its parameters are more consistent with the null hypothesis of interest (that the difference between the two groups equals zero).\nEffects parameterisation\r\r\\[ y_i = \\beta_0 + \\beta_{j}x_i + \\epsilon_i, \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma). \\]\nEach \\(y_i\\) is modelled by an intercept \\(\\beta_0\\) (mean of group A) plus a difference parameter \\(\\beta_j\\) (difference between mean of group A and group B) multiplied by an indicator of which group the observation came from (\\(x_i\\)), plus a residual drawn from a normal distribution with mean \\(0\\) and standard deviation \\(\\sigma\\). Actually, there are as many \\(\\beta_j\\) parameters as there are groups but one of them (typically the first) is set to be equal to zero (to avoid over-parameterization). Expected values of \\(y\\) are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of effect parameters and whose variance is defined by the degree of variability in this mean. The parameters are: \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\).\nMeans parameterisation\r\r\\[ y_i = \\beta_{j} + \\epsilon_i, \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma). \\]\nEach \\(y_i\\) is modelled as the mean \\(\\beta_j\\) of each group (\\(j=1,2\\)) plus a residual drawn from a normal distribution with a mean of zero and a standard deviation of \\(\\sigma\\). Actually, \\(\\boldsymbol \\beta\\) is a set of \\(j\\) coefficients corresponding to the \\(j\\) dummy coded factor levels. Expected values of \\(y\\) are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of means parameters and whose variance is defined by the degree of variability in this mean. The parameters are: \\(\\beta_1\\), \\(\\beta_2\\) and \\(\\sigma\\).\nWhilst the STAN language broadly resembles BUGS/JAGS, there are numerous important differences. Some of these differences are to support translation to c++ for compilation (such as declaring variables). Others reflect leveraging of vectorization to speed up run time. Here are some important notes about STAN:\n\rAll variables must be declared\n\rVariables declared in the parameters block will be collected\n\rAnything in the transformed block will be collected as samples. Also, checks will be made every loop\n\r\rNow I will demonstrate fitting the models with STAN. Note, I am using the refresh=0 option so as to suppress the larger regular output in the interest of keeping output to what is necessary for this tutorial. When running outside of a tutorial context, the regular verbose output is useful as it provides a way to gauge progress.\nEffects Parameterisation\n\u0026gt; stanString = \u0026quot; + data {\r+ int n;\r+ vector [n] y;\r+ vector [n] x;\r+ }\r+ parameters {\r+ real \u0026lt;lower=0, upper=100\u0026gt; sigma;\r+ real beta0;\r+ real beta;\r+ }\r+ transformed parameters {\r+ }\r+ model {\r+ vector [n] mu;\r+ + //Priors\r+ beta0 ~ normal(0,1000);\r+ beta ~ normal(0,1000);\r+ sigma ~ cauchy(0,25);\r+ + mu = beta0 + beta*x;\r+ //Likelihood\r+ y ~ normal(mu, sigma);\r+ }\r+ generated quantities {\r+ vector [2] Group_means;\r+ real CohensD;\r+ //Other Derived parameters + //# Group means (note, beta is a vector)\r+ Group_means[1] = beta0;\r+ Group_means[2] = beta0+beta;\r+ + CohensD = beta /sigma; + }\r+ + \u0026quot;\r\u0026gt; ## write the model to a text file\r\u0026gt; writeLines(stanString, con = \u0026quot;ttestModel.stan\u0026quot;)\rMeans Parameterisation\n\u0026gt; stanString.means = \u0026quot; + data {\r+ int n;\r+ int nX;\r+ vector [n] y;\r+ matrix [n,nX] x;\r+ }\r+ parameters {\r+ real \u0026lt;lower=0, upper=100\u0026gt; sigma;\r+ vector [nX] beta;\r+ }\r+ transformed parameters {\r+ }\r+ model {\r+ vector [n] mu;\r+ + //Priors\r+ beta ~ normal(0,1000);\r+ sigma ~ cauchy(0,25);\r+ + mu = x*beta;\r+ //Likelihood\r+ y ~ normal(mu, sigma);\r+ }\r+ generated quantities {\r+ vector [2] Group_means;\r+ real CohensD;\r+ + //Other Derived parameters + Group_means[1] = beta[1];\r+ Group_means[2] = beta[1]+beta[2];\r+ + CohensD = beta[2] /sigma; + }\r+ + \u0026quot;\r\u0026gt; ## write the model to a text file\r\u0026gt; writeLines(stanString.means, con = \u0026quot;ttestModelMeans.stan\u0026quot;)\rArrange the data as a list (as required by STAN).\n\u0026gt; data.list \u0026lt;- with(data, list(y = y, x = (xn - 1), n = nrow(data)))\r\u0026gt; X \u0026lt;- model.matrix(~x, data)\r\u0026gt; data.list.means = with(data, list(y = y, x = X, n = nrow(data), nX = ncol(X)))\rDefine the initial values for the chain. Reasonable starting points can be gleaned from the data themselves.\n\u0026gt; inits \u0026lt;- list(beta0 = mean(data$y), beta = c(NA, diff(tapply(data$y,\r+ data$x, mean))), sigma = sd(data$y/2))\r\u0026gt; inits.means \u0026lt;- list(beta = tapply(data$y, data$x, mean), sigma = sd(data$y/2))\rDefine the nodes (parameters and derivatives) to monitor.\n\u0026gt; params \u0026lt;- c(\u0026quot;beta0\u0026quot;, \u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;Group_means\u0026quot;, \u0026quot;CohensD\u0026quot;)\r\u0026gt; params.means \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;sigma\u0026quot;, \u0026quot;Group_means\u0026quot;,\u0026quot;CohensD\u0026quot;)\rDefine the chain parameters.\n\u0026gt; burnInSteps = 500 # the number of initial samples to discard\r\u0026gt; nChains = 2 # the number of independed sampling chains to perform \u0026gt; thinSteps = 1 # the thinning rate\r\u0026gt; nIter = 2000\rStart the STAN model (check the model, load data into the model, specify the number of chains and compile the model). Load the rstan package.\n\u0026gt; library(rstan)\rWhen using the stan function (rtsan package), it is not necessary to provide initial values. However, if they are to be supplied, the inital values must be provided as a list of the same length as the number of chains.\nEffects Parameterisation\n\u0026gt; data.stan = stan(file = \u0026quot;ttestModel.stan\u0026quot;, + data = data.list, + pars = params,\r+ iter = nIter,\r+ warmup = burnInSteps, + chains = nChains, + thin = thinSteps, + init = \u0026quot;random\u0026quot;, #or inits=list(inits,inits)\r+ refresh = 0)\r\u0026gt; \u0026gt; #print results\r\u0026gt; print(data.stan)\rInference for Stan model: ttestModel.\r2 chains, each with iter=2000; warmup=500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5%\rbeta0 105.20 0.01 0.36 104.47 104.95 105.20 105.44 105.91\rbeta -27.32 0.01 0.57 -28.43 -27.72 -27.33 -26.93 -26.22\rsigma 2.79 0.00 0.21 2.41 2.64 2.77 2.92 3.23\rGroup_means[1] 105.20 0.01 0.36 104.47 104.95 105.20 105.44 105.91\rGroup_means[2] 77.88 0.01 0.45 77.01 77.59 77.87 78.18 78.76\rCohensD -9.85 0.02 0.75 -11.36 -10.35 -9.86 -9.35 -8.36\rlp__ -150.78 0.04 1.25 -154.05 -151.31 -150.44 -149.88 -149.34\rn_eff Rhat\rbeta0 1802 1\rbeta 1731 1\rsigma 2187 1\rGroup_means[1] 1802 1\rGroup_means[2] 2826 1\rCohensD 2238 1\rlp__ 1272 1\rSamples were drawn using NUTS(diag_e) at Mon Feb 10 14:10:29 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\rMeans Parameterisation\n\u0026gt; data.stan.means = stan(file = \u0026quot;ttestModelMeans.stan\u0026quot;, + data = data.list.means, + pars = params.means,\r+ iter = nIter,\r+ warmup = burnInSteps, + chains = nChains, + thin = thinSteps, + init = \u0026quot;random\u0026quot;, #or inits=list(inits.means,inits.means)\r+ refresh = 0)\r\u0026gt; \u0026gt; #print results\r\u0026gt; print(data.stan.means)\rInference for Stan model: ttestModelMeans.\r2 chains, each with iter=2000; warmup=500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5%\rbeta[1] 105.21 0.01 0.37 104.51 104.96 105.20 105.44 105.92\rbeta[2] -27.33 0.01 0.58 -28.47 -27.71 -27.31 -26.93 -26.23\rsigma 2.78 0.00 0.20 2.43 2.64 2.76 2.90 3.22\rGroup_means[1] 105.21 0.01 0.37 104.51 104.96 105.20 105.44 105.92\rGroup_means[2] 77.88 0.01 0.44 77.02 77.59 77.88 78.17 78.77\rCohensD -9.88 0.02 0.74 -11.35 -10.40 -9.89 -9.40 -8.40\rlp__ -150.74 0.03 1.26 -153.85 -151.33 -150.42 -149.83 -149.33\rn_eff Rhat\rbeta[1] 1439 1\rbeta[2] 1654 1\rsigma 1955 1\rGroup_means[1] 1439 1\rGroup_means[2] 3595 1\rCohensD 2056 1\rlp__ 1397 1\rSamples were drawn using NUTS(diag_e) at Mon Feb 10 14:11:08 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\rNotes\n\rIf inits=\"random\" the stan function will randomly generate initial values between \\(-2\\) and \\(2\\) on the unconstrained support. The optional additional parameter init_r can be set to some value other than \\(2\\) to change the range of the randomly generated inits. Other available options include: set inits=\"0\" to initialize all parameters to zero on the unconstrained support; set inital values by providing a list equal in length to the number of chains; set initial values by providing a function that returns a list for specifying the initial values of parameters for a chain.\n\rIn addition to the mean and quantiles of each of the sample nodes, the stan function will calculate.\nThe effective sample size for each sample - if n.eff for a node is substantially less than the number of iterations, then it suggests poor mixing.\n\rThe Potential scale reduction factor or Rhat values for each sample - these are a convergence diagnostic (values of \\(1\\) indicate full convergence, values greater than \\(1.01\\) are indicative of non-convergence.\n\r\r\rThe total number samples collected is \\(3000\\). That is, there are \\(3000\\) samples collected from the multidimensional posterior distribution and thus, \\(3000\\) samples collected from the posterior distributions of each parameter. The effective number of samples column indicates the number of independent samples represented in the total. It is clear that for all parameters the chains were well mixed.\n\r\rMCMC diagnostics\rAgain, prior to examining the summaries, we should have explored the convergence diagnostics. There are numerous ways of working with STAN model fits (for exploring diagnostics and summarisation).\nextract the mcmc samples and convert them into a mcmc.list to leverage the various mcmcplots routines\n\ruse the numerous routines that come with the rstan package\n\ruse the routines that come with the bayesplot package\n\r\rWe will explore all of these.\n\rmcmcplots\r\rFirst, we need to convert the rtsan object into an mcmc.list object to apply the functions in the mcmcplots package.\n\u0026gt; library(mcmcplots)\r\u0026gt; s = as.array(data.stan.means)\r\u0026gt; mcmc \u0026lt;- do.call(mcmc.list, plyr:::alply(s[, , -(length(s[1, 1, ]))], 2, as.mcmc))\rNext we look at density and trace plots.\n\u0026gt; denplot(mcmc, parms = c(\u0026quot;Group_means\u0026quot;, \u0026quot;CohensD\u0026quot;))\r\u0026gt; traplot(mcmc, parms = c(\u0026quot;Group_means\u0026quot;, \u0026quot;CohensD\u0026quot;))\rThese plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.\n\rrstan\r\rMCMC diagnostic measures that can be directly applied to rstan objects via the rstan package include: traceplots, autocorrelation, effective sample size and Rhat diagnostics.\n\u0026gt; #traceplots\r\u0026gt; stan_trace(data.stan.means, pars = c(\u0026quot;Group_means\u0026quot;, \u0026quot;CohensD\u0026quot;))\r\u0026gt; \u0026gt; #autocorrelation\r\u0026gt; stan_ac(data.stan.means, pars = c(\u0026quot;Group_means\u0026quot;, \u0026quot;CohensD\u0026quot;))\r\u0026gt; \u0026gt; #rhat\r\u0026gt; stan_rhat(data.stan.means, pars = c(\u0026quot;Group_means\u0026quot;, \u0026quot;CohensD\u0026quot;))\r\u0026gt; \u0026gt; #ess\r\u0026gt; stan_ess(data.stan.means, pars = c(\u0026quot;Group_means\u0026quot;, \u0026quot;CohensD\u0026quot;))\rNote:\n\rRhat values are a measure of sampling efficiency/effectiveness. Ideally, all values should be less than \\(1.05\\). If there are values of 1.05 or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentiall slower than it could have been, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\rESS indicates the number samples (or proportion of samples that the sampling algorithm) deamed effective. The sampler rejects samples on the basis of certain criterion and when it does so, the previous sample value is used. Hence while the MCMC sampling chain may contain \\(1000\\) samples, if there are only \\(10\\) effective samples (\\(1\\)%), the estimated properties are not likely to be reliable.\n\rbayesplot\n\r\rAnother alternative is to use the package bayesplot, which provides a range of standardised diagnostic measures for assessing MCMC convergence and issues, which can be directly applied to the rstan object.\n\u0026gt; library(bayesplot)\r\u0026gt; \u0026gt; #density and trace plots\r\u0026gt; mcmc_combo(as.array(data.stan.means), regex_pars = \u0026quot;Group_means|CohensD\u0026quot;)\r\rModel validation\rResiduals are not computed directly within rstan. However, we can calculate them manually form the posteriors.\n\u0026gt; library(ggplot2)\r\u0026gt; mcmc = as.matrix(data.stan.means)[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; # generate a model matrix\r\u0026gt; newdata = data.frame(x = data$x)\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; ## get median parameter estimates\r\u0026gt; coefs = apply(mcmc, 2, median)\r\u0026gt; fit = as.vector(coefs %*% t(Xmat))\r\u0026gt; resid = data$y - fit\r\u0026gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit))\rThere is no evidence that the mcmc chain did not converge on a stable posterior distribution. We are now in a position to examine the summaries of the parameters.\n\rParameter estimates\rA quick look at posterior summaries can be obtained through the command summary which can be directly applied to our rstan object.\n\u0026gt; summary(data.stan.means)\r$summary\rmean se_mean sd 2.5% 25%\rbeta[1] 105.205981 0.009650332 0.3660680 104.512893 104.95847\rbeta[2] -27.327670 0.014175541 0.5765494 -28.471465 -27.70858\rsigma 2.779295 0.004564259 0.2017951 2.425126 2.63877\rGroup_means[1] 105.205981 0.009650332 0.3660680 104.512893 104.95847\rGroup_means[2] 77.878310 0.007293288 0.4372737 77.017179 77.58974\rCohensD -9.883908 0.016318680 0.7398548 -11.345145 -10.39596\rlp__ -150.744310 0.033765022 1.2622380 -153.847632 -151.32845\r50% 75% 97.5% n_eff Rhat\rbeta[1] 105.197887 105.442341 105.923970 1438.928 1.0006369\rbeta[2] -27.313058 -26.929462 -26.228003 1654.222 0.9996207\rsigma 2.761057 2.904130 3.220382 1954.702 1.0008448\rGroup_means[1] 105.197887 105.442341 105.923970 1438.928 1.0006369\rGroup_means[2] 77.881198 78.173471 78.765424 3594.677 0.9997923\rCohensD -9.893648 -9.396558 -8.403284 2055.526 1.0013095\rlp__ -150.420841 -149.826519 -149.327836 1397.489 1.0006469\r$c_summary\r, , chains = chain:1\rstats\rparameter mean sd 2.5% 25% 50%\rbeta[1] 105.194598 0.3722763 104.485138 104.943830 105.189222\rbeta[2] -27.316749 0.5909082 -28.503315 -27.700926 -27.303076\rsigma 2.787113 0.2017944 2.439039 2.649487 2.769964\rGroup_means[1] 105.194598 0.3722763 104.485138 104.943830 105.189222\rGroup_means[2] 77.877849 0.4452879 76.953676 77.589838 77.884335\rCohensD -9.851471 0.7306980 -11.291742 -10.351622 -9.856804\rlp__ -150.774304 1.3031195 -154.143552 -151.358639 -150.446011\rstats\rparameter 75% 97.5%\rbeta[1] 105.430335 105.928130\rbeta[2] -26.900706 -26.189346\rsigma 2.905763 3.220038\rGroup_means[1] 105.430335 105.928130\rGroup_means[2] 78.167639 78.777570\rCohensD -9.358039 -8.394201\rlp__ -149.844014 -149.328052\r, , chains = chain:2\rstats\rparameter mean sd 2.5% 25% 50%\rbeta[1] 105.217363 0.3595164 104.544008 104.970466 105.208509\rbeta[2] -27.338592 0.5618086 -28.444722 -27.716894 -27.323423\rsigma 2.771476 0.2015598 2.417028 2.631247 2.750654\rGroup_means[1] 105.217363 0.3595164 104.544008 104.970466 105.208509\rGroup_means[2] 77.878771 0.4292579 77.031912 77.589743 77.878030\rCohensD -9.916344 0.7477366 -11.431004 -10.435551 -9.924630\rlp__ -150.714316 1.2196850 -153.673281 -151.305580 -150.383196\rstats\rparameter 75% 97.5%\rbeta[1] 105.450568 105.916257\rbeta[2] -26.963106 -26.265061\rsigma 2.898644 3.219905\rGroup_means[1] 105.450568 105.916257\rGroup_means[2] 78.179664 78.753253\rCohensD -9.430001 -8.422613\rlp__ -149.795340 -149.327597\rThe Group A is typically \\(27.3\\) units greater than Group B. The \\(95\\)% confidence interval for the difference between Group A and B does not overlap with \\(0\\) implying a significant difference between the two groups.\n\rGraphical summaries\rA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group. We do this by loading functions in the package broom and dplyr.\n\u0026gt; library(broom)\r\u0026gt; library(dplyr)\r\u0026gt; mcmc = as.matrix(data.stan.means)\r\u0026gt; ## Calculate the fitted values\r\u0026gt; newdata = data.frame(x = levels(data$x))\r\u0026gt; Xmat = model.matrix(~x, newdata)\r\u0026gt; coefs = mcmc[, c(\u0026quot;beta[1]\u0026quot;, \u0026quot;beta[2]\u0026quot;)]\r\u0026gt; fit = coefs %*% t(Xmat)\r\u0026gt; newdata = newdata %\u0026gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;))\r\u0026gt; newdata\rx estimate std.error conf.low conf.high\r1 A 105.20598 0.3660680 104.52503 105.93588\r2 B 77.87831 0.4372737 76.99792 78.74455\r\u0026gt; \u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_pointrange(aes(ymin = conf.low,\r+ ymax = conf.high)) + scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) +\r+ theme_classic()\rIf you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.\n\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data, aes(y = y,\r+ x = x), color = \u0026quot;gray\u0026quot;) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\r+ scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) + theme_classic()\rA more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since \\(\\text{resid}=\\text{obs}−\\text{fitted}\\) and the fitted values depend only on the single predictor we are interested in.\n\u0026gt; ## Calculate partial residuals fitted values\r\u0026gt; fdata = rdata = data\r\u0026gt; fMat = rMat = model.matrix(~x, fdata)\r\u0026gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))\r\u0026gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\r\u0026gt; rdata = rdata %\u0026gt;% mutate(partial.resid = resid + fit)\r\u0026gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\r+ color = \u0026quot;gray\u0026quot;) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\r+ scale_y_continuous(\u0026quot;Y\u0026quot;) + scale_x_discrete(\u0026quot;X\u0026quot;) + theme_classic()\r\rEffect sizes\rWe can compute summaries for our effect size of interest (e.g. Cohen’s or the percentage ES) by post-processing our posterior distributions.\n\u0026gt; mcmc = as.matrix(data.stan.means)\r\u0026gt; ## Cohen\u0026#39;s D\r\u0026gt; cohenD = mcmc[, \u0026quot;beta[2]\u0026quot;]/mcmc[, \u0026quot;sigma\u0026quot;]\r\u0026gt; tidyMCMC(as.mcmc(cohenD), conf.int = TRUE, conf.method = \u0026quot;HPDinterval\u0026quot;)\r# A tibble: 1 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 var1 -9.88 0.740 -11.3 -8.38\r\u0026gt; \u0026gt; # Percentage change (relative to Group A)\r\u0026gt; ES = 100 * mcmc[, \u0026quot;beta[2]\u0026quot;]/mcmc[, \u0026quot;beta[1]\u0026quot;]\r\u0026gt; \u0026gt; # Probability that the effect is greater than 10% (a decline of \u0026gt;10%)\r\u0026gt; sum(-1 * ES \u0026gt; 10)/length(ES)\r[1] 1\r\rProbability statements\rAny sort of probability statements of interest about our effect size can be computed in a relatively easy way by playing around with the posteriors.\n\u0026gt; mcmc = as.matrix(data.stan.means)\r\u0026gt; \u0026gt; # Percentage change (relative to Group A)\r\u0026gt; ES = 100 * mcmc[, \u0026quot;beta[2]\u0026quot;]/mcmc[, \u0026quot;beta[1]\u0026quot;]\r\u0026gt; hist(ES)\r\u0026gt; \u0026gt; # Probability that the effect is greater than 10% (a decline of \u0026gt;10%)\r\u0026gt; sum(-1 * ES \u0026gt; 10)/length(ES)\r[1] 1\r\u0026gt; \u0026gt; # Probability that the effect is greater than 25% (a decline of \u0026gt;25%)\r\u0026gt; sum(-1 * ES \u0026gt; 25)/length(ES)\r[1] 0.978\r\rFinite population standard deviations\rEstimates for the variability associated with between and within group differences can also be easily obtained.\n# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x 19.3 0.408 18.5 20.1 2 sd.resid 2.75 0.0207 2.74 2.79\r# A tibble: 2 x 5\rterm estimate std.error conf.low conf.high\r\u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 sd.x 87.5 0.238 87.1 87.8\r2 sd.resid 12.5 0.238 12.2 12.9\r\rUnequally varied populations\rWe can also generate data assuming two populations with different variances, e.g. between male and female subgroups.\n\u0026gt; set.seed(123)\r\u0026gt; n1 \u0026lt;- 60 #sample size from population 1\r\u0026gt; n2 \u0026lt;- 40 #sample size from population 2\r\u0026gt; mu1 \u0026lt;- 105 #population mean of population 1\r\u0026gt; mu2 \u0026lt;- 77.5 #population mean of population 2\r\u0026gt; sigma1 \u0026lt;- 3 #standard deviation of population 1\r\u0026gt; sigma2 \u0026lt;- 2 #standard deviation of population 2\r\u0026gt; n \u0026lt;- n1 + n2 #total sample size\r\u0026gt; y1 \u0026lt;- rnorm(n1, mu1, sigma1) #population 1 sample\r\u0026gt; y2 \u0026lt;- rnorm(n2, mu2, sigma2) #population 2 sample\r\u0026gt; y \u0026lt;- c(y1, y2)\r\u0026gt; x \u0026lt;- factor(rep(c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;), c(n1, n2))) #categorical listing of the populations\r\u0026gt; xn \u0026lt;- rep(c(0, 1), c(n1, n2)) #numerical version of the population category\r\u0026gt; data2 \u0026lt;- data.frame(y, x, xn) # dataset\r\u0026gt; head(data2) #print out the first six rows of the data set\ry x xn\r1 103.3186 A 0\r2 104.3095 A 0\r3 109.6761 A 0\r4 105.2115 A 0\r5 105.3879 A 0\r6 110.1452 A 0\rStart by defining the model\n\\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon, \\]\nwhere \\(\\epsilon_1 \\sim \\text{Normal}(0,\\sigma_1)\\) for \\(x_1=0\\) (females), and \\(\\epsilon_2 \\sim \\text{Normal}(0,\\sigma_2)\\) for \\(x_2=1\\) (males). In STAN code, the model becomes:\n\u0026gt; stanStringv3 = \u0026quot; + data {\r+ int n;\r+ vector [n] y;\r+ vector [n] x;\r+ int\u0026lt;lower=1,upper=2\u0026gt; xn[n];\r+ }\r+ parameters {\r+ vector \u0026lt;lower=0, upper=100\u0026gt;[2] sigma;\r+ real beta0;\r+ real beta;\r+ }\r+ transformed parameters {\r+ }\r+ model {\r+ vector [n] mu;\r+ //Priors\r+ beta0 ~ normal(0,1000);\r+ beta ~ normal(0,1000);\r+ sigma ~ cauchy(0,25);\r+ + mu = beta0 + beta*x;\r+ //Likelihood\r+ for (i in 1:n) y[i] ~ normal(mu[i], sigma[xn[i]]);\r+ }\r+ generated quantities {\r+ vector [2] Group_means;\r+ real CohensD;\r+ real CLES;\r+ + Group_means[1] = beta0;\r+ Group_means[2] = beta0+beta;\r+ CohensD = beta /(sum(sigma)/2);\r+ CLES = normal_cdf(beta /sum(sigma),0,1); + }\r+ + \u0026quot;\r\u0026gt; \u0026gt; ## write the model to a text file \u0026gt; writeLines(stanStringv3,con=\u0026quot;ttestModelv3.stan\u0026quot;)\rWe specify priors directly on \\(\\sigma_1\\) and \\(\\sigma_2\\) using Cauchy distributions with a scale of \\(25\\). Next, arrange the data as a list (as required by STAN) and define the MCMC parameters.\n\u0026gt; data2.list \u0026lt;- with(data, list(y = y, x = (xn - 1), xn = xn, n = nrow(data)))\r\u0026gt; paramsv3 \u0026lt;- c(\u0026quot;beta0\u0026quot;,\u0026quot;beta\u0026quot;,\u0026quot;sigma\u0026quot;,\u0026quot;Group_means\u0026quot;,\u0026quot;CohensD\u0026quot;, \u0026quot;CLES\u0026quot;)\r\u0026gt; burnInSteps = 500\r\u0026gt; nChains = 2\r\u0026gt; thinSteps = 1\r\u0026gt; nIter = 2000\rFinally, fit the model in STAN and print the results.\n\u0026gt; data.stanv3 = stan(file = \u0026quot;ttestModelv3.stan\u0026quot;, + data = data2.list, + pars = paramsv3,\r+ iter = nIter,\r+ warmup = burnInSteps, + chains = nChains, + thin = thinSteps, + init = \u0026quot;random\u0026quot;, #or inits=list(inits,inits)\r+ refresh = 0)\r\u0026gt; \u0026gt; #print results\r\u0026gt; print(data.stanv3)\rInference for Stan model: ttestModelv3.\r2 chains, each with iter=2000; warmup=500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=3000.\rmean se_mean sd 2.5% 25% 50% 75% 97.5%\rbeta0 105.21 0.01 0.36 104.51 104.97 105.21 105.44 105.92\rbeta -27.34 0.01 0.57 -28.45 -27.71 -27.35 -26.96 -26.21\rsigma[1] 2.79 0.01 0.27 2.31 2.60 2.77 2.97 3.38\rsigma[2] 2.88 0.01 0.34 2.31 2.63 2.84 3.07 3.65\rGroup_means[1] 105.21 0.01 0.36 104.51 104.97 105.21 105.44 105.92\rGroup_means[2] 77.86 0.01 0.44 77.00 77.57 77.86 78.15 78.75\rCohensD -9.70 0.02 0.76 -11.23 -10.23 -9.69 -9.17 -8.26\rCLES 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\rlp__ -150.30 0.04 1.42 -153.88 -151.02 -149.99 -149.25 -148.53\rn_eff Rhat\rbeta0 2426 1\rbeta 2359 1\rsigma[1] 2166 1\rsigma[2] 2547 1\rGroup_means[1] 2426 1\rGroup_means[2] 3478 1\rCohensD 2468 1\rCLES 1875 1\rlp__ 1277 1\rSamples were drawn using NUTS(diag_e) at Mon Feb 10 14:11:55 2020.\rFor each parameter, n_eff is a crude measure of effective sample size,\rand Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1).\r\rReferences\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rGelman, Andrew, and others. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).” Bayesian Analysis 1 (3): 515–34.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\r\r\r","date":1580609594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580609594,"objectID":"1800c435d174d6f0595943438a103ae4","permalink":"/stan/comparing-two-populations-stan/comparing-two-populations-stan/","publishdate":"2020-02-01T21:13:14-05:00","relpermalink":"/stan/comparing-two-populations-stan/comparing-two-populations-stan/","section":"STAN","summary":"This tutorial will focus on the use of Bayesian estimation to explore differences between two populations. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nOpenBUGS - written in component pascal.\n\rJAGS - (Just Another Gibbs Sampler) - written in C++.","tags":["tutorials","STAN","population differences"],"title":"Comparing Two Populations - STAN","type":"STAN"},{"authors":["A Gabrio"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"a259569c4eec6977fc373574bdeecc7e","permalink":"/publication/gabrio2020/","publishdate":"2019-11-22T00:00:00Z","relpermalink":"/publication/gabrio2020/","section":"publication","summary":"Statistical modelling of sports data has become more and more popular in the recent years and different types of models have been proposed to achieve a variety of objectives: from identifying the key characteristics which lead a team to win or lose to predicting the outcome of a game or the team rankings in national leagues. Although not as popular as football or basketball, volleyball is a team sport with both national and international level competitions in almost every country. However, there is almost no study investigating the prediction of volleyball game outcomes and team rankings in national leagues. We propose a Bayesian hierarchical model for the prediction of the rankings of volleyball national teams, which also allows to estimate the results of each match in the league. We consider two alternative model specifications of different complexity which are validated using data from the women's volleyball Italian Serie A1 2017-2018 season.","tags":["Bayesian methods","Volleyball","Hierarchical Models"],"title":"Bayesian Hierarchical Models for the Prediction of Volleyball Results","type":"publication"},{"authors":["Andrea Gabrio"],"categories":["news","talks","PRIMENT"],"content":"The new year is finally taking off for me and I have a couple of updates. First, I would like to remind everyone about the exciting new course \u0026ldquo;understanding health economics in clinical trials\u0026rdquo; that me and the rest of our research team HEART have put together to support the dissemination of health economics among all people involved in the design and analysis of clinical trials. I look forward to deliver this one-day short course together with my colleagues from the UCL PCPH department which will be structured into different sessions during the day of Feb 11th at the UCL CCTU - 2nd Floor, 90 High Holborn, London. The course is specifically intended for those who would like to know more about health economics, which has become an important component in the design, analysis and most crucially, for the funding approval of clinical trials. The course will focus on the following aspects:\n  A short intorduction to the basic concepts of health economics and why these can be relevent to different people\n  A review of different types of intruments and tools used to collect health economic data in clinical trials\n  A quick look at decision models with some examples\n  A summary of the typical results from health economic analyses and how to interpret them\n  The course is still in its pilot form and therfore it is free of charge. If there are still places available, you are very welcome to join and give us your feedback!.\nSecond, I am happy to announce that my recent paper about the use of Bayesian Hierarchical Models for the Prediction of Volleyball Results has finally been published on the Journal of Applied Statistics. I am really proud of this paper as it is my first solo paper publiched and because I have always been very invested in the general topic of predicting sport results using probability models. To be able to publish something about this based on my own efforts is very rewarding in terms of the (small) contribution to research that I hope I was able to provide.\nFinally, I have submitted an abstract to the 2020 European Health Economics Association Conference, which this year will be held in Oslo, Norway.\nI have now to patiently wait for the review of the abstracts and see if my work made it, either as an oral presenation or as a poster. Fingers crossed!.\n","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580556843,"objectID":"1a17de6f401df35d356e42a6fcb7311e","permalink":"/post/update-february/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/post/update-february/","section":"post","summary":"The new year is finally taking off for me and I have a couple of updates. First, I would like to remind everyone about the exciting new course \u0026ldquo;understanding health economics in clinical trials\u0026rdquo; that me and the rest of our research team HEART have put together to support the dissemination of health economics among all people involved in the design and analysis of clinical trials. I look forward to deliver this one-day short course together with my colleagues from the UCL PCPH department which will be structured into different sessions during the day of Feb 11th at the UCL CCTU - 2nd Floor, 90 High Holborn, London.","tags":["News","Academic","Publication"],"title":"Finally here ...","type":"post"},{"authors":[],"categories":null,"content":"","date":1580210100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580210100,"objectID":"834821180f97e5e82b96a2f72eda0a02","permalink":"/talk/priment2020/","publishdate":"2019-11-24T00:00:00Z","relpermalink":"/talk/priment2020/","section":"talk","summary":"Invited presentation","tags":["Economic Evaluations","Missing Data"],"title":"Choosing the Missing Data Method in Trial-Based Economic Evaluations. How to Make the Right Choice?","type":"talk"},{"authors":["Andrea Gabrio"],"categories":["news","talks","PRIMENT"],"content":"After the terrible start of this year, things are going ok now and I am quite busy with different projects that I left a bit behind. First, I can confirm that me and my colleagues from the HEART group are going to give an introductory course to health economic evaluations next month for different groups of people from academia and clinical trial units. The course has been generally structured based on our \u0026ldquo;pilot\u0026rdquo; we gave last year (which went really well by the way) and involves many different topics that will cover the entire day of February 11th. The attending list is already full and thw waiting list is also quite big; happy to see so much interest in economic evaluations.\nSecond, I will give a talk at the PRIMENT statistics and health economics and methodology seminar about an on-going project on missing data in trial-based analysis on Tuesday 28th, at UCL PRIMENT CTU. I am really happy to be back at these seminars which I feel I really nice and where you have the opportunity to interact with people from different backgrounds and job positions who may give some useful feedback on my work. Hopefully, people will find my research interesting!. I would also like to mention the fact that one of my HEART colleague, Marie, will give another talk at the same seminar just before me. Her topic is the economic analysis plan for a trial she has been involved with and I think she is really good, so may worth check her presentaiton out.\nThird, I have finalised a long-waited submission for a paper which has been discussed, written and re-written many times. I really hope we can get some useful feedback on it as I personally worked very hard to keep this work alive. Let see if my efforts have not been in vain and fingers crossed!\nFourth, as a side note, I have recently bought a new book on missing data called Semiparametric Thoery and Missing Data by Tsiatis, which looks very interesting. To be honest, the book is quite technical with many theoretical concpets and proofs which sometimes I find hard to follow. However, so far it gives a nice introduction to semiparametric models and I look forward to see how it approaches the missing data topic from a non likelihood-based approach. If you are into non/semiparametric statistics and want to find out more about this, I recommend the reading.\nFinally, more work is also coming up in the next weeks and some of this is not going to be very enjoyable, I think. Anyway, let us go through this busy period at our best and see how things will go.\n","date":1578528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578569643,"objectID":"86a4b6d8880e417ff7d095dfbed94999","permalink":"/post/update-january2/","publishdate":"2020-01-09T00:00:00Z","relpermalink":"/post/update-january2/","section":"post","summary":"After the terrible start of this year, things are going ok now and I am quite busy with different projects that I left a bit behind. First, I can confirm that me and my colleagues from the HEART group are going to give an introductory course to health economic evaluations next month for different groups of people from academia and clinical trial units. The course has been generally structured based on our \u0026ldquo;pilot\u0026rdquo; we gave last year (which went really well by the way) and involves many different topics that will cover the entire day of February 11th.","tags":["News","Academic","talks"],"title":"Let us do some work","type":"post"},{"authors":[],"categories":null,"content":"","date":1575896400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575896400,"objectID":"ef55746dc1f839190eecabdfbe1d4252","permalink":"/talk/priment2019/","publishdate":"2019-11-24T00:00:00Z","relpermalink":"/talk/priment2019/","section":"talk","summary":"Invited presentation","tags":null,"title":"MissingHE An R package to deal with missing data in trial based health economic evaluations","type":"talk"},{"authors":["Andrea Gabrio"],"categories":["news","2020"],"content":"After some nice holiday break, I came back to work ready for an exciting 2020 \u0026hellip; or so I thought. Unfortunately, I have recently been caught by a terrible flu which forced me to postpone my flight back to London of a week. The worst part is that I was basically a dead corpse moving around with high fever and an awful condition for more than 4 days. It was quite a bad experience which I rarely had in my life. I am just glad I survived this.\nGoing back to more interesting news. Before my cursed period, I was smart enough to work on different things and I am happy to announce a new update for my missingHE package, which is available both on my GitHub page and on the CRAN repository. Its new version is 1.3.2 and has the nice addition of making available more choices for the parametric distributions that can be selected in all main functions of the package to handle missing data in trial-based economic evaluations. In particular, it is now possible to choose among new probability distributions for the health outcomes, including continuous (Gamma, Weibull, Exponential, Logistic), discrete (Poisson, Negative Binomial) and binary (Bernoulli) distributions. These may be useful when the analysis is not based on utilities scores but some other types of effects, such as survival time, number of events or binary outcomes. I have also included some examples for each type of outcome in the MenSS dataset (available directly once installed the package on your machine) so that people can play around with the new distributions.\nAnother good news is that the last paper written with Michael about missing data handling in economic evaluations will soon be publiched in the February issue of JRSSA, which will make the final and official version of the article that can be cited, I think.\nFinally, an announcement about the one-day course I am holding together with my mates from the HEART group about an introduction to economic evaluations to people who are not familiar with health economics. The course will take place next month, I believe on Feb 11th, in central London (soon an update about the exact location) and, as the previous edition, I am happy to see that all spots have been taken and everything is sold out (well, to be precise the course is free \u0026hellip;). Need to meet up with the others to make the last changes and prepare the slides but I am quite excited about this, given also the good response we got last time.\nNow I am (hopefully) ready to start the new year and there are many things already piling up on my list of things to do in the next days. Let\u0026rsquo;s try again 2020.\n","date":1575849600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575891243,"objectID":"8839c0783a511af23ea9bcb595ecdbf7","permalink":"/post/update-january/","publishdate":"2019-12-09T00:00:00Z","relpermalink":"/post/update-january/","section":"post","summary":"After some nice holiday break, I came back to work ready for an exciting 2020 \u0026hellip; or so I thought. Unfortunately, I have recently been caught by a terrible flu which forced me to postpone my flight back to London of a week. The worst part is that I was basically a dead corpse moving around with high fever and an awful condition for more than 4 days. It was quite a bad experience which I rarely had in my life.","tags":["News","Academic"],"title":"Not a very good start...","type":"post"},{"authors":["A Gabrio"],"categories":null,"content":"","date":1574380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574380800,"objectID":"303356de7837fb8e5dc4516c3402f840","permalink":"/publication/gabrio2019e/","publishdate":"2019-11-22T00:00:00Z","relpermalink":"/publication/gabrio2019e/","section":"publication","summary":"Statistical modelling of sports data has become more and more popular in the recent years and different types of models have been proposed to achieve a variety of objectives: from identifying the key characteristics which lead a team to win or lose to predicting the outcome of a game or the team rankings in national leagues. Although not as popular as football or basketball, volleyball is a team sport with both national and international level competitions in almost every country. However, there is almost no study investigating the prediction of volleyball game outcomes and team rankings in national leagues. We propose a Bayesian hierarchical model for the prediction of the rankings of volleyball national teams, which also allows to estimate the results of each match in the league. We consider two alternative model specifications of different complexity which are validated using data from the women's volleyball Italian Serie A1 2017-2018 season.","tags":["Bayesian methods","Volleyball","Hierarchical Models"],"title":"Bayesian Hierarchical Models for the Prediction of Volleyball Results","type":"publication"},{"authors":["Andrea Gabrio"],"categories":["news","Christmas"],"content":"I did not have much time to post anything this month until now as it has been a quite busy period. I have been involved in many different works and I have also involved other people in what I think could be some very interesting new projects. Not that I complain about having many different things to do (most of them are actually cool) but doing everything in a short period is not the best.\nA couple of things have come/are coming up. First, I have seriously started working on the coding of a decision model for some health economic evaluation project I have been involved in since last year. Everything seems ok after I spent lots of days and time fixing some small bugs in my code. I am about half way through the model and I hope I will be able to finish it before Christmas (I doubt it though).\nSecond, I have finished reviewing an interesting paper about some new methods for improving current practice for dealing with missing data, which I kinda enjoy reading (very good!).\nThird, I would like to quickly summarise my first experience at ISPOR Europe in Copenhagen. I was really excited to attend this conference which, as expected, revealed itself as huge with people coming from all over the world and with many interesting sessions and discussion topics. I had the chance to meet new and old people, such as professor Andrea Manca and the always very kind Chris Sampson for whom I was like a stalker asking for more and more information about himself and his work. I also met some of my old collegues from MapiGroup, now under ICON plc. It was very fun to hang out with these old friends and see what they have been up to during this time. Among them, I gladly caught up with my dear friend Ryan Pulleyblank, now doing a PhD at the University of Southern Denmark. My poster was a success with (unexpectedly) many people stopping by and asking for more information on my work. I was genuinely surprised by this as ISPOR is mostly a conference dedicated to companies rather than academic works and networking. To sum up, it was a very nice and fun experience and despite the level of statistical methodology was not particularly high I enjoyed my time there and I also had the chance to visit Copenhagen for the first time.\nFinally, as a side note, I have found the time to upload on my arXiv page a nice application of Bayesian hierarchical models for the prediction of volleyball matches which I have been working on the past summer, taking inspiration from the work of Gianluca about predicting football macthes. I hope my work can turn out in something cool as well.\nThis is all for the moment but soon I will be heading back to another quite busy period for me. I hope this will be the last for some time, especially given that Christmas is coming and I would like to have some free time to properly enjoy this period, which I really like, even more than Christmas itself.\n","date":1573257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573299243,"objectID":"4d4989c8f96fb85ba1f81a1451bbaa83","permalink":"/post/update-november/","publishdate":"2019-11-09T00:00:00Z","relpermalink":"/post/update-november/","section":"post","summary":"I did not have much time to post anything this month until now as it has been a quite busy period. I have been involved in many different works and I have also involved other people in what I think could be some very interesting new projects. Not that I complain about having many different things to do (most of them are actually cool) but doing everything in a short period is not the best.","tags":["News","Academic","Christmas"],"title":"Too many things, again....","type":"post"},{"authors":[],"categories":null,"content":"","date":1572872400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572872400,"objectID":"a980e034b0bf0d1396268ed836bcf343","permalink":"/talk/isporeu2019/","publishdate":"2019-08-02T00:00:00Z","relpermalink":"/talk/isporeu2019/","section":"talk","summary":"Contibuted poster","tags":["Economic Evaluations","Missing Data"],"title":"A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations","type":"talk"},{"authors":["Andrea Gabrio"],"categories":["news","conferences"],"content":"Finally the time of ISPOR Europe 2019 has arrived and I will depart in a few days for Copenhagen, where the conference is held this year. I am actually looking forward to this as I am curious to see what type of conference ISPOR is, that is, whether I will be able to find some interesting works and have some \u0026ldquo;applied statistics\u0026rdquo;-related discussions or the attention is more placed on \u0026ldquo;economics and clinical\u0026rdquo; matters. From what I heard by other people who routinely attend the conference, there should be a bit of both sides, even though I really hope I will be able to see some intersting methods and engage in discussion with some authors.\nI know the conference is mainly related to address the needs of pharmaceutical and consultancy companies, but I hope I will be able to see some familiar faces there. Well, to be honest I know that some people I already know are going, which is good considering that their work is really cool. As for me, I will present the same work that I showed at ICTMC 2019 (some slides available here), but this time in the format of a poster, of which I am kind of very proud in terms of the final output, if I may say so.\nApart from this nice event, there are many things coming up when I will be back from the conference, which I really need to start working on. Mostly, these are related to some routine work for some trial analyses at PRIMENT, which by the way is advertising a new health economist job vacancy for those who might be interested. Other tasks include writing down and code a decision model on which I have been working since ages, papers review, other collaborations with different people, starting my co-supervison for a new PhD student at stats and, after I can find some free time, do some reasearch work on my beloved missing data. Am I ready? not sure about that \u0026hellip;\n","date":1572220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572262443,"objectID":"4290b3a1cb0e2b13693c9588a7c6a6cd","permalink":"/post/update3-october/","publishdate":"2019-10-28T00:00:00Z","relpermalink":"/post/update3-october/","section":"post","summary":"Finally the time of ISPOR Europe 2019 has arrived and I will depart in a few days for Copenhagen, where the conference is held this year. I am actually looking forward to this as I am curious to see what type of conference ISPOR is, that is, whether I will be able to find some interesting works and have some \u0026ldquo;applied statistics\u0026rdquo;-related discussions or the attention is more placed on \u0026ldquo;economics and clinical\u0026rdquo; matters.","tags":["News","Academic","Conferences"],"title":"Copenhagen, I am coming ...","type":"post"},{"authors":["Andrea Gabrio"],"categories":["news","conferences"],"content":"Just a quick update about some talks I gave/am about to give to advertise my research work. The one in Brighton, which I gave a couple of weeks a go at ICTMC, went really well and I was glad to hear that some people were very interested in what I presented. For more info, here a link to my presentation about missing data methods for trial-based economic evaluations that I discussed. Honestly, since the conference was mainly directed towards people working in clinical trials, I did not expect a huge interest in the use of Bayesian methods for economic evaluations, but apparently (and I am happy about that) I was wrong.\nI had the chance to chat a bit with few people that I did not know, including William Hollingworth from Bristol and Ines Rombach from Oxford, with whom I had very nice conversations about my work and other interesting topics. I was also glad to meet some known faces, including the always lovely Catrin Plumpton from Bangor University, who I met for the first time at HESG this summer and with whom I share the interest in missing data methods (even though she is a STATA and multiple imputation user, sadly). I am also glad that I met my previous PhD secondary supervisor, Alexina Mason, with whom it is always a pleasure to talk with. Unfortunately, we both missed the talk of each other becuase of time problems but it was good to catch up with her again. I am also sad that I could not attend Baptiste\u0026lsquo;s presentation which was the last day of the conference (I had to leave the same day of my talk, the first day) and I was not also able to actually meet him. I hope we will be able to see him soon at some other conference in the near future.\nGiven this past experience, I am now looking forward to meet new people at my next conference at the Bella Center in Copenhagen (thumbnail) where this year ISPOR Europe 2019 will be held. However, I believe this will be a much larger conference and therefore I will probably not have many chances to talk with people as I did at ICTMC. Plus I am only preseting a poster this time, so it will be less likely that some people will actually notice my work, especially given the typically huge amount of presenters of this type of conferences. In the wrost case, I will enjoy Copenhagen and meet up with some old friends who live in Denmark and who will come at ISPOR to present some other work.\n","date":1571097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571139243,"objectID":"2d6798f4902280dea56962da08530a02","permalink":"/post/update2-october/","publishdate":"2019-10-15T00:00:00Z","relpermalink":"/post/update2-october/","section":"post","summary":"Just a quick update about some talks I gave/am about to give to advertise my research work. The one in Brighton, which I gave a couple of weeks a go at ICTMC, went really well and I was glad to hear that some people were very interested in what I presented. For more info, here a link to my presentation about missing data methods for trial-based economic evaluations that I discussed.","tags":["News","Academic","Publication"],"title":"Conferences updates and news","type":"post"},{"authors":[],"categories":null,"content":"","date":1570446000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570446000,"objectID":"df74c48b54aa2fc2fe3c5c3c71c7f2a8","permalink":"/talk/ictmc2019/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/talk/ictmc2019/","section":"talk","summary":"Contibuted presentation","tags":["Economic Evaluations","Missing Data"],"title":"A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations","type":"talk"},{"authors":["Andrea Gabrio"],"categories":["news","conferences"],"content":"I have got two news coming up. First, the paper I wrote with Michael and Gianluca on Bayesian methdos for longitudinal data in trial-based economic evaluations has finally been published as early view on JRSSA. As I said in some earlier posts, I am super happy about this collaboration and I hope I can continue working on similar projects in the future.\nSecond, I will soon give a talk about this work at the ICTMC conference in Brighton, next Monday. This will be the first time at this conference and unfortunately I will only be able to remain around for one day as I need to go back to London pretty soon. I hope I will be able to enjoy my day at the conference, even though I will miss the talks of Baptiste and Alexina which are scheduled for the last day of the conference. I hope I can at least have a quick chat with them the day I am around.\nI am also excited to visit Brighton, since many people keep telling me that I should go and visit this sort of british version of \u0026ldquo;Rimini\u0026rdquo;. To be honest, I do not expect to find a nice wheather, given that in this period it is raining a lot in London, but I hope I will be lucky and get the only sunny day of the week.\nFinally, I have started a rubric called missing data on my website, where I try to describe some of the most popular methods to handle missing data and to provide some references for anyone who could be interested in this field. I am really fascinated by statistical methods for dealing with missingness, perhaps because it was the main focus of my PhD, but I am eager to review different methods and see if I can find something really interesting. Of course, to complete this it will take more time, which I hope I will be able to find in the next months.\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569929643,"objectID":"4c3b10d9662be9e28d6bd91a6e8d7d29","permalink":"/post/update-october/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/post/update-october/","section":"post","summary":"I have got two news coming up. First, the paper I wrote with Michael and Gianluca on Bayesian methdos for longitudinal data in trial-based economic evaluations has finally been published as early view on JRSSA. As I said in some earlier posts, I am super happy about this collaboration and I hope I can continue working on similar projects in the future.\nSecond, I will soon give a talk about this work at the ICTMC conference in Brighton, next Monday.","tags":["News","Academic","Publication"],"title":"More good news...","type":"post"},{"authors":["Andrea Gabrio"],"categories":["package","R"],"content":"I have finally found some time to update the version for my R package missingHE, for which version 1.2.1 is now available on CRAN. I included two main features to the previous version of the package.\nFirst, I have added a new type of identifying restriction when fitting pattern mixture models through the function \u0026ldquo;pattern\u0026rdquo;. Before, only the complete case restriction was available, which identifies the distributions of the missing data with those from the completers. Now the alternative available case restriction is can also be selected, which relies on the distributions that can be identified among the non-completers to identify the distributions of the missing data. In this way, people can choose among at least two options for the type of restrictions and compare how this choice may affect the final estimates.\nSecond, I added a new accessory function called \u0026ldquo;ppc\u0026rdquo;, which allows to perform posterior predictive checks using the conditional parameters saved from the fitted model to generate replications of the data at each posterior iteration of the model. The function implements a relatively large number of checks, mostly taken from the R package bayesplot, which allow to assess the fit of the model to the observed data by type of outcome (effects and costs) and treatment group (control and intervention). For example, overalyed density plots can be generated to compare the empirical and replicated densities of the data to detect possible failures of the model.\n\rDensity plots for the observed and replicated data\r\r\rI feel this is very important as when fitting a Bayesian model it is crucial to assess whether the model seems to adequately capture the different characteristics of the observed data (e.g. skewness, structural values, etc.). A wide range of predictive checks are available, including histograms (see thumbnail pciture), scatterplots, error intervals, empirical cumulative distribution fucntions, statistcis of interest and many others. In addition , these checks can be performed for each type of missingness model and parametric distribution chosen within missingHE.\nOf course, it is important to remember that, when dealing with missing data the fit of the model can only be checked with respect to the observed values and therefore this check is only partial since the fit to the unibserved values can never be checked. This is also why it is not meaningful to assess the fit of a model fitted under a missing not at random assumption because this is based on information which is not directly available from the data at hand and thus impossible to check.\n","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569411243,"objectID":"cbb05f40714cf0203644f57a8c8c1d48","permalink":"/post/missinghe-version121/","publishdate":"2019-09-25T00:00:00Z","relpermalink":"/post/missinghe-version121/","section":"post","summary":"I have finally found some time to update the version for my R package missingHE, for which version 1.2.1 is now available on CRAN. I included two main features to the previous version of the package.\nFirst, I have added a new type of identifying restriction when fitting pattern mixture models through the function \u0026ldquo;pattern\u0026rdquo;. Before, only the complete case restriction was available, which identifies the distributions of the missing data with those from the completers.","tags":["Academic","Health economics","Missing data"],"title":"MissingHE 1.2.1","type":"post"},{"authors":["Andrea Gabrio"],"categories":["thesis"],"content":"I have been kindly invited by the amazing person Chris Sampson to talk about the work I inlcuded in my PhD thesis for his monthly rubric entitled \u0026ldquo;Thesis Thursday\u0026rdquo; on the The Academic Health Economists blog.\nI happily accepted Chris\u0026rsquo;s invitation as I beleive this initiative is really interesting and represents a nice way for newly graduated PhD students to advertise their work while also giving the chance to people interested in health economics to read about some academic work which is typically freely available to everyone.\nHere you can find the full interview, which is not very long and resolves around 5 questions that Chris asked me about my work. I already new this blog but I have never had a proper chance to read through its posts carefully, which is a shame.\nI shall promise myself to try to check it more often from now on, using this interview as a nice motivation to do so. In fact, there are not many blogs around health economics matters (here a non-comprehensive list), among which The Academic Health Economists and Gianluca\u0026rsquo;s blog are my favourites.\nI hope I will be able to find some time to write some nice posts about some health economic applications of my work in the next future as this is still the most interesting field for me at the moment. I am also the maintainer of another small blog called the Health Economics Analysis and Research Methods Team (HEART) blog, where I occasionally write some posts on health economics together with my colleagues from the UCL department of Primary Care and Population Health. The blog is still new but I hope it can become more active in the next months.\n","date":1568505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568547243,"objectID":"a40e8115aed0c811b887304de7afd3e2","permalink":"/post/update-interview/","publishdate":"2019-09-15T00:00:00Z","relpermalink":"/post/update-interview/","section":"post","summary":"I have been kindly invited by the amazing person Chris Sampson to talk about the work I inlcuded in my PhD thesis for his monthly rubric entitled \u0026ldquo;Thesis Thursday\u0026rdquo; on the The Academic Health Economists blog.\nI happily accepted Chris\u0026rsquo;s invitation as I beleive this initiative is really interesting and represents a nice way for newly graduated PhD students to advertise their work while also giving the chance to people interested in health economics to read about some academic work which is typically freely available to everyone.","tags":["Academic","Health Economics","Interview"],"title":"Discussing my thesis","type":"post"},{"authors":["Andrea Gabrio"],"categories":["publication"],"content":"With the approaching of the new academic here I have received some good news for my most recently submitted paper on Bayesian parametric modelling in health economics for missing longitudinal data, which at the moment is only available on arXiv.\nI am happy to announce that, after a couple of rounds of reviews, the paper has been finally accepted for publication in JSS: Series A. I believe that the reviewers provided a very nice feedback for improving the work and I am quite satisfied with the final version of the article which, I hope, will be of interest for anyone involved in the analsysi of partially-observed longitudinal data. I hope the pre-print of the paper will be available soon and I will \u0026ldquo;advertise\u0026rdquo; my work in two conferences in the next couple of months, where I will present the content of the paper, namely ICTMC this October in Brighton, and ISPOR Europe this November in Copenhagen.\nI am really excited about this paper which represented the last part of my PhD thesis and on which I worked really hard in the last year of my studies. Here you can find a general summary of the content of the article, while here there are some slides that describe the main idea behind the proposed model.\nI just want to conlcude with some thanks with my co-authors of the paper, Michael and Ginaluca, without whom I would have not been able to write this paper. This was my first work with Mike, with whom I had a wonderful collaboration and I was able to visit the beatiful city of Gainesville (FL) during my first visiting period at the University of Florida (see thumbnail picture). I hope this will be the first of many works together in the furture!.\n","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567337643,"objectID":"f8c1f56f754f503c6c6cba3c2d501be9","permalink":"/post/update-september/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/post/update-september/","section":"post","summary":"With the approaching of the new academic here I have received some good news for my most recently submitted paper on Bayesian parametric modelling in health economics for missing longitudinal data, which at the moment is only available on arXiv.\nI am happy to announce that, after a couple of rounds of reviews, the paper has been finally accepted for publication in JSS: Series A. I believe that the reviewers provided a very nice feedback for improving the work and I am quite satisfied with the final version of the article which, I hope, will be of interest for anyone involved in the analsysi of partially-observed longitudinal data.","tags":["Academic","Publication"],"title":"Some good news...","type":"post"},{"authors":["Andrea Gabrio"],"categories":["discussion"],"content":"Today, I would like to briefly comment an interesting research article written by Goodman, who provided a clear and exemplary discussion about the typical incorrect interpretation of a standard frequentist analysis in the field of medical research. I will now briefly summarise the main argument of the paper and then add some personal comments.\nEssentially, the article describes the characteristics of the dominant school of medical statistics and highlights the logical fallacy at the heart of the typical frequentist analysis in clinical studies. This is based on a deductive inferential approach, which starts with a given hypothesis and makes conclusions under the assumption that the hypothesis is true. This is in contrast with a inductive approach, which uses the observed evidence to evaluate what hypothesis is most tenable. The two most popular methods of the frequentist paradigm are the P value proposed by Fisher and the hypothesis testing developed by Neyman and Pearson.\nThe P value is defined as the probability, under the assumption of no effect (null hypothesis), of obtaining a result equal to or more extreme than what was actually observed. Fisher proposed it as an informal index to be used as a measure of discrepancy between the data and the null hypothesis and therefore should not be interpreted as a formal inferential method. For example, since the P value can only be calculated on the assumption that the null hypothesis is true, it cannot be a direct measure of the probability that the null hypothesis is false. However, the main criticism to the P value is perhaps that it does not take into account the size of the observed effect, i.e. a small effect in a study with a large sample size can have the same P value as a large effect in a small study.\nHypothesis testing was proposed by Neyman and Pearson as an alternative approach to the P value, which assumes the existence of a null hypothesis (e.g. no effect) and an alternative hypothesis (e.g. nonzero effect). The outcome of the test is then simply to reject one hypothesis in favour of the other, solely based on the data. This exposes the researcher to two types of errors: type I error or false-positive ($\\alpha$) and type II error or false-negative ($\\beta$) result. Rather than focussing on single experiments, like the P value, hypothesis testing is effectively based on a deductive approach to minimise the errors over a large number of experiments. However, the price to pay to obtain this objectivity is the impossibility to make any inferential statement about a single experiment. The procedure only guarantees that in the long run, i.e. after considering many experiments, we shall not often be wrong.\nOver time a combination between the P value and hypothesis testing was developed under the assumption that the two approaches can be complementary. The idea was that the P value could be used to measure evidence in a single experiment while not violating the long run logic of hypothesis testing. The combined method is characterized by setting $\\alpha$ and power $\\beta$ before the experiment, then calculating a P value and rejecting the null hypothesis if the P value is less than the preset type I error rate. This means that the P value is considered a false-positive error rate specific to the data and also a measure of evidence against the null hypothesis. The P value fallacy is born from this statement, which assumes that an event can be seen simultaneously from a long run perspective (where the observed results are put together with other results that might have occurred in hypothetical repetitions of the experiment) and from a short run perspective (where the observed results are interpreted only with respect to the single experiment). However, these views are not reconcilable since a result cannot be at the same time an interchangeable (long-run) and unique (short-run) member of a group of results.\nI personally find this discussion fascinating and I believe that it is important to recognise the inconsistencies between the two alternative approaches to inference. The original authors of the two paradigms were well aware of the implications of their methods and never supported the combination of these. However, the combined approach has somehow become widely accepted in practice while its internal inconsistencies and conceptual limitations are hardly recognised.\nI feel that, since the two methods are perceived as \u0026ldquo;objective\u0026rdquo;, it is generally accepted that, if combined, they can produce reliable conclusions. This, however, is not necessarily true. Accepting at face value the significance result as a binary indicator of whether or not a relation is real is dangeroues and potentially misleading. This practice wants to show that conclusions are being drawn directly from the data, without any external influence, because direct inference from data to hypothesis is thought to result in mistaken conclusions only rarely and is therefore regarded as \u0026ldquo;scientific\u0026rdquo;.\nThis misguided approach has led to a much stronger emphasis towards the quantitative results alone (without any external input). In contrast, I believe that such perspective has the serious drawback of ignoring potentially useful information which is available (e.g. relevant medical knowledge or historical data) and which should be included in the analysis. Of course, I am aware of the potential issues that may arise from the selection and incorporation of external evidence, but I believe this should not be considered as \u0026ldquo;less reliable\u0026rdquo; or \u0026ldquo;more prone to mistakes\u0026rdquo; compared with the evidence from the available data. It is important that an agreement is reached about the selection of the type of evidence and methods to be used to perform the analysis solely based on their relevance with respect to the context analysed.\n","date":1564790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565004843,"objectID":"2e3b25e6b65aa152b5e106f8693672fa","permalink":"/post/p-value-fallacy/","publishdate":"2019-08-03T00:00:00Z","relpermalink":"/post/p-value-fallacy/","section":"post","summary":"Today, I would like to briefly comment an interesting research article written by Goodman, who provided a clear and exemplary discussion about the typical incorrect interpretation of a standard frequentist analysis in the field of medical research. I will now briefly summarise the main argument of the paper and then add some personal comments.\nEssentially, the article describes the characteristics of the dominant school of medical statistics and highlights the logical fallacy at the heart of the typical frequentist analysis in clinical studies.","tags":["Academic","Frequentist statistics"],"title":"The P value fallacy","type":"post"},{"authors":[],"categories":null,"content":"","date":1562256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562256000,"objectID":"83ff58a85903f76d2402e728102afadf","permalink":"/talk/hesg2019/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/talk/hesg2019/","section":"talk","summary":"Contibuted presentation","tags":["Economic Evaluations","Missing Data"],"title":"Adjusting for partially-observed utilities and costs in trial-based cost-effectiveness analysis: a comparison of different methods and their performance","type":"talk"},{"authors":["Andrea Gabrio"],"categories":["R"],"content":"\rThe focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using STAN via R.\nPrerequisites:\n\rThe latest version of R, which can be downloaded and installed for Windows, Mac or Linux OS from the CRAN website\rI also strongly recommend to download and install Rstudio, an integrated development environment which provides an “user-friendly” interaction with R (e.g. many drop-down menus, tabs, customisation options)\r\rPreliminaries\rWhat is STAN?\rStan provides full Bayesian inference for continuous-variable models through Markov Chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling\nSTAN is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (Gelman, Lee, and Guo (2015)). STAN is a free software and a probabilistic programming language for specifying statistical models using a specific class of MCMC algorithms known as Hamiltonian Monte Carlo methods (HMC). The latest version of STAN can be dowloaded from the web repository and is available for different OS. There are different R packages which function as frontends for STAN. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the rstan package (Stan Development Team (2018)) and show how to fit STAN models using this package.\n\rInstalling STAN and rstan\rUnlike other Bayesian software, such as JAGS or OpenBUGS, it is not required to separately install the program and the corresponding frontend R package. Indeed, installing the R package rstan will automatically install STAN on your machine. However, you will also need to make sure to having installed on your pc a C++ compiler which is used by rstan to fit the models. Under a Windows OS, for example, this can be done by installing Rtools, a collection of resources for building packages for R, which is freely available from the web repository.\nNext, install the package rstan from within R or Rstudio, via the package installer or by typing in the command line\n\u0026gt; install.packages(\u0026quot;rstan\u0026quot;, dependencies = TRUE)\rThe dependencies = TRUE option will automatically install all the packages on which the functions in the rstan package rely.\n\r\rBasic model\rSimulate data\rFor an example dataset, I simulate my own data in R. I create a continuous outcome variable \\(y\\) as a function of one predictor \\(x\\) and a disturbance term \\(\\epsilon\\). I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) coefficients, i.e.\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon \\]\nThe R commands which I use to simulate the data are the following:\n\u0026gt; n_sim=100; set.seed(123)\r\u0026gt; x=rnorm(n_sim, mean = 5, sd = 2)\r\u0026gt; epsilon=rnorm(n_sim, mean = 0, sd = 1)\r\u0026gt; beta0=1.5\r\u0026gt; beta1=1.2\r\u0026gt; y=beta0 + beta1 * x + epsilon\r\u0026gt; n_sim=as.integer(n_sim)\rThen, I define all the data for STAN in a list object\n\u0026gt; datalist=list(\u0026quot;y\u0026quot;=y,\u0026quot;x\u0026quot;=x,\u0026quot;n_sim\u0026quot;=n_sim)\r\rModel file\rNow, I write the model for STAN and save it as a stan file named \"basic.mod.stan\" in the current working directory\n\u0026gt; basic.mod= \u0026quot;\r+ data {\r+ int\u0026lt;lower=0\u0026gt; n_sim;\r+ vector[n_sim] y;\r+ vector[n_sim] x;\r+ }\r+ parameters {\r+ real beta0;\r+ real beta1;\r+ real\u0026lt;lower=0\u0026gt; sigma;\r+ }\r+ transformed parameters {\r+ vector[n_sim] mu;\r+ mu=beta0 + beta1*x;\r+ } + model {\r+ sigma~uniform(0,100);\r+ beta0~normal(0,1000);\r+ beta1~normal(0,1000);\r+ y~normal(mu,sigma);\r+ }\r+ + \u0026quot;\rSTAN models are written using an imperative programming language, which means that the order in which you write the elements in your model file matters, i.e. you first need to define your variables (e.g. integers, vectors, matrices, etc.), the constraints which define the range of values your variable can take (e.g. only positive values for standard deviations), and finally define the relationship among the variables (e.g. one is a liner function of another).\nA Stan model is defined by six program blocks:\n\rData (required). The data block reads external information – e.g. data vectors, matrices, integers, etc.\rTransformed data (optional). The transformed data block allows for preprocessing of the data – e.g. transformation or rescaling of the data.\rParameters (required). The parameters block defines the sampling space – e.g. parameters to which prior distributions must be assigned.\rTransformed parameters (optional). The transformed parameters block allows for parameter processing before the posterior is computed – e.g. tranformation or rescaling of the parameters.\rModel (required). In the model block we define our posterior distributions – e.g. choice of distributions for all variables.\rGenerated quantities (optional). The generated quantities block allows for postprocessing – e.g. backtranformation of the parameters using the posterior samples.\r\rFor this introduction I consider a very simple model which only requires the specification of four blocks in the STAN model. In the data block, I first define the size of the sample n_sim as a positive integer number using the expression int\u0026lt;lower=0\u0026gt; n_sim; then I declare the two variables y and x as reals (or vectors) with length equal to N. In the parameters block, I define the coefficients for the linear regression beta0 and beta1 (as two real numbers) and the standard deviation parameter sigma (as a positive real number). In the transformed parameters block, I define the conditional mean mu (a real vector of length N) as a linear function of the intercept beta0, the slope beta1, and the covariate x. Finally, in the model block, I assign weakly informative priors to the regression coefficients and the standard deviation parameters, and I model the outcome data y using a normal distribution indexed by the conditional mean mu and the standard deviation sigma parameters. In many cases, STAN uses sampling statements which can be vectorised, i.e. you do not need to use for loop statements.\nTo write and save the model as the text file “basic.mod.stan” in the current working directory, I use the writeLines function\n\u0026gt; writeLines(basic.mod, \u0026quot;basic.mod.stan\u0026quot;)\r\rPre-processing\rDefine the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in STAN\n\u0026gt; params=c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;)\r\u0026gt; inits=function(){list(\u0026quot;beta0\u0026quot;=rnorm(1), \u0026quot;beta1\u0026quot;=rnorm(1))}\rThe function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters in the object inits which is then passed to the stan function in rstan. However, for more complex models, this may not be immediate and a lot of trial and error may be required. However, STAN can automatically select the initial values for all parameters randomly. This can be achieved by setting inits=\"random\", which is then passed to the stan function in rstan.\nBefore using rstan for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable\n\u0026gt; library(rstan)\r\u0026gt; set.seed(123)\r\rFit the model\rNow, we can fit the model in STAN using the stan function in the rstan package and save it in the object basic.mod\n\u0026gt; basic.mod\u0026lt;-stan(data = datalist, pars = params, iter = 9000, + warmup = 1000, init = inits, chains = 2, file = \u0026quot;basic.mod.stan\u0026quot;)\rSAMPLING FOR MODEL \u0026#39;basic\u0026#39; NOW (CHAIN 1).\rChain 1: Chain 1: Gradient evaluation took 0 seconds\rChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 1: Adjust your expectations accordingly!\rChain 1: Chain 1: Chain 1: Iteration: 1 / 9000 [ 0%] (Warmup)\rChain 1: Iteration: 900 / 9000 [ 10%] (Warmup)\rChain 1: Iteration: 1001 / 9000 [ 11%] (Sampling)\rChain 1: Iteration: 1900 / 9000 [ 21%] (Sampling)\rChain 1: Iteration: 2800 / 9000 [ 31%] (Sampling)\rChain 1: Iteration: 3700 / 9000 [ 41%] (Sampling)\rChain 1: Iteration: 4600 / 9000 [ 51%] (Sampling)\rChain 1: Iteration: 5500 / 9000 [ 61%] (Sampling)\rChain 1: Iteration: 6400 / 9000 [ 71%] (Sampling)\rChain 1: Iteration: 7300 / 9000 [ 81%] (Sampling)\rChain 1: Iteration: 8200 / 9000 [ 91%] (Sampling)\rChain 1: Iteration: 9000 / 9000 [100%] (Sampling)\rChain 1: Chain 1: Elapsed Time: 0.078 seconds (Warm-up)\rChain 1: 0.593 seconds (Sampling)\rChain 1: 0.671 seconds (Total)\rChain 1: SAMPLING FOR MODEL \u0026#39;basic\u0026#39; NOW (CHAIN 2).\rChain 2: Chain 2: Gradient evaluation took 0 seconds\rChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\rChain 2: Adjust your expectations accordingly!\rChain 2: Chain 2: Chain 2: Iteration: 1 / 9000 [ 0%] (Warmup)\rChain 2: Iteration: 900 / 9000 [ 10%] (Warmup)\rChain 2: Iteration: 1001 / 9000 [ 11%] (Sampling)\rChain 2: Iteration: 1900 / 9000 [ 21%] (Sampling)\rChain 2: Iteration: 2800 / 9000 [ 31%] (Sampling)\rChain 2: Iteration: 3700 / 9000 [ 41%] (Sampling)\rChain 2: Iteration: 4600 / 9000 [ 51%] (Sampling)\rChain 2: Iteration: 5500 / 9000 [ 61%] (Sampling)\rChain 2: Iteration: 6400 / 9000 [ 71%] (Sampling)\rChain 2: Iteration: 7300 / 9000 [ 81%] (Sampling)\rChain 2: Iteration: 8200 / 9000 [ 91%] (Sampling)\rChain 2: Iteration: 9000 / 9000 [100%] (Sampling)\rChain 2: Chain 2: Elapsed Time: 0.078 seconds (Warm-up)\rChain 2: 0.594 seconds (Sampling)\rChain 2: 0.672 seconds (Total)\rChain 2: \rDifferent packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the bayesplot package (Gabry and Mahr (2017)) to obtain graphical diagnostics and results.\n\u0026gt; install.packages(\u0026quot;bayesplot\u0026quot;)\r\u0026gt; library(bayesplot)\rFor example, density and trace plots can be obtained by typing\n\u0026gt; mcmc_combo(as.array(basic.mod),regex_pars=\u0026quot;beta0|beta1\u0026quot;)\rBoth types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains).\n\r\rConclusions\rThis tutorial was simply a brief introduction on how simple linear regression models can be fitted using the Bayesian software STAN via the rstan package. Although this may seem a complex procedure compared with simply fitting a linear model under the frequentist framework, however, the real advantages of Bayesian methods become evident when the complexity of the analysis is increased (which is often the case in real applications). Indeed, the flexibility in Bayesian modelling allows to account for increasingly complex models in a relatively easy way. In addition, Bayesian methods are ideal when the interest is in taking into account the potential impact that different sources of uncertainty may have on the final results, as they allow the natural propagation of uncertainty throughout each quantity in the model.\n\rReferences\rGabry, J, and T Mahr. 2017. “Bayesplot: Plotting for Bayesian Models.” R Package Version 1.\n\rGelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” Journal of Educational and Behavioral Statistics 40 (5): 530–43.\n\rStan Development Team. 2018. “RStan: The R Interface to Stan.” http://mc-stan.org/.\n\r\r\r","date":1562206394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562206394,"objectID":"16a77cc23dd55d8ba6afba62a396f822","permalink":"/stan/basic-introduction-to-stan/super-basic-introduction-to-stan/","publishdate":"2019-07-03T21:13:14-05:00","relpermalink":"/stan/basic-introduction-to-stan/super-basic-introduction-to-stan/","section":"STAN","summary":"The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using STAN via R.\nPrerequisites:\n\rThe latest version of R, which can be downloaded and installed for Windows, Mac or Linux OS from the CRAN website\rI also strongly recommend to download and install Rstudio, an integrated development environment which provides an “user-friendly” interaction with R (e.g. many drop-down menus, tabs, customisation options)\r\rPreliminaries\rWhat is STAN?","tags":["linear regression","STAN","introduction"],"title":"Super basic introduction to STAN","type":"STAN"},{"authors":["Andrea Gabrio"],"categories":["R"],"content":"\rThe focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using OpenBUGS via R.\nPrerequisites:\n\rThe latest version of R, which can be downloaded and installed for Windows, Mac or Linux OS from the CRAN website\rI also strongly recommend to download and install Rstudio, an integrated development environment which provides an “user-friendly” interaction with R (e.g. many drop-down menus, tabs, customisation options)\r\rPreliminaries\rWhat is OpenBUGS?\rOpenBUGS is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (Spiegelhalter et al. (2007)). OpenBUGS is a free software based on the Bayesian inference Using Gibbs Sampling (informally BUGS) language at the base of WinBUGS but, unlike this program, is platform independent. The latest version of OpenBUGS can be dowloaded from the web repository and is available for different OS. There are different R packages which function as frontends for OpenBUGS. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the R2OpenBUGS package (Sturtz, Ligges, and Gelman (2010)) and show how to fit OpenBUGS models using this package.\n\rInstalling OpenBUGS and R2OpenBUGS\rInstall the latest version of OpenBUGS for your OS. Next, install the package R2OpenBUGS from within R or Rstudio, via the package installer or by typing in the command line\n\u0026gt; install.packages(\u0026quot;R2OpenBUGS\u0026quot;, dependencies = TRUE)\rThe dependencies = TRUE option will automatically install all the packages on which the functions in the R2OpenBUGS package rely.\n\r\rBasic model\rSimulate data\rFor an example dataset, I simulate my own data in R. I create a continuous outcome variable \\(y\\) as a function of one predictor \\(x\\) and a disturbance term \\(\\epsilon\\). I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) coefficients, i.e.\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon \\]\nThe R commands which I use to simulate the data are the following:\n\u0026gt; n.sim=100; set.seed(123)\r\u0026gt; x=rnorm(n.sim, mean = 5, sd = 2)\r\u0026gt; epsilon=rnorm(n.sim, mean = 0, sd = 1)\r\u0026gt; beta0=1.5\r\u0026gt; beta1=1.2\r\u0026gt; y=beta0 + beta1 * x + epsilon\rThen, I define all the data for JAGS in a list object\n\u0026gt; datalist=list(\u0026quot;y\u0026quot;,\u0026quot;x\u0026quot;,\u0026quot;n.sim\u0026quot;)\r\rModel file\rNow, I write the model for OpenBUGS and save it as a text file named \"basicmodbugs.txt\" in the current working directory\n\u0026gt; basic.mod= \u0026quot;\r+ model {\r+ #model\r+ for(i in 1:n.sim){\r+ y[i] ~ dnorm(mu[i], tau)\r+ mu[i] \u0026lt;- beta0 + beta1 * x[i]\r+ }\r+ #priors\r+ beta0 ~ dnorm(0, 0.01)\r+ beta1 ~ dnorm(0, 0.01)\r+ tau ~ dgamma(0.01,0.01)\r+ }\r+ \u0026quot;\rThe part of the model inside the for loop denotes the likelihood, which is evaluated for each individual in the sample using a Normal distribution parameterised by some mean mu and precision tau (where, precision = 1/variance). The covariate x is included at the mean level using a linear regression, which is indexed by the intercept beta0 and slope beta1 terms. The second part defines the prior distributions for all parameters of the model, namely the regression coefficients and the precision. Weakly informative priors are used since I assume that I do not have any prior knowledge about these parameters.\nTo write and save the model as the text file “basicmodbugs.txt” in the current working directory, I use the writeLines function\n\u0026gt; writeLines(basic.mod, \u0026quot;basicmodbugs.txt\u0026quot;)\r\rPre-processing\rDefine the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in OpenBUGS\n\u0026gt; params=c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;)\r\u0026gt; inits=function(){list(\u0026quot;beta0\u0026quot;=rnorm(1), \u0026quot;beta1\u0026quot;=rnorm(1), \u0026quot;tau\u0026quot;=rgamma(1,1,1))}\rThe function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters in the object inits which is then passed to the bugs function in R2OpenBUGS. However, for more complex models, this may not be immediate and a lot of trial and error may be required.\nBefore using R2OpenBUGS for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable\n\u0026gt; library(R2OpenBUGS)\r\u0026gt; set.seed(123)\r\rFit the model\rNow, we can fit the model in OpenBUGS using the bugs function in the R2openBUGS package and save it in the object basic.mod\n\u0026gt; basic.mod.bugs=bugs(data = datalist, inits = inits, + parameters.to.save = params, n.chains = 2, n.iter = 2000,\r+ n.burnin = 1000, model.file = \u0026quot;basicmodbugs.txt\u0026quot;)\rWhile the model is running, the function prints out some information related to the Bayesian graph (corresponding to the specification used for the model) underneath OpenBUGS, such as number of observed and unobserved nodes and graph size.\n\rPost-processing\rOnce the model has finished running, a summary of the posteiror estimates and convergence diagnostics for all parameters specified can be seen by typing print(basic.mod) or, alternatively,\n\u0026gt; print(basic.mod.bugs$summary)\r mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 1.5 0.293 0.99 1.3 1.5 1.7 2.1 1 1700\rbeta1 1.2 0.053 1.06 1.1 1.2 1.2 1.3 1 2000\rdeviance 278.8 2.439 276.00 277.1 278.2 280.0 285.2 1 2000\rThe posterior distribution of each parameter is summarised in terms of:\n\rThe mean, sd and some percentiles\rPotential scale reduction factor Rhat and effective sample size n.eff (Gelman (2013)). The first is a measure to assess issues in convergence of the MCMC algorithm (typically a value below \\(1.05\\) for all parameters is considered ok). The second is a measure which assesses the adequacy of the posterior sample (typically values close to the total number of iterations are desirable for all parameters).\r\rThe deviance is a goodness of fit statistic and is used in the construction of the “Deviance Information Criterion” or DIC (Spiegelhalter et al. (2014)), which is a relative measure of model comparison. The DIC of the model can be accessed by typing\n\u0026gt; basic.mod.bugs$DIC\r[1] 282\r\rDiagnostics\rMore diagnostics are available when we convert the model output into an MCMC object using the command\n\u0026gt; install.packages(\u0026quot;coda\u0026quot;)\r\u0026gt; library(coda)\r\u0026gt; basic.mod.mcmc.bugs=as.mcmc.list(basic.mod.bugs)\rDifferent packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the mcmcplots package (Curtis (2015)) to obtain graphical diagnostics and results.\n\u0026gt; install.packages(\u0026quot;mcmcplots\u0026quot;)\r\u0026gt; library(mcmcplots)\rFor example, density and trace plots can be obtained by typing\n\u0026gt; denplot(basic.mod.mcmc.bugs, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;))\r\u0026gt; traplot(basic.mod.mcmc.bugs, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;))\rBoth types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains).\n\r\rConclusions\rThis tutorial was simply a brief introduction on how simple linear regression models can be fitted using the Bayesian software OpenBUGS via the R2OpenBUGS package. Although this may seem a complex procedure compared with simply fitting a linear model under the frequentist framework, however, the real advantages of Bayesian methods become evident when the complexity of the analysis is increased (which is often the case in real applications). Indeed, the flexibility in Bayesian modelling allows to account for increasingly complex models in a relatively easy way. In addition, Bayesian methods are ideal when the interest is in taking into account the potential impact that different sources of uncertainty may have on the final results, as they allow the natural propagation of uncertainty throughout each quantity in the model.\n\rReferences\rCurtis, SM. 2015. “Mcmcplots: Create Plots from Mcmc Output.” R Package Version 0.4 2.\n\rGelman, Andrew. 2013. Bayesian Data Analysis. Chapman; Hall/CRC.\n\rSpiegelhalter, David J, Nicola G Best, Bradley P Carlin, and Angelika Van Der Linde. 2014. “The Deviance Information Criterion: 12 Years on.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 76 (3): 485–93.\n\rSpiegelhalter, David, Andrew Thomas, Nicky Best, and Dave Lunn. 2007. “OpenBUGS User Manual, Version 3.0. 2.” MRC Biostatistics Unit, Cambridge.\n\rSturtz, Sibylle, Uwe Ligges, and Andrew Gelman. 2010. “R2OpenBUGS: A Package for Running Openbugs from R.” URL Http://Cran. Rproject. Org/Web/Packages/R2OpenBUGS/Vignettes/R2OpenBUGS. Pdf.\n\r\r\r","date":1562119874,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562119874,"objectID":"dd38afcc7412084a4fc897a7e681d0f6","permalink":"/openbugs/basic-introduction-to-openbugs/super-basic-introduction-to-openbugs/","publishdate":"2019-07-02T21:11:14-05:00","relpermalink":"/openbugs/basic-introduction-to-openbugs/super-basic-introduction-to-openbugs/","section":"OpenBUGS","summary":"The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using OpenBUGS via R.\nPrerequisites:\n\rThe latest version of R, which can be downloaded and installed for Windows, Mac or Linux OS from the CRAN website\rI also strongly recommend to download and install Rstudio, an integrated development environment which provides an “user-friendly” interaction with R (e.g. many drop-down menus, tabs, customisation options)\r\rPreliminaries\rWhat is OpenBUGS?","tags":["linear regression","OpenBUGS","introduction"],"title":"Super basic introduction to OpenBUGS","type":"OpenBUGS"},{"authors":["Andrea Gabrio"],"categories":["conference"],"content":"I have just come back form my first Health Economists\u0026rsquo; Study Group (HESG) meeting, which this year was held at the University of East Anglia in the beautiful city of Norwich, south east of England, and where I presented some preliminary results from one of my on-going works. I have to say, it was a remarkable experience which I really liked thanks to a wonderful and welcoming environment. I had the pleasure to talk to many people from different research areas involved in health economics (both from academia and industry) and to see many different projects and works.\nI particularly enjoy the structure of the meeting, which requires some chair and discussant who have to present and discuss the paper of the authors, who are only allowed to provide some clarification if needed. At first I thought this structure of the sessions was strange, but after attending many sessions and experiencing this for my own paper, I feel that it is a very good way to encourage discussion about works from different people rather than just focussing on your own presentation. Plus, the weather and always sunny, it felt like Italy for a few days.\n\rThe beautiful Norwich\u0026rsquo;s cathedral\r\r\rOther nice people and colleagues from HEART and other UCL department came to HESG with me, including Caroline and Ekaterina (aka Katia), you can see them in thumbnail of this post. I was also pleased to meet Baptiste from LSHTM, who shares with me the interest in missing data methods for cost-effectiveness analysis and who presented some very nice work on that. I had the chance to give some feedback to him and he did the same for me. It felt so nice when we started discussing about some aspects of our analyses and after some minutes we simply lost track of time and everyone else disappeared. I also had the opportunity to talk about my work with the discussant of my session, Catrin Plumpton from the Centre for Health Economics and Medicines Evaluation, who gave me some nice feedback which I really appreciated, especially given her mathematical background.\nAn important contribution to the success of the meeting was also given by the wonderful organisation of the event, including an accommodation located very closely to the main building of the meeting, plenty of food provided during each day, a nice bus tour of the city and a wonderful conference dinner. I must thank all the people, who organised the event who were very extremely nice to us and who were always ready to help us for whatever need we had, with a special mention for Emma Mcmanus who was amazing.\nIn summary, everything was good. Well, almost. Going back to the works presented, as usual, the only less positive note that I would like to make is the almost total absence of Bayesian applications. Some authors mentioned that they used some popular Bayesian program, such as WinBUGS, but this was mainly related to the usual meta-analysis stuff which is pretty standardised. I hope next time I will be able to see more people going Bayesian as this is what I am.\n","date":1562112000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562326443,"objectID":"39454d30ea7c6161ee89bc00abd6b10b","permalink":"/post/hesg-summer-meeting-2019/","publishdate":"2019-07-03T00:00:00Z","relpermalink":"/post/hesg-summer-meeting-2019/","section":"post","summary":"I have just come back form my first Health Economists\u0026rsquo; Study Group (HESG) meeting, which this year was held at the University of East Anglia in the beautiful city of Norwich, south east of England, and where I presented some preliminary results from one of my on-going works. I have to say, it was a remarkable experience which I really liked thanks to a wonderful and welcoming environment. I had the pleasure to talk to many people from different research areas involved in health economics (both from academia and industry) and to see many different projects and works.","tags":["Academic","Industry","Health economics"],"title":"HESG Summer Meeting 2019","type":"post"},{"authors":["Andrea Gabrio"],"categories":["courses"],"content":"As member of the Health Economics Analysis and Research Methods Team (HEART), together with my colleagues, on Tuesday 2 July I took part in a 1-day introductory short course entitled “Understanding health economics in clinical trials”, which was designed and delivered by the team. HEART is a new group of health economists who are based in UCL’s Institute of Clinical Trials and Methodology (ICTM), led by Rachael Hunter, and is involved at different levels in the economic components of clinical trials in different trial units at UCL. This short course was aimed at ICTM staff who are not health economists (e.g. trial managers, CIs/PIs, statisticians, data managers, research assistants, etc.) and was designed in response to the need we have identified over the last few years in working on trials as well as in response to colleagues across ICTM. This course was unique as it was intended specifically for non health economists working in trials, who wish to better understand the health economics in their study, and/or the health economist on their study. The course used a mix of lectures, group discussions and practical exercises to help participants consolidate their learning and see how to apply information from the sessions to real studies. No prior knowledge of health economics was assumed.\nI believe the course was a success both in terms of the quality/quantity of the material covered during the six sessions throughout the day, as well as in terms of the positive feedback we received from the participants (almost entirely women, with the exception of two men). Many key and typically not well understood economic topics were discussed during the day, e.g. what are and how QALYs and costs are calculated, the potential limitations and issues of an economic analysis within a trial, or the role played by the protocol and analysis plan in the economic evaluation. My session was related to reporting and interpreting health economic results and I realised that most people who do not routinely deal with health economics may find difficult to grasp certain concepts or tools used in the economic analysis (e.g. what is a cost-effectiveness acceptability curve and how it can be computed). Nevertheless, I must admit that I was surprised by how many people were very motivated to learn these concepts and these \u0026ldquo;difficult\u0026rdquo; methods, often asking questions and making good comments (despite the fact that my session was the last of the course at the end of the day). We ran this course as a trial as we did not have clear ideas of what an optimal design should be or the number of topics that should be covered for this type of course. We are now confident that the course has a solid structure and that there is a clear demand to learn the basic concepts of health economics, at least among people involved in trial analyses. Following the successful delivery of the course, we are planning to replicate the experience in the future, improving certain aspects of the sessions based on the feedback we received and also considering to open the course to meet the demand of a wider audience.\nI have to say that this was an extremely positive experience for me as it was the first time I was involved in this type of projects. Me and my colleagues worked hard to design and prepare the different sessions of the course over the last few months, find the best way to link the arguments across the sessions, provide interesting group activities and materials for the practicals, etc. I have to thank all my colleagues who contributed to the promotion and realisation of this project, with a special mention for Caroline Clarke, who spent a lot of time and effort to organise the course and who personally contributed in giving one of the session of the course. Finally, I would also like to thank my colleague and health economist Ekaterina, with whom I had the pleasure to share the presentation and practical of my session in the course.\nPerhaps the only true negative aspect of the course was the absence of a Bayesian perspective, especially related to the interpretation of the results and the statistical methods that can be used to perform the analysis. Given the generally low familiarity of the people attending the course with statistics, I believe it was reasonable not to further confuse them with another new element into the picture. However, I truly hope that people will become more and more familiar with the importance of using tailored statistical methods in economic evaluations to avoid biased results, and from that point to justify a Bayesian approach, well, at least for me, the step is straightforward!.\n","date":1562112000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562153643,"objectID":"32751f66aea8e2316ec4874bd6c1d290","permalink":"/post/understanding-health-economics-in-clinical-trials/","publishdate":"2019-07-03T00:00:00Z","relpermalink":"/post/understanding-health-economics-in-clinical-trials/","section":"post","summary":"As member of the Health Economics Analysis and Research Methods Team (HEART), together with my colleagues, on Tuesday 2 July I took part in a 1-day introductory short course entitled “Understanding health economics in clinical trials”, which was designed and delivered by the team. HEART is a new group of health economists who are based in UCL’s Institute of Clinical Trials and Methodology (ICTM), led by Rachael Hunter, and is involved at different levels in the economic components of clinical trials in different trial units at UCL.","tags":["Academic","Clinical Trials","Health economics","introduction"],"title":"Understanding health economics in clinical trials","type":"post"},{"authors":[],"categories":null,"content":"","date":1562058000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562058000,"objectID":"cd757bbca2053fdc8ded948ff62c2cc3","permalink":"/talk/heartcourse2019/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/talk/heartcourse2019/","section":"talk","summary":"One day short course","tags":["Economic Evaluations","Clinical Trials","HEART"],"title":"Understanding Health Economics in Clinical Trials","type":"talk"},{"authors":["Andrea Gabrio"],"categories":["R"],"content":"\rThe focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using JAGS via R.\nPrerequisites:\n\rThe latest version of R, which can be downloaded and installed for Windows, Mac or Linux OS from the CRAN website\rI also strongly recommend to download and install Rstudio, an integrated development environment which provides an “user-friendly” interaction with R (e.g. many drop-down menus, tabs, customisation options)\r\rPreliminaries\rWhat is JAGS?\rJAGS or Just Another Gibbs Sampler is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (Plummer (2004)). JAGS is a free software based on the Bayesian inference Using Gibbs Sampling (informally BUGS) language at the base of WinBUGS/OpenBUGS but, unlike these programs, it is written in C++ and is platform independent. The latest version of JAGS can be dowloaded from Martyn Plummer’s repository and is available for different OS. There are different R packages which function as frontends for JAGS. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the R2jags package (Su et al. (2015)) and show how to fit JAGS models using this package.\n\rInstalling JAGS and R2jags\rInstall the latest version of JAGS for your OS. Next, install the package R2jags from within R or Rstudio, via the package installer or by typing in the command line\n\u0026gt; install.packages(\u0026quot;R2jags\u0026quot;, dependencies = TRUE)\rThe dependencies = TRUE option will automatically install all the packages on which the functions in the R2jags package rely.\n\r\rBasic model\rSimulate data\rFor an example dataset, I simulate my own data in R. I create a continuous outcome variable \\(y\\) as a function of one predictor \\(x\\) and a disturbance term \\(\\epsilon\\). I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) coefficients, i.e.\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon \\]\nThe R commands which I use to simulate the data are the following:\n\u0026gt; n.sim=100; set.seed(123)\r\u0026gt; x=rnorm(n.sim, mean = 5, sd = 2)\r\u0026gt; epsilon=rnorm(n.sim, mean = 0, sd = 1)\r\u0026gt; beta0=1.5\r\u0026gt; beta1=1.2\r\u0026gt; y=beta0 + beta1 * x + epsilon\rThen, I define all the data for JAGS in a list object\n\u0026gt; datalist=list(\u0026quot;y\u0026quot;,\u0026quot;x\u0026quot;,\u0026quot;n.sim\u0026quot;)\r\rModel file\rNow, I write the model for JAGS and save it as a text file named \"basic.mod.txt\" in the current working directory\n\u0026gt; basic.mod= \u0026quot;\r+ model {\r+ #model\r+ for(i in 1:n.sim){\r+ y[i] ~ dnorm(mu[i], tau)\r+ mu[i] = beta0 + beta1 * x[i]\r+ }\r+ #priors\r+ beta0 ~ dnorm(0, 0.01)\r+ beta1 ~ dnorm(0, 0.01)\r+ tau ~ dgamma(0.01,0.01)\r+ }\r+ \u0026quot;\rThe part of the model inside the for loop denotes the likelihood, which is evaluated for each individual in the sample using a Normal distribution parameterised by some mean mu and precision tau (where, precision = 1/variance). The covariate x is included at the mean level using a linear regression, which is indexed by the intercept beta0 and slope beta1 terms. The second part defines the prior distributions for all parameters of the model, namely the regression coefficients and the precision. Weakly informative priors are used since I assume that I do not have any prior knowledge about these parameters.\nTo write and save the model as the text file “basic.mod.txt” in the current working directory, I use the writeLines function\n\u0026gt; writeLines(basic.mod, \u0026quot;basic.mod.txt\u0026quot;)\r\rPre-processing\rDefine the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in JAGS\n\u0026gt; params=c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;)\r\u0026gt; inits=function(){list(\u0026quot;beta0\u0026quot;=rnorm(1), \u0026quot;beta1\u0026quot;=rnorm(1))}\rThe function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters. However, for more complex models, this may not be immediate and a lot of trial and error may be required. However, JAGS can automatically select the initial values for all parameters in an efficient way even for relatively complex models. This can be achieved by setting inits=NULL, which is then passed to the jags function in R2jags.\nBefore using R2jags for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable\n\u0026gt; library(R2jags)\r\u0026gt; set.seed(123)\r\rFit the model\rNow, we can fit the model in JAGS using the jags function in the R2jags package and save it in the object basic.mod\n\u0026gt; basic.mod=jags(data = datalist, inits = inits,\r+ parameters.to.save = params, n.chains = 2, n.iter = 2000, + n.burnin = 1000, model.file = \u0026quot;basic.mod.txt\u0026quot;)\rCompiling model graph\rResolving undeclared variables\rAllocating nodes\rGraph information:\rObserved stochastic nodes: 100\rUnobserved stochastic nodes: 3\rTotal graph size: 406\rInitializing model\rWhile the model is running, the function prints out some information related to the Bayesian graph (corresponding to the specification used for the model) underneath JAGS, such as number of observed and unobserved nodes and graph size.\n\rPost-processing\rOnce the model has finished running, a summary of the posteiror estimates and convergence diagnostics for all parameters specified can be seen by typing print(basic.mod) or, alternatively,\n\u0026gt; print(basic.mod$BUGSoutput$summary)\r mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff\rbeta0 1.5 0.294 0.95 1.3 1.5 1.7 2.1 1 2000\rbeta1 1.2 0.054 1.07 1.1 1.2 1.2 1.3 1 2000\rdeviance 278.8 2.475 276.03 277.1 278.2 279.9 285.1 1 2000\rThe posterior distribution of each parameter is summarised in terms of:\n\rThe mean, sd and some percentiles\rPotential scale reduction factor Rhat and effective sample size n.eff (Gelman (2013)). The first is a measure to assess issues in convergence of the MCMC algorithm (typically a value below \\(1.05\\) for all parameters is considered ok). The second is a measure which assesses the adequacy of the posterior sample (typically values close to the total number of iterations are desirable for all parameters).\r\rThe deviance is a goodness of fit statistic and is used in the construction of the “Deviance Information Criterion” or DIC (Spiegelhalter et al. (2014)), which is a relative measure of model comparison. The DIC of the model can be accessed by typing\n\u0026gt; basic.mod$BUGSoutput$DIC\r[1] 282\r\rDiagnostics\rMore diagnostics are available when we convert the model output into an MCMC object using the command\n\u0026gt; basic.mod.mcmc=as.mcmc(basic.mod)\rDifferent packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the mcmcplots package (Curtis (2015)) to obtain graphical diagnostics and results.\n\u0026gt; install.packages(\u0026quot;mcmcplots\u0026quot;)\r\u0026gt; library(mcmcplots)\rFor example, density and trace plots can be obtained by typing\n\u0026gt; denplot(basic.mod.mcmc, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;))\r\u0026gt; traplot(basic.mod.mcmc, parms = c(\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;))\rBoth types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains).\n\r\rConclusions\rThis tutorial was simply a brief introduction on how simple linear regression models can be fitted using the Bayesian software JAGS via the R2jags package. Although this may seem a complex procedure compared with simply fitting a linear model under the frequentist framework, however, the real advantages of Bayesian methods become evident when the complexity of the analysis is increased (which is often the case in real applications). Indeed, the flexibility in Bayesian modelling allows to account for increasingly complex models in a relatively easy way. In addition, Bayesian methods are ideal when the interest is in taking into account the potential impact that different sources of uncertainty may have on the final results, as they allow the natural propagation of uncertainty throughout each quantity in the model.\n\rReferences\rCurtis, SM. 2015. “Mcmcplots: Create Plots from Mcmc Output.” R Package Version 0.4 2.\n\rGelman, Andrew. 2013. Bayesian Data Analysis. Chapman; Hall/CRC.\n\rPlummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”\n\rSpiegelhalter, David J, Nicola G Best, Bradley P Carlin, and Angelika Van Der Linde. 2014. “The Deviance Information Criterion: 12 Years on.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 76 (3): 485–93.\n\rSu, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags.\n\r\r\r","date":1562033594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562033594,"objectID":"3e17280ebd0c2f5fb338cae0d6c210e8","permalink":"/jags/basic-introduction-to-jags/super-basic-introduction-to-jags/","publishdate":"2019-07-01T21:13:14-05:00","relpermalink":"/jags/basic-introduction-to-jags/super-basic-introduction-to-jags/","section":"JAGS","summary":"The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using JAGS via R.\nPrerequisites:\n\rThe latest version of R, which can be downloaded and installed for Windows, Mac or Linux OS from the CRAN website\rI also strongly recommend to download and install Rstudio, an integrated development environment which provides an “user-friendly” interaction with R (e.g. many drop-down menus, tabs, customisation options)\r\rPreliminaries\rWhat is JAGS?","tags":["linear regression","JAGS","introduction"],"title":"Super basic introduction to JAGS","type":"JAGS"},{"authors":[],"categories":null,"content":"","date":1560247200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560247200,"objectID":"56bfdbfa104cbed858b670d8954cd275","permalink":"/talk/albacete2019/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/talk/albacete2019/","section":"talk","summary":"Pre-conference workshop","tags":["Bayesian Statistics","Economic Evaluations","Missing Data"],"title":"Bayesian methods for addressing missing data in health economic evaluations","type":"talk"},{"authors":["A Gabrio","MJ Daniels","G Baio"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"6069360c460078f6e7c2128b47e6d1d2","permalink":"/publication/gabrio2019c/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/gabrio2019c/","section":"publication","summary":"Trial-based economic evaluations are typically performed on cross-sectional variables, derived from the responses for only the completers in the study, using methods that ignore the complexities of utility and cost data (e.g. skewness and spikes). We present an alternative and more efficient Bayesian parametric approach to handle missing longitudinal outcomes in economic evaluations, while accounting for the complexities of the data. We specify a flexible parametric model for the observed data and partially identify the distribution of the missing data with partial identifying restrictions and sensitivity parameters. We explore alternative nonignorable scenarios through different priors for the sensitivity parameters, calibrated on the observed data. Our approach is motivated by, and applied to, data from a trial assessing the cost-effectiveness of a new treatment for intellectual disability and challenging behaviour.","tags":["Missing Data","Bayesian Statistics","Economic Evaluations"],"title":"A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations","type":"publication"},{"authors":["A Gabrio","G Baio","A Manca"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"58b637ad9c4945c334e3dbfbf3a955fe","permalink":"/publication/gabrio2019b/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/gabrio2019b/","section":"publication","summary":"The evidence produced by healthcare economic evaluation studies is a key component of any health technology assessment (HTA) process designed to inform resource allocation decisions in a budget limited context. To improve the quality (and harmonize the generation process) of such evidence, many HTA agencies have established methodological guidelines describing the normative framework inspiring their decision-making process. The information requirements that economic evaluation analyses for HTA must satisfy typically involve the use of complex quantitative syntheses of multiple available datasets, handling mixtures of aggregate and patient-level information, and the use of sophisticated statistical models for the analysis of non-Normal data (e.g. time-to-event, quality of life and costs). Much of the recent methodological research in economic evaluation for healthcare has developed in response to these needs, in terms of sound statistical decision-theoretic foundations, and is increasingly being formulated within a Bayesian paradigm. The rationale for this preference lies in the fact that by taking a probabilistic approach, based on decision rules and available information, a Bayesian economic evaluation study can explicitly account for relevant sources of uncertainty in the decision process and produce information to identify an optimal course of actions.  Moreover, the Bayesian approach naturally allows the incorporation of an element of judgement or evidence from different sources (e.g.~expert opinion or multiple studies) into the analysis. This is particularly important when, as often occurs in economic evaluation for HTA, the evidence base is sparse and requires some inevitable mathematical modelling to bridge the gaps in the available data. The availability of free and open source software in the last two decades has greatly reduced the computational costs and facilitated the application of Bayesian methods and has the potential to improve the work of modellers and regulators alike, thus advancing the fields of economic evaluation of health care interventions. This chapter provides an overview of the areas where Bayesian methods have contributed to the address the methodological needs that stem from the normative framework adopted by a number of HTA agencies.","tags":["Economic Evaluations","Bayesian Statistics"],"title":"Bayesian Statistical Economic Evaluation Methods for Health Technology Assessment","type":"publication"},{"authors":["A Gabrio","AJ Mason","G Baio"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"e4282ca40728b2e297a1792c11cd58b6","permalink":"/publication/gabrio2019a/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/publication/gabrio2019a/","section":"publication","summary":"Economic evaluations from individual level data are an important component of the process of technology appraisal, with a view to informing resource allocation decisions. A critical problem in these analyses is that both effectiveness and cost data typically present some complexity (eg, nonnormality, spikes, and missingness) that should be addressed using appropriate methods. However, in routine analyses, standardised approaches are typically used, possibly leading to biassed inferences. We present a general Bayesian framework that can handle the complexity. We show the benefits of using our approach with a motivating example, the MenSS trial, for which there are spikes at one in the effectiveness and missingness in both outcomes. We contrast a set of increasingly complex models and perform sensitivity analysis to assess the robustness of the conclusions to a range of plausible missingness assumptions. We demonstrate the flexibility of our approach with a second example, the PBS trial, and extend the framework to accommodate the characteristics of the data in this study. This paper highlights the importance of adopting a comprehensive modelling approach to economic evaluations and the strategic advantages of building these complex models within a Bayesian framework. ","tags":["Missing Data","Economic Evaluations","Bayesian Statistics"],"title":"A Full Bayesian Model to Handle Structural Ones and Missingness in Economic Evaluations from Individual-Level Data","type":"publication"},{"authors":[],"categories":null,"content":"","date":1527858000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527858000,"objectID":"ea9f4848b6ad6d57bd35e16b751992b0","permalink":"/talk/priment2018/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/talk/priment2018/","section":"talk","summary":"Invited presentation","tags":["Economic Evaluations","Missing Data"],"title":"A Bayesian Parametric Approach to Handle Nonignorable Missingness in Economic Evaluations","type":"talk"},{"authors":null,"categories":null,"content":"Modelling Framework We extend and adapt the modelling frameworks typically used for the analysis of football data and propose a novel Bayesian hierarchical modelling framework for the analysis and prediction of volleyball results in regular seasons. Three different sub-models or \u0026ldquo;modules\u0026rdquo; form our framework: (1) The module of the observed number of points scored by the two opposing teams in a match ($y_h$ and $y_a$); (2) the module of the binary indicator for the number of sets played ($d^s$); (3) the module of the binary indicator for the winner of the match ($d^m$). These three modules are jointly modelled using a flexible Bayesian parametric approach, which allows to fully propagate the uncertainty for each unobserved quantity and to assess the predictive performance of the model in a relatively easy way. In the following, we describe the notation and the model used in each of the three modules.\nModule 1: Modelling the Scoring Intensity In the first module of the framework, we model the number of points scored by the home and away team in the $i$-th match of the season $\\boldsymbol y=(y_{hi},y_{ai})$ using two independent Poisson distributions\n\\[ y_{hi} \\sim Poisson(\\theta_{hi}), \\]\n\\[ y_{ai} \\sim Poisson(\\theta_{ai}), \\]\nconditionally on the set of parameters $\\boldsymbol \\theta=(\\theta_{hi},\\theta_{ai})$, representing the scoring intensity in the $i$-th match for the home and away team, respectively. These parameters are then modelled using the log-linear regressions\n\\[ log(\\theta_{hi}) =\\mu + \\lambda + att_{h(i)} + def_{a(i)}, \\]\n\\[ log(\\theta_{ai}) =\\mu + att_{a(i)} + def_{h(i)}, \\]\nwhich corresponds to a Poisson log-linear model. Within these formulae, $\\mu$ is a constant, while $\\lambda$ can be identified as the home effect and represents the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The overall offensive and defensive performances of the $k$-th team is captured by the parameters $att$ and $def$, whose nested indexes $h(i), a(i)=1,\\ldots,K$ identify the home and away team in the $i$-th game of the season, where $K$ denotes the total number of the teams.\nWe then expand the modelling framework to incorporate match-specific statistics related to the offensive and defensive performances of the home and away teams. More specifically, the effects associated with the attack intensity of the home teams and the defence effect of the away teams are:\n\\[ att_{h(i)} =\\alpha_{0h(i)} + \\alpha_{1h(i)}att^{eff}_{hi} + \\alpha_{2h(i)}ser^{eff}_{hi}, \\]\n\\[ def_{a(i)} =\\beta_{0a(i)} + \\beta_{1a(i)}def^{eff}_{ai} + \\beta_{2a(i)}blo^{eff}_{ai}. \\]\nWe omit the index $i$ from the terms to the left-hand side of the above formulae to ease notation, i.e. $att_{h(i)}=att_{h(i)i}$ and $def_{a(i)}=def_{a(i)i}$. The overall offensive effect of the home teams is a function of a baseline team specific parameter $\\alpha_{0h(i)}$, and the attack and serve efficiencies of the home team, whose impact is captured by the parameters $\\alpha_{1h(i)}$ and $\\alpha_{2h(i)}$. The overall defensive effect of the away team is a function of a baseline team-specific parameter $\\beta_{0a(i)}$, and the defence and block efficiencies of the away team, whose impact is captured by the parameters $\\beta_{1a(i)}$ and $\\beta_{2a(i)}$, respectively. Similarly, the effects associated with the attack intensity of the away teams and the defence effect of the home teams are:\n\\[ att_{a(i)} =\\alpha_{0a(i)} + \\alpha_{1a(i)}att^{eff}_{ai}+ \\alpha_{2a(i)}ser^{eff}_{ai}, \\]\n\\[ def_{h(i)} =\\beta_{0h(i)} + \\beta_{1h(i)}def^{eff}_{hi}+ \\beta_{2h(i)}blo^{eff}_{hi}, \\]\nTo achieve identifiability of the model, a set of parametric constraints needs to be imposed. We impose sum-to-zero constraints on the team-specific parameters, i.e. we set $\\sum_{k=1}^{K}\\alpha_{jk}=0$ and $\\sum_{k=1}^{K}\\beta_{jk}=0$, for $k=1,\\ldots,K$ and $j=(0,1,2)$. Under this set of constraints, the overall offensive and defensive effects of the teams are expressed as departures from a team of average offensive and defensive performance. Within a Bayesian framework, prior distributions need to be specified for all random parameters in the model. Weakly informative Normal distributions centred at $0$ with a relatively large variances are specified for the fixed effect parameters.\nModule 2: Modelling the Probability of Playing 5 Sets In the second module, we explicitly model the chance of playing $5$ sets in the $i$-th match of the season, i.e. the sum of the sets won by the home ($s_{hi}$) and away ($s_{ai}$) team is equal to $5$. This is necessary when generating predictions in order to correctly assign the points to the winning/losing teams throughout the season and evaluate the rankings of the teams at the end of the season. We model the indicator variable $d^s_{i}$, taking value $1$ if $5$ sets were played in the $i-$th match and $0$ otherwise, using a Bernoulli distribution\n\\[ d^s_{i}:=\\mathbb{I}(s_{hi}+s_{ai}=5)\\sim\\mbox{Bernoulli}(\\pi^s_{i}), \\]\nwhere\n\\[ logit(\\pi^s_{i})= \\gamma_0 + \\gamma_1y_{hi} + \\gamma_2y_{ai}.\n\\]\nModule 3: Modelling the Probability of Winning the Match The last module deals with the chance of the home team to win the $i$-th match, i.e. the total number of sets won by the home team ($s_{hi}$) is larger than that of the away team ($s_{ai}$) \u0026ndash; we note that we could have also equivalently decided to model the chance of the away team to win the $i$-th match. This part of the model is again necessary when predicting the results for future matches, since the team associated with the higher number of points scored in the $i$-th match may not correspond to the winning team. We model the indicator variable $d^m_{i}$, taking value $1$ if the home team won the $i-$th match and $0$ otherwise, using another Bernoulli distribution\n\\[ d^m_{i}:=\\mathbb{I}(s_{hi}\u0026gt;s_{ai}) \\sim\\mbox{Bernoulli}(\\pi^m_{i}), \\]\nwhere\n\\[ logit(\\pi^m_{i})= \\eta_0 + \\eta_1y_{hi} + \\eta_2y_{ai} + \\eta_3 d^s_i. \\]\nThe next figure shows a graphical representation of the modelling framework proposed.\n\rGraphical representation of the modelling framework.\r\r\rThe framework corresponds to a joint distribution for all the observed quantities which are explicitly modelled. This is factored into the product of the marginal distribution of the total number of points scored by the two teams in each match, Module 1 \u0026ndash; $p(\\boldsymbol y)$, the conditional distribution of the probability of playing $5$ sets in a match given $\\boldsymbol y$, Module 2 \u0026ndash; $p(d^s_i \\mid \\boldsymbol y)$, and the conditional probability of winning the match given $\\boldsymbol y$ and $d^s_i$, Module 3 \u0026ndash; $p(d^m_i\\mid \\boldsymbol y, d^s_i)$. Module 1 also includes the different in-game statistics as covariates in the model. These are related to the either the offensive (serve and attack efficiency) or defensive (defence and block efficiency) effects of the home and away teams in each match of the season, and are respectively denoted in the graph as $\\boldsymbol x^{att}_{ti}=(ser^{eff}_{ti}, att^{eff}_{ti})$ and $\\boldsymbol x^{def}_{ti}=(def^{eff}_{ti}, blo^{eff}_{ti})$ to ease notation, for $t=(h,a)$.\nAccounting for the multilevel correlation \nAlthough the individual-level correlation between the observable variables $y_{hi}$ and $y_{ai}$ is taken into account through the hierarchical structure of the framework, a potential limitation of the model is that it ignores the possible multilevel correlation between the team-specific offensive $\\alpha_{jk}$ and defensive $\\beta_{jk}$ coefficients, for $j=(0,1,2)$ and $k=1,\\ldots,K$. In an alternative analysis, we account for the multilevel correlation using Inverse-Wishart distributions on the covariance matrix of the team specific parameters $ \\boldsymbol \\Sigma_{\\boldsymbol \\alpha}$ and $ \\boldsymbol \\Sigma_{\\boldsymbol \\beta}$, which are scaled in order to facilitate the specification of the priors.\nResults Overall, the predicted results from both the basic and the scaled IW model seem to replicate the observed data relatively well for most of the teams. The total number of points scored and conceded are similar between the observed and replicated data, with the teams scoring (conceding) the most being also associated with the highest replicated points scored (conceded) and vice versa. Relatively small discrepancies are observed between the results of the two models for some of the teams. The total number of wins and league points are almost identical between the observed and replicated data, with the scaled IW model being associated with slightly m ore accurate predictions compared with the basic model.\nThe following figure compares the cumulative points derived from the observed results throughout the season (the black line) and the predictions from both the basic model (in red), and the scaled Inverse-Wishart model (in blue).\n\rPosterior predictive validation of the basic (red) and IW (blue) model with respect to the observed data (black).\r\r\rFor almost all teams the predicted results are relatively close to the observed data and suggest a good performance of both models.\nConclusions To our knowledge, this is the first modelling framework which jointly allows to predict team rankings and the outcomes of the matches during a season in volleyball. The two alternative specifications implemented in our analysis show generally good predictive performances; between the two models, the scaled IW model seems to be slightly more accurate compared with the basic model, but is also associated with a higher level of complexity.\n","date":1524787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524787200,"objectID":"9bc5501ddff6119ed99359ce345d8b23","permalink":"/project/volleyball/","publishdate":"2018-04-27T00:00:00Z","relpermalink":"/project/volleyball/","section":"project","summary":"Modelling Framework We extend and adapt the modelling frameworks typically used for the analysis of football data and propose a novel Bayesian hierarchical modelling framework for the analysis and prediction of volleyball results in regular seasons. Three different sub-models or \u0026ldquo;modules\u0026rdquo; form our framework: (1) The module of the observed number of points scored by the two opposing teams in a match ($y_h$ and $y_a$); (2) the module of the binary indicator for the number of sets played ($d^s$); (3) the module of the binary indicator for the winner of the match ($d^m$).","tags":["Bayesian Modelling"],"title":"Bayesian Hierarchical Models for the Prediction of Volleyball Results","type":"project"},{"authors":[],"categories":null,"content":"","date":1517824800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517824800,"objectID":"6595e4086447a19af1feb033b3a8201a","permalink":"/talk/hesymposium2018/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/talk/hesymposium2018/","section":"talk","summary":"Invited presentation","tags":["Bayesian Statistics","Economic Evaluations","Missing Data"],"title":"A Full Bayesian Model to Handle Structural Ones and Missingness in Health Economic Evaluations from Individual-Level Data","type":"talk"},{"authors":null,"categories":null,"content":"missingHE is a R package, available on CRAN which is aimed at providing some useful tools to analysts in order to handle missing outcome data under a full Bayesian framework in economic evaluations. The package relies on the R package R2jags to implement Bayesian methods via the statistical software JAGS to obtain inferences using Markov Chain Monte Carlo (MCMC) methods. Different types of missing data models are implemented in the package, including selection models, pattern mixture models and hurdle models. A range of parametric distributions can be specified when modelling the typical outcomes in an trial-based economic evaluations, namely the effectiveness and cost variabels, while simultaneously incorporating different assumptions about the missingness mechanism, which allows to easily perform sensitvity analysis to a range of alternative missing data assumptions according to the modelling choices selected by the user.\nmissingHE also provides functions, taken and adapted from other R packages, to assess the results of each type of model, including summaries of the posterior distributions of each model parameter, range and imputations of the missing values, different types of model diagnostics to assess convergence of the algorithm, posterior predictive checks, model assessment measures based on the fit to the observed data, and a general summary of the economic evaluations, including the results from probabilistic sensitivity analyses which are automatically performed within a Bayesian modelling framework.\nFor example, the function plot, when applied to the output of a model fitted using missingHE, produces graphs which compare the observed and imputed values for both cost and benefit measures in each treatment group to detect possible concerns about the plausibility of the imputations.\n\rExample of a graphical output from missingHE\r\r\rMore information, including new updates, about missingHE can be found at this dedicated page\n","date":1517788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517788800,"objectID":"28eb798e628125aa2f3089fd4e6e3707","permalink":"/project/missinghe/","publishdate":"2018-02-05T00:00:00Z","relpermalink":"/project/missinghe/","section":"project","summary":"An R package to handle missingness in health economic evaluations","tags":["Missing Data","Bayesian Modelling"],"title":"missingHE","type":"project"},{"authors":["A Gabrio","AJ Mason","G Baio"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"bbef5e786cc53704ee1f92102539d5e4","permalink":"/publication/gabrio2017/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/gabrio2017/","section":"publication","summary":"Cost-effectiveness analyses (CEAs) alongside randomised controlled trials (RCTs) are increasingly designed to collect resource use and preference-based health status data for the purpose of healthcare technology assessment. However, because of the way these measures are collected, they are prone to missing data, which can ultimately affect the decision of whether an intervention is good value for money. We examine how missing cost and effect outcome data are handled in RCT-based CEAs, complementing a previous review (covering 2003-2009, 88 articles) with a new systematic review (2009-2015, 81 articles) focussing on two different perspectives. First, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. We propose to address this issue by means of a quality evaluation scheme, providing a structured approach that can be used to guide the collection of information, elicitation of the assumptions, choice of methods and considerations of possible limitations of the given missingness problem. Second, we review the description of the missing data, the statistical methods used to deal with them and the quality of the judgement underpinning the choice of these methods. Our review shows that missing data in within-RCT CEAs are still often inadequately handled and the overall level of information provided to support the chosen methods is rarely satisfactory. ","tags":["Missing Data","Economic Evaluations","Literature Review"],"title":"Handling Missing Data in Within-Trial Cost-Effectiveness Analysis: A Review with Future Recommendations","type":"publication"},{"authors":null,"categories":null,"content":"Introduction We performed a systematic literature review that assesses the quality of the information reported and type of methods used to handle missing outcome data in trial-based economic evaluations. The purpose of this review is to critically appraise the current literature in within-trial CEAs with respect to the quality of the information reported and the methods used to deal with missingness for both effectiveness and costs. The review complements previous work, covering 2003-2009 (88 articles) with a new systematic review, covering 2009-2015 (81 articles) and focuses on two~perspectives.\nFirst, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. We propose to address this issue by means of a quality evaluation scheme, providing a structured approach that can be used to guide the collection of information, formulation of the assumptions, choice of methods, and considerations of possible limitations for the given missingness problem. Second, we review the description of the missing data, the statistical methods used to deal with them and the quality of the judgement underpinning the choice of these methods.\nQuality Evaluation Scheme In order to judge whether missing data in CEAs have been adequately handled, we assembled guidelines from previous review articles on how information relating to the missing data should be reported. In particular, we defined three broad components of the analysis that are related to the description of the missingness problem (Description), details of the methods used to address it (Methods) and a discussion on the uncertainty in the conclusions resulting from the missingness (Limitations). For each component, information that is considered to be vital for transparency is listed under key considerations, while other details that could usefully be provided as supplementary material are suggested under optimal considerations.\nUsing the list of key considerations, we determine whether null (all key considerations absent), partial (one or more key considerations absent) or full (all key considerations present) information has been provided for each component. The set of key considerations is defined to ensure a full assessment of the impact that missingness may have on the final conclusions of the analysis with respect to all three components. However, providing a certain level of information on one component (e.g.~full information on Description) typically has a different impact on the results with respect to providing the same level of information on another component (e.g.~full information on Limitations). Based on this, we suggest computing a numerical score that weights each component by the impact that it may have on the final results to summarise the overall information provided on missingness.\nDifferent score values are calculated based on whether full, partial or null information content is provided in each component and by weighting the three components in a ratio of 3:2:1 (Description: Method: Limitations). This weighting scheme has been chosen according to the impact that each component is likely to have on the final conclusions based on assumptions that we deemed to be~reasonable. Specifically, the Limitations component typically has the least importance among the three because of its limited impact on the conclusions. In the same way, the Description component has potentially a higher impact on the results than the Method component as it generally drives the choice for the initial assumptions about the missingness.\nFinally, the relevance of the scores in terms of decision analysis is mainly associated with a qualitative assessment of the articles. Therefore, we suggest converting the scores into ordered grades (A-E) to evaluate the studies based on the overall information reported on the handling of the missing data. Studies that are graded in the top categories should be associated with a higher degree of confidence in their results, whereas more caution should be given in the consideration of results coming from studies that are graded in the bottom categories. When qualitatively assessing the articles, the different grading assigned to each of them could be an indication of a lack in the robustness of the conclusions provided due to missingness uncertainty. With respect to the quality assessment of the studies, the aggregation of the quality scores on the components of the analysis (Description, Method and Limitations) into ordered grades could lead to some loss of information compared with the direct use of the quality scores on each component. However, merging the scores into a fewer number of categories ensures a relatively easy comparison of the quality of the information provided across the three analysis components and provides a useful indication about the different degree of confidence to assign to the results obtained by each study.\nThe Figure below shows a visual representation of the grade (and score) assignment in the quality evaluation scheme. Although the importance between the different components is subjective, the chosen structure represents a reasonable and relatively straightforward assessment scheme.\n\rQuality Evaluation Scheme.\r\r\rThe articles reviewed for the two periods are presented and compared by type of analysis performed. First, the base-case methods are considered, i.e.~those used in the main analysis. Second, any alternative methods in these analyses are discussed; when present, these assess the robustness of the results obtained in the main analysis against departures from the initial assumptions on missingness.\nSummary of the findings Our review is based on a sample of recently published studies and should therefore provide a picture of current missing data handling in within-trial CEAs. However, the quality assessment of the articles is based on the information reported in the articles. It is possible that authors had assessed the robustness of their conclusions to the missing data using alternative approaches that were not reported in the published version because of space limitations in journals. In these cases, it is important that on-line appendices and supplementary material are used to report these~alternatives. In our literature review, information about missing data information and methods was available from $4$ and $9$ on-line supplementary materials for the period 2003-2009 and 2009-2015, respectively. Both the larger number of on-line materials and more detailed information reported about missingness handling in the analyses indicate an increased use of this tool in the later period (2009-2015) compared to the first period (2003-2009).\nDescriptive Review \rMissingness methods by outcome and period.\r\r\rFrom the comparison of the base-case methods used for the costs and effects between 2009 and 2015, the Figure above shows a marked reduction in the number of methods not clearly described for the effects, compared to those for the costs. A possible reason for this is that, while clinical effectiveness measures are often collected through self-reported questionnaires, which are naturally prone to missingness, cost measures rely more on clinical patient files which may ensure a higher completeness rate. It was not possible to confirm this interpretation in the reviewed studies due to the high proportions of articles not clearly reporting the missing rates in both 2003-2009 and 2009-2015 periods, for effects ($\\approx 45%$ and $\\approx 38%$) and costs ( $\\approx 50%$ and $\\approx 62%$). In addition, clinical outcomes are almost invariably the main objective of RCTs and are usually subject to more advanced and standardised analyses. Arguably, costs are often considered as an add-on to the standard trial: for instance, sample size calculations are almost always performed with the effectiveness measure as the only outcome of interest. Consequently, missing data methods are less frequently well thought through for the analysis of the costs. However, this situation is likely to change as cost data from different perspectives (e.g. caregivers, patients, society, etc.) are being increasingly used in trials, leading to the more frequent adoption of self-report cost data which may start to exhibit similar missingness characteristics to effect data.\nThe review identified only a few articles using more than one alternative method. In addition, these analyses are typically conducted without any clear justification about their underlying missing data assumptions and may therefore not provide a concrete assessment of the impact of missingness uncertainty. This situation indicates a gap in the literature associated with an under-implementation of sensitivity analysis, which may significantly affect the whole decision-making process outcome, under the perspective of a body who is responsible for providing recommendations about the implementation of alternative interventions for health care matters.\nLimiting the assessment of missingness assumptions to a single case is unlikely to provide a reliable picture of the underlying mechanism. This, in turn, may have a significant impact on the CEA and mislead its conclusions, suggesting the implementation of non-cost-effective treatments. Robustness analyses assess the sensitivity of the results to alternative missing data methods but do not justify the choice of these methods and their underlying assumptions about missingness which may therefore be inappropriate in the specific context analysed. By contrast, sensitivity analyses, which rely on external information to explore plausible alternative methods and missingness assumptions, represent an important and more appropriate tool to provide realistic assessments of the impact of missing data uncertainty on the final conclusions.\nQuality assessment Generally speaking, most of the reviewed papers achieved an unsatisfactory quality score under the Quality Evaluation Scheme. Indeed, the benchmark area on the top-right corner of the graphs is barely reached by less than $7%$ of the articles, both for cost and effect data.\nOverall, the proportions of the studies associated with the lowest category (E) prevails in the majority of the years, with a similar pattern over time between missing costs and effects. All the articles that are associated with the top category (A) belong to the period 2013-2015, with the highest proportions of articles falling in this category being observed in 2015 for both outcomes. The opportunity of reaching such a target might be precluded by the choice of the method adopted, which may not be able to support less restrictive assumptions about missingness, even when this would be desirable. As a result, when simple methods cannot be fully justified it is necessary to replace them with more flexible ones that can relax assumptions and incorporate more alternatives. In settings such as those involving MNAR, sensitivity analysis might represent the only possible approach to account for the uncertainty due to the missingness in a principled way. However, due to the lack of studies either performing a sensitivity analysis or providing high quality scores on the assumptions, missingness is not adequately addressed in most studies. This could have the serious consequence of imposing too restrictive assumptions about missingness and affect the outcome of decision making.\nConclusions Our review shows, over time, a significant change from more to less restrictive methods in terms of the assumptions on the missingness mechanism. This is an encouraging movement towards a more suitable and careful missing data analysis. The results from the disaggregated analysis by year of publication in the later period (2009-2015) indicates the rise of a better and more transparent approach to handle missingness in the latest years of the review, especially in 2015. In particular, compared to the previous years, the articles reviewed from 2015 are associated with a higher proportion of MI methods used in the base-case analysis, a substantial increase in the number of robustness methods implemented, and a better quality score assignment.\nNevertheless, improvements are still needed as, overall, only a small number of articles provide transparent information about the missing data and almost no study performs a sensitivity~analysis. These failings are probably due to the fact that the implications of using methods that do not handle missingness in a principled way are not well-known among practitioners. In addition, the choice of the missing data methods may also be guided by their ease of implementation in standard software packages rather than methodological reasons. This is a potentially serious issue for bodies such as the NICE who use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new treatment options.\nThe Quality Evaluation Scheme represents a valuable tool to improve missing data handling. By carefully thinking about each component in the analysis we are forced to explicitly consider all the assumptions we make about missingness and assess the impact of their variation on final conclusions. The main advantage is a more comparable formalisation of the uncertainty as well as a better indication of possible issues in assessing the cost-effectiveness of new treatments.\n","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"264b6a73ec81e06fc20279d323edf442","permalink":"/project/missing-data-review/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/project/missing-data-review/","section":"project","summary":"With [Alexina Mason](https://www.lshtm.ac.uk/aboutus/people/mason.alexina) and [Gianluca Baio](https://www.ucl.ac.uk/statistics/people/gianlucabaio)","tags":["Missing Data"],"title":"Missingness Methods in trial-based CEA","type":"project"},{"authors":[],"categories":null,"content":"","date":1473346800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473346800,"objectID":"a177d4c811e51520cedd357bee093b9f","permalink":"/talk/euhea2016/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/talk/euhea2016/","section":"talk","summary":"Contributed presentation","tags":["Economic Evaluations","Missing Data"],"title":"Handling Missing Data in Within-Trial Cost-Effectiveness Analysis: a Review with Future Recommendations","type":"talk"},{"authors":null,"categories":null,"content":"Introduction The type of data used in economic evaluations typically come from a range of sources, whose evidence is combined to inform HTA decision-making. Traditionally, relative effectiveness data are derived from randomised controlled clinical trials (RCTs), while healthcare resource utilisation, costs and preference-based quality of life data may come from the same study that estimated the clinical effectiveness or not. A number of HTA agencies have developed their own methodological guidelines to support the generation of the evidence required to inform their decisions. In this context, the primary role of economic evaluation for HTA is not the estimation of the quantities of interest (e.g. the computation of point or interval estimation, or hypothesis testing), but to aid decision making. The implication of this is that the standard frequentist analyses that rely on power calculations and $P$-values to estimate statistical and clinical significance, typically used in RCTs, are not well-suited for addressing these HTA requirements.\nIt has been argued that, to be consistent with its intended role in HTA, economic evaluation should embrace a decision-theoretic paradigm and develop ideally within a Bayesian statistical framework to inform two decisions\n whether the treatments under evaluation are cost-effective given the available evidence and whether the level of uncertainty surrounding the decision is acceptable (i.e. the potential benefits are worth the costs of making the wrong decision).  This corresponds to quantify the impact of the uncertainty in the evidence on the entire decision-making process (e.g. to what extent the uncertainty in the estimation of the effectiveness of a new intervention affects the decision about whether it is paid for by the public provider).\nBayesian methods in HTA There are several reasons that make the use of Bayesian methods in economic evaluations particularly appealing. First, Bayesian modelling is naturally embedded in the wider scheme of decision theory; by taking a probabilistic approach, based on decision rules and available information, it is possible to explicitly account for relevant sources of uncertainty in the decision process and obtain an optimal course of action. Second, Bayesian methods allow extreme flexibility in modelling using computational algorithms such as Markov Chain Monte Carlo (MCMC) methods; this allows to handle in a relatively easy way the generally sophisticated structure of the relationships and complexities that characterise effectiveness, quality of life and cost data. Third, through the use of prior distributions, the Bayesian approach naturally allows the incorporation of evidence from different sources in the analysis (e.g. expert opinion or multiple studies), which may improve the estimation of the quantities of interest; the process is generally referred to as evidence synthesis and finds its most common application in the use of meta-analytic tools. This may be extremely important when, as it often happens, there is only some partial (imperfect) information to identify the model parameters. In this case analysts are required to develop chain-of-evidence models. When required by the limitations in the evidence base, subjective prior distributions can be specified based on the synthesis and elicitation of expert opinion to identify the model, and their impact on the results can be assessed by presenting or combining the results across a range of plausible alternatives. Finally, under a Bayesian approach, it is straightforward to conduct sensitivity analysis to properly account for the impact of uncertainty in all inputs of the decision process; this is a required component in the approval or reimbursement of a new intervention for many decision-making bodies, such as NICE in the UK.\nThe general process of conducting a Bayesian analysis (with a view of using the results of the model to perform an economic evaluation) can be broken down in several steps, which are graphically summarized in the Figure below.\n\rProcess of health economic evaluation.\r\r\rThe starting point is the identification of the decision problem, which defines the objective of the economic evaluation (e.g. the interventions being compared, the target population, the relevant time horizon). In line with the decision problem, a statistical model is constructed to describe the (by necessity, limited) knowledge of the underlying clinical pathways. This implies, for example, the definition of suitable models to describe variability in potentially observed data (e.g. the number of patients recovering from the disease because of a given treatment), as well as the epistemic uncertainty in the population parameters (e.g.~the underlying probability that a random individual in the target population is cured, if given the treatment under study). At this point, all the relevant data are identified, collected and quantitatively sytnthesised to derive the estimates of the input parameters of interest for the model.\nThese parameter estimates (and associated uncertainties) are then fed to the economic model, with the objective of obtaining some relevant summaries indicating the benefits and costs for each intervention under evaluation. Uncertainty analysis represents some sort of detour from the straight path going from the statistical model to the decision analysis: if the output of the statistical model allowed us to know with perfect certainty the true value of the model parameters, then it would be possible to simply run the decision analysis and make the decision. Of course, even if the statistical model were the true representation of the underlying data generating process (which it most certainly is not), because the data may be limited in terms of length of follow up, or sample size, the uncertainty in the value of the model parameters would still remain. This parameter (and structural) uncertainty is propagated throughout the whole process to evaluate its impact on the decision-making. In some cases, although there might be substantial uncertainty in the model inputs, this may not turn out to modify substantially the output of the decision analysis, i.e. the new treatment would be deemed as optimal irrespectively. In other cases, however, even a small amount of uncertainty in the inputs could be associated with very serious consequences. In such circumstances, the decision-maker may conclude that the availbale evidence is not sufficient to decide on which intervention to select and require more information before a decision can be made.\nThe results of the above analysis can be used to inform policy makers about two related decisions:\n whether the new intervention is to be considered (on average) value for money, given the evidence base available at the time of decision, and whether the consequences (in terms of net health loss) of making the wrong decision would warrant further research to reduce this decision uncertaint.  While the type and specification of the statistical and economic models vary with the nature of the underlying data (e.g. individual (ILD) level versus aggregated (ALD) data, the decision and uncertainty analyses have a more standardised set up.\nConclusions HTA has been slow to adopt Bayesian methods; this could be due to a reluctance to use prior opinions, unfamiliarity, mathematical complexity, lack of software, or conservatism of the healthcare establishment and, in particular, the regulatory~authorities. However, the use of Bayesian approach has been increasingly advocated as an efficient tool to integrate statistical evidence synthesis and parameter estimation with probabilistic decision analysis in an unified framework for HTA. This enables a transparent evidence-based decision modelling, reflecting the uncertainty and the structural relationships in all the available~data.\nWith respect to trial-based analyses, the flexibility and modularity of the Bayesian modelling structure are well-suited to jointly account for the typical complexities that affect ILD. In addition, prior distributions can be used as convenient means to incorporate external information into the model when the evidence from the data is limited or absent (e.g. for missing values). In the context of evidence synthesis, the Bayesian approach is particularly appealing in that it allows for all the uncertainty and correlation induced by the often heterogeneous nature of the evidence (either ALD only or both ALD and ILD) to be synthesised in a way that can be easily integrated within a decision modelling framework.\nThe availability and spread of Bayesian software among practitioners since the late 1990s, such as OpenBUGS or JAGS, has greatly improved the applicability and reduced the computational costs of these models. Thus, analysts are provided with a powerful framework, which has been termed comprehensive decision modelling, for simultaneously estimating posterior distributions for parameters based on specified prior knowledge and data evidence, and for translating this into the ultimate measures used in the decision analysis to inform cost-effectiveness conclusions.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"971bed50bf7791bf6b8581ec7b3373d8","permalink":"/project/health-economics/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/health-economics/","section":"project","summary":"With [Andrea Manca](https://www.york.ac.uk/che/staff/research/andrea-manca/) and [Gianluca Baio](https://www.ucl.ac.uk/statistics/people/gianlucabaio)","tags":["Health Economics","Bayesian Modelling"],"title":"Bayesian Methods for Health Technology Assessment","type":"project"},{"authors":null,"categories":null,"content":"Modelling Framework We propose a unified Bayesian framework that jointly accounts for the typical complexities of the data (e.g. correlation, skewness, spikes at the boundaries and missingness), and that can be implemented in a relatively easy way.\nConsider the usual cross-sectional bivariate outcome formed by the QALYs and total cost variables $(e_{it}, c_{it})$ calculated for the $i-$th person in group $t$ of the trial. To simplify the notation, unless necessary, we suppress the treatment indicator $t$. We specify the joint distribution $p(e_i,c_i)$ as\n\\[ p(e_i,c_i) = p(c_i)p(e_i\\mid c_i) = p(e_i)p(c_i\\mid e_i) \\]\nwhere, for example, $p(e_i)$ is the marginal distribution of the QALYs and $p(c_i\\mid e_i)$ is the conditional distribution of the costs given the QALYs. Note that, although the two factorisations are mathematically equivalent, the choice of which to use has different practical implications. From a statistical point of view, the factorisations require the specifications of different statistical models, e.g. $p(e_i)$ or $p(e_i\\mid c_i)$, which may have different approximation errors. From a clinical point of view, the two versions make different assumptions about the casual relationships between the outcomes, i.e. either $e_i$ determines $c_i$ or vice versa. We describe our analysis under the assumption that the costs are determined by the effectiveness measures and therefore we specify the joint distribution $p(e_i,c_i)$ in terms of a marginal distribution for the QALYs and a conditional distribution for the costs.\nFor each individual we consider a marginal distribution $p(e_i \\mid \\boldsymbol \\theta_e)$ indexed by a set of parameters $\\boldsymbol \\theta_e$ comprising a location $\\boldsymbol \\phi_{ie}$ and a set of *ancillary* parameters $\\boldsymbol\\psi_e$ typically including some measure of *marginal* variance $\\sigma^2_e$. We can model the location parameter using a generalised linear structure, e.g.\n\\[ g_e(\\phi_{ie})= \\alpha_0 ,,[+ \\ldots] \\]\nwhere $\\alpha_0$ is the intercept and the notation $[+\\ldots]$ indicates that other terms (e.g. quantifying the effect of relevant covariates) may or may not be included. In the absence of covariates or assuming that a centered version $x_i^{\\star} = (x_i - \\bar{x})$ is used, the parameter $\\mu_e = g_e^{-1}(\\alpha_0)$ represents the population average QALYs. For the costs, we consider a conditional model $p(c_i\\mid e_i,\\boldsymbol\\theta_c)$, which explicitly depends on the QALYs, as well as on a set of quantities $\\boldsymbol\\theta_c$, again comprising a location $\\phi_{ic}$ and ancillary parameters $\\boldsymbol \\psi_{c}$. For example, when normal distributions are assumed for both $p(e_i \\mid \\boldsymbol \\theta_e)$ and $p(c_i \\mid e_i, \\boldsymbol \\theta_c)$, i.e. bivariate normal on both outcomes, the ancillary parameters $\\boldsymbol\\psi_c$ include a *conditional* variance $\\tau^2_c$, which can be expressed as a function of the marginal variance $\\sigma^2_c$. More specifically, the conditional variance of $p(c_i \\mid e_i, \\boldsymbol \\theta_c)$ is a function of the marginal effectiveness and cost variances and has the closed form $\\tau^2_c=\\sigma^2_c - \\sigma^2_e \\beta^2$, where $\\beta=\\rho \\frac{\\sigma_c}{\\sigma_e}$ and $\\rho$ is the parameter capturing the correlation between the variables.\nThe location can be modelled as a function of the QALYs as\n\\[ g_c(\\phi_{ic}) = \\beta_{0} + \\beta_{1}(e_{i}-\\mu_{e}),,[+\\ldots] \\]\nHere, $(e_i-\\mu_e)$ is the centered version of the QALYs, while $\\beta_{1}$ quantifies the correlation between costs and QALYs. Assuming other covariates are either also centered or absent, $\\mu_c = g_c^{-1}(\\beta_{0})$ is the estimated population average cost. The Figure below shows a graphical representation of the general modelling framework.\n\rModelling framework.\r\r\rThe QALYs and cost distributions are represented in terms of combined modules, the blue and the red boxes, in which the random quantities are linked through logical relationships. This ensures the full characterisation of the uncertainty for each variable in the model. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in either or both the modules.\nThe proposed framework allows jointly tackling of the different complexities that affect the data in a relatively easy way by means of its modular structure and flexible choice for the distributions of the QALYs and cost variables. Using the MenSS trial as motivating example, we start from the original analysis and expand the model using alternative specifications that progressively account for an increasing number of complexities in the outcomes. We specifically focus on appropriately modelling spikes at the boundary and missingness, as they have substantial implications in terms of inferences and, crucially, cost-effectiveness results.\nExample Three model specifications are considered and applied to QALY data from a RCT case study: 1) Normal marginal for the QALYs and Normal conditional for the costs (which is identical to a Bivariate Normal distribution for the two outcomes); 2) Beta marginal for the QALYs and Gamma conditional for the costs; and 3) Hurdle Model. The following Figure shows the observed QALYs in both treatment groups (indicated with black crosses) as well as summaries of the posterior distributions for the imputed values, obtained from each model. Imputations are distinguished based on whether the corresponding baseline utility value is observed or missing (blue or red lines and dots, respectively) and are summarised in terms of posterior mean and $90%$ HPD intervals.\n\rImputed QALYs under alternative model specifications.\r\r\rThere are clear differences in the imputed values and corresponding credible intervals between the three models in both treatment groups. Neither the Bivariate Normal nor the Beta-Gamma models produce imputed values that capture the structural one component in the data. In addition, as to be expected, the Bivariate Normal fails to respect the natural support for the observed QALYs, with many of the imputations exceeding the unit threshold bound. These unrealistic imputed values highlight the inadequacy of the Normal distribution for the data and may lead to distorted inferences. Conversely, imputations under the Hurdle Model are more realistic, as they can replicate values in the whole range of the observed data, including the structural ones. Imputed unit QALYs with no discernible interval are only observed in the intervention group due to the original data composition, i.e. individuals associated with a unit baseline utility and missing QALYs are almost exclusively present in the intervention group.\nConclusions We have presented a flexible Bayesian framework that can handle the typical complexities affecting outcome data in CEA, while also being relatively easy to implement using freely available Bayesian software. This is a key advantage that can encourage practitioners to move away from likely biased methods and promote the use of our framework in routine analyses. In conclusion, the proposed framework can:\n Jointly model costs and QALYs; Account for skewness and structural values; Assess the robustness of the results under a set of differing missingness assumptions.  The original contribution of this work consists in the joint implementation of methods that account for the complexities of the data within a unique and flexible framework that is relatively easy to apply. In the next chapter we will take a step forward in the analysis and present a longitudinal model that can use all observed utility and cost data in the analysis, explore alternative nonignorable missing data assumptions, while simultaneously handling the complexities that affect the data.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"159ff484bc950a41748df648cbd474ee","permalink":"/project/bayesian-modelling/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/bayesian-modelling/","section":"project","summary":"With [Alexina Mason](https://www.lshtm.ac.uk/aboutus/people/mason.alexina) and [Gianluca Baio](https://www.ucl.ac.uk/statistics/people/gianlucabaio)","tags":["Bayesian Modelling","Health Economics"],"title":"Bayesian Modelling for Health Economic Evaluations","type":"project"},{"authors":null,"categories":null,"content":"Introduction Economic evaluation alongside Randomised Clinical Trials (RCTs) is an important and increasingly popular component of the process of technology appraisal. The typical analysis of individual level data involves the comparison of two interventions for which suitable measures of clinical benefits and costs are observed on each patient enrolled in the trial at different time points throughout the follow up. Individual level data from RCTs are almost invariably affected by missingness. The recorded outcome process is often incomplete due to individuals who drop out or are observed intermittently throughout the study, causing some observations to be missing. In most applications, the economic evaluation is performed on the cross-sectional variables, computed using only the data from the individuals who are observed at each time point in the trial (completers), with at most limited sensitivity analysis to missingness assumptions. This, however, is an extremely inefficient approach as the information from the responses of all partially observed subjects is completely lost and it is also likely biased unless the completers are a random sample of the subjects on each arm. The problem of missingness is often embedded within a more complex framework, which makes the modelling task in economic evaluations particularly challenging. Specifically, the effectiveness and cost data typically present a series of complexities that need to be simultaneously addressed to avoid biased results.\nUsing a recent randomised trial as our motivating example, we present a Bayesian parametric model for conducting inference on a bivariate health economic longitudinal response. We specify our model to account for the different types of complexities affecting the data while accommodating a sensitivity analysis to explore the impact of alternative missingness assumptions on the inferences and on the decision-making process for health technology assessment.\nStandard approach To perform the economic evaluation, aggregated measures for both utilities and costs are typically derived from the longitudinal responses recorded in the study. QALYs ($e_{it}$) and total costs ($c_{it}$) measures are computed as:\n\\[ e_{it}=\\sum_{j=1}^{J}(u_{ijt}+u_{ij-1t})\\frac{\\delta_{j}}{2} ;;; \\text{and} ;;;\\ c_{it}=\\sum_{j=1}^{J}c_{ijt}, \\]\nwhere $t$ denotes the treatment group, while $\\delta_{j}=\\frac{\\text{Time}_{j}-\\text{Time}_{j-1}}{\\text{Unit of time}}$ is the percentage of the time unit (typically one year) which is covered between time $j-1$ and $j$ in the trial. The economic evaluation is then performed by applying some parametric model $p(e_{it},c_{it}\\mid \\boldsymbol \\theta)$, indexed by a set of parameters $\\boldsymbol \\theta$, to these cross-sectional quantities, typically using linear regression methods to account for the imbalance in some baseline variables between treatments. We note that the term cross-sectional here refers to analyses based on variables derived from the combination of repeated measurements collected at different times over the trial duration and not on data collected at a single point in time. Finally, QALYs and total costs population mean values are derived from the model:\n\\[ \\mu_{et} = \\text{E}\\left(e_{it} \\mid \\boldsymbol \\theta\\right) ;;; \\text{and} ;;; \\mu_{ct} = \\text{E}\\left(c_{it} \\mid \\boldsymbol \\theta \\right). \\]\nThe differences in $\\mu_{et}$ and $\\mu_{ct}$ between the treatment groups represent the quantities of interest in the economic evaluation and are used in assessing the relative cost-effectiveness of the interventions. This modelling approach has the limitation that $\\mu_{et}$ and $\\mu_{ct}$ are derived based only on the completers in the study and does not assess the robustness of the results to a range of plausible missingness assumptions. The model also fails to account for the different complexities that affect the utility and cost data in the trial: from the correlation between variables to the skewness and the presence of structural values (zero for the costs and one for the utilities) in both outcomes.\nLongitudinal model to deal with missingness We propose an alternative approach to deal with a missing bivariate outcome in economic evaluations, while simultaneously allowing for the different complexities that typically affect utility and cost data. Our approach includes a longitudinal model that improves the current practice by taking into account the information from all observed data as well as the time dependence between the responses.\nLet $\\boldsymbol u_i=(u_{i0},\\ldots,u_{iJ})$ and $\\boldsymbol c_i=(c_{i0},\\ldots,c_{iJ})$ denote the vectors of utilities and costs that were supposed to be observed for subject $i$ at time $j$ in the study, with $j \\in {0,1,J}$. We denote with $\\boldsymbol y_{ij}=(u_{ij},c_{ij})$ the bivariate outcome for subject $i$ formed by the utility and cost pair at time $j$. We group the individuals according to the missingness patterns and denote with $\\boldsymbol r_{ij}=(r^u_{ij},r^c_{ij})$ a pair of indicator variables that take value $1$ if the corresponding outcome for subject $i$ at time $j$ is observed and $0$ otherwise. We denote with $\\boldsymbol r_i = (\\boldsymbol r_{i0}, \\ldots, \\boldsymbol r_{iJ})$ the missingness pattern to which subject $i$ belongs, where each pattern is associated with different values for $\\boldsymbol r_{ij}$.\nWe then define our modelling strategy and factor the joint distribution for the response and missingness as:\n\\[ p(\\boldsymbol y, \\boldsymbol r \\mid \\boldsymbol \\omega) = p(\\boldsymbol y^{\\boldsymbol r}_{obs}, \\boldsymbol r \\mid \\boldsymbol \\omega)p(\\boldsymbol y^{\\boldsymbol r}_{mis} \\mid \\boldsymbol y^{\\boldsymbol r}_{obs}, \\boldsymbol r, \\boldsymbol \\omega) \\]\nwhere $\\boldsymbol y^{\\boldsymbol r}_{obs}$ and $\\boldsymbol y^{\\boldsymbol r}_{mis}$ indicate the observed and missing responses within pattern $\\boldsymbol r$, respectively. This is the extrapolation factorisation and factors the joint into two components, of which the extrapolation distribution $p(\\boldsymbol y^{\\boldsymbol r}_{mis} \\mid \\boldsymbol y^{\\boldsymbol r}_{obs}, \\boldsymbol r, \\boldsymbol \\omega)$ remains unidentified by the data in the absence of unverifiable assumptions about the full data.\nTo specify the observed data distribution $p(\\boldsymbol y^{\\boldsymbol r}_{obs}, \\boldsymbol r \\mid \\boldsymbol \\omega)$ we use a working model $p^{\\star}$ for the joint distribution of the response and missingness. Essentially, the idea is to use the working model $p^{\\star}(\\boldsymbol y, \\boldsymbol{r} \\mid \\boldsymbol \\omega)$ to draw inferences about the distribution of the observed data $p(\\boldsymbol y^{\\boldsymbol r}_{obs}, \\boldsymbol r \\mid \\boldsymbol \\omega)$ by integrating out the missing responses:\n\\[ p(\\boldsymbol y^{\\boldsymbol r}_{obs}, \\boldsymbol r \\mid \\boldsymbol \\omega) = \\int p^{\\star}(\\boldsymbol y, \\boldsymbol{r} \\mid \\boldsymbol \\omega)d \\boldsymbol y^{\\boldsymbol r}_{mis}. \\]\nThis approach avoids direct specification of the joint distribution of the observed and missing data $p(\\boldsymbol y, \\boldsymbol r\\mid \\boldsymbol \\omega)$, which has the undesirable consequence of identifying the extrapolation distribution with assumptions that are difficult to check. Indeed, since we use $p^{\\star}(\\boldsymbol y, \\boldsymbol r \\mid \\boldsymbol \\omega)$ only to obtain a model for $p(\\boldsymbol y^{\\boldsymbol r}_{obs}, \\boldsymbol r \\mid \\boldsymbol \\omega)$ and not as a basis for inference, the extrapolation distribution is left unidentified. Any inference depending on the observed data distribution may be obtained using the working model as the true model, with the advantage that it is often easier to specify a model for the the full data $p(\\boldsymbol y,\\boldsymbol r)$ compared with a model for the observed data $p(\\boldsymbol y^{\\boldsymbol r}_{obs},\\boldsymbol r)$.\nWe specify $p^{\\star}$ using a pattern mixture approach, factoring the joint $p(\\boldsymbol y,\\boldsymbol r \\mid \\boldsymbol \\omega)$ as the product between the marginal distribution of the missingness patterns $p(\\boldsymbol r\\mid \\boldsymbol \\psi)$ and the distribution of the response conditional on the patterns $p(\\boldsymbol y\\mid \\boldsymbol r,\\boldsymbol \\theta)$, respectively indexed by the distinct parameter vectors $\\boldsymbol \\psi$ and $\\boldsymbol \\theta$. If missingness is monotone it is possible to summarise the patterns by dropout time and directly model the dropout process. Unfortunately, as it often occurs in trial-based health economic data, missingness in the case study is mostly nonmonotone and the sparsity of the data in most patterns makes it infeasible to fit the response model within each pattern, with the exception of the completers ($\\boldsymbol r = \\boldsymbol 1$). Thus, we decided to collapse together all the non-completers patterns ($\\boldsymbol r \\neq \\boldsymbol 1$) and fit the model separately to this aggregated pattern and to the completers. The joint distribution has three components. The first is given by the model for the patterns and the model for the completers ($\\boldsymbol r = \\boldsymbol 1$), where no missingness occurs. The second component is a model for the observed data in the collapsed patterns $\\boldsymbol r \\neq \\boldsymbol 1$ that, together with the first component, form the observed data distribution. The last component is the extrapolation distribution.\nBecause the targeted quantities of interest can be derived based on the marginal utility and cost means at each time $j$, in our analysis we do not require the full identification of $p(\\boldsymbol y^{\\boldsymbol r}_{mis} \\mid \\boldsymbol y^{\\boldsymbol r}_{obs}, \\boldsymbol r,\\boldsymbol \\xi)$. Instead, we only partially identify the extrapolation distribution using partial identifying restrictions. Specifically, we only require the identification of the marginal means for the missing responses in each pattern. We identify the marginal mean of $\\boldsymbol y^{\\boldsymbol r}_{mis}$ using the observed values, averaged across $\\boldsymbol r^\\prime \\neq \\boldsymbol 1$, and some sensitivity parameters $\\boldsymbol \\Delta = (\\Delta_u,\\Delta_c)$. Therefore, we compute the marginal means by averaging only across the observed components in pattern ${\\boldsymbol r}^\\prime$ and ignore the components that are missing.\nWe start by setting a benchmark assumption with $\\boldsymbol \\Delta = \\boldsymbol 0$, and then explore the sensitivity of the results to alternative scenarios by using different prior distributions on $\\boldsymbol \\Delta$, calibrated on the observed data. This provides a convenient benchmark scenario from which departures can be explored using alternative informative priors on $\\boldsymbol \\Delta$. Once the working model has been fitted to the observed data and the extrapolation distribution has been identified, the overall marginal mean for the response model can be computed by marginalising over $\\boldsymbol r$, i.e. $\\text{E}\\left[\\boldsymbol Y\\right] = \\sum_{\\boldsymbol r} p(\\boldsymbol r)\\text{E}\\left[\\boldsymbol Y \\mid \\boldsymbol r \\right]$.\nModelling framework The distribution of the observed responses $\\boldsymbol y_{ijt}=(u_{ijv},c_{ijt})$ is specified in terms of a model for the utility and cost variables at time $j=0,1,2$, which are jointly modelled without using a multilevel approach and separately by treatment group. In particular, the joint distribution for $\\boldsymbol y_{ijt}$ is specified as a series of conditional distributions that capture the dependence between utilities and costs as well as the time dependence.\nFollowing the recommendations from the published literature, we account for the skewness using Beta and Log-Normal distributions for the utilities and costs, respectively. Since the Beta distribution does not allow for negative values, we scaled the utilities on $[0,1]$ through the transformation $u^{\\star}_{ij}=\\frac{u_{ij}-\\text{min}(\\boldsymbol u_{j})}{\\text{max}(\\boldsymbol u_{j})-\\text{min}(\\boldsymbol u_{j})}$, and fit the model to these transformed variables. To account for the structural values $u_{ij} = 1$ and $c_{ij} = 0$ we use a hurdle approach by including in the model the indicator variables $d^u_{ij}:=\\mathbb{I}(u_{ij}=1)$ and $d^c_{ij}:=\\mathbb{I}(c_{ij}=0)$, which take value $1$ if subject $i$ is associated with a structural value at time $j$ and 0 otherwise. The probabilities of observing these values, as well as the mean of each variable, are then modelled conditionally on other variables via linear regressions defined on the logit or log scale. Specifically, at time $j=1,2$, the probability of observing a zero and the mean costs are modelled conditionally on the utilities and costs at the previous times, while the probability of observing a one and the mean utilities are modelled conditionally on the current costs (also at $j=0$) and the utilities at the previous times (only at $j=1,2$). The model is summarised by the following Figure.\n\rLongitudinal model for missingness.\r\r\rWe use partial identifying restrictions to link the observed data distribution $p(\\boldsymbol y_{obs},\\boldsymbol r)$ to the extrapolation distribution $p(\\boldsymbol y_{mis} \\mid \\boldsymbol y_{obs},\\boldsymbol r)$ and consider interpretable deviations from a benchmark scenario to assess how inferences are driven by our assumptions. Specifically, we identify the marginal mean of the missing responses in each pattern $\\boldsymbol y^{\\boldsymbol r}_{mis}$ by averaging across the corresponding components that are observed and add the sensitivity parameters $\\boldsymbol \\Delta_j$.\nWe define $\\boldsymbol \\Delta_j=(\\Delta_{c_{j}},\\Delta_{u_{j}})$ to be time-specific location shifts at the marginal mean in each pattern and set $\\boldsymbol \\Delta_j = \\boldsymbol 0$ as the benchmark scenario. We then explore departures from this benchmark using alternative priors on $\\boldsymbol \\Delta_j$, which are calibrated using the observed standard deviations for costs and utilities at each time $j$ to define the amplitude of the departures from $\\boldsymbol \\Delta_j=\\boldsymbol 0$.\nConlcusions Missingness represents a threat to economic evaluations as, when dealing with partially-observed data, any analysis makes assumptions about the missing values that cannot be verified from the data at hand. Trial-based analyses are typically conducted on cross-sectional quantities, e.g. QALYs and total costs, which are derived based only on the observed data from the completers in the study. This is an inefficient approach which may discard a substantial proportion of the sample, especially when there is a relatively large number of time points, where individuals are more likely to have some missing value or to drop out from the study. In addition, when there are systematic differences between the responses of the completers and non-completers, which is typically the case when dealing with self-reported outcomes in trial-based analyses, the results based only on the former may be biased and mislead the final assessment. A further concern is that routine analyses typically rely on standard models that ignore or at best fail to properly account for potentially important features in the data such as correlation, skewness, and the presence of structural values.\nOur framework represents a considerable step forward for the handling of missingness in economic evaluations compared with the current practice, which typically relies on methods that assume an ignorable MAR and rarely conducts sensitivity analysis to MNAR departures. Nevertheless, further improvements are certainly possible. For example, a potential area for future work is to increase the flexibility of our approach through a semi-parametric or nonparametric specification for the observed data distribution, which would allow a weakening of the model assumptions and likely further improve the fit of the model to the observed data and address sparse patterns in an automated way. As for the extrapolation distribution, alternative identifying restrictions that introduce the sensitivity parameters via the conditional mean (rather than the marginal mean) could be considered, and their impact on the conclusions assessed in a sensitivity analysis.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"64ed545a4c3df615031db54dc419fefc","permalink":"/project/missing-data/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/missing-data/","section":"project","summary":"With [Michael Daniels](http://users.stat.ufl.edu/~daniels/) and [Gianluca Baio](https://www.ucl.ac.uk/statistics/people/gianlucabaio)","tags":["Missing Data"],"title":"Nonignorable Missingness Models in HTA","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"38ae74f025de673d7e353e9b7732fa99","permalink":"/rubric/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/rubric/","section":"","summary":"","tags":null,"title":"Missing Data","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"/research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"","tags":null,"title":"Research","type":"widget_page"}]