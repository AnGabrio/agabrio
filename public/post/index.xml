<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My blog on Andrea Gabrio</title>
    <link>/post/</link>
    <description>Recent content in My blog on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>`{year}`</copyright>
    <lastBuildDate>Fri, 07 Aug 2020 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>What is Bayesian inference?</title>
      <link>/post/update-august/</link>
      <pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-august/</guid>
      <description>


&lt;p&gt;What is probability ? The answer to this question is generally acknowledged to be the one that respects the so called Kolmogorov axioms which can be brutally simplified to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Probabilities are non-negative&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Probabilities sum to one&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The joint probability of disjoint events is the sum of the probabilities of the events&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One of the ways in which Bayesian statistics differs from classical statistics is in the &lt;strong&gt;interpretation&lt;/strong&gt; of probability. Differences in interpretation continue to be controversial, are critical to the distinction between Bayesian and non-Bayesian statistics.&lt;/p&gt;
&lt;p&gt;In classical statistics probability is often understood as a &lt;em&gt;property of the phenomenon being studied&lt;/em&gt;: for instance, the probability that a tossed coin will come up heads is a characteristic of the coin. Thus, by tossing the coin many times under more or less identical conditions, and noting the result of each toss, we can estimate the probability of a head, with the precision of the estimate monotonically increasing with the number of tosses. In this view, probability is the limit of a long-run, relative frequency; i.e. if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is an event of interest (e.g. the coin lands heads up) then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \text{Pr}(A) = \lim_{n\rightarrow\infty}\frac{m}{n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is the probabilty of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is the number of times we observe the event &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of repetitions. Given this definition of probability, we can understand why classicial statistics is sometimes referred to as &lt;em&gt;frequentist&lt;/em&gt; and &lt;em&gt;objectivist&lt;/em&gt;. However, historians of science stress that at least two notions of probability were under development from the late &lt;span class=&#34;math inline&#34;&gt;\(1600\)&lt;/span&gt;s onwards: the objectivist view described above, and a subjectivist view. With regard to the latter, we can consider different ‘degrees’ of belief to interpret probability, ‘from the very neighborhourhood of certainty and demonstration, quite down to improbability and unlikeliness, even to the confines of impossibility’. For Locke, ‘Probability is likeliness to be true’, a definition in which (repeated) games of chance play no part. For Bernoulli, ‘Probability is degree of certainty and differs from absolute certainty as the part differs from the whole’, it being unequivocal that the ‘certainty’ referred to is a state of mind, but, critically, (1) varied from person to person (depending on one’s knowledge and experience) and (2) was quantifiable. Ramsey and de Finetti, working independently, showed that subjective probability is not just any set of subjective beliefs, but beliefs that conform to the axioms of probability. The Ramsey-de Finetti Theorem states that if &lt;span class=&#34;math inline&#34;&gt;\(p_1, p_2, \ldots\)&lt;/span&gt; are a set of betting quotients on hypotheses &lt;span class=&#34;math inline&#34;&gt;\(h_1, h_2,\ldots\)&lt;/span&gt; , then if the &lt;span class=&#34;math inline&#34;&gt;\(p_j\)&lt;/span&gt; do not satisfy the probability axioms, there exists a betting strategy and a set of stakes such that whoever follows this betting strategy will lose a finite sum whatever the truth values of the hypotheses turn out to be. In de Finetti’s terminology, subjective probabilities that fail to conform to the axioms of probability are incoherent or inconsistent. Thus, subjective probabilities are whatever a particular person believes, provided they satisfy the axioms of probability. Thus, if I do not update my subjective beliefs in light of new information (data) in a manner consistent with the probability axioms, and you can convince me to gamble with you, you have the opportunity to take advantage of my irrationality, and are guaranteed to profit at my expense. That is, while probability may be subjective, Bayes Rule governs how rational people should update subjective beliefs.&lt;/p&gt;
&lt;div id=&#34;subjective-probability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Subjective probability&lt;/h2&gt;
&lt;p&gt;Bayesian probability statements are thus about states of mind over states of the world, and not about states of the world per se. Indeed, whatever one believes about determinism or chance in social processes, the meaningful uncertainty is that which resides in our brains, upon which we will base decisions and actions. This is why, in one of the more memorable and strongest statements of the subjectivist position, de Finetti writes &lt;strong&gt;probability does not exist&lt;/strong&gt;: “The abandonment of superstitious beliefs about … Fairies and Witches was an essential step along the road to scientific thinking. Probability, too, if regarded as something endowed with some kind of objective existence, is not less a misleading misconception, an illusory attempt to exteriorize or materialize our true probabilistic beliefs. In investigating the reasonableness of our own modes of thought and behaviour under uncertainty, all we require, and all that we are reasonably entitled to, is consistency among these beliefs, and their reasonable relation to any kind of relevant objective data”.&lt;/p&gt;
&lt;p&gt;The use of subjective probability also means that Bayesians can report probabilities without a “practically unlimited” sequence of observations. What is the frequentist probability of the truth of the proposition “Jackson was the eighth president”? Since there is only one relevant experiment for this problem, the frequentist probability is either zero (if Jackson was not the eighth president) or one (if Jackson was the eighth president). Non-trivial frequentist probabilities, it seems, are reserved for phenomena that are standardized and repeatable. Bayes Theorem itself is uncontroversial: it is merely an accounting identity that follows from the axioms of probability discussed above, plus the following additional definition.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Conditional probability&lt;/strong&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; be events with &lt;span class=&#34;math inline&#34;&gt;\(P(B)&amp;gt;0\)&lt;/span&gt;. Then the conditional probability of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(A\mid B) = \frac{P(A \cap B)}{P(B)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The following two useful results are also implied by the probability axioms, plus the definition of conditional probability&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Multiplication rule&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(A \cap B) = P(A\mid B)P(B) = P(B\mid A)P(A)\]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Law of total probability&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(B) = P(A\cap B)+ P\overline{(A\cap B)} = P(B\mid A)P(A) + P(B \mid \overline{A})P(\overline{A})\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayes-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayes theorem&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Bayes Theorem&lt;/em&gt; can now be stated, following immediately from the definition of conditional probability. If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are events with &lt;span class=&#34;math inline&#34;&gt;\(P(B)&amp;gt;0\)&lt;/span&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we consider the event &lt;span class=&#34;math inline&#34;&gt;\(A=H\)&lt;/span&gt; to be an hypothesis and the event &lt;span class=&#34;math inline&#34;&gt;\(B=E\)&lt;/span&gt; to be observing some evidence, then &lt;span class=&#34;math inline&#34;&gt;\(Pr(H\mid E)\)&lt;/span&gt; is the probability of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; after obtaining &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\text{Pr}(H)\)&lt;/span&gt; is the prior probability of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; before considering &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;. The conditional probability on the left-hand side of the theorem, &lt;span class=&#34;math inline&#34;&gt;\(\text{Pr}(H\mid E)\)&lt;/span&gt;, is usually referred to as the posterior probability of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;. Bayes Theorem thus supplies a solution to the general problem of inference or induction, providing a mechanism for learning about the plausibility of a hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; from data &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In most analyses in the social sciences, we want to learn about a continuous parameter, rather than the discrete parameters considered in the discussion thus far. Examples include the mean of a continuous variable, a proportion (a continuous parameter on the unit interval), a correlation, or a regression coefficient. In general, let the unknown parameter be &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and denote the data available for analysis as &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y = (y_1, \ldots , y_n)\)&lt;/span&gt;. In the case of continuous parameters, beliefs about the parameter are represented as probability density
functions or pdfs; we denote the prior pdf as &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; and the posterior pdf as &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid \boldsymbol y)\)&lt;/span&gt;. Then, Bayes Theorem for a continuous parameter is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p(\theta \mid \boldsymbol y) = \frac{p(\boldsymbol y \mid \theta) p(\theta)}{\int p(\boldsymbol y \mid \theta) p(\theta) d\theta}\]&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;which is often approximated by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p(\theta \mid \boldsymbol y) \propto p(\boldsymbol y \mid \theta) p(\theta) \]&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;where the proportionality constant is &lt;span class=&#34;math inline&#34;&gt;\(\left[ \int p(\boldsymbol y \mid \theta) p(\theta) d\theta \right]^{-1}\)&lt;/span&gt; which ensures that the posterior density integrates to one, as a proper probability density. The first term on the right hand side of the Equation is the &lt;em&gt;likelihood function&lt;/em&gt;, the probability density of the data &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y\)&lt;/span&gt;, considered as a function of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. This formulation of Bayes Rule highlights a particularly elegant feature of the Bayesian approach, showing how the likelihood function &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol y|\theta)\)&lt;/span&gt; can be “inverted” to generate a
probability statement about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, given data &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y\)&lt;/span&gt;. Thus, from a Bayesian perspective, likelihood based analyses of data assume prior ignorance, although seldom is this assumption made explicit, even if it were plausible. In other cases, when working with the so-called conjugate priors in the exponential family, the mean of the posterior distribution is a precision-weighted average of the prior and the likelihood. Suppose a prior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; belongs to a class of parametric of densities, &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;. More specifically, the prior density is said to be conjugate with respect to a likelihood &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol y\mid \theta)\)&lt;/span&gt; if the posterior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid \boldsymbol y )\)&lt;/span&gt; is also in &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Bayesian statistical inference is equivalent to combining information, marrying the information in the prior with the information in the data, with the relative contributions of prior and data to the posterior being proportional to their respective precisions. That is, Bayesian analysis with conjugate priors over a parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is equivalent to taking a precision-weighted average of prior information about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and the information in the data about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Thus:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Thus, when prior beliefs about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; are ‘vague’, ‘diffuse’, or, in the limit, uninformative, the posterior density will be dominated by the likelihood (i.e. the data contains much more information than the prior about the parameters);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When prior information is available, the posterior incorporates it, and rationally, in the sense of being consistent with the laws of probability via Bayes Theorem. In fact, when prior beliefs are quite precise relative to the data, it is possible that the likelihood is largely ignored, and the posterior distribution will look almost exactly like the prior&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note also that via Bayes Rule, if a particular region of the parameter space has zero prior probability, then it also has zero posterior probability. This feature of Bayesian updating
has been dubbed &lt;em&gt;Cromwell’s Rule&lt;/em&gt; by Lindley. The point here is that posterior distributions can sometimes look quite unusual, depending on the form of the prior and the likelihood for a particular problem. The fact that a posterior distribution may have a peculiar shape is of no great concern in a Bayesian analysis: provided one is updating prior beliefs via Bayes Rule, all is well. Unusual looking posterior distributions might suggest that one’s prior distribution was poorly specified, but, as a general rule, one should be extremely wary of engaging this kind of procedure. Bayes Rule is a procedure for generating posterior distributions over parameters in light of data. Although one can always re-run a Bayesian analysis with different priors (and indeed, this is usually a good idea), Bayesian procedures should not be used to hunt for priors that generate the most pleasing looking posterior distribution given a particular data set and likelihood. Indeed, such a practice would amount to an inversion of the Bayesian approach: i.e. if the researcher has strong ideas as to what values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; are more likely than others, aside from the information in the data, then that auxiliary information should be considered a prior, with Bayes Rule providing a procedure for rationally combining that auxiliary information with the information in the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-updating-of-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian updating of information&lt;/h2&gt;
&lt;p&gt;Bayesian procedures are often equivalent to combining the information in one set of data with another set of data. In fact, if prior beliefs represent the result of a previous data analysis (or perhaps many previous data analyses), then Bayesian analysis is equivalent to &lt;em&gt;pooling information&lt;/em&gt;. This is a particularly compelling feature of Bayesian analysis, and one that takes on special significance when working with cojugate priors. In these cases, Bayesian procedures accumulate information in the sense that the posterior distribution is more precise than either the prior distribution or the likelihood alone. Further, as the amount of data increases, say through repeated applications of the data generation process, the posterior precision will continue to increase, eventually overwhelming any non-degenerate prior; the upshot is that analysts with different (non-degenerate) prior beliefs over a parameter will eventually find their beliefs coinciding, provided they (1) see enough data and (2) update their beliefs using Bayes Theorem. In this way Bayesian analysis has been proclaimed as a model for scientific practice acknowledging that while reasonable people may differ (at least prior to seeing data), our views will tend to converge as scientific knowledge accumulates, provided we update our views rationally, consistent with the laws of probability.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters-as-random-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameters as random variables&lt;/h2&gt;
&lt;p&gt;One of the critical ways in which Bayesian statistical inference differs from frequentist
inference is that the result of a Bayesian analysis, the posterior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid \boldsymbol y)\)&lt;/span&gt; is just that, a probability density. Given a subjectivist interpretation of probabilty that most Bayesians adopt, the ‘randomness’ summarized by the posterior density is a reflection of the researcher’s uncertainty over &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, conditional on having observed data. Contrast the frequentist approach, in which &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is not random, but a fixed (but unknown) property of a population from which we randomly sample data &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y\)&lt;/span&gt;. Repeated applications of the sampling process, if undertaken, would yield different y, and different sample based estimates of θ, denoted &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta} = \hat{\theta}(\boldsymbol y)\)&lt;/span&gt;, this notation reminding us that estimates of parameters are functions of data. In the frequentist scheme, the &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}(\boldsymbol y)\)&lt;/span&gt; vary randomly across data sets (or would, if repeated sampling was undertaken), while the parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is a constant feature of the population from which data sets are drawn. The distribution of values of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}(\boldsymbol y)\)&lt;/span&gt; that would result from repeated application of the sampling process is called the sampling distribution, and is the basis of inference in the frequentist approach; the standard deviation of the sampling distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is the standard error of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt;, which plays a key role in frequentist inference. The Bayesian approach does not rely on how &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; might vary over repeated applications of random sampling. Instead, Bayesian procedures center on a simple question: “what should I believe about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in light of the data available for analysis, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol y\)&lt;/span&gt; ?”&lt;/p&gt;
&lt;p&gt;The critical point to grasp is that in the Bayesian approach, the roles of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; are reversed relative to their roles in classical, frequentist inference: &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is random, in the sense that the researcher is uncertain about its value, while &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is fixed, a feature of the data at hand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;So, we have seen a couple of interesting things about Bayesian statistics which people may not be aware of. First, Bayesian statistics is a scientific approach in that it provides a rational way to update subjective beliefs based on the available evidence through Bayes theorem which conforms the rules of probability. This ensures the scientific credibility of the posterior results while also providing a way to solve the inductive problem of learning from the data and update our belief about a parameter/hypothesis. Second, in contrast to the classical approach, Bayesian statistics do not rely on asymptotic results of a series of repeateable events in order to hold and therefore can be used to answer questions which do not have any meaning in the context of repeated events. Finally, Bayesian statistics sees any unknown quantity (e.g. parameters) as random variables and attach to them a probability distribution expressing the uncertainty around the estimates. Since the entire posterior distribution is derived based on Bayes theorem, this ensures correct propagation of uncertainty from the data and prior and does not require the additional step of classical statistics of deriving uncertainty measures in an “artifical way” or relying on asymptotic results.&lt;/p&gt;
&lt;p&gt;I hope this was a bit interesting for those who would like to get more familiar with the Bayesian philosophy and its underlying implications in terms of statistical assumptions and methods. Of course, being a Bayesian, this is the best way to go for me when doing an analysis and I would love to see more people embracing the Bayesian way as a new way of thinking statistics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/qav3a2OPBdZoQ/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why be Bayesian?</title>
      <link>/post/update-july/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-july/</guid>
      <description>


&lt;p&gt;Many times I have been asked by co-workers and people around me who are a bit familiar with statistics why I choose to be Bayesian and whether I feel confident in using this approach for my data analysis rather than the most widely accepted frequentist methods, at least in my research area. Well, I am sure there are many valid arguments I could use to reply to this question but if I have to summarise my answer in two words I would say: &lt;strong&gt;why not&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;Now, a bit more into the details for those who were not extremely annoyed by my previous sentence. So, I truly believe that the Bayesian approach can be considered as a complement rather than a substitute to the frequentist paradigm. The main reason is relate to its much stronger links with probability theory compared with the classical approach in that not only are sampling distributions required for summaries of data, but also a wide range of distributions are used to represent prior opinion about proportions, event rates, and other unknown quantities. In a nutshell, the key difference between the two approaches is how they confront the concept of &lt;strong&gt;probability&lt;/strong&gt; of a certain event. In fact, although there is general consensus about the &lt;em&gt;rules&lt;/em&gt; of probability, that there is no universal &lt;em&gt;concept&lt;/em&gt; of probability, and two quite different definitions come from the frequentist and Bayesian approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The most widely known definition is: the proportion of times a will occur in an infinitely long series of repeated identical situations. This is known as the &lt;strong&gt;frequentist&lt;/strong&gt; perspective, as it rests on the &lt;em&gt;frequency&lt;/em&gt; with which specific events occur.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In contrast, the &lt;strong&gt;Bayesian&lt;/strong&gt; approach rests on an essentially subjective interpretation of probability, which is allowed to express generic uncertainty or &lt;em&gt;degree of belief&lt;/em&gt; about any unknown but potentially observable quantity, whether or not it is one of a number of repeatable experiments.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Rather than debating on philosophical debates about the foundations of statistics I prefer to focus on those aspects which I believe make the Bayesian approach, if not more intuitive than the frequentist counterpart, at least more attractive. Be worn I am not trying to start a war as I think both approaches could be used without the need to completely discard the other. The simple fact of being able to choose between two methods, rather than restricting themselves to a single option, seems a good enough reason for me to advocate the use of &lt;strong&gt;both&lt;/strong&gt; approaches. I terms of your own knowledge, experience and skills, You do not gain anything by saying “I will never be Bayesian” or “I will never be a frequentist”. On the contrary, by opening your mind and explore the use of one or the other method you will be able to have more options at your disposal that you can use to tackle the different problems you will face in your analyses.&lt;/p&gt;
&lt;p&gt;For the purpose of this post I just want to highlight some aspects which make the Bayesian approach particularly useful and, in some cases, even arguably preferable than the frequentist approach. Note that I am well aware there could be cases where the opposite holds and this is precisely why I believe it is important that statisticians should become familiar with both methods. By doing so they will be able to overcome the limitations/concerns associated with one method for a specific problem at hand using the instruments made available from the other method. Since I am a Bayesian, here I want to report the reasons and situations in which the Bayesian approach could provide a powerful tool.&lt;/p&gt;
&lt;p&gt;Let us start with a quick recap of the basic principle behind Bayesian methods. Bayesian statistical analysis relies on &lt;strong&gt;Bayes’s Theorem&lt;/strong&gt;, which tells us how to update prior
beliefs about parameters and hypotheses in light of data, to yield posterior beliefs. The theorem itself is utterly uncontroversial and follows directly from the conventional definition of conditional probability. If &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is some object of interest, but subject to uncertainty, e.g. a parameter, a hypothesis, a model, a data point, then Bayes Theorem tells us how to rationally
revise prior beliefs about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;, in light of the data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, to yield posterior beliefs &lt;span class=&#34;math inline&#34;&gt;\(p(y \mid \theta)\)&lt;/span&gt;. In this way Bayes Theorem provides a solution to the general problem of &lt;em&gt;induction&lt;/em&gt;, while in the specific case of statistical inference, Bayes Theorem provides a solution to problem of &lt;em&gt;how to learn from data&lt;/em&gt;. Thus, in a general sense, Bayesian statistical analysis is remarkably simple and even elegant, relying on this same simple recipe in each and every application.&lt;/p&gt;
&lt;p&gt;As I see it, there are a few major reasons why statisticians should consider learning about the Bayesian approach to statistical inference, and in the social sciences in particular:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Bayesian inference is simple and direct&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The result of a Bayesian analysis is a posterior probability statement, ‘posterior’ in the literal sense, in that such a statement characterizes beliefs after looking at data. Examples include: the posterior probability that a regression coefficient is positive, negative or lies in a particular interval; the posterior probability that a subject belongs to a particular latent class; the posterior probabilities that a particular statistical model is true model among a
family of statistical models.&lt;/p&gt;
&lt;p&gt;Note that the posterior probability statements produced by a Bayesian analysis are probability
statements over the quantities or objects of direct substantive interest to the researcher (e.g. parameters, hypotheses, models, predictions from models). Bayesian procedures condition on the data at hand to produce posterior probability statements about parameters and hypotheses. Frequentist procedures do just the reverse: one conditions on a null hypothesis to assess the plausibility of the data one observes (and more ‘extreme’ data sets that one did not observe but we might have had we done additional sampling), with another step of reasoning required to either reject or fail to reject the null hypothesis. Thus, compared to frequentist procedures, Bayesian procedures are simple and straightforward, at least conceptually.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical modeling&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The prior density also provides a way for model expansion when we work with data sets that pool data over multiple units and/or time periods. Data sets of this sort abound in the social sciences. Individuals live in different locations, with environmental factors that are constant for anyone within that location, but vary across locations. key question in research of this type is how the causal structure that operates at one level of analysis (e.g. individuals) varies across a ‘higher’ level of analysis (e.g. localities or time periods). The Bayesian approach to statistical inference is extremely well-suited to answering this question. Recall that in the Bayesian approach parameters are always random variables, typically (and most basically) in the sense that the researcher is unsure as to their value, but can characterize that uncertainty in the form of a prior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can replace the prior with a stochastic model formalizing the researcher’s assumptions about the way that parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; might vary across groups &lt;span class=&#34;math inline&#34;&gt;\(j = 1,..., J\)&lt;/span&gt; , perhaps as a function of observable characteristics of the groups; e.g., &lt;span class=&#34;math inline&#34;&gt;\(\theta_j \sim f (z_j, \gamma )\)&lt;/span&gt;, where now &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is a set of unknown hyperparameters. That is, the model is now comprised of a nested hierarchy of stochastic relations: the data from unit &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;, are modeled as a function of covariates and parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; , while cross-unit heterogeneity in the &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; is modeled as function of unit-specific covariates &lt;span class=&#34;math inline&#34;&gt;\(z_j\)&lt;/span&gt; and hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;. Models of this sort are known to Bayesians as &lt;em&gt;hierarchical models&lt;/em&gt;, but go by many different names in different parts of the social sciences depending on the specific form of the model and the estimation strategy being used (e.g. ‘random’ or ‘varying’ coefficients models, ‘multilevel’ or ‘mixed’ models). Compared with the frequentist counterpart, thanks to the use of &lt;em&gt;Markov chain Monte Carlo&lt;/em&gt; (MCMC) methods, Bayesian computation for these models has also become rather simple. Indeed, MCMC algorithms have proven themselves amazingly powerful and flexible, and have brought wide classes of models and data sets out of the ‘too hard’ basket. Other modelling examples include data sets with lots of missing data, or models with lots of parameters, model with latent variables, mixture
models, and flexible semi-and non-parametric models.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Statistically significant?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Frequentist inference asks assuming hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; is true, how often would we obtain a result at least as extreme as the result actually obtained?’, where ‘extreme’ is relative to the hypothesis being tested. If results such as the one obtained are sufficiently rare under hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; (e.g. generate a sufficiently small p value), then we conclude that &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; is incorrect, rejecting it in favor of some alternative hypothesis. Indeed, we teach our students to say that when the preceding conditions hold, we have a &lt;em&gt;statistically significant&lt;/em&gt; result. My experience is that in substituting this phrase for the much longer textbook definition, people quickly forget the frequentist underpinnings of what it is they are really asserting, and, hence seldom question whether the appeal to the long-run, repeated sampling properties of a statistical procedure is logical or realistic.&lt;/p&gt;
&lt;p&gt;In the Bayesian approach we condition on the data at hand to assess the plausibility of a hypothesis (via Bayes Rule), while the frequentist approach conditions on a hypothesis to assess the plausibility of the data (or more extreme data sets), with another step of reasoning required to either reject or fail to reject hypotheses. The frequentist p-value is the relative frequency of obtaining a result at least as extreme as the result actually obtained, assuming hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; to be true, where the sampling distribution of the result tells us how to assess relative frequencies of possible different results, under &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;. But what about cases where repeated sampling makes no sense, even as a thought experiment?&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Intervals&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Recall that in the frequentist approach, parameters are fixed characteristics of populations, so &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; either lies in the interval or it doesn’t. The correct interpretation of a frequentist confidence interval concerns the repeated sampling characteristics of a sample statistic. In the case of a &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; confidence interval, the correct frequentist interpretation is that &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; confidence intervals one would draw in repeated samples will include &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. Now, is the &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; confidence interval that one constructs from the data set at hand one of the lucky &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; that actually contains &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, or not? No ones knows.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Rational subjectivity&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, aside from acknowledging the subjectivity inherent to the general scientific exercise, the Bayesian approach rests on a subjective notion of probability, but demands that subjective
beliefs conform to the laws of probability. Put differently, in the Bayesian approach, the subjectivity of scientists is acknowledged, but simultaneously insists that subjectivity be
rational, in the sense that when confronted with evidence, subjective beliefs are updated rationally, in accord with the axioms of probability. Again, it is in this sense that Bayesian procedures offer a more direct path to inference; as I put it earlier, the Bayesian approach lets researchers mean what they say and say what they mean. For instance, the statement, having looked at the data, I am &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; sure that &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is included in an interval is a natural product of a Bayesian analysis, a characterization of the researcher’s beliefs about a parameter in formal, probabilistic terms, rather than a statement about the repeated sampling properties of a statistical procedure.&lt;/p&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The mathematics and computation underlying Bayesian analysis has been dramatically simplified
via a suite of MCMC algorithms. The combination of the popularization of MCMC and vast
increases in the computing power available to social scientists means that Bayesian analysis
is now well and truly part of the mainstream of quantitative social science. Despite these important pragmatic reasons for adopting the Bayesian approach, it is important to remember that MCMC algorithms are Bayesian algorithms: they are tools that simplify the computation of posterior densities. So, before we can fully and sensibly exploit the power of MCMC algorithms, it is important that we understand the foundations of Bayesian inference.&lt;/p&gt;
&lt;p&gt;This time I went overboard with the discussion but I thought it could be interesting to clarify here the key points, in my opinion, which make the Bayesian approach not only valid and efficient, but even a powerful tool that, once grasped the underlying phylosophy, can be used to overcome the difficulties of standard methods, especially when dealing with complex analyses.&lt;/p&gt;
&lt;p&gt;So what are you waiting for? do not sit in your frequentist comfort zone but expand your statistical knowledge! Evolve!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/u1k1kpDZSw5sA/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>New tutorials for missingHE</title>
      <link>/post/update-june/</link>
      <pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-june/</guid>
      <description>&lt;p&gt;Nothing major to report for the past month, mostly spent at home still in lockdown. A few offices and shops have already opened in London but all the public stuff, including my office at UCL will remain close until who knows when. So, in the meantime I put some work into prpearing some tutorials about how to use the different functions from my &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;missingHE&lt;/code&gt;. These are built directly into the package in the form of &lt;code&gt;vignettes&lt;/code&gt; which can be easily accessed from the &lt;code&gt;R&lt;/code&gt; terminal once the package is installed locally.&lt;/p&gt;
&lt;p&gt;I have worked on three main tutorials dedicated to explain the basic functions of the package and to show how to customise the different models using different combinations of input choices. The three &lt;code&gt;vignettes&lt;/code&gt; have each a specific target of users, starting from the beginners in using &lt;code&gt;R&lt;/code&gt; to those who would like to have a more flexibile specification of the models based on different modelling assumptions. I use the built-in dataset in the package, the &lt;code&gt;MenSS&lt;/code&gt; study, to give practical examples of how the different changes to the models may affect the results in a standard analysis.&lt;/p&gt;
&lt;p&gt;The three tutorials are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Introduction to missingHE&lt;/strong&gt;&lt;/em&gt;, which is intended for those who have little familiarity with &lt;code&gt;R&lt;/code&gt; and just want an overview of the different functions of the package, what they do and how to extract the relevant information from the fitted models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Fitting MNAR models in missingHE&lt;/strong&gt;&lt;/em&gt;, which is intended for those who already know about the main functions of the package and would like to explore more deeply how to perform sensitivity analysis to missing not at random assumtpions using the arguments of each function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Model Customisation in missingHE&lt;/strong&gt;&lt;/em&gt;, which is intended for those who have already grasped the basic idea behind the different functions and would like to customise their models and not just stick with the default settings. Examples include how to specify random effects, different prior distributions and so on.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I believe these tutorials provide a reasonable summary about the key elements for anyone who would like to use the package but is a bit uncertain about what he or she can actually do with the functions and to which extent customisation is possible. Fot the moment the &lt;code&gt;vignettes&lt;/code&gt; are only available from my &lt;a href=&#34;https://github.com/AnGabrio/missingHE&#34;&gt;GitHub&lt;/a&gt; version of the package (1.4.1) and can be accessed by installing the package using the command&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;devtools&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;install_github&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;AnGabrio/missingHE&amp;#34;&lt;/span&gt;, build_vignettes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and then typing&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;browseVignettes&lt;/span&gt;(package &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;missingHE&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that you need to locally install the packages &lt;code&gt;devtools&lt;/code&gt; and &lt;code&gt;utils&lt;/code&gt; to access their functions. As soon as I have a bit of time I will update the version on &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE&#34;&gt;CRAN&lt;/a&gt; to make them available from there as well. I spent a bit of time creating these tutorials and I hope people will find them useful to undestand the package. In case anything is still unclear, feel free to contact me to ask questions.&lt;/p&gt;
&lt;p&gt;That is pretty much it for the moment from me. I should add new tutorials for using &lt;code&gt;JAGS&lt;/code&gt; and &lt;code&gt;STAN&lt;/code&gt; on the website but time is always never enough. Naaah, I am just very lazy these days.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/vjCkGbbSm7bhe/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sorry, an error occurred</title>
      <link>/post/update-may/</link>
      <pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-may/</guid>
      <description>&lt;p&gt;It has been a while from my last update on this website, but this has been an incredibly busy period with lots of routine work that I had to do. Now the situation has clamed down a bit, and I have also some news to report. So, here I am finally.&lt;/p&gt;
&lt;p&gt;In the past few days, I had an extremely interesting email correspondence with one guy (don&amp;rsquo;t want to say the name for privacy) interested in using my R package &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE/&#34;&gt;missingHE&lt;/a&gt; to do some trial-based CEA. Awesome, I thought. He was looking for some advice for how to customise some models in the package and how to get the results he wanted. Unfortunately for my pride, but fortunately for my package, we discovered a small bug in the code when trying to specify a hurdle model (two-part model) to handle the zero costs when not including any covariate inside the logit model to estimate the probability of having a zero cost. Essentially, under these specific circumstances, the function did not correctly backtransformed the estimate of the mean costs on the appropriate scale and the results provided were incorrect. Sorry about that!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/80TEu4wOBdPLG/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To be honest, you can get away by simply including some baseline covariate into the logit model for the structural zero costs, in which case the estimates produced by the function are correct. I have immediately updated the package version to correct this bug on my &lt;a href=&#34;https://github.com/AnGabrio/missingHE&#34;&gt;GitHub page&lt;/a&gt;, where you can find the most up to date version (1.4.1). The version on CRAN will be updated at the next iteration as I have recently uploaded the 1.4.0 version in May. In the meantime, if you want to avoid having that issue above, you can download and install the updated version from my GitHub page.&lt;/p&gt;
&lt;p&gt;I think this is also a good chance to tell that I have updated an old paper on my on my &lt;a href=&#34;https://arxiv.org/pdf/1805.07149.pdf&#34;&gt;Arxiv account&lt;/a&gt;. Both the content and title of the paper have changed considerably, but overall I feel that the overall message and quality of the article has improved. It is still an on-going version, but I am quite satisfied with its current status given all the effort I put into it. Have a lool in case you are interested. New title : &amp;ldquo;Joint longitudinal models for dealing with missing at random data in trial-based economic evaluations&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;More information on the paper can be found at this &lt;a href=&#34;/publication/gabrio2019d/&#34;&gt;dedicated page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I believe that is all for this update. Not much going on due to the whole lockdown situation here in the UK but hopefully things are improving a little in a two-months time we will be able to at least go to the office. Let&amp;rsquo;s see.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New updates for missingHE</title>
      <link>/post/update2-april/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update2-april/</guid>
      <description>&lt;p&gt;In spite of how incredibly busy I am at the moment, which is also weird considering the whole lockdown situation still going on, I managed to upload a new version (1.4.0) of my R package &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE/index.html&#34;&gt;missingHE&lt;/a&gt; with exciting updates!&lt;/p&gt;
&lt;p&gt;For those who do not know, &lt;code&gt;missingHE&lt;/code&gt; is specifically designed to implement Bayesian models for the analysis of trial-based economic evaluations and provides different methods to handle missingness in either or both the effectiveness and cost outcomes. The cool new things in this version are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First, random effects can now be specified for each model implemented in missingHE (I know, Bayesians should not talk about &amp;ldquo;random&amp;rdquo; or &amp;ldquo;fixed&amp;rdquo; effects as we know that there are no real &amp;ldquo;fixed&amp;rdquo; effects but the terms have become quite popular and many people would prefer this way). These include selection, hurdle and pattern mixture models. The package allows a flexible implementation of either random intercept only, random slope only and both random intecept and slope models based on the input given by the user. The random effects term is specified via the formula y ~ x + (x | z) where x is a covariate included also as a fixed effects in the model and z is the clustering variable over which the random effects for x are specified. It is possible to remove the random intercept if desired by adding 0 + inside the brackets (by default this is included).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Second, new types of posterior predictive checks can now be chosen using the function &lt;code&gt;ppc&lt;/code&gt; for each type of model fitted using the function of the package. These include plotting the Bayesian posterior p-values (which should not be confused with the usual p-values as they are completely different) based on the posterior replications of the models and a given statistics computed from the observed data. The statistic can be provided by the user under the form of a univariate function (e.g. mean or sd) or a specific type of bivariate function (e.g. cor).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Third, a new generic function called coef has been added which allows to extract the regression coefficients from each type of model, either in terms of fixed effects or random effects (if specified).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I am quite proud of this new update as it is something I considered for a long time which is now available. If even one person find this useful, I think it will be worth all my effort. Very nice.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/ghccKVv6mSpXy/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Oh, and yes you can also find the new version of &lt;code&gt;missingHE&lt;/code&gt; on my &lt;a href=&#34;https://github.com/AnGabrio/missingHE&#34;&gt;GitHub page&lt;/a&gt;. I plan to upload a more serious tutorial on how to use all the functions of the package at some point (hopefully not too far from now).&lt;/p&gt;
&lt;p&gt;So, now that all the fun part is done, I need to go back to doing meetings, reviews, writing papers, etc&amp;hellip;. It will be a quite busy period again but now I feel motivated. Let&amp;rsquo;s see for how long this will last.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>So much time but also not really</title>
      <link>/post/update-april/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-april/</guid>
      <description>&lt;p&gt;The lockdown proceeds also here in the UK, as in the rest of the world, and at the moment we have no clear idea how long it will last. Not much we can do apart from staying at home all the time and practicing social distancing. I am still ok with living at home 24/7 but this has affected my productivity, especially in terms of collaborating with other people.&lt;/p&gt;
&lt;p&gt;Although I have a lot of time to dedicate to some works, I am making a slow progress and the time of the day seems to fly in an instant with so many things to do. This months is particularly busy as I am trying to submit a revision for a paper which hanted me for quite a lot of time now and which I must finish by the first days of May. I am also working on side projects but these have been slowed down due to the current situation. I hope I can find the time (and the will) to do some more updates to my website by adding more tutorials and similar stuff. I do have some nice ideas about possible projects and collaboration but I need to wait until after this weird period.&lt;/p&gt;
&lt;p&gt;I am also planning to prepare a new version for my R package &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE&#34;&gt;missingHE&lt;/a&gt; to add some nice additional features for post-processing the results of Bayesian models and to implement new types of models. These, however, will take time, which at the moment is one thing that I do have but that I also do not have.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/xCBE0RPfYsyWI/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Anyway, not much of an update this one, but I hope things will move quicker in the next couple of months or so.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Living and working at home is nice, right?</title>
      <link>/post/update2-march/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update2-march/</guid>
      <description>&lt;p&gt;It has been roughtly a week and a half now since this whole shutdown started here in London and things are not going to be easy in the next few weeks. I am lucky, in that my job allows me to work remotely with limited inconveniences. Other people have to go outide for working and, if not risking thier life, at least put at risk the life of those who they care most. Last week was particularly bad in terms of supermarket products which were sold out for the most part. This week is a bit better as people may have realised that for the moment, if we just buy products as usual, we still have food and toilet paper for everyone.&lt;/p&gt;
&lt;p&gt;To be honest, not much to update on my work which has slowed down due to this whole situation and also to me not feeling at my best. I hope I will have some time to look at the different projects I am involved with in the next few days. In the meantime, I worked a bit on my website with new JAGS and STAN tutorials and I have also uploaded on my &lt;a href=&#34;https://github.com/AnGabrio&#34;&gt;GitHub page&lt;/a&gt; some materials (e.g. software code) related to some of the projects I did. For example, &lt;a href=&#34;https://github.com/AnGabrio/Code/tree/master/volleyball&#34;&gt;here&lt;/a&gt; the link to the JAGS and STAN code for the model I used to predict volleyball results&lt;/p&gt;
&lt;p&gt;Not sure what gif to use this time to conclude the post. So I guess I will just go for a random cat picture, which does not make any sense but which is always nice to look at.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/10dU7AN7xsi1I4/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lockdown</title>
      <link>/post/update-march/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-march/</guid>
      <description>&lt;p&gt;I have to admit, although I expected some fear to spread because of the virus which is currently and quickly infecting the world, I was surprised by the frenzy surrounding
us, especially in my homecountry (Italy) and particularly in my parents&amp;rsquo; region which is at the moment under lockdown. I will also probably cancel my planned trip home for Eastern
and perhaps also after that since the situations is still unclear and I may be unable to come back to the UK in the short time. This is pretty scary to all the people living in those
territories, who are now forbidden to have any sort of public meeting and are strongly recommended to stay at home. I am afraid this will not be enough to stop the virus 
from spreading but of course it is useful as it is the only way we have if we want to contain it. The hope is that by summer time the heath will reduce the ability of the virus
to spread and give us some time to come up with a possible vaccine in the next months.&lt;/p&gt;
&lt;p&gt;There have already been attempts to estimate the fatality ratio of the virus using statistical methods. Here I post the tweet from Andrew Gelman&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Post Edited: Coronavirus age-specific fatality ratio, estimated using Stan, and (attempting) to account for underreporting of cases and the time delay to death.  Now with data and code.  And now a link to another paper (also with data and code). &lt;a href=&#34;https://t.co/CSHnXRMtyp&#34;&gt;https://t.co/CSHnXRMtyp&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andrew Gelman (@StatModeling) &lt;a href=&#34;https://twitter.com/StatModeling/status/1236845267275702273?ref_src=twsrc%5Etfw&#34;&gt;March 9, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;which refers to epidemiologists who tried to use &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.03.04.20031104v1.full.pdf&#34;&gt;STAN&lt;/a&gt; for achieving this objective, although an additional reference
to another work based on the use of &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.02.14.20023127v1&#34;&gt;differential equation analysis&lt;/a&gt; is also made. However, results 
are still preliminary and subject to limitations for the type of data and assumptions used. From a statistical perspective I am sure this new epidemic will be interesting to
study and I guess lots of funding will be devoted to analyse the upcoming data to get a better idea of the actual threat it represents for the people. I am not an epidemiologist,
so I do not have a big statistical interest in this, although I am pretty much worried as any other person. Hopefully, this will become clearer as time passes and let us just hope
the number of deaths will not be very high.&lt;/p&gt;
&lt;p&gt;Sorry about talking about this here, but from time to time I would also like to highlight was is currently happening around me. As for my research, nothing has changed much for me
at the moment and life as usual continues with another busy upcoming period with lots of boring meetings, reports and standard analyses to do, but hopefully I can also save some time
to do some methodological work. I am also waiting for the decision about my abstract which I submitted to the &lt;a href=&#34;https://cfp.euhea.eu/conf/euhea2020&#34;&gt;EuHEA conference 2020&lt;/a&gt; and which is supposed to
be held in Oslo this July. I really hope I have a chance to presenting my work there as I have never been to this specific health economic conference. Fingers crossed! Of course, 
nobody knows what will happen from here till July and much is to be discussed also with respect to how the spreading of the virus may affect everyone&amp;rsquo;s schedule in the next months.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/3ohhwxaw5OlElXzv8s/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Something old, something new, something borrowed, something blue</title>
      <link>/post/update2-february/</link>
      <pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update2-february/</guid>
      <description>&lt;p&gt;The moment for the second edition of the HEART&amp;rsquo;s one-day introductory course to health economics arrived at last! The course, led by &lt;a href=&#34;https://iris.ucl.ac.uk/iris/browse/profile?upi=RMHUN48&#34;&gt;Rachael Hunter&lt;/a&gt; and called &amp;ldquo;Understanding health economics in clinical trials&amp;rdquo;, took place on Tuesday 11 February and was prepared in collaboration between the HEART group and the Institute of Clinical Trials and Methodology (ICTM). I believe this second edition of the course was a success both in terms of the quality/quantity of the material covered during the six sessions throughout the day, as well as in terms of the positive feedback we received from the participants. Also, this time a new HEART member (&lt;a href=&#34;https://iris.ucl.ac.uk/iris/browse/profile?upi=MCCLE13&#34;&gt;Marie&lt;/a&gt;) joined the group and very nicely delivered the session about patient reported outcome measures (PROMs), engaging in nice discussions with the audience.&lt;/p&gt;
&lt;p&gt;Finally, a couple of personal notes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I recently attended a very interesting meeting about missing data methodology which was held by an international group of very talented senior and junior statisticians from different universities, including people like &lt;a href=&#34;https://www.ctu.mrc.ac.uk/about-us/senior-staff/ian-white/&#34;&gt;Ian White&lt;/a&gt; and &lt;a href=&#34;https://www.lshtm.ac.uk/aboutus/people/carpenter.james&#34;&gt;James Carpenter&lt;/a&gt; from UCL and the LSHTM. It was really an amazing experience to meet so many people working in different stats area but with a common passion about missing data methods (also mine!). From what I understood this series of meetings (called &amp;ldquo;MiDIA&amp;rdquo;) have been held since years but do not have a very regular schedule due to people being busy I guess, which makes totally sense. Not sure when the next one will be held but now I am definitely looking forward to the next meetings!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/kFBJt0yqZD315EFJiJ/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I would also like to highlight a recent tweet from &lt;a href=&#34;https://www.ucl.ac.uk/priment/&#34;&gt;UCL PRIMENT CTU&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/Vacancy?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Vacancy&lt;/a&gt; &lt;br&gt;Come join our team of health economists! &lt;br&gt;Closing date: 15 March 2020&lt;br&gt;Apply here: &lt;a href=&#34;https://t.co/IUg8JIeANX&#34;&gt;https://t.co/IUg8JIeANX&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://twitter.com/hashtag/clinicaltrials?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#clinicaltrials&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/healtheconomics?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#healtheconomics&lt;/a&gt;&lt;/p&gt;&amp;mdash; Priment Clinical Trials Unit (@PRIMENT_UCL) &lt;a href=&#34;https://twitter.com/PRIMENT_UCL/status/1230124818244173824?ref_src=twsrc%5Etfw&#34;&gt;February 19, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;which advertises a new position as health economist in our HEART group for performing health economics using data from clinical trials. I would encourage anyone interested in some good applied health economic work to apply for this position. Deadline 15 March 2020.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/kHZu4LDtvpY63RT1He/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To conclude, I would also like to say that I have done some updates to this website. From the inclusion of new tutorials on the use of &lt;a href=&#34;/jags/&#34;&gt;JAGS&lt;/a&gt; and &lt;a href=&#34;/stan/&#34;&gt;STAN&lt;/a&gt; on different statistical topics, to a restyle of the website. In particular I had fun by playing around with some &lt;code&gt;Markdown&lt;/code&gt; code to add new features, e.g. customised alert notes and emoji, for example. Somthing like this:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alertqueen alertqueen-note&#34;&gt;
    &lt;h4 class=&#34;alertqueen-heading;&#34;&gt;&amp;#9819; Queen&#39;s note! &amp;#9819; &lt;/h4&gt;
  &lt;div&gt;
        &lt;hr&gt;
    I 👍 $\rm \LaTeX$ very much.
        &lt;hr&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;This took me so much time but I am quite satisfied with the result if I may say so. You really never stop learning new things!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Finally here ...</title>
      <link>/post/update-february/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-february/</guid>
      <description>&lt;p&gt;The new year is finally taking off for me and I have a couple of updates. First, I would like to remind everyone about the exciting new course &amp;ldquo;understanding health economics in clinical trials&amp;rdquo; that me and 
the rest of our research team &lt;a href=&#34;https://hearteam.blogspot.com/&#34;&gt;HEART&lt;/a&gt; have put together to support the dissemination of health economics among all people involved in the design and analysis of clinical trials.
I look forward to deliver this one-day short course together with my colleagues from the &lt;a href=&#34;https://www.ucl.ac.uk/epidemiology-health-care/research/pcph&#34;&gt;UCL PCPH department&lt;/a&gt; which will be structured into different sessions
during the day of Feb 11th at the &lt;a href=&#34;https://www.ucl.ac.uk/comprehensive-clinical-trials-unit/contact-us&#34;&gt;UCL CCTU&lt;/a&gt; - 2nd Floor, 90 High Holborn, London. The course is specifically intended for those who would like to know more 
about health economics, which has become an important component in the design, analysis and most crucially, for the funding approval of clinical trials. The course will focus on the following aspects:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A short intorduction to the basic concepts of health economics and why these can be relevent to different people&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A review of different types of intruments and tools used to collect health economic data in clinical trials&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A quick look at decision models with some examples&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A summary of the typical results from health economic analyses and how to interpret them&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The course is still in its pilot form and therfore it is free of charge. If there are still places available, you are very welcome to join and give us your feedback!.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/RMTQiRYAuvvJb1k6al/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Second, I am happy to announce that my recent paper about the use of &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/02664763.2020.1723506&#34;&gt;Bayesian Hierarchical Models for the Prediction of Volleyball Results&lt;/a&gt; has finally 
been published on the &lt;a href=&#34;https://www.tandfonline.com/loi/cjas20&#34;&gt;Journal of Applied Statistics&lt;/a&gt;. I am really proud of this paper as it is my first solo paper publiched and because I have always been very invested in the general topic of 
predicting sport results using probability models. To be able to publish something about this based on my own efforts is very rewarding in terms of the (small) contribution to research that I hope I was able to provide.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/l0He0cVv8lGggpruo/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally, I have submitted an abstract to the &lt;a href=&#34;https://euhea.eu/welcome_conference_2020.html&#34;&gt;2020 European Health Economics Association Conference&lt;/a&gt;, which this year will be held in Oslo, Norway.&lt;br&gt;
I have now to patiently wait for the review of the abstracts and see if my work made it, either as an oral presenation or as a poster. Fingers crossed!.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Let us do some work</title>
      <link>/post/update-january2/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-january2/</guid>
      <description>&lt;p&gt;After the terrible start of this year, things are going ok now and I am quite busy with different projects that I left a bit behind. First, I can confirm that me and my colleagues from the &lt;a href=&#34;https://hearteam.blogspot.com/&#34;&gt;HEART group&lt;/a&gt;
are going to give an introductory course to health economic evaluations next month for different groups of people from academia and clinical trial units. The course has been generally structured based on our &amp;ldquo;pilot&amp;rdquo; we gave last year (which went really well by the way)
and involves many different topics that will cover the entire day of February 11th. The attending list is already full and thw waiting list is also quite big; happy to see so much interest in economic evaluations.&lt;/p&gt;
&lt;p&gt;Second, I will give a talk at the PRIMENT statistics and health economics and methodology seminar about an on-going project on missing data in trial-based analysis on Tuesday 28th, at UCL &lt;a href=&#34;https://www.ucl.ac.uk/priment/&#34;&gt;PRIMENT CTU&lt;/a&gt;.
I am really happy to be back at these seminars which I feel I really nice and where you have the opportunity to interact with people from different backgrounds and job positions who may give some useful feedback on my work. Hopefully, 
people will find my research interesting!. I would also like to mention the fact that one of my HEART colleague, &lt;a href=&#34;https://iris.ucl.ac.uk/iris/browse/profile?upi=MCCLE13&#34;&gt;Marie&lt;/a&gt;, will give another talk at the same seminar just before me.
Her topic is the economic analysis plan for a trial she has been involved with and I think she is really good, so may worth check her presentaiton out.&lt;/p&gt;
&lt;p&gt;Third, I have finalised a long-waited submission for a paper which has been discussed, written and re-written many times. I really hope we can get some useful feedback on it as 
I personally worked very hard to keep this work alive. Let see if my efforts have not been in vain and fingers crossed!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/4KxeicCUTvhrW/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Fourth, as a side note, I have recently bought a new book on missing data called &lt;a href=&#34;https://www.springer.com/gp/book/9780387324487&#34;&gt;Semiparametric Thoery and Missing Data&lt;/a&gt; by Tsiatis, which looks very interesting.
To be honest, the book is quite technical with many theoretical concpets and proofs which sometimes I find hard to follow. However, so far it gives a nice introduction to semiparametric models and I look forward to see 
how it approaches the missing data topic from a non likelihood-based approach. If you are into non/semiparametric statistics and want to find out more about this, I recommend the reading.&lt;/p&gt;
&lt;p&gt;Finally, more work is also coming up in the next weeks and some of this is not going to be very enjoyable, I think. Anyway, let us go through this busy period at our best and see how things will go.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Not a very good start...</title>
      <link>/post/update-january/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update-january/</guid>
      <description>&lt;p&gt;After some nice holiday break, I came back to work ready for an exciting 2020 &amp;hellip; or so I thought. Unfortunately, I have recently been caught by a terrible flu which forced me to postpone my flight back to London of a week. 
The worst part is that I was basically a dead corpse moving around with high fever and an awful condition for more than 4 days. It was quite a bad experience which I rarely had in my life. I am just glad I survived this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/12Eo3WBLbH9HRS/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Going back to more interesting news. Before my cursed period, I was smart enough to work on different things and I am happy to announce a new update for my &lt;code&gt;missingHE&lt;/code&gt; package, which is available both on my &lt;a href=&#34;https://github.com/AnGabrio/missingHE&#34;&gt;GitHub page&lt;/a&gt; and on the &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE&#34;&gt;CRAN repository&lt;/a&gt;. 
Its new version is 1.3.2 and has the nice addition of making available more choices for the parametric distributions that can be selected in all main functions of the package to handle missing data in trial-based economic evaluations. In particular, it is now possible
to choose among new probability distributions for the health outcomes, including continuous (Gamma, Weibull, Exponential, Logistic), discrete (Poisson, Negative Binomial) and binary (Bernoulli) distributions. These may be useful when the analysis is not based on utilities scores but some other
types of effects, such as survival time, number of events or binary outcomes. I have also included some examples for each type of outcome in the MenSS dataset (available directly once installed the package on your machine) so that people can play around with the new distributions.&lt;/p&gt;
&lt;p&gt;Another good news is that the last paper written with &lt;a href=&#34;http://users.stat.ufl.edu/~daniels/&#34;&gt;Michael&lt;/a&gt; about missing data handling in economic evaluations will soon be publiched in the February issue of JRSSA, which will make the final and official version of the article that can be cited, I think.&lt;/p&gt;
&lt;p&gt;Finally, an announcement about the one-day course I am holding together with my mates from the &lt;a href=&#34;https://hearteam.blogspot.com/&#34;&gt;HEART group&lt;/a&gt; about an introduction to economic evaluations to people who are not familiar with health economics. The course will take place next month, I believe on Feb 11th, 
in central London (soon an update about the exact location) and, as the previous edition, I am happy to see that all spots have been taken and everything is sold out (well, to be precise the course is free &amp;hellip;). Need to meet up with the others to make the last changes and prepare the slides but I am quite excited about this, given also the good response we got last time.&lt;/p&gt;
&lt;p&gt;Now I am (hopefully) ready to start the new year and there are many things already piling up on my list of things to do in the next days. Let&amp;rsquo;s try again 2020.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Too many things, again....</title>
      <link>/post/update-november/</link>
      <pubDate>Sat, 09 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update-november/</guid>
      <description>&lt;p&gt;I did not have much time to post anything this month until now as it has been a quite busy period. I have been involved in many different works and I have also involved other people in what I think could be some very interesting new projects. Not that I complain about having many different things to do (most of them are actually cool) but doing everything in a short period is not the best.&lt;/p&gt;
&lt;p&gt;A couple of things have come/are coming up. First, I have seriously started working on the coding of a decision model for some health economic evaluation project I have been involved in since last year. Everything seems ok after I spent lots of days and time fixing some small bugs in my code. I am about half way through the model and I hope I will be able to finish it before Christmas (I doubt it though).&lt;/p&gt;
&lt;p&gt;Second, I have finished reviewing an interesting paper about some new methods for improving current practice for dealing with missing data, which I kinda enjoy reading (very good!).&lt;/p&gt;
&lt;p&gt;Third, I would like to quickly summarise my first experience at &lt;a href=&#34;https://www.ispor.org/conferences-education/conferences/past-conferences/ispor-europe-2019&#34;&gt;ISPOR Europe&lt;/a&gt; in Copenhagen. I was really excited to attend this conference which, as expected, revealed itself as huge with people coming from all over the world and with many interesting sessions and discussion topics. I had the chance to meet new and old people, such as professor &lt;a href=&#34;https://www.york.ac.uk/che/staff/research/andrea-manca/&#34;&gt;Andrea Manca&lt;/a&gt; and the always very kind &lt;a href=&#34;https://www.ohe.org/about-us/meet-team/chris-sampson&#34;&gt;Chris Sampson&lt;/a&gt; for whom I was like a stalker asking for more and more information about himself and his work. I also met some of my old collegues from MapiGroup, now under &lt;a href=&#34;https://iconplc.com/&#34;&gt;ICON plc&lt;/a&gt;. It was very fun to hang out with these old friends and see what they have been up to during this time. Among them, I gladly caught up with my dear friend &lt;a href=&#34;https://www.iqce.uni-hamburg.de/people/iqce-fellows/ryan-pulleyblank.html&#34;&gt;Ryan Pulleyblank&lt;/a&gt;, now doing a PhD at the University of Southern Denmark. My poster was a success with (unexpectedly) many people stopping by and asking for more information on my work. I was genuinely surprised by this as ISPOR is mostly a conference dedicated to companies rather than academic works and networking. To sum up, it was a very nice and fun experience and despite the level of statistical methodology was not particularly high I enjoyed my time there and I also had the chance to visit Copenhagen for the first time.&lt;/p&gt;
&lt;p&gt;Finally, as a side note, I have found the time to upload on my arXiv page a nice application of &lt;a href=&#34;https://arxiv.org/abs/1911.08791&#34;&gt;Bayesian hierarchical models for the prediction of volleyball matches&lt;/a&gt; which I have been working on the past summer, taking inspiration from the work of Gianluca about &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/02664760802684177?journalCode=cjas20&#34;&gt;predicting football macthes&lt;/a&gt;. I hope my work can turn out in something cool as well.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/oWA8lD03GUew8/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is all for the moment but soon I will be heading back to another quite busy period for me. I hope this will be the last for some time, especially given that Christmas is coming and I would like to have some free time to properly enjoy this period, which I really like, even more than Christmas itself.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Copenhagen, I am coming ...</title>
      <link>/post/update3-october/</link>
      <pubDate>Mon, 28 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update3-october/</guid>
      <description>&lt;p&gt;Finally the time of &lt;a href=&#34;https://www.ispor.org/conferences-education/conferences/upcoming-conferences/ispor-europe-2019&#34;&gt;ISPOR Europe 2019&lt;/a&gt; has arrived and I will depart in a few days for 
Copenhagen, where the conference is held this year. I am actually looking forward to this as I am curious to see what type of conference ISPOR is, that is, whether I will be able to find
some interesting works and have some &amp;ldquo;applied statistics&amp;rdquo;-related discussions or the attention is more placed on &amp;ldquo;economics and clinical&amp;rdquo; matters. From what I heard by other people who 
routinely attend the conference, there should be a bit of both sides, even though I really hope I will be able to see some intersting methods and engage in discussion with some authors.&lt;/p&gt;
&lt;p&gt;I know the conference is mainly related to address the needs of pharmaceutical and consultancy companies, but I hope I will be able to see some familiar faces there. Well, to be
honest I know that some people I already know are going, which is good considering that their work is really cool. As for me, I will present the same work that I showed at ICTMC 2019 (some slides available &lt;a href=&#34;https://www.luminpdf.com/viewer/5dbd43939a40480018633f2e&#34;&gt;here&lt;/a&gt;),
but this time in the format of a poster, of which I am kind of very proud in terms of the final output, if I may say so.&lt;/p&gt;
&lt;p&gt;Apart from this nice event, there are many things coming up when I will be back from the conference, which I really need to start working on. Mostly, these are related to some
routine work for some trial analyses at &lt;a href=&#34;https://www.ucl.ac.uk/priment/&#34;&gt;PRIMENT&lt;/a&gt;, which by the way is advertising a new health economist &lt;a href=&#34;https://www.jobs.ac.uk/job/BWK840/research-fellow-in-health-economics&#34;&gt;job vacancy&lt;/a&gt; for those who might be interested. 
Other tasks include writing down and code a decision model on which I have been working since ages, papers review, other collaborations with different people, starting my co-supervison for a new PhD student at stats and, after I can find some free time, do some reasearch work on my beloved missing data. 
Am I ready? not sure about that &amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/fy0gLJtIkZj8I/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conferences updates and news</title>
      <link>/post/update2-october/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update2-october/</guid>
      <description>&lt;p&gt;Just a quick update about some talks I gave/am about to give to advertise my research work. The one in Brighton, which I gave a couple of weeks a go at &lt;a href=&#34;https://ictmc2019.com/&#34;&gt;ICTMC&lt;/a&gt;, went really well and I was glad to hear that some people were very interested in what I presented. For more info, here a &lt;a href=&#34;https://www.luminpdf.com/viewer/5daad5f7ad8625001932b9a4&#34;&gt;link&lt;/a&gt; to my presentation about missing data methods for trial-based economic evaluations that I discussed.
Honestly, since the conference was mainly directed towards people working in clinical trials, I did not expect a huge interest in the use of Bayesian methods for economic evaluations, but apparently (and I am happy about that) I was wrong.&lt;/p&gt;
&lt;p&gt;I had the chance to chat a bit with few people that I did not know, including &lt;a href=&#34;http://www.bristol.ac.uk/social-community-medicine/people/william-hollingworth/overview.html&#34;&gt;William Hollingworth&lt;/a&gt; from Bristol and &lt;a href=&#34;https://www.ndorms.ox.ac.uk/team/ines-rombach&#34;&gt;Ines Rombach&lt;/a&gt; from Oxford, with whom I had very nice conversations about my work and other interesting topics.
I was also glad to meet some known faces, including the always lovely &lt;a href=&#34;https://cheme.bangor.ac.uk/CatrinPlumptonBiography.php&#34;&gt;Catrin Plumpton&lt;/a&gt; from Bangor University, who I met for the first time at HESG this summer and with whom I share the interest in missing data methods (even though she is a STATA and multiple imputation user, sadly). 
I am also glad that I met my previous PhD secondary supervisor, &lt;a href=&#34;https://www.lshtm.ac.uk/aboutus/people/mason.alexina&#34;&gt;Alexina Mason&lt;/a&gt;, with whom it is always a pleasure to talk with. Unfortunately, we both missed the talk of each other becuase of time problems but it was good to catch up with her again. I am also sad that I could
not attend &lt;a href=&#34;https://www.lshtm.ac.uk/aboutus/people/leurent.baptiste&#34;&gt;Baptiste&lt;/a&gt;&amp;rsquo;s presentation which was the last day of the conference (I had to leave the same day of my talk, the first day) and I was not also able to actually meet him. I hope we will be able to see him soon at some other conference in the near future.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/YATNr2oXRo0IE/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Given this past experience, I am now looking forward to meet new people at my next conference at the Bella Center in Copenhagen (thumbnail) where this year &lt;a href=&#34;https://www.ispor.org/conferences-education/conferences/upcoming-conferences/ispor-europe-2019&#34;&gt;ISPOR Europe 2019&lt;/a&gt; will be held. However, I believe this will be a much larger conference and therefore I will probably not have many chances to talk with people as I did at ICTMC.
Plus I am only preseting a poster this time, so it will be less likely that some people will actually notice my work, especially given the typically huge amount of presenters of this type of conferences. In the wrost case, I will enjoy Copenhagen and meet up with some old friends who live in Denmark and who will come at ISPOR to present some other work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>More good news...</title>
      <link>/post/update-october/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update-october/</guid>
      <description>&lt;p&gt;I have got two news coming up. First, the paper I wrote with Michael and Gianluca on Bayesian methdos for longitudinal data in trial-based economic evaluations has finally been published as early view on &lt;a href=&#34;https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12522&#34;&gt;JRSSA&lt;/a&gt;. As I said in some earlier posts, I am super happy about this collaboration and I hope I can continue working on similar projects in the future.&lt;/p&gt;
&lt;p&gt;Second, I will soon give a talk about this work at the &lt;a href=&#34;https://ictmc2019.com/&#34;&gt;ICTMC&lt;/a&gt; conference in Brighton, next Monday. This will be the first time at this conference and unfortunately I will only be able to remain around for one day as I need to go back to London pretty soon. I hope I will be able to enjoy my day at the conference, even though I will miss the talks of &lt;a href=&#34;https://www.lshtm.ac.uk/aboutus/people/leurent.baptiste&#34;&gt;Baptiste&lt;/a&gt; and &lt;a href=&#34;https://www.lshtm.ac.uk/aboutus/people/mason.alexina&#34;&gt;Alexina&lt;/a&gt; which are scheduled for the last day of the conference. I hope I can at least have a quick chat with them the day I am around.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/efDT7dqlF5N2LVHG8C/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I am also excited to visit &lt;a href=&#34;https://en.wikipedia.org/wiki/Brighton&#34;&gt;Brighton&lt;/a&gt;, since many people keep telling me that I should go and visit this sort of british version of &amp;ldquo;Rimini&amp;rdquo;. To be honest, I do not expect to find a nice wheather, given that in this period it is raining a lot in London, but I hope I will be lucky and get the only sunny day of the week.&lt;/p&gt;
&lt;p&gt;Finally, I have started a rubric called &lt;a href=&#34;https://agabrioblog.onrender.com/missingdata/&#34;&gt;missing data&lt;/a&gt; on my website, where I try to describe some of the most popular methods to handle missing data and to provide some references for anyone who could be interested in this field. I am really fascinated by statistical methods for dealing with missingness, perhaps because it was the main focus of my PhD, but I am eager to review different methods and see if I can find something really interesting. Of course, to complete this it will take more time, which I hope I will be able to find in the next months.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MissingHE 1.2.1</title>
      <link>/post/missinghe-version121/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/missinghe-version121/</guid>
      <description>&lt;p&gt;I have finally found some time to update the version for my R package &lt;a href=&#34;https://agabrioblog.onrender.com/missingHE/&#34;&gt;missingHE&lt;/a&gt;, for which version 1.2.1 is now available on &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE/&#34;&gt;CRAN&lt;/a&gt;. 
I included two main features to the previous version of the package.&lt;/p&gt;
&lt;p&gt;First, I have added a new type of identifying restriction when fitting pattern mixture models through the function &amp;ldquo;pattern&amp;rdquo;. Before, only the complete case
restriction was available, which identifies the distributions of the missing data with those from the completers. Now the alternative available case restriction is can also be selected, which relies on the distributions that can be identified 
among the non-completers to identify the distributions of the missing data. In this way, people can choose among at least two options for the type of restrictions and compare how this choice may affect the final estimates.&lt;/p&gt;
&lt;p&gt;Second, I added a new accessory function called &amp;ldquo;ppc&amp;rdquo;, which allows to perform posterior predictive checks using the conditional parameters saved from the fitted model to generate replications of the data at each posterior iteration of the model.
The function implements a relatively large number of checks, mostly taken from the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/bayesplot/&#34;&gt;bayesplot&lt;/a&gt;, which allow to assess the fit of the model to the observed data by type of outcome (effects and costs) and treatment group (control and intervention).
For example, overalyed density plots can be generated to compare the empirical and replicated densities of the data to detect possible failures of the model.&lt;/p&gt;



  




&lt;figure&gt;

&lt;img src=&#34;/img/plotec.png&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Density plots for the observed and replicated data&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;I feel this is very important as when fitting a Bayesian model it is crucial to assess whether the model seems to adequately capture the different characteristics of the observed data (e.g. skewness, structural values, etc.). 
A wide range of predictive checks are available, including histograms (see thumbnail pciture), scatterplots, error intervals, empirical cumulative distribution fucntions, statistcis of interest and many others. In addition ,
these checks can be performed for each type of missingness model and parametric distribution chosen within missingHE.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/NEvPzZ8bd1V4Y/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Of course, it is important to remember that, when dealing with missing data the fit of the model can only be checked with respect to the observed values and therefore this 
check is only partial since the fit to the unibserved values can never be checked. This is also why it is not meaningful to assess the fit of a model fitted under a missing not at random assumption
because this is based on information which is not directly available from the data at hand and thus impossible to check.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Discussing my thesis</title>
      <link>/post/update-interview/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update-interview/</guid>
      <description>&lt;p&gt;I have been kindly invited by the amazing person &lt;a href=&#34;https://www.ohe.org/about-us/meet-team/chris-sampson&#34;&gt;Chris Sampson&lt;/a&gt; to talk about the work I inlcuded in my PhD thesis for his monthly rubric entitled  &amp;ldquo;Thesis Thursday&amp;rdquo; on the &lt;a href=&#34;https://aheblog.com/&#34;&gt;The Academic Health Economists blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I happily accepted Chris&amp;rsquo;s invitation as I beleive this initiative is really interesting and represents a nice way for newly graduated PhD students to advertise their work while also giving the chance to people interested in health economics to read about some academic work which is typically freely available to everyone.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://aheblog.com/2019/09/19/thesis-thursday-andrea-gabrio/&#34;&gt;Here&lt;/a&gt; you can find the full interview, which is not very long and resolves around 5 questions that Chris asked me about my work. I already new this blog but I have never had a proper chance to read through its posts carefully, which is a shame.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/13lTgtSUmqMrlu/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I shall promise myself to try to check it more often from now on, using this interview as a nice motivation to do so.
In fact, there are not many blogs around health economics matters (here a &lt;a href=&#34;https://blog.feedspot.com/health_economics_blogs/&#34;&gt;non-comprehensive list&lt;/a&gt;), among which The Academic Health Economists and &lt;a href=&#34;http://www.statistica.it/gianluca/blog/&#34;&gt;Gianluca&amp;rsquo;s blog&lt;/a&gt; are my favourites.&lt;/p&gt;
&lt;p&gt;I hope I will be able to find some time to write some nice posts about some health economic applications of my work in the next future as this is still the most interesting field for me at the moment. I am also the maintainer of another small blog called the &lt;a href=&#34;https://hearteam.blogspot.com/&#34;&gt;Health Economics Analysis and Research Methods Team (HEART) blog&lt;/a&gt;, where I occasionally write some posts on health economics together with my colleagues from the UCL department of &lt;a href=&#34;https://www.ucl.ac.uk/epidemiology-health-care/research/pcph&#34;&gt;Primary Care and Population Health&lt;/a&gt;. The blog is still new but I hope it can become more active in the next months.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some good news...</title>
      <link>/post/update-september/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update-september/</guid>
      <description>&lt;p&gt;With the approaching of the new academic here I have received some good news for my most recently submitted paper on Bayesian parametric modelling in health economics for missing longitudinal data, which at the moment is only available on &lt;a href=&#34;https://arxiv.org/abs/1805.07147&#34;&gt;arXiv&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I am happy to announce that, after a couple of rounds of reviews, the paper has been finally accepted for publication in &lt;a href=&#34;https://rss.onlinelibrary.wiley.com/journal/1467985X&#34;&gt;JSS: Series A&lt;/a&gt;. I believe that the reviewers provided a very nice feedback for improving the work and I am quite satisfied with the final version of the article which, I hope, will be of interest for anyone involved in the analsysi of partially-observed longitudinal data. I hope the pre-print of the paper will be available soon and I will &amp;ldquo;advertise&amp;rdquo; my work in two conferences in the next couple of months, where I will present the content of the paper, namely &lt;a href=&#34;https://ictmc2019.com/&#34;&gt;ICTMC&lt;/a&gt; this October in Brighton, and &lt;a href=&#34;http://www.ispor.org/conferences-education/conferences/upcoming-conferences/ispor-europe-2019&#34;&gt;ISPOR Europe&lt;/a&gt; this November in Copenhagen.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/vMEjhlxsBR7Fe/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I am really excited about this paper which represented the last part of my PhD thesis and on which I worked really hard in the last year of my studies. &lt;a href=&#34;https://agabrioblog.onrender.com/project/missing-data/&#34;&gt;Here&lt;/a&gt; you can find a general summary of the content of the article, while &lt;a href=&#34;https://www.ucl.ac.uk/statistics/sites/statistics/files/presentation_priment_1.pdf&#34;&gt;here&lt;/a&gt; there are some slides that describe the main idea behind the proposed model.&lt;/p&gt;
&lt;p&gt;I just want to conlcude with some thanks with my co-authors of the paper, &lt;a href=&#34;http://users.stat.ufl.edu/~daniels/&#34;&gt;Michael&lt;/a&gt; and &lt;a href=&#34;https://www.ucl.ac.uk/statistics/people/gianlucabaio&#34;&gt;Ginaluca&lt;/a&gt;, without whom I would have not been able to write this paper. This was my first work with Mike, with whom I had a wonderful collaboration and I was able to visit the beatiful city of &lt;a href=&#34;https://en.wikipedia.org/wiki/Gainesville,_Florida&#34;&gt;Gainesville&lt;/a&gt; (FL) during my first visiting period at the &lt;a href=&#34;https://www.ufl.edu/&#34;&gt;University of Florida&lt;/a&gt; (see thumbnail picture). I hope this will be the first of many works together in the furture!.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The P value fallacy</title>
      <link>/post/p-value-fallacy/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/p-value-fallacy/</guid>
      <description>&lt;p&gt;Today, I would like to briefly comment an interesting research article written by &lt;a href=&#34;https://jhu.pure.elsevier.com/en/publications/toward-evidence-based-medical-statistics-1-the-p-value-fallacy-4&#34;&gt;Goodman&lt;/a&gt;, who provided a clear and exemplary discussion about the typical incorrect interpretation of a standard frequentist analysis in the field of medical research. I will now briefly summarise the main argument of the paper and then add some personal comments.&lt;/p&gt;
&lt;p&gt;Essentially, the article describes the characteristics of the dominant school of medical statistics and highlights the logical fallacy at the heart of the typical frequentist analysis in clinical studies. This is based on a &lt;em&gt;deductive&lt;/em&gt; inferential approach, which starts with a given hypothesis and makes conclusions under the assumption that the hypothesis is true. This is in contrast with a &lt;em&gt;inductive&lt;/em&gt; approach, which uses the observed evidence to evaluate what hypothesis is most tenable. The two most popular methods of the frequentist paradigm are the &lt;em&gt;P value&lt;/em&gt; proposed by &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_6&#34;&gt;Fisher&lt;/a&gt; and the &lt;em&gt;hypothesis testing&lt;/em&gt; developed by &lt;a href=&#34;https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1933.0009?casa_token=sbSkualIaPYAAAAA%3ACxPsFTFEUK7vaxMPi5dJwUr4HoUWjrkxNh7Hl2q0owjtcU2wJHnakG-Xug7y95v1Tyqbbc8Mymaq_Q&amp;amp;&#34;&gt;Neyman and Pearson&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The P value is defined as the probability, under the assumption of no effect (null hypothesis), of obtaining a result equal to or more extreme than what was actually observed. Fisher proposed it as an informal index to be used as a measure of discrepancy between the data and the null hypothesis and therefore should not be interpreted as a formal inferential method. For example, since the P value can only be calculated on the assumption that the null hypothesis is true, it cannot be a direct measure of the probability that the null hypothesis is false. However, the main criticism to the P value is perhaps that it does not take into account the size of the observed effect, i.e. a small effect in a study with a large sample size can have the same P value as a large effect in a small study.&lt;/p&gt;
&lt;p&gt;Hypothesis testing was proposed by Neyman and Pearson as an alternative approach to the P value, which assumes the existence of a null hypothesis (e.g. no effect) and an alternative hypothesis (e.g. nonzero effect). The outcome of the test is then simply to reject one hypothesis in favour of the other, solely based on the data. This exposes the researcher to two types of errors: type I error or false-positive ($\alpha$) and type II error or false-negative ($\beta$) result. Rather than focussing on single experiments, like the P value, hypothesis testing is effectively based on a deductive approach to minimise the errors over a large number of experiments. However, the price to pay to obtain this &lt;em&gt;objectivity&lt;/em&gt; is the impossibility to make any inferential statement about a single experiment. The procedure only guarantees that in the long run, i.e. after considering many experiments, we shall not often be wrong.&lt;/p&gt;
&lt;p&gt;Over time a combination between the P value and hypothesis testing was developed under the assumption that the two approaches can be complementary. The idea was that the P value could be used to measure evidence in a single experiment while not violating the long run logic of hypothesis testing. The combined method is characterized by setting $\alpha$ and power $\beta$ before the experiment, then calculating a P value and rejecting the null hypothesis if the P value is less than the preset type I error rate. This means that the P value is considered a false-positive error rate specific to the data and also a measure of evidence against the null hypothesis. The &lt;strong&gt;P value fallacy&lt;/strong&gt; is born from this statement, which assumes that an event can be seen simultaneously from a long run perspective (where the observed results are put together with other results that might have occurred in hypothetical repetitions of the experiment) and from a short run perspective (where the observed results are interpreted only with respect to the single experiment). However, these views are not reconcilable since a result cannot be at the same time an interchangeable (long-run) and unique (short-run) member of a group of results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/JszzkKOlV6gTK/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I personally find this discussion fascinating and I believe that it is important to recognise the inconsistencies between the two alternative approaches to inference. The original authors of the two paradigms were well aware of the implications of their methods and never supported the combination of these. However, the combined approach has somehow become widely accepted in practice while its internal inconsistencies and conceptual limitations are hardly recognised.&lt;/p&gt;
&lt;p&gt;I feel that, since the two methods are perceived as &amp;ldquo;objective&amp;rdquo;, it is generally accepted that, if combined, they can produce reliable conclusions. This, however, is not necessarily true. Accepting at face value the significance result as a binary indicator of whether or not a relation is real is dangeroues and potentially misleading. This practice wants to show that conclusions are being drawn directly from the data, without any external influence, because direct inference from data to hypothesis is thought to result in mistaken conclusions only rarely and is therefore regarded as &amp;ldquo;scientific&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;This misguided approach has led to a much stronger emphasis towards the quantitative results alone (without any external input). In contrast, I believe that such perspective has the serious drawback of ignoring potentially useful information which is available (e.g. relevant medical knowledge or historical data) and which should be included in the analysis. Of course, I am aware of the potential issues that may arise from the selection and incorporation of external evidence, but I believe this should not be considered as &amp;ldquo;less reliable&amp;rdquo; or &amp;ldquo;more prone to mistakes&amp;rdquo; compared with the evidence from the available data. It is important that an agreement is reached about the selection of the type of evidence and methods to be used to perform the analysis solely based on their relevance with respect to the context analysed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HESG Summer Meeting 2019</title>
      <link>/post/hesg-summer-meeting-2019/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/hesg-summer-meeting-2019/</guid>
      <description>&lt;p&gt;I have just come back form my first Health Economists&amp;rsquo; Study Group (&lt;a href=&#34;https://hesg.org.uk/meetings/summer-2019-university-of-east-anglia/&#34;&gt;HESG&lt;/a&gt;) meeting, which this year was held at the &lt;a href=&#34;https://www.uea.ac.uk/&#34;&gt;University of East Anglia&lt;/a&gt; in the beautiful city of Norwich, 
south east of England, and where I presented some preliminary results from one of my on-going works. I have to say, it was a remarkable experience which 
I really liked thanks to a wonderful and welcoming environment. I had the pleasure to talk to many people from different research areas involved in 
health economics (both from academia and industry) and to see many different projects and works.&lt;/p&gt;
&lt;p&gt;I particularly enjoy the structure of the meeting, which requires some chair and discussant who have to present and discuss the paper of the authors, 
who are only allowed to provide some clarification if needed. At first I thought this structure of the sessions was strange, but after attending many 
sessions and experiencing this for my own paper, I feel that it is a very good way to encourage discussion about works from different people rather than 
just focussing on your own presentation. Plus, the weather and always sunny, it felt like Italy for a few days.&lt;/p&gt;



  




&lt;figure&gt;

&lt;img src=&#34;/img/Norwich.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;The beautiful Norwich&amp;rsquo;s cathedral&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Other nice people and colleagues from HEART and other UCL department came to HESG with me, including &lt;a href=&#34;https://iris.ucl.ac.uk/iris/browse/profile?upi=CSCLA53&#34;&gt;Caroline&lt;/a&gt; and &lt;a href=&#34;https://www.ucl.ac.uk/comprehensive-clinical-trials-unit/meet-team/health-economics/junior-health-economist-ekaterina-kuznetsova&#34;&gt;Ekaterina&lt;/a&gt; (aka Katia), 
you can see them in thumbnail of this post. I was also pleased to meet &lt;a href=&#34;https://www.lshtm.ac.uk/aboutus/people/leurent.baptiste&#34;&gt;Baptiste&lt;/a&gt; from &lt;a href=&#34;https://www.lshtm.ac.uk/&#34;&gt;LSHTM&lt;/a&gt;, who shares with me the interest in missing data 
methods for cost-effectiveness analysis and who presented some very nice work on that. I had the chance to give some feedback to him and he did the same for me. 
It felt so nice when we started discussing about some aspects of our analyses and after some minutes we simply lost track of time and everyone else disappeared. 
I also had the opportunity to talk about my work with the discussant of my session, &lt;a href=&#34;https://cheme.bangor.ac.uk/CatrinPlumptonBiography.php&#34;&gt;Catrin Plumpton&lt;/a&gt; from the &lt;a href=&#34;https://cheme.bangor.ac.uk/&#34;&gt;Centre for Health Economics and Medicines Evaluation&lt;/a&gt;, 
who gave me some nice feedback which I really appreciated, especially given her mathematical background.&lt;/p&gt;
&lt;p&gt;An important contribution to the success of the meeting was also given by the wonderful organisation of the event, including an accommodation located very closely 
to the main building of the meeting, plenty of food provided during each day, a nice bus tour of the city and a wonderful conference dinner. I must thank all the people, 
who organised the event who were very extremely nice to us and who were always ready to help us for whatever need we had, with a special mention for &lt;a href=&#34;https://people.uea.ac.uk/emma_mcmanus&#34;&gt;Emma Mcmanus&lt;/a&gt; who 
was amazing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/cWnICjtVkJJsgGKhyX/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In summary, everything was good. Well, almost. Going back to the works presented, as usual, the only less positive note that I would like to make 
is the almost total absence of Bayesian applications. Some authors mentioned that they used some popular Bayesian program, such as &lt;a href=&#34;https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-winbugs/&#34;&gt;WinBUGS&lt;/a&gt;, but this was 
mainly related to the usual meta-analysis stuff which is pretty standardised. I hope next time I will be able to see more people going Bayesian as this is what I am.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding health economics in clinical trials</title>
      <link>/post/understanding-health-economics-in-clinical-trials/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/understanding-health-economics-in-clinical-trials/</guid>
      <description>&lt;p&gt;As member of the Health Economics Analysis and Research Methods Team (&lt;a href=&#34;https://hearteam.blogspot.com/&#34;&gt;HEART&lt;/a&gt;), together with my colleagues, on Tuesday 2 July I took part in a 1-day introductory short course entitled “Understanding health economics in clinical trials”, which was designed and delivered by the team. HEART is a new group of health economists who are based in UCL’s Institute of Clinical Trials and Methodology (ICTM), led by &lt;a href=&#34;https://iris.ucl.ac.uk/iris/browse/profile?upi=RMHUN48&#34;&gt;Rachael Hunter&lt;/a&gt;, and is involved at different levels in the economic components of clinical trials in different trial units at UCL. This short course was aimed at ICTM staff who are not health economists (e.g. trial managers, CIs/PIs, statisticians, data managers, research assistants, etc.) and was designed in response to the need we have identified over the last few years in working on trials as well as in response to colleagues across ICTM. This course was unique as it was intended specifically for non health economists working in trials, who wish to better understand the health economics in their study, and/or the health economist on their study. The course used a mix of lectures, group discussions and practical exercises to help participants consolidate their learning and see how to apply information from the sessions to real studies. No prior knowledge of health economics was assumed.&lt;/p&gt;
&lt;p&gt;I believe the course was a success both in terms of the quality/quantity of the material covered during the six sessions throughout the day, as well as in terms of the positive feedback we received from the participants (almost entirely women, with the exception of two men). Many key and typically not well understood economic topics were discussed during the day, e.g. what are and how QALYs and costs are calculated, the potential limitations and issues of an economic analysis within a trial, or the role played by the protocol and analysis plan in the economic evaluation. My session was related to reporting and interpreting health economic results and I realised that most people who do not routinely deal with health economics may find difficult to grasp certain concepts or tools used in the economic analysis (e.g. what is a cost-effectiveness acceptability curve and how it can be computed). Nevertheless, I must admit that I was surprised by how many people were very motivated to learn these concepts and these &amp;ldquo;difficult&amp;rdquo; methods, often asking questions and making good comments (despite the fact that my session was the last of the course at the end of the day).   &lt;br&gt;
We ran this course as a trial as we did not have clear ideas of what an optimal design should be or the number of topics that should be covered for this type of course. We are now confident that the course has a solid structure and that there is a clear demand to learn the basic concepts of health economics, at least among people involved in trial analyses. Following the successful delivery of the course, we are planning to replicate the experience in the future, improving certain aspects of the sessions based on the feedback we received and also considering to open the course to meet the demand of a wider audience.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/bQrVMr3CO3QaY/giphy.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I have to say that this was an extremely positive experience for me as it was the first time I was involved in this type of projects. Me and my colleagues worked hard to design and prepare the different sessions of the course over the last few months, find the best way to link the arguments across the sessions, provide interesting group activities and materials for the practicals, etc. I have to thank all my colleagues who contributed to the promotion and realisation of this project, with a special mention for &lt;a href=&#34;https://iris.ucl.ac.uk/iris/browse/profile?upi=CSCLA53&#34;&gt;Caroline Clarke&lt;/a&gt;, who spent a lot of time and effort to organise the course and who personally contributed in giving one of the session of the course. Finally, I would also like to thank my colleague and health economist &lt;a href=&#34;https://www.ucl.ac.uk/comprehensive-clinical-trials-unit/meet-team/health-economics/junior-health-economist-ekaterina-kuznetsova&#34;&gt;Ekaterina&lt;/a&gt;, with whom I had the pleasure to share the presentation and practical of my session in the course.&lt;/p&gt;
&lt;p&gt;Perhaps the only true negative aspect of the course was the absence of a Bayesian perspective, especially related to the interpretation of the results and the statistical methods that can be used to perform the analysis. Given the generally low familiarity of the people attending the course with statistics, I believe it was reasonable not to further confuse them with another new element into the picture. However, I truly hope that people will become more and more familiar with the importance of using tailored statistical methods in economic evaluations to avoid biased results, and from that point to justify a Bayesian approach, well, at least for me, the step is straightforward!.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
