<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Andrea Gabrio</title>
    <link>/</link>
    <description>Recent content on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2020 11:15:00 +0000</lastBuildDate>
    
	    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Choosing the Missing Data Method in Trial-Based Economic Evaluations. How to Make the Right Choice?</title>
      <link>/talk/priment2020/</link>
      <pubDate>Tue, 28 Jan 2020 11:15:00 +0000</pubDate>
      
      <guid>/talk/priment2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MissingHE An R package to deal with missing data in trial based health economic evaluations</title>
      <link>/talk/priment2019/</link>
      <pubDate>Mon, 09 Dec 2019 13:00:00 +0000</pubDate>
      
      <guid>/talk/priment2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Not a very good start...</title>
      <link>/post/update-january/update-november/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update-january/update-november/</guid>
      <description>&lt;p&gt;After some nice holiday break, I came back to work ready for an exciting 2020 &amp;hellip; or so I thought. Unfortunately, I have recently been caught by a terrible flu which forced me to postpone my flight back to London of a week.
The worst part is that I was basically a dead corpse moving around with high fever and an awful condition for more than 4 days. It was quite a bad experience which I rarely had in my life. I am just glad I survived this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/12Eo3WBLbH9HRS/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Going back to more interesting news. Before my cursed period, I was smart enough to work on different things and I am happy to announce a new update for my &lt;code&gt;missingHE&lt;/code&gt; package, which is available both on my &lt;a href=&#34;https://github.com/AnGabrio/missingHE&#34; target=&#34;_blank&#34;&gt;GitHub page&lt;/a&gt; and on the &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE&#34; target=&#34;_blank&#34;&gt;CRAN repository&lt;/a&gt;.
Its new version is 1.3.2 and has the nice addition of making available more choices for the parametric distributions that can be selected in all main functions of the package to handle missing data in trial-based economic evaluations. In particular, it is now possible
to choose among new probability distributions for the health outcomes, including continuous (Gamma, Weibull, Exponential, Logistic), discrete (Poisson, Negative Binomial) and binary (Bernoulli) distributions. These may be useful when the analysis is not based on utilities scores but some other
types of effects, such as survival time, number of events or binary outcomes. I have also included some examples for each type of outcome in the MenSS dataset (available directly once installed the package on your machine) so that people can play around with the new distributions.&lt;/p&gt;

&lt;p&gt;Another good news is that the last paper written with &lt;a href=&#34;http://users.stat.ufl.edu/~daniels/&#34; target=&#34;_blank&#34;&gt;Michael&lt;/a&gt; about missing data handling in economic evaluations will soon be publiched in the February issue of JRSSA, which will make the final and official version of the article that can be cited, I think.&lt;/p&gt;

&lt;p&gt;Finally, an announcement about the one-day course I am holding together with my mates from the &lt;a href=&#34;https://hearteam.blogspot.com/&#34; target=&#34;_blank&#34;&gt;HEART group&lt;/a&gt; about an introduction to economic evaluations to people who are not familiar with health economics. The course will take place next month, I believe on Feb 11th,
in central London (soon an update about the exact location) and, as the previous edition, I am happy to see that all spots have been taken and everything is sold out (when to be precise the course is free&amp;hellip;). Need to meet up with the others to make the last changes and prepare the slides but I am quite excited about this, given also the good response we got last time.&lt;/p&gt;

&lt;p&gt;Now I am (hopefully) ready to start the new year and there are many things already piling up on my list of things to do in the next days. Let&amp;rsquo;s try again 2020.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Hierarchical Models for the Prediction of Volleyball Results</title>
      <link>/publication/gabrio2019e/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2019e/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Too many things, again....</title>
      <link>/post/update-november/update-november/</link>
      <pubDate>Sat, 09 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update-november/update-november/</guid>
      <description>&lt;p&gt;I did not have much time to post anything this month until now as it has been a quite busy period. I have been involved in many different works and I have also involved other people in what I think could be some very interesting new projects. Not that I complain about having many different things to do (most of them are actually cool) but doing everything in a short period is not the best.&lt;/p&gt;

&lt;p&gt;A couple of things have come/are coming up. First, I have seriously started working on the coding of a decision model for some health economic evaluation project I have been involved in since last year. Everything seems ok after I spent lots of days and time fixing some small bugs in my code. I am about half way through the model and I hope I will be able to finish it before Christmas (I doubt it though).&lt;/p&gt;

&lt;p&gt;Second, I have finished reviewing an interesting paper about some new methods for improving current practice for dealing with missing data, which I kinda enjoy reading (very good!).&lt;/p&gt;

&lt;p&gt;Third, I would like to quickly summarise my first experience at &lt;a href=&#34;https://www.ispor.org/conferences-education/conferences/past-conferences/ispor-europe-2019&#34; target=&#34;_blank&#34;&gt;ISPOR Europe&lt;/a&gt; in Copenhagen. I was really excited to attend this conference which, as expected, revealed itself as huge with people coming from all over the world and with many interesting sessions and discussion topics. I had the chance to meet new and old people, such as professor &lt;a href=&#34;https://www.york.ac.uk/che/staff/research/andrea-manca/&#34; target=&#34;_blank&#34;&gt;Andrea Manca&lt;/a&gt; and the always very kind &lt;a href=&#34;https://www.ohe.org/about-us/meet-team/chris-sampson&#34; target=&#34;_blank&#34;&gt;Chris Sampson&lt;/a&gt; for whom I was like a stalker asking for more and more information about himself and his work. I also met some of my old collegues from MapiGroup, now under &lt;a href=&#34;https://iconplc.com/&#34; target=&#34;_blank&#34;&gt;ICON plc&lt;/a&gt;. It was very fun to hang out with these old friends and see what they have been up to during this time. Among them, I gladly caught up with my dear friend &lt;a href=&#34;https://www.iqce.uni-hamburg.de/people/iqce-fellows/ryan-pulleyblank.html&#34; target=&#34;_blank&#34;&gt;Ryan Pulleyblank&lt;/a&gt;, now doing a PhD at the University of Southern Denmark. My poster was a success with (unexpectedly) many people stopping by and asking for more information on my work. I was genuinely surprised by this as ISPOR is mostly a conference dedicated to companies rather than academic works and networking. To sum up, it was a very nice and fun experience and despite the level of statistical methodology was not particularly high I enjoyed my time there and I also had the chance to visit Copenhagen for the first time.&lt;/p&gt;

&lt;p&gt;Finally, as a side note, I have found the time to upload on my arXiv page a nice application of &lt;a href=&#34;https://arxiv.org/abs/1911.08791&#34; target=&#34;_blank&#34;&gt;Bayesian hierarchical models for the prediction of volleyball matches&lt;/a&gt; which I have been working on the past summer, taking inspiration from the work of Gianluca about &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/02664760802684177?journalCode=cjas20&#34; target=&#34;_blank&#34;&gt;predicting football macthes&lt;/a&gt;. I hope my work can turn out in something cool as well.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/oWA8lD03GUew8/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is all for the moment but soon I will be heading back to another quite busy period for me. I hope this will be the last for some time, especially given that Christmas is coming and I would like to have some free time to properly enjoy this period, which I really like, even more than Christmas itself.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations</title>
      <link>/talk/isporeu2019/</link>
      <pubDate>Mon, 04 Nov 2019 13:00:00 +0000</pubDate>
      
      <guid>/talk/isporeu2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Copenhagen, I am coming ...</title>
      <link>/post/update3-october/update3-october/</link>
      <pubDate>Mon, 28 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update3-october/update3-october/</guid>
      <description>&lt;p&gt;Finally the time of &lt;a href=&#34;https://www.ispor.org/conferences-education/conferences/upcoming-conferences/ispor-europe-2019&#34; target=&#34;_blank&#34;&gt;ISPOR Europe 2019&lt;/a&gt; has arrived and I will depart in a few days for
Copenhagen, where the conference is held this year. I am actually looking forward to this as I am curious to see what type of conference ISPOR is, that is, whether I will be able to find
some interesting works and have some &amp;ldquo;applied statistics&amp;rdquo;-related discussions or the attention is more placed on &amp;ldquo;economics and clinical&amp;rdquo; matters. From what I heard by other people who
routinely attend the conference, there should be a bit of both sides, even though I really hope I will be able to see some intersting methods and engage in discussion with some authors.&lt;/p&gt;

&lt;p&gt;I know the conference is mainly related to address the needs of pharmaceutical and consultancy companies, but I hope I will be able to see some familiar faces there. Well, to be
honest I know that some people I already know are going, which is good considering that their work is really cool. As for me, I will present the same work that I showed at ICTMC 2019 (some slides available &lt;a href=&#34;https://www.luminpdf.com/viewer/5dbd43939a40480018633f2e&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;),
but this time in the format of a poster, of which I am kind of very proud in terms of the final output, if I may say so.&lt;/p&gt;

&lt;p&gt;Apart from this nice event, there are many things coming up when I will be back from the conference, which I really need to start working on. Mostly, these are related to some
routine work for some trial analyses at &lt;a href=&#34;https://www.ucl.ac.uk/priment/&#34; target=&#34;_blank&#34;&gt;PRIMENT&lt;/a&gt;, which by the way is advertising a new health economist &lt;a href=&#34;https://www.jobs.ac.uk/job/BWK840/research-fellow-in-health-economics&#34; target=&#34;_blank&#34;&gt;job vacancy&lt;/a&gt; for those who might be interested.
Other tasks include writing down and code a decision model on which I have been working since ages, papers review, other collaborations with different people, starting my co-supervison for a new PhD student at stats and, after I can find some free time, do some reasearch work on my beloved missing data.
Am I ready? not sure about that &amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/fy0gLJtIkZj8I/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conferences updates and news</title>
      <link>/post/update2-october/update2-october/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update2-october/update2-october/</guid>
      <description>&lt;p&gt;Just a quick update about some talks I gave/am about to give to advertise my research work. The one in Brighton, which I gave a couple of weeks a go at &lt;a href=&#34;https://ictmc2019.com/&#34; target=&#34;_blank&#34;&gt;ICTMC&lt;/a&gt;, went really well and I was glad to hear that some people were very interested in what I presented. For more info, here a &lt;a href=&#34;https://www.luminpdf.com/viewer/5daad5f7ad8625001932b9a4&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to my presentation about missing data methods for trial-based economic evaluations that I discussed.
Honestly, since the conference was mainly directed towards people working in clinical trials, I did not expect a huge interest in the use of Bayesian methods for economic evaluations, but apparently (and I am happy about that) I was wrong.&lt;/p&gt;

&lt;p&gt;I had the chance to chat a bit with few people that I did not know, including &lt;a href=&#34;http://www.bristol.ac.uk/social-community-medicine/people/william-hollingworth/overview.html&#34; target=&#34;_blank&#34;&gt;William Hollingworth&lt;/a&gt; from Bristol and &lt;a href=&#34;https://www.ndorms.ox.ac.uk/team/ines-rombach&#34; target=&#34;_blank&#34;&gt;Ines Rombach&lt;/a&gt; from Oxford, with whom I had very nice conversations about my work and other interesting topics.
I was also glad to meet some known faces, including the always lovely &lt;a href=&#34;https://cheme.bangor.ac.uk/CatrinPlumptonBiography.php&#34; target=&#34;_blank&#34;&gt;Catrin Plumpton&lt;/a&gt; from Bangor University, who I met for the first time at HESG this summer and with whom I share the interest in missing data methods (even though she is a STATA and multiple imputation user, sadly).
I am also glad that I met my previous PhD secondary supervisor, &lt;a href=&#34;https://www.lshtm.ac.uk/aboutus/people/mason.alexina&#34; target=&#34;_blank&#34;&gt;Alexina Mason&lt;/a&gt;, with whom it is always a pleasure to talk with. Unfortunately, we both missed the talk of each other becuase of time problems but it was good to catch up with her again. I am also sad that I could
not attend &lt;a href=&#34;https://www.lshtm.ac.uk/aboutus/people/leurent.baptiste&#34; target=&#34;_blank&#34;&gt;Baptiste&lt;/a&gt;&amp;rsquo;s presentation which was the last day of the conference (I had to leave the same day of my talk, the first day) and I was not also able to actually meet him. I hope we will be able to see him soon at some other conference in the near future.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/YATNr2oXRo0IE/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Given this past experience, I am now looking forward to meet new people at my next conference at the Bella Center in Copenhagen (thumbnail) where this year &lt;a href=&#34;https://www.ispor.org/conferences-education/conferences/upcoming-conferences/ispor-europe-2019&#34; target=&#34;_blank&#34;&gt;ISPOR Europe 2019&lt;/a&gt; will be held. However, I believe this will be a much larger conference and therefore I will probably not have many chances to talk with people as I did at ICTMC.
Plus I am only preseting a poster this time, so it will be less likely that some people will actually notice my work, especially given the typically huge amount of presenters of this type of conferences. In the wrost case, I will enjoy Copenhagen and meet up with some old friends who live in Denmark and who will come at ISPOR to present some other work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations</title>
      <link>/talk/ictmc2019/</link>
      <pubDate>Mon, 07 Oct 2019 11:00:00 +0000</pubDate>
      
      <guid>/talk/ictmc2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>More good news...</title>
      <link>/post/update-october/update-october/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update-october/update-october/</guid>
      <description>&lt;p&gt;I have got two news coming up. First, the paper I wrote with Michael and Gianluca on Bayesian methdos for longitudinal data in trial-based economic evaluations has finally been published as early view on &lt;a href=&#34;https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12522&#34; target=&#34;_blank&#34;&gt;JRSSA&lt;/a&gt;. As I said in some earlier posts, I am super happy about this collaboration and I hope I can continue working on similar projects in the future.&lt;/p&gt;

&lt;p&gt;Second, I will soon give a talk about this work at the &lt;a href=&#34;https://ictmc2019.com/&#34; target=&#34;_blank&#34;&gt;ICTMC&lt;/a&gt; conference in Brighton, next Monday. This will be the first time at this conference and unfortunately I will only be able to remain around for one day as I need to go back to London pretty soon. I hope I will be able to enjoy my day at the conference, even though I will miss the talks of &lt;a href=&#34;https://www.lshtm.ac.uk/aboutus/people/leurent.baptiste&#34; target=&#34;_blank&#34;&gt;Baptiste&lt;/a&gt; and &lt;a href=&#34;https://www.lshtm.ac.uk/aboutus/people/mason.alexina&#34; target=&#34;_blank&#34;&gt;Alexina&lt;/a&gt; which are scheduled for the last day of the conference. I hope I can at least have a quick chat with them the day I am around.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/efDT7dqlF5N2LVHG8C/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I am also excited to visit &lt;a href=&#34;https://en.wikipedia.org/wiki/Brighton&#34; target=&#34;_blank&#34;&gt;Brighton&lt;/a&gt;, since many people keep telling me that I should go and visit this sort of british version of &amp;ldquo;Rimini&amp;rdquo;. To be honest, I do not expect to find a nice wheather, given that in this period it is raining a lot in London, but I hope I will be lucky and get the only sunny day of the week.&lt;/p&gt;

&lt;p&gt;Finally, I have started a rubric called &lt;a href=&#34;https://agabrioblog.onrender.com/missingdata/&#34; target=&#34;_blank&#34;&gt;missing data&lt;/a&gt; on my website, where I try to describe some of the most popular methods to handle missing data and to provide some references for anyone who could be interested in this field. I am really fascinated by statistical methods for dealing with missingness, perhaps because it was the main focus of my PhD, but I am eager to review different methods and see if I can find something really interesting. Of course, to complete this it will take more time, which I hope I will be able to find in the next months.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MissingHE 1.2.1</title>
      <link>/post/missinghe-121/missinghe-version121/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/missinghe-121/missinghe-version121/</guid>
      <description>&lt;p&gt;I have finally found some time to update the version for my R package &lt;a href=&#34;https://agabrioblog.onrender.com/missingHE/&#34; target=&#34;_blank&#34;&gt;missingHE&lt;/a&gt;, for which version 1.2.1 is now available on &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE/&#34; target=&#34;_blank&#34;&gt;CRAN&lt;/a&gt;.
I included two main features to the previous version of the package.&lt;/p&gt;

&lt;p&gt;First, I have added a new type of identifying restriction when fitting pattern mixture models through the function &amp;ldquo;pattern&amp;rdquo;. Before, only the complete case
restriction was available, which identifies the distributions of the missing data with those from the completers. Now the alternative available case restriction is can also be selected, which relies on the distributions that can be identified
among the non-completers to identify the distributions of the missing data. In this way, people can choose among at least two options for the type of restrictions and compare how this choice may affect the final estimates.&lt;/p&gt;

&lt;p&gt;Second, I added a new accessory function called &amp;ldquo;ppc&amp;rdquo;, which allows to perform posterior predictive checks using the conditional parameters saved from the fitted model to generate replications of the data at each posterior iteration of the model.
The function implements a relatively large number of checks, mostly taken from the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/bayesplot/&#34; target=&#34;_blank&#34;&gt;bayesplot&lt;/a&gt;, which allow to assess the fit of the model to the observed data by type of outcome (effects and costs) and treatment group (control and intervention).
For example, overalyed density plots can be generated to compare the empirical and replicated densities of the data to detect possible failures of the model.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/plotec.png&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Density plots for the observed and replicated data&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;I feel this is very important as when fitting a Bayesian model it is crucial to assess whether the model seems to adequately capture the different characteristics of the observed data (e.g. skewness, structural values, etc.).
A wide range of predictive checks are available, including histograms (see thumbnail pciture), scatterplots, error intervals, empirical cumulative distribution fucntions, statistcis of interest and many others. In addition ,
these checks can be performed for each type of missingness model and parametric distribution chosen within missingHE.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/NEvPzZ8bd1V4Y/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, it is important to remember that, when dealing with missing data the fit of the model can only be checked with respect to the observed values and therefore this
check is only partial since the fit to the unibserved values can never be checked. This is also why it is not meaningful to assess the fit of a model fitted under a missing not at random assumption
because this is based on information which is not directly available from the data at hand and thus impossible to check.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Discussing my thesis</title>
      <link>/post/update-interview/update-interview/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update-interview/update-interview/</guid>
      <description>&lt;p&gt;I have been kindly invited by the amazing person &lt;a href=&#34;https://www.ohe.org/about-us/meet-team/chris-sampson&#34; target=&#34;_blank&#34;&gt;Chris Sampson&lt;/a&gt; to talk about the work I inlcuded in my PhD thesis for his monthly rubric entitled  &amp;ldquo;Thesis Thursday&amp;rdquo; on the &lt;a href=&#34;https://aheblog.com/&#34; target=&#34;_blank&#34;&gt;The Academic Health Economists blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I happily accepted Chris&amp;rsquo;s invitation as I beleive this initiative is really interesting and represents a nice way for newly graduated PhD students to advertise their work while also giving the chance to people interested in health economics to read about some academic work which is typically freely available to everyone.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://aheblog.com/2019/09/19/thesis-thursday-andrea-gabrio/&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; you can find the full interview, which is not very long and resolves around 5 questions that Chris asked me about my work. I already new this blog but I have never had a proper chance to read through its posts carefully, which is a shame.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/13lTgtSUmqMrlu/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I shall promise myself to try to check it more often from now on, using this interview as a nice motivation to do so.
In fact, there are not many blogs around health economics matters (here a &lt;a href=&#34;https://blog.feedspot.com/health_economics_blogs/&#34; target=&#34;_blank&#34;&gt;non-comprehensive list&lt;/a&gt;), among which The Academic Health Economists and &lt;a href=&#34;http://www.statistica.it/gianluca/blog/&#34; target=&#34;_blank&#34;&gt;Gianluca&amp;rsquo;s blog&lt;/a&gt; are my favourites.&lt;/p&gt;

&lt;p&gt;I hope I will be able to find some time to write some nice posts about some health economic applications of my work in the next future as this is still the most interesting field for me at the moment. I am also the maintainer of another small blog called the &lt;a href=&#34;https://hearteam.blogspot.com/&#34; target=&#34;_blank&#34;&gt;Health Economics Analysis and Research Methods Team (HEART) blog&lt;/a&gt;, where I occasionally write some posts on health economics together with my colleagues from the UCL department of &lt;a href=&#34;https://www.ucl.ac.uk/epidemiology-health-care/research/pcph&#34; target=&#34;_blank&#34;&gt;Primary Care and Population Health&lt;/a&gt;. The blog is still new but I hope it can become more active in the next months.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some good news...</title>
      <link>/post/update-september/update-september/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update-september/update-september/</guid>
      <description>&lt;p&gt;With the approaching of the new academic here I have received some good news for my most recently submitted paper on Bayesian parametric modelling in health economics for missing longitudinal data, which at the moment is only available on &lt;a href=&#34;https://arxiv.org/abs/1805.07147&#34; target=&#34;_blank&#34;&gt;arXiv&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am happy to announce that, after a couple of rounds of reviews, the paper has been finally accepted for publication in &lt;a href=&#34;https://rss.onlinelibrary.wiley.com/journal/1467985X&#34; target=&#34;_blank&#34;&gt;JSS: Series A&lt;/a&gt;. I believe that the reviewers provided a very nice feedback for improving the work and I am quite satisfied with the final version of the article which, I hope, will be of interest for anyone involved in the analsysi of partially-observed longitudinal data. I hope the pre-print of the paper will be available soon and I will &amp;ldquo;advertise&amp;rdquo; my work in two conferences in the next couple of months, where I will present the content of the paper, namely &lt;a href=&#34;https://ictmc2019.com/&#34; target=&#34;_blank&#34;&gt;ICTMC&lt;/a&gt; this October in Brighton, and &lt;a href=&#34;http://www.ispor.org/conferences-education/conferences/upcoming-conferences/ispor-europe-2019&#34; target=&#34;_blank&#34;&gt;ISPOR Europe&lt;/a&gt; this November in Copenhagen.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/vMEjhlxsBR7Fe/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I am really excited about this paper which represented the last part of my PhD thesis and on which I worked really hard in the last year of my studies. &lt;a href=&#34;https://agabrioblog.onrender.com/project/missing-data/&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; you can find a general summary of the content of the article, while &lt;a href=&#34;https://www.ucl.ac.uk/statistics/sites/statistics/files/presentation_priment_1.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; there are some slides that describe the main idea behind the proposed model.&lt;/p&gt;

&lt;p&gt;I just want to conlcude with some thanks with my co-authors of the paper, &lt;a href=&#34;http://users.stat.ufl.edu/~daniels/&#34; target=&#34;_blank&#34;&gt;Michael&lt;/a&gt; and &lt;a href=&#34;https://www.ucl.ac.uk/statistics/people/gianlucabaio&#34; target=&#34;_blank&#34;&gt;Ginaluca&lt;/a&gt;, without whom I would have not been able to write this paper. This was my first work with Mike, with whom I had a wonderful collaboration and I was able to visit the beatiful city of &lt;a href=&#34;https://en.wikipedia.org/wiki/Gainesville,_Florida&#34; target=&#34;_blank&#34;&gt;Gainesville&lt;/a&gt; (FL) during my first visiting period at the &lt;a href=&#34;https://www.ufl.edu/&#34; target=&#34;_blank&#34;&gt;University of Florida&lt;/a&gt; (see thumbnail picture). I hope this will be the first of many works together in the furture!.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The P value fallacy</title>
      <link>/post/p-value-fallacy/p-value-fallacy/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/p-value-fallacy/p-value-fallacy/</guid>
      <description>&lt;p&gt;Today, I would like to briefly comment an interesting research article written by &lt;a href=&#34;https://jhu.pure.elsevier.com/en/publications/toward-evidence-based-medical-statistics-1-the-p-value-fallacy-4&#34; target=&#34;_blank&#34;&gt;Goodman&lt;/a&gt;, who provided a clear and exemplary discussion about the typical incorrect interpretation of a standard frequentist analysis in the field of medical research. I will now briefly summarise the main argument of the paper and then add some personal comments.&lt;/p&gt;

&lt;p&gt;Essentially, the article describes the characteristics of the dominant school of medical statistics and highlights the logical fallacy at the heart of the typical frequentist analysis in clinical studies. This is based on a &lt;em&gt;deductive&lt;/em&gt; inferential approach, which starts with a given hypothesis and makes conclusions under the assumption that the hypothesis is true. This is in contrast with a &lt;em&gt;inductive&lt;/em&gt; approach, which uses the observed evidence to evaluate what hypothesis is most tenable. The two most popular methods of the frequentist paradigm are the &lt;em&gt;P value&lt;/em&gt; proposed by &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_6&#34; target=&#34;_blank&#34;&gt;Fisher&lt;/a&gt; and the &lt;em&gt;hypothesis testing&lt;/em&gt; developed by &lt;a href=&#34;https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1933.0009?casa_token=sbSkualIaPYAAAAA%3ACxPsFTFEUK7vaxMPi5dJwUr4HoUWjrkxNh7Hl2q0owjtcU2wJHnakG-Xug7y95v1Tyqbbc8Mymaq_Q&amp;amp;&#34; target=&#34;_blank&#34;&gt;Neyman and Pearson&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The P value is defined as the probability, under the assumption of no effect (null hypothesis), of obtaining a result equal to or more extreme than what was actually observed. Fisher proposed it as an informal index to be used as a measure of discrepancy between the data and the null hypothesis and therefore should not be interpreted as a formal inferential method. For example, since the P value can only be calculated on the assumption that the null hypothesis is true, it cannot be a direct measure of the probability that the null hypothesis is false. However, the main criticism to the P value is perhaps that it does not take into account the size of the observed effect, i.e. a small effect in a study with a large sample size can have the same P value as a large effect in a small study.&lt;/p&gt;

&lt;p&gt;Hypothesis testing was proposed by Neyman and Pearson as an alternative approach to the P value, which assumes the existence of a null hypothesis (e.g. no effect) and an alternative hypothesis (e.g. nonzero effect). The outcome of the test is then simply to reject one hypothesis in favour of the other, solely based on the data. This exposes the researcher to two types of errors: type I error or false-positive ($\alpha$) and type II error or false-negative ($\beta$) result. Rather than focussing on single experiments, like the P value, hypothesis testing is effectively based on a deductive approach to minimise the errors over a large number of experiments. However, the price to pay to obtain this &lt;em&gt;objectivity&lt;/em&gt; is the impossibility to make any inferential statement about a single experiment. The procedure only guarantees that in the long run, i.e. after considering many experiments, we shall not often be wrong.&lt;/p&gt;

&lt;p&gt;Over time a combination between the P value and hypothesis testing was developed under the assumption that the two approaches can be complementary. The idea was that the P value could be used to measure evidence in a single experiment while not violating the long run logic of hypothesis testing. The combined method is characterized by setting $\alpha$ and power $\beta$ before the experiment, then calculating a P value and rejecting the null hypothesis if the P value is less than the preset type I error rate. This means that the P value is considered a false-positive error rate specific to the data and also a measure of evidence against the null hypothesis. The &lt;strong&gt;P value fallacy&lt;/strong&gt; is born from this statement, which assumes that an event can be seen simultaneously from a long run perspective (where the observed results are put together with other results that might have occurred in hypothetical repetitions of the experiment) and from a short run perspective (where the observed results are interpreted only with respect to the single experiment). However, these views are not reconcilable since a result cannot be at the same time an interchangeable (long-run) and unique (short-run) member of a group of results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/JszzkKOlV6gTK/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I personally find this discussion fascinating and I believe that it is important to recognise the inconsistencies between the two alternative approaches to inference. The original authors of the two paradigms were well aware of the implications of their methods and never supported the combination of these. However, the combined approach has somehow become widely accepted in practice while its internal inconsistencies and conceptual limitations are hardly recognised.&lt;/p&gt;

&lt;p&gt;I feel that, since the two methods are perceived as &amp;ldquo;objective&amp;rdquo;, it is generally accepted that, if combined, they can produce reliable conclusions. This, however, is not necessarily true. Accepting at face value the significance result as a binary indicator of whether or not a relation is real is dangeroues and potentially misleading. This practice wants to show that conclusions are being drawn directly from the data, without any external influence, because direct inference from data to hypothesis is thought to result in mistaken conclusions only rarely and is therefore regarded as &amp;ldquo;scientific&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;This misguided approach has led to a much stronger emphasis towards the quantitative results alone (without any external input). In contrast, I believe that such perspective has the serious drawback of ignoring potentially useful information which is available (e.g. relevant medical knowledge or historical data) and which should be included in the analysis. Of course, I am aware of the potential issues that may arise from the selection and incorporation of external evidence, but I believe this should not be considered as &amp;ldquo;less reliable&amp;rdquo; or &amp;ldquo;more prone to mistakes&amp;rdquo; compared with the evidence from the available data. It is important that an agreement is reached about the selection of the type of evidence and methods to be used to perform the analysis solely based on their relevance with respect to the context analysed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adjusting for partially-observed utilities and costs in trial-based cost-effectiveness analysis: a comparison of different methods and their performance</title>
      <link>/talk/hesg2019/</link>
      <pubDate>Thu, 04 Jul 2019 16:00:00 +0000</pubDate>
      
      <guid>/talk/hesg2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Super basic introduction to STAN</title>
      <link>/stan/basic-introduction-to-stan/super-basic-introduction-to-stan/</link>
      <pubDate>Wed, 03 Jul 2019 21:13:14 -0500</pubDate>
      
      <guid>/stan/basic-introduction-to-stan/super-basic-introduction-to-stan/</guid>
      <description>


&lt;p&gt;The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using &lt;code&gt;STAN&lt;/code&gt; via &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Prerequisites:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latest version of &lt;code&gt;R&lt;/code&gt;, which can be downloaded and installed for Windows, Mac or Linux OS from the &lt;a href=&#34;https://www.r-project.org/%7D&#34;&gt;CRAN&lt;/a&gt; website&lt;/li&gt;
&lt;li&gt;I also &lt;strong&gt;strongly&lt;/strong&gt; recommend to download and install &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;Rstudio&lt;/a&gt;, an integrated development environment which provides an “user-friendly” interaction with &lt;code&gt;R&lt;/code&gt; (e.g. many drop-down menus, tabs, customisation options)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Preliminaries&lt;/h1&gt;
&lt;div id=&#34;what-is-stan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is STAN?&lt;/h2&gt;
&lt;p&gt;Stan provides full Bayesian inference for continuous-variable models through Markov Chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling&lt;/p&gt;
&lt;p&gt;&lt;code&gt;STAN&lt;/code&gt; is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (&lt;span class=&#34;citation&#34;&gt;Gelman, Lee, and Guo (2015)&lt;/span&gt;). &lt;code&gt;STAN&lt;/code&gt; is a free software and a probabilistic programming language for specifying statistical models using a specific class of MCMC algorithms known as &lt;strong&gt;H&lt;/strong&gt;amiltonian &lt;strong&gt;M&lt;/strong&gt;onte &lt;strong&gt;C&lt;/strong&gt;arlo methods (HMC). The latest version of &lt;code&gt;STAN&lt;/code&gt; can be dowloaded from the web &lt;a href=&#34;https://mc-stan.org/users/interfaces/&#34;&gt;repository&lt;/a&gt; and is available for different OS. There are different &lt;code&gt;R&lt;/code&gt; packages which function as frontends for &lt;code&gt;STAN&lt;/code&gt;. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the &lt;code&gt;rstan&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Stan Development Team (2018)&lt;/span&gt;) and show how to fit &lt;code&gt;STAN&lt;/code&gt; models using this package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-stan-and-rstan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing STAN and rstan&lt;/h2&gt;
&lt;p&gt;Unlike other Bayesian software, such as &lt;code&gt;JAGS&lt;/code&gt; or &lt;code&gt;OpenBUGS&lt;/code&gt;, it is not required to separately install the program and the corresponding frontend &lt;code&gt;R&lt;/code&gt; package. Indeed, installing the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;rstan&lt;/code&gt; will automatically install &lt;code&gt;STAN&lt;/code&gt; on your machine. However, you will also need to make sure to having installed on your pc a &lt;code&gt;C++&lt;/code&gt; compiler which is used by &lt;code&gt;rstan&lt;/code&gt; to fit the models. Under a Windows OS, for example, this can be done by installing &lt;code&gt;Rtools&lt;/code&gt;, a collection of resources for building packages for &lt;code&gt;R&lt;/code&gt;, which is freely available from the web &lt;a href=&#34;https://cran.r-project.org/bin/windows/Rtools/&#34;&gt;repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next, install the package &lt;code&gt;rstan&lt;/code&gt; from within &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;Rstudio&lt;/code&gt;, via the package installer or by typing in the command line&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;rstan&amp;quot;, dependencies = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;dependencies = TRUE&lt;/code&gt; option will automatically install all the packages on which the functions in the &lt;code&gt;rstan&lt;/code&gt; package rely.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic model&lt;/h1&gt;
&lt;div id=&#34;simulate-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulate data&lt;/h2&gt;
&lt;p&gt;For an example dataset, I simulate my own data in &lt;code&gt;R&lt;/code&gt;. I create a continuous outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as a function of one predictor &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and a disturbance term &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficients, i.e.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \beta_1 x + \epsilon  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;R&lt;/code&gt; commands which I use to simulate the data are the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; n_sim=100; set.seed(123)
&amp;gt; x=rnorm(n_sim, mean = 5, sd = 2)
&amp;gt; epsilon=rnorm(n_sim, mean = 0, sd = 1)
&amp;gt; beta0=1.5
&amp;gt; beta1=1.2
&amp;gt; y=beta0 + beta1 * x + epsilon
&amp;gt; n_sim=as.integer(n_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I define all the data for &lt;code&gt;STAN&lt;/code&gt; in a list object&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; datalist=list(&amp;quot;y&amp;quot;=y,&amp;quot;x&amp;quot;=x,&amp;quot;n_sim&amp;quot;=n_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model file&lt;/h2&gt;
&lt;p&gt;Now, I write the model for &lt;code&gt;STAN&lt;/code&gt; and save it as a stan file named &lt;code&gt;&#34;basic.mod.stan&#34;&lt;/code&gt; in the current working directory&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod= &amp;quot;
+ data {
+ int&amp;lt;lower=0&amp;gt; n_sim;
+ vector[n_sim] y;
+ vector[n_sim] x;
+ }
+ parameters {
+ real beta0;
+ real beta1;
+ real&amp;lt;lower=0&amp;gt; sigma;
+ }
+ transformed parameters {
+ vector[n_sim] mu;
+ mu=beta0 + beta1*x;
+ } 
+ model {
+ sigma~uniform(0,100);
+ beta0~normal(0,1000);
+ beta1~normal(0,1000);
+ y~normal(mu,sigma);
+ }
+ 
+ &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;STAN&lt;/code&gt; models are written using an imperative programming language, which means that the order in which you write the elements in your model file matters, i.e. you first need to define your variables (e.g. integers, vectors, matrices, etc.), the constraints which define the range of values your variable can take (e.g. only positive values for standard deviations), and finally define the relationship among the variables (e.g. one is a liner function of another).&lt;/p&gt;
&lt;p&gt;A Stan model is defined by six program blocks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data (required). The &lt;em&gt;data block&lt;/em&gt; reads external information – e.g. data vectors, matrices, integers, etc.&lt;/li&gt;
&lt;li&gt;Transformed data (optional). The &lt;em&gt;transformed data block&lt;/em&gt; allows for preprocessing of the data – e.g. transformation or rescaling of the data.&lt;/li&gt;
&lt;li&gt;Parameters (required). The &lt;em&gt;parameters block&lt;/em&gt; defines the sampling space – e.g. parameters to which prior distributions must be assigned.&lt;/li&gt;
&lt;li&gt;Transformed parameters (optional). The &lt;em&gt;transformed parameters block&lt;/em&gt; allows for parameter processing before the posterior is computed – e.g. tranformation or rescaling of the parameters.&lt;/li&gt;
&lt;li&gt;Model (required). In the &lt;em&gt;model block&lt;/em&gt; we define our posterior distributions – e.g. choice of distributions for all variables.&lt;/li&gt;
&lt;li&gt;Generated quantities (optional). The &lt;em&gt;generated quantities block&lt;/em&gt; allows for postprocessing – e.g. backtranformation of the parameters using the posterior samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this introduction I consider a very simple model which only requires the specification of four blocks in the &lt;code&gt;STAN&lt;/code&gt; model. In the data block, I first define the size of the sample &lt;code&gt;n_sim&lt;/code&gt; as a positive integer number using the expression &lt;code&gt;int&amp;lt;lower=0&amp;gt; n_sim&lt;/code&gt;; then I declare the two variables &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; as reals (or vectors) with length equal to N. In the parameters block, I define the coefficients for the linear regression &lt;code&gt;beta0&lt;/code&gt; and &lt;code&gt;beta1&lt;/code&gt; (as two real numbers) and the standard deviation parameter &lt;code&gt;sigma&lt;/code&gt; (as a positive real number). In the transformed parameters block, I define the conditional mean &lt;code&gt;mu&lt;/code&gt; (a real vector of length &lt;code&gt;N&lt;/code&gt;) as a linear function of the intercept &lt;code&gt;beta0&lt;/code&gt;, the slope &lt;code&gt;beta1&lt;/code&gt;, and the covariate &lt;code&gt;x&lt;/code&gt;. Finally, in the model block, I assign weakly informative priors to the regression coefficients and the standard deviation parameters, and I model the outcome data &lt;code&gt;y&lt;/code&gt; using a normal distribution indexed by the conditional mean &lt;code&gt;mu&lt;/code&gt; and the standard deviation &lt;code&gt;sigma&lt;/code&gt; parameters. In many cases, &lt;code&gt;STAN&lt;/code&gt; uses sampling statements which can be vectorised, i.e. you do not need to use for loop statements.&lt;/p&gt;
&lt;p&gt;To write and save the model as the text file “basic.mod.stan” in the current working directory, I use the &lt;code&gt;writeLines&lt;/code&gt; function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; writeLines(basic.mod, &amp;quot;basic.mod.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pre-processing&lt;/h2&gt;
&lt;p&gt;Define the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in &lt;code&gt;STAN&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params=c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;)
&amp;gt; inits=function(){list(&amp;quot;beta0&amp;quot;=rnorm(1), &amp;quot;beta1&amp;quot;=rnorm(1))}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters in the object &lt;code&gt;inits&lt;/code&gt; which is then passed to the &lt;code&gt;stan&lt;/code&gt; function in &lt;code&gt;rstan&lt;/code&gt;. However, for more complex models, this may not be immediate and a lot of trial and error may be required. However, &lt;code&gt;STAN&lt;/code&gt; can automatically select the initial values for all parameters randomly. This can be achieved by setting &lt;code&gt;inits=&#34;random&#34;&lt;/code&gt;, which is then passed to the &lt;code&gt;stan&lt;/code&gt; function in &lt;code&gt;rstan&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before using &lt;code&gt;rstan&lt;/code&gt; for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(rstan)
&amp;gt; set.seed(123)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit the model&lt;/h2&gt;
&lt;p&gt;Now, we can fit the model in &lt;code&gt;STAN&lt;/code&gt; using the &lt;code&gt;stan&lt;/code&gt; function in the &lt;code&gt;rstan&lt;/code&gt; package and save it in the object &lt;code&gt;basic.mod&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod&amp;lt;-stan(data = datalist, pars = params, iter = 9000, 
+   warmup = 1000, init = inits, chains = 2, file = &amp;quot;basic.mod.stan&amp;quot;)

SAMPLING FOR MODEL &amp;#39;basic&amp;#39; NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 9000 [  0%]  (Warmup)
Chain 1: Iteration:  900 / 9000 [ 10%]  (Warmup)
Chain 1: Iteration: 1001 / 9000 [ 11%]  (Sampling)
Chain 1: Iteration: 1900 / 9000 [ 21%]  (Sampling)
Chain 1: Iteration: 2800 / 9000 [ 31%]  (Sampling)
Chain 1: Iteration: 3700 / 9000 [ 41%]  (Sampling)
Chain 1: Iteration: 4600 / 9000 [ 51%]  (Sampling)
Chain 1: Iteration: 5500 / 9000 [ 61%]  (Sampling)
Chain 1: Iteration: 6400 / 9000 [ 71%]  (Sampling)
Chain 1: Iteration: 7300 / 9000 [ 81%]  (Sampling)
Chain 1: Iteration: 8200 / 9000 [ 91%]  (Sampling)
Chain 1: Iteration: 9000 / 9000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.078 seconds (Warm-up)
Chain 1:                0.593 seconds (Sampling)
Chain 1:                0.671 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL &amp;#39;basic&amp;#39; NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 9000 [  0%]  (Warmup)
Chain 2: Iteration:  900 / 9000 [ 10%]  (Warmup)
Chain 2: Iteration: 1001 / 9000 [ 11%]  (Sampling)
Chain 2: Iteration: 1900 / 9000 [ 21%]  (Sampling)
Chain 2: Iteration: 2800 / 9000 [ 31%]  (Sampling)
Chain 2: Iteration: 3700 / 9000 [ 41%]  (Sampling)
Chain 2: Iteration: 4600 / 9000 [ 51%]  (Sampling)
Chain 2: Iteration: 5500 / 9000 [ 61%]  (Sampling)
Chain 2: Iteration: 6400 / 9000 [ 71%]  (Sampling)
Chain 2: Iteration: 7300 / 9000 [ 81%]  (Sampling)
Chain 2: Iteration: 8200 / 9000 [ 91%]  (Sampling)
Chain 2: Iteration: 9000 / 9000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.078 seconds (Warm-up)
Chain 2:                0.594 seconds (Sampling)
Chain 2:                0.672 seconds (Total)
Chain 2: &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Different packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the &lt;code&gt;bayesplot&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Gabry and Mahr (2017)&lt;/span&gt;) to obtain graphical diagnostics and results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;bayesplot&amp;quot;)
&amp;gt; library(bayesplot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, density and trace plots can be obtained by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc_combo(as.array(basic.mod),regex_pars=&amp;quot;beta0|beta1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/basic-introduction-to-stan/2018-07-06-super-basic-introduction-to-stan_files/figure-html/diagnostic3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;This tutorial was simply a brief introduction on how simple linear regression models can be fitted using the Bayesian software &lt;code&gt;STAN&lt;/code&gt; via the &lt;code&gt;rstan&lt;/code&gt; package. Although this may seem a complex procedure compared with simply fitting a linear model under the frequentist framework, however, the real advantages of Bayesian methods become evident when the complexity of the analysis is increased (which is often the case in real applications). Indeed, the flexibility in Bayesian modelling allows to account for increasingly complex models in a relatively easy way. In addition, Bayesian methods are ideal when the interest is in taking into account the potential impact that different sources of uncertainty may have on the final results, as they allow the natural propagation of uncertainty throughout each quantity in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-gabry2017bayesplot&#34;&gt;
&lt;p&gt;Gabry, J, and T Mahr. 2017. “Bayesplot: Plotting for Bayesian Models.” &lt;em&gt;R Package Version&lt;/em&gt; 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2015stan&#34;&gt;
&lt;p&gt;Gelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” &lt;em&gt;Journal of Educational and Behavioral Statistics&lt;/em&gt; 40 (5): 530–43.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rstanpackage&#34;&gt;
&lt;p&gt;Stan Development Team. 2018. “RStan: The R Interface to Stan.” &lt;a href=&#34;http://mc-stan.org/&#34;&gt;http://mc-stan.org/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Super basic introduction to OpenBUGS</title>
      <link>/openbugs/basic-introduction-to-openbugs/super-basic-introduction-to-openbugs/</link>
      <pubDate>Tue, 02 Jul 2019 21:11:14 -0500</pubDate>
      
      <guid>/openbugs/basic-introduction-to-openbugs/super-basic-introduction-to-openbugs/</guid>
      <description>


&lt;p&gt;The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using &lt;code&gt;OpenBUGS&lt;/code&gt; via &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Prerequisites:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latest version of &lt;code&gt;R&lt;/code&gt;, which can be downloaded and installed for Windows, Mac or Linux OS from the &lt;a href=&#34;https://www.r-project.org/%7D&#34;&gt;CRAN&lt;/a&gt; website&lt;/li&gt;
&lt;li&gt;I also &lt;strong&gt;strongly&lt;/strong&gt; recommend to download and install &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;Rstudio&lt;/a&gt;, an integrated development environment which provides an “user-friendly” interaction with &lt;code&gt;R&lt;/code&gt; (e.g. many drop-down menus, tabs, customisation options)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Preliminaries&lt;/h1&gt;
&lt;div id=&#34;what-is-openbugs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is OpenBUGS?&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;OpenBUGS&lt;/code&gt; is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (&lt;span class=&#34;citation&#34;&gt;Spiegelhalter et al. (2007)&lt;/span&gt;). &lt;code&gt;OpenBUGS&lt;/code&gt; is a free software based on the &lt;strong&gt;B&lt;/strong&gt;ayesian inference &lt;strong&gt;U&lt;/strong&gt;sing &lt;strong&gt;G&lt;/strong&gt;ibbs &lt;strong&gt;S&lt;/strong&gt;ampling (informally &lt;code&gt;BUGS&lt;/code&gt;) language at the base of &lt;code&gt;WinBUGS&lt;/code&gt; but, unlike this program, is platform independent. The latest version of &lt;code&gt;OpenBUGS&lt;/code&gt; can be dowloaded from the web &lt;a href=&#34;http://www.openbugs.net/w/FrontPage&#34;&gt;repository&lt;/a&gt; and is available for different OS. There are different &lt;code&gt;R&lt;/code&gt; packages which function as frontends for &lt;code&gt;OpenBUGS&lt;/code&gt;. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the &lt;code&gt;R2OpenBUGS&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Sturtz, Ligges, and Gelman (2010)&lt;/span&gt;) and show how to fit &lt;code&gt;OpenBUGS&lt;/code&gt; models using this package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-openbugs-and-r2openbugs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing OpenBUGS and R2OpenBUGS&lt;/h2&gt;
&lt;p&gt;Install the latest version of &lt;code&gt;OpenBUGS&lt;/code&gt; for your OS. Next, install the package &lt;code&gt;R2OpenBUGS&lt;/code&gt; from within &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;Rstudio&lt;/code&gt;, via the package installer or by typing in the command line&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;R2OpenBUGS&amp;quot;, dependencies = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;dependencies = TRUE&lt;/code&gt; option will automatically install all the packages on which the functions in the &lt;code&gt;R2OpenBUGS&lt;/code&gt; package rely.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic model&lt;/h1&gt;
&lt;div id=&#34;simulate-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulate data&lt;/h2&gt;
&lt;p&gt;For an example dataset, I simulate my own data in &lt;code&gt;R&lt;/code&gt;. I create a continuous outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as a function of one predictor &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and a disturbance term &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficients, i.e.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \beta_1 x + \epsilon  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;R&lt;/code&gt; commands which I use to simulate the data are the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; n.sim=100; set.seed(123)
&amp;gt; x=rnorm(n.sim, mean = 5, sd = 2)
&amp;gt; epsilon=rnorm(n.sim, mean = 0, sd = 1)
&amp;gt; beta0=1.5
&amp;gt; beta1=1.2
&amp;gt; y=beta0 + beta1 * x + epsilon&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I define all the data for &lt;code&gt;JAGS&lt;/code&gt; in a list object&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; datalist=list(&amp;quot;y&amp;quot;,&amp;quot;x&amp;quot;,&amp;quot;n.sim&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model file&lt;/h2&gt;
&lt;p&gt;Now, I write the model for &lt;code&gt;OpenBUGS&lt;/code&gt; and save it as a text file named &lt;code&gt;&#34;basicmodbugs.txt&#34;&lt;/code&gt; in the current working directory&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod= &amp;quot;
+ model {
+ #model
+  for(i in 1:n.sim){
+   y[i] ~ dnorm(mu[i], tau)
+   mu[i] &amp;lt;- beta0 + beta1 * x[i]
+  }
+ #priors
+ beta0 ~ dnorm(0, 0.01)
+ beta1 ~ dnorm(0, 0.01)
+ tau ~ dgamma(0.01,0.01)
+ }
+ &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The part of the model inside the for loop denotes the likelihood, which is evaluated for each individual in the sample using a Normal distribution parameterised by some mean &lt;code&gt;mu&lt;/code&gt; and precision &lt;code&gt;tau&lt;/code&gt; (where, precision = 1/variance). The covariate &lt;code&gt;x&lt;/code&gt; is included at the mean level using a linear regression, which is indexed by the intercept &lt;code&gt;beta0&lt;/code&gt; and slope &lt;code&gt;beta1&lt;/code&gt; terms. The second part defines the prior distributions for all parameters of the model, namely the regression coefficients and the precision. Weakly informative priors are used since I assume that I do not have any prior knowledge about these parameters.&lt;/p&gt;
&lt;p&gt;To write and save the model as the text file “basicmodbugs.txt” in the current working directory, I use the &lt;code&gt;writeLines&lt;/code&gt; function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; writeLines(basic.mod, &amp;quot;basicmodbugs.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pre-processing&lt;/h2&gt;
&lt;p&gt;Define the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in &lt;code&gt;OpenBUGS&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params=c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;)
&amp;gt; inits=function(){list(&amp;quot;beta0&amp;quot;=rnorm(1), &amp;quot;beta1&amp;quot;=rnorm(1), &amp;quot;tau&amp;quot;=rgamma(1,1,1))}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters in the object &lt;code&gt;inits&lt;/code&gt; which is then passed to the &lt;code&gt;bugs&lt;/code&gt; function in &lt;code&gt;R2OpenBUGS&lt;/code&gt;. However, for more complex models, this may not be immediate and a lot of trial and error may be required.&lt;/p&gt;
&lt;p&gt;Before using &lt;code&gt;R2OpenBUGS&lt;/code&gt; for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(R2OpenBUGS)
&amp;gt; set.seed(123)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit the model&lt;/h2&gt;
&lt;p&gt;Now, we can fit the model in &lt;code&gt;OpenBUGS&lt;/code&gt; using the &lt;code&gt;bugs&lt;/code&gt; function in the &lt;code&gt;R2openBUGS&lt;/code&gt; package and save it in the object &lt;code&gt;basic.mod&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod.bugs=bugs(data = datalist, inits = inits, 
+   parameters.to.save = params, n.chains = 2, n.iter = 2000,
+   n.burnin = 1000, model.file = &amp;quot;basicmodbugs.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the model is running, the function prints out some information related to the Bayesian graph (corresponding to the specification used for the model) underneath &lt;code&gt;OpenBUGS&lt;/code&gt;, such as number of observed and unobserved nodes and graph size.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;post-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Post-processing&lt;/h2&gt;
&lt;p&gt;Once the model has finished running, a summary of the posteiror estimates and convergence diagnostics for all parameters specified can be seen by typing &lt;code&gt;print(basic.mod)&lt;/code&gt; or, alternatively,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; print(basic.mod.bugs$summary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          mean    sd   2.5%   25%   50%   75% 97.5% Rhat n.eff
beta0      1.5 0.293   0.99   1.3   1.5   1.7   2.1    1  1700
beta1      1.2 0.053   1.06   1.1   1.2   1.2   1.3    1  2000
deviance 278.8 2.439 276.00 277.1 278.2 280.0 285.2    1  2000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The posterior distribution of each parameter is summarised in terms of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mean, sd and some percentiles&lt;/li&gt;
&lt;li&gt;Potential scale reduction factor &lt;code&gt;Rhat&lt;/code&gt; and effective sample size &lt;code&gt;n.eff&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Gelman (2013)&lt;/span&gt;). The first is a measure to assess issues in convergence of the MCMC algorithm (typically a value below &lt;span class=&#34;math inline&#34;&gt;\(1.05\)&lt;/span&gt; for all parameters is considered ok). The second is a measure which assesses the adequacy of the posterior sample (typically values close to the total number of iterations are desirable for all parameters).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The deviance is a goodness of fit statistic and is used in the construction of the “Deviance Information Criterion” or DIC (&lt;span class=&#34;citation&#34;&gt;Spiegelhalter et al. (2014)&lt;/span&gt;), which is a &lt;em&gt;relative&lt;/em&gt; measure of model comparison. The DIC of the model can be accessed by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod.bugs$DIC
[1] 282&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;diagnostics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagnostics&lt;/h2&gt;
&lt;p&gt;More diagnostics are available when we convert the model output into an MCMC object using the command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;coda&amp;quot;)
&amp;gt; library(coda)
&amp;gt; basic.mod.mcmc.bugs=as.mcmc.list(basic.mod.bugs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Different packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the &lt;code&gt;mcmcplots&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Curtis (2015)&lt;/span&gt;) to obtain graphical diagnostics and results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;mcmcplots&amp;quot;)
&amp;gt; library(mcmcplots)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, density and trace plots can be obtained by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; denplot(basic.mod.mcmc.bugs, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/OpenBUGS/basic-introduction-to-openbugs/2018-07-23-super-basic-introduction-to-openbugs_files/figure-html/diagnostic3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(basic.mod.mcmc.bugs, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/OpenBUGS/basic-introduction-to-openbugs/2018-07-23-super-basic-introduction-to-openbugs_files/figure-html/diagnostic3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;This tutorial was simply a brief introduction on how simple linear regression models can be fitted using the Bayesian software &lt;code&gt;OpenBUGS&lt;/code&gt; via the &lt;code&gt;R2OpenBUGS&lt;/code&gt; package. Although this may seem a complex procedure compared with simply fitting a linear model under the frequentist framework, however, the real advantages of Bayesian methods become evident when the complexity of the analysis is increased (which is often the case in real applications). Indeed, the flexibility in Bayesian modelling allows to account for increasingly complex models in a relatively easy way. In addition, Bayesian methods are ideal when the interest is in taking into account the potential impact that different sources of uncertainty may have on the final results, as they allow the natural propagation of uncertainty throughout each quantity in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-curtis2015mcmcplots&#34;&gt;
&lt;p&gt;Curtis, SM. 2015. “Mcmcplots: Create Plots from Mcmc Output.” &lt;em&gt;R Package Version 0.4&lt;/em&gt; 2.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34;&gt;
&lt;p&gt;Gelman, Andrew. 2013. &lt;em&gt;Bayesian Data Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-spiegelhalter2014deviance&#34;&gt;
&lt;p&gt;Spiegelhalter, David J, Nicola G Best, Bradley P Carlin, and Angelika Van Der Linde. 2014. “The Deviance Information Criterion: 12 Years on.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 76 (3): 485–93.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-spiegelhalter2007openbugs&#34;&gt;
&lt;p&gt;Spiegelhalter, David, Andrew Thomas, Nicky Best, and Dave Lunn. 2007. “OpenBUGS User Manual, Version 3.0. 2.” &lt;em&gt;MRC Biostatistics Unit, Cambridge&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-sturtz2010r2openbugs&#34;&gt;
&lt;p&gt;Sturtz, Sibylle, Uwe Ligges, and Andrew Gelman. 2010. “R2OpenBUGS: A Package for Running Openbugs from R.” &lt;em&gt;URL Http://Cran. Rproject. Org/Web/Packages/R2OpenBUGS/Vignettes/R2OpenBUGS. Pdf&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>HESG Summer Meeting 2019</title>
      <link>/post/hesg-norwich-2019/hesg-summer-meeting-2019/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/hesg-norwich-2019/hesg-summer-meeting-2019/</guid>
      <description>&lt;p&gt;I have just come back form my first Health Economists&amp;rsquo; Study Group (&lt;a href=&#34;https://hesg.org.uk/meetings/summer-2019-university-of-east-anglia/&#34; target=&#34;_blank&#34;&gt;HESG&lt;/a&gt;) meeting, which this year was held at the &lt;a href=&#34;https://www.uea.ac.uk/&#34; target=&#34;_blank&#34;&gt;University of East Anglia&lt;/a&gt; in the beautiful city of Norwich,
south east of England, and where I presented some preliminary results from one of my on-going works. I have to say, it was a remarkable experience which
I really liked thanks to a wonderful and welcoming environment. I had the pleasure to talk to many people from different research areas involved in
health economics (both from academia and industry) and to see many different projects and works.&lt;/p&gt;

&lt;p&gt;I particularly enjoy the structure of the meeting, which requires some chair and discussant who have to present and discuss the paper of the authors,
who are only allowed to provide some clarification if needed. At first I thought this structure of the sessions was strange, but after attending many
sessions and experiencing this for my own paper, I feel that it is a very good way to encourage discussion about works from different people rather than
just focussing on your own presentation. Plus, the weather and always sunny, it felt like Italy for a few days.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/Norwich.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;The beautiful Norwich&amp;rsquo;s cathedral&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Other nice people and colleagues from HEART and other UCL department came to HESG with me, including &lt;a href=&#34;https://iris.ucl.ac.uk/iris/browse/profile?upi=CSCLA53&#34; target=&#34;_blank&#34;&gt;Caroline&lt;/a&gt; and &lt;a href=&#34;https://www.ucl.ac.uk/comprehensive-clinical-trials-unit/meet-team/health-economics/junior-health-economist-ekaterina-kuznetsova&#34; target=&#34;_blank&#34;&gt;Ekaterina&lt;/a&gt; (aka Katia),
you can see them in thumbnail of this post. I was also pleased to meet &lt;a href=&#34;https://www.lshtm.ac.uk/aboutus/people/leurent.baptiste&#34; target=&#34;_blank&#34;&gt;Baptiste&lt;/a&gt; from &lt;a href=&#34;https://www.lshtm.ac.uk/&#34; target=&#34;_blank&#34;&gt;LSHTM&lt;/a&gt;, who shares with me the interest in missing data
methods for cost-effectiveness analysis and who presented some very nice work on that. I had the chance to give some feedback to him and he did the same for me.
It felt so nice when we started discussing about some aspects of our analyses and after some minutes we simply lost track of time and everyone else disappeared.
I also had the opportunity to talk about my work with the discussant of my session, &lt;a href=&#34;https://cheme.bangor.ac.uk/CatrinPlumptonBiography.php&#34; target=&#34;_blank&#34;&gt;Catrin Plumpton&lt;/a&gt; from the &lt;a href=&#34;https://cheme.bangor.ac.uk/&#34; target=&#34;_blank&#34;&gt;Centre for Health Economics and Medicines Evaluation&lt;/a&gt;,
who gave me some nice feedback which I really appreciated, especially given her mathematical background.&lt;/p&gt;

&lt;p&gt;An important contribution to the success of the meeting was also given by the wonderful organisation of the event, including an accommodation located very closely
to the main building of the meeting, plenty of food provided during each day, a nice bus tour of the city and a wonderful conference dinner. I must thank all the people,
who organised the event who were very extremely nice to us and who were always ready to help us for whatever need we had, with a special mention for &lt;a href=&#34;https://people.uea.ac.uk/emma_mcmanus&#34; target=&#34;_blank&#34;&gt;Emma Mcmanus&lt;/a&gt; who
was amazing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/cWnICjtVkJJsgGKhyX/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In summary, everything was good. Well, almost. Going back to the works presented, as usual, the only less positive note that I would like to make
is the almost total absence of Bayesian applications. Some authors mentioned that they used some popular Bayesian program, such as &lt;a href=&#34;https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-winbugs/&#34; target=&#34;_blank&#34;&gt;WinBUGS&lt;/a&gt;, but this was
mainly related to the usual meta-analysis stuff which is pretty standardised. I hope next time I will be able to see more people going Bayesian as this is what I am.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding health economics in clinical trials</title>
      <link>/post/introduction-to-health-economics/understanding-health-economics-in-clinical-trials/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/introduction-to-health-economics/understanding-health-economics-in-clinical-trials/</guid>
      <description>&lt;p&gt;As member of the Health Economics Analysis and Research Methods Team (&lt;a href=&#34;https://hearteam.blogspot.com/&#34; target=&#34;_blank&#34;&gt;HEART&lt;/a&gt;), together with my colleagues, on Tuesday 2 July I took part in a 1-day introductory short course entitled “Understanding health economics in clinical trials”, which was designed and delivered by the team. HEART is a new group of health economists who are based in UCL’s Institute of Clinical Trials and Methodology (ICTM), led by &lt;a href=&#34;https://iris.ucl.ac.uk/iris/browse/profile?upi=RMHUN48&#34; target=&#34;_blank&#34;&gt;Rachael Hunter&lt;/a&gt;, and is involved at different levels in the economic components of clinical trials in different trial units at UCL. This short course was aimed at ICTM staff who are not health economists (e.g. trial managers, CIs/PIs, statisticians, data managers, research assistants, etc.) and was designed in response to the need we have identified over the last few years in working on trials as well as in response to colleagues across ICTM. This course was unique as it was intended specifically for non health economists working in trials, who wish to better understand the health economics in their study, and/or the health economist on their study. The course used a mix of lectures, group discussions and practical exercises to help participants consolidate their learning and see how to apply information from the sessions to real studies. No prior knowledge of health economics was assumed.&lt;/p&gt;

&lt;p&gt;I believe the course was a success both in terms of the quality/quantity of the material covered during the six sessions throughout the day, as well as in terms of the positive feedback we received from the participants (almost entirely women, with the exception of two men). Many key and typically not well understood economic topics were discussed during the day, e.g. what are and how QALYs and costs are calculated, the potential limitations and issues of an economic analysis within a trial, or the role played by the protocol and analysis plan in the economic evaluation. My session was related to reporting and interpreting health economic results and I realised that most people who do not routinely deal with health economics may find difficult to grasp certain concepts or tools used in the economic analysis (e.g. what is a cost-effectiveness acceptability curve and how it can be computed). Nevertheless, I must admit that I was surprised by how many people were very motivated to learn these concepts and these &amp;ldquo;difficult&amp;rdquo; methods, often asking questions and making good comments (despite the fact that my session was the last of the course at the end of the day).&lt;br /&gt;
We ran this course as a trial as we did not have clear ideas of what an optimal design should be or the number of topics that should be covered for this type of course. We are now confident that the course has a solid structure and that there is a clear demand to learn the basic concepts of health economics, at least among people involved in trial analyses. Following the successful delivery of the course, we are planning to replicate the experience in the future, improving certain aspects of the sessions based on the feedback we received and also considering to open the course to meet the demand of a wider audience.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/bQrVMr3CO3QaY/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I have to say that this was an extremely positive experience for me as it was the first time I was involved in this type of projects. Me and my colleagues worked hard to design and prepare the different sessions of the course over the last few months, find the best way to link the arguments across the sessions, provide interesting group activities and materials for the practicals, etc. I have to thank all my colleagues who contributed to the promotion and realisation of this project, with a special mention for &lt;a href=&#34;https://iris.ucl.ac.uk/iris/browse/profile?upi=CSCLA53&#34; target=&#34;_blank&#34;&gt;Caroline Clarke&lt;/a&gt;, who spent a lot of time and effort to organise the course and who personally contributed in giving one of the session of the course. Finally, I would also like to thank my colleague and health economist &lt;a href=&#34;https://www.ucl.ac.uk/comprehensive-clinical-trials-unit/meet-team/health-economics/junior-health-economist-ekaterina-kuznetsova&#34; target=&#34;_blank&#34;&gt;Ekaterina&lt;/a&gt;, with whom I had the pleasure to share the presentation and practical of my session in the course.&lt;/p&gt;

&lt;p&gt;Perhaps the only true negative aspect of the course was the absence of a Bayesian perspective, especially related to the interpretation of the results and the statistical methods that can be used to perform the analysis. Given the generally low familiarity of the people attending the course with statistics, I believe it was reasonable not to further confuse them with another new element into the picture. However, I truly hope that people will become more and more familiar with the importance of using tailored statistical methods in economic evaluations to avoid biased results, and from that point to justify a Bayesian approach, well, at least for me, the step is straightforward!.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Health Economics in Clinical Trials</title>
      <link>/talk/heartcourse2019/</link>
      <pubDate>Tue, 02 Jul 2019 09:00:00 +0000</pubDate>
      
      <guid>/talk/heartcourse2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Super basic introduction to JAGS</title>
      <link>/jags/basic-introduction-to-jags/super-basic-introduction-to-jags/</link>
      <pubDate>Mon, 01 Jul 2019 21:13:14 -0500</pubDate>
      
      <guid>/jags/basic-introduction-to-jags/super-basic-introduction-to-jags/</guid>
      <description>


&lt;p&gt;The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using &lt;code&gt;JAGS&lt;/code&gt; via &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Prerequisites:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latest version of &lt;code&gt;R&lt;/code&gt;, which can be downloaded and installed for Windows, Mac or Linux OS from the &lt;a href=&#34;https://www.r-project.org/%7D&#34;&gt;CRAN&lt;/a&gt; website&lt;/li&gt;
&lt;li&gt;I also &lt;strong&gt;strongly&lt;/strong&gt; recommend to download and install &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;Rstudio&lt;/a&gt;, an integrated development environment which provides an “user-friendly” interaction with &lt;code&gt;R&lt;/code&gt; (e.g. many drop-down menus, tabs, customisation options)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Preliminaries&lt;/h1&gt;
&lt;div id=&#34;what-is-jags&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is JAGS?&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;JAGS&lt;/code&gt; or &lt;strong&gt;J&lt;/strong&gt;ust &lt;strong&gt;A&lt;/strong&gt;nother &lt;strong&gt;G&lt;/strong&gt;ibbs &lt;strong&gt;S&lt;/strong&gt;ampler is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (&lt;span class=&#34;citation&#34;&gt;Plummer (2004)&lt;/span&gt;). &lt;code&gt;JAGS&lt;/code&gt; is a free software based on the &lt;strong&gt;B&lt;/strong&gt;ayesian inference &lt;strong&gt;U&lt;/strong&gt;sing &lt;strong&gt;G&lt;/strong&gt;ibbs &lt;strong&gt;S&lt;/strong&gt;ampling (informally &lt;code&gt;BUGS&lt;/code&gt;) language at the base of &lt;code&gt;WinBUGS/OpenBUGS&lt;/code&gt; but, unlike these programs, it is written in &lt;code&gt;C++&lt;/code&gt; and is platform independent. The latest version of &lt;code&gt;JAGS&lt;/code&gt; can be dowloaded from Martyn Plummer’s &lt;a href=&#34;https://sourceforge.net/projects/mcmc-jags/files/JAGS/&#34;&gt;repository&lt;/a&gt; and is available for different OS. There are different &lt;code&gt;R&lt;/code&gt; packages which function as frontends for &lt;code&gt;JAGS&lt;/code&gt;. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the &lt;code&gt;R2jags&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Su et al. (2015)&lt;/span&gt;) and show how to fit &lt;code&gt;JAGS&lt;/code&gt; models using this package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-jags-and-r2jags&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing JAGS and R2jags&lt;/h2&gt;
&lt;p&gt;Install the latest version of &lt;code&gt;JAGS&lt;/code&gt; for your OS. Next, install the package &lt;code&gt;R2jags&lt;/code&gt; from within &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;Rstudio&lt;/code&gt;, via the package installer or by typing in the command line&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;R2jags&amp;quot;, dependencies = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;dependencies = TRUE&lt;/code&gt; option will automatically install all the packages on which the functions in the &lt;code&gt;R2jags&lt;/code&gt; package rely.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic model&lt;/h1&gt;
&lt;div id=&#34;simulate-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulate data&lt;/h2&gt;
&lt;p&gt;For an example dataset, I simulate my own data in &lt;code&gt;R&lt;/code&gt;. I create a continuous outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as a function of one predictor &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and a disturbance term &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficients, i.e.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \beta_1 x + \epsilon  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;R&lt;/code&gt; commands which I use to simulate the data are the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; n.sim=100; set.seed(123)
&amp;gt; x=rnorm(n.sim, mean = 5, sd = 2)
&amp;gt; epsilon=rnorm(n.sim, mean = 0, sd = 1)
&amp;gt; beta0=1.5
&amp;gt; beta1=1.2
&amp;gt; y=beta0 + beta1 * x + epsilon&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I define all the data for &lt;code&gt;JAGS&lt;/code&gt; in a list object&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; datalist=list(&amp;quot;y&amp;quot;,&amp;quot;x&amp;quot;,&amp;quot;n.sim&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model file&lt;/h2&gt;
&lt;p&gt;Now, I write the model for &lt;code&gt;JAGS&lt;/code&gt; and save it as a text file named &lt;code&gt;&#34;basic.mod.txt&#34;&lt;/code&gt; in the current working directory&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod= &amp;quot;
+ model {
+ #model
+  for(i in 1:n.sim){
+   y[i] ~ dnorm(mu[i], tau)
+   mu[i] = beta0 + beta1 * x[i]
+  }
+ #priors
+ beta0 ~ dnorm(0, 0.01)
+ beta1 ~ dnorm(0, 0.01)
+ tau ~ dgamma(0.01,0.01)
+ }
+ &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The part of the model inside the for loop denotes the likelihood, which is evaluated for each individual in the sample using a Normal distribution parameterised by some mean &lt;code&gt;mu&lt;/code&gt; and precision &lt;code&gt;tau&lt;/code&gt; (where, precision = 1/variance). The covariate &lt;code&gt;x&lt;/code&gt; is included at the mean level using a linear regression, which is indexed by the intercept &lt;code&gt;beta0&lt;/code&gt; and slope &lt;code&gt;beta1&lt;/code&gt; terms. The second part defines the prior distributions for all parameters of the model, namely the regression coefficients and the precision. Weakly informative priors are used since I assume that I do not have any prior knowledge about these parameters.&lt;/p&gt;
&lt;p&gt;To write and save the model as the text file “basic.mod.txt” in the current working directory, I use the &lt;code&gt;writeLines&lt;/code&gt; function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; writeLines(basic.mod, &amp;quot;basic.mod.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pre-processing&lt;/h2&gt;
&lt;p&gt;Define the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in &lt;code&gt;JAGS&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params=c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;)
&amp;gt; inits=function(){list(&amp;quot;beta0&amp;quot;=rnorm(1), &amp;quot;beta1&amp;quot;=rnorm(1))}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters. However, for more complex models, this may not be immediate and a lot of trial and error may be required. However, &lt;code&gt;JAGS&lt;/code&gt; can automatically select the initial values for all parameters in an efficient way even for relatively complex models. This can be achieved by setting &lt;code&gt;inits=NULL&lt;/code&gt;, which is then passed to the &lt;code&gt;jags&lt;/code&gt; function in &lt;code&gt;R2jags&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before using &lt;code&gt;R2jags&lt;/code&gt; for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(R2jags)
&amp;gt; set.seed(123)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit the model&lt;/h2&gt;
&lt;p&gt;Now, we can fit the model in &lt;code&gt;JAGS&lt;/code&gt; using the &lt;code&gt;jags&lt;/code&gt; function in the &lt;code&gt;R2jags&lt;/code&gt; package and save it in the object &lt;code&gt;basic.mod&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod=jags(data = datalist, inits = inits,
+   parameters.to.save = params, n.chains = 2, n.iter = 2000, 
+   n.burnin = 1000, model.file = &amp;quot;basic.mod.txt&amp;quot;)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 100
   Unobserved stochastic nodes: 3
   Total graph size: 406

Initializing model&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the model is running, the function prints out some information related to the Bayesian graph (corresponding to the specification used for the model) underneath &lt;code&gt;JAGS&lt;/code&gt;, such as number of observed and unobserved nodes and graph size.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;post-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Post-processing&lt;/h2&gt;
&lt;p&gt;Once the model has finished running, a summary of the posteiror estimates and convergence diagnostics for all parameters specified can be seen by typing &lt;code&gt;print(basic.mod)&lt;/code&gt; or, alternatively,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; print(basic.mod$BUGSoutput$summary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          mean    sd   2.5%   25%   50%   75% 97.5% Rhat n.eff
beta0      1.5 0.294   0.95   1.3   1.5   1.7   2.1    1  2000
beta1      1.2 0.054   1.07   1.1   1.2   1.2   1.3    1  2000
deviance 278.8 2.475 276.03 277.1 278.2 279.9 285.1    1  2000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The posterior distribution of each parameter is summarised in terms of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mean, sd and some percentiles&lt;/li&gt;
&lt;li&gt;Potential scale reduction factor &lt;code&gt;Rhat&lt;/code&gt; and effective sample size &lt;code&gt;n.eff&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Gelman (2013)&lt;/span&gt;). The first is a measure to assess issues in convergence of the MCMC algorithm (typically a value below &lt;span class=&#34;math inline&#34;&gt;\(1.05\)&lt;/span&gt; for all parameters is considered ok). The second is a measure which assesses the adequacy of the posterior sample (typically values close to the total number of iterations are desirable for all parameters).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The deviance is a goodness of fit statistic and is used in the construction of the “Deviance Information Criterion” or DIC (&lt;span class=&#34;citation&#34;&gt;Spiegelhalter et al. (2014)&lt;/span&gt;), which is a &lt;em&gt;relative&lt;/em&gt; measure of model comparison. The DIC of the model can be accessed by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod$BUGSoutput$DIC
[1] 282&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;diagnostics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagnostics&lt;/h2&gt;
&lt;p&gt;More diagnostics are available when we convert the model output into an MCMC object using the command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod.mcmc=as.mcmc(basic.mod)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Different packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the &lt;code&gt;mcmcplots&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Curtis (2015)&lt;/span&gt;) to obtain graphical diagnostics and results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;mcmcplots&amp;quot;)
&amp;gt; library(mcmcplots)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, density and trace plots can be obtained by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; denplot(basic.mod.mcmc, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/basic-introduction-to-jags/2018-08-23-super-basic-introduction-to-jags_files/figure-html/diagnostic3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(basic.mod.mcmc, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/basic-introduction-to-jags/2018-08-23-super-basic-introduction-to-jags_files/figure-html/diagnostic3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;This tutorial was simply a brief introduction on how simple linear regression models can be fitted using the Bayesian software &lt;code&gt;JAGS&lt;/code&gt; via the &lt;code&gt;R2jags&lt;/code&gt; package. Although this may seem a complex procedure compared with simply fitting a linear model under the frequentist framework, however, the real advantages of Bayesian methods become evident when the complexity of the analysis is increased (which is often the case in real applications). Indeed, the flexibility in Bayesian modelling allows to account for increasingly complex models in a relatively easy way. In addition, Bayesian methods are ideal when the interest is in taking into account the potential impact that different sources of uncertainty may have on the final results, as they allow the natural propagation of uncertainty throughout each quantity in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-curtis2015mcmcplots&#34;&gt;
&lt;p&gt;Curtis, SM. 2015. “Mcmcplots: Create Plots from Mcmc Output.” &lt;em&gt;R Package Version 0.4&lt;/em&gt; 2.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34;&gt;
&lt;p&gt;Gelman, Andrew. 2013. &lt;em&gt;Bayesian Data Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-plummer2004jags&#34;&gt;
&lt;p&gt;Plummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-spiegelhalter2014deviance&#34;&gt;
&lt;p&gt;Spiegelhalter, David J, Nicola G Best, Bradley P Carlin, and Angelika Van Der Linde. 2014. “The Deviance Information Criterion: 12 Years on.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 76 (3): 485–93.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-su2015package&#34;&gt;
&lt;p&gt;Su, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” &lt;em&gt;R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian methods for addressing missing data in health economic evaluations</title>
      <link>/talk/albacete2019/</link>
      <pubDate>Tue, 11 Jun 2019 10:00:00 +0000</pubDate>
      
      <guid>/talk/albacete2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations</title>
      <link>/publication/gabrio2019c/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2019c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Statistical Economic Evaluation Methods for Health Technology Assessment</title>
      <link>/publication/gabrio2019b/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2019b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pitfalls of adjusting for mean baseline utilities/costs in trial-based cost-effectiveness analysis with missing data</title>
      <link>/publication/gabrio2019d/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2019d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Full Bayesian Model to Handle Structural Ones and Missingness in Economic Evaluations from Individual-Level Data</title>
      <link>/publication/gabrio2019a/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2019a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Bayesian Parametric Approach to Handle Nonignorable Missingness in Economic Evaluations</title>
      <link>/talk/priment2018/</link>
      <pubDate>Fri, 01 Jun 2018 13:00:00 +0000</pubDate>
      
      <guid>/talk/priment2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Hierarchical Models for the Prediction of Volleyball Results</title>
      <link>/project/volleyball/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/volleyball/</guid>
      <description>

&lt;h1 id=&#34;modelling-framework&#34;&gt;Modelling Framework&lt;/h1&gt;

&lt;p&gt;We extend and adapt the modelling frameworks typically used for the analysis of football data and propose a novel Bayesian hierarchical modelling framework for the analysis and
prediction of volleyball results in regular seasons. Three different sub-models or &amp;ldquo;modules&amp;rdquo; form our framework: (1) The module of the observed number of points scored by the two opposing teams in a match ($y_h$ and $y_a$);
(2) the module of the binary indicator for the number of sets played ($d^s$); (3) the module of the binary indicator for the winner of the match ($d^m$).
These three modules are jointly modelled using a flexible Bayesian parametric approach, which allows to fully propagate the uncertainty for each unobserved quantity
and to assess the predictive performance of the model in a relatively easy way. In the following, we describe the notation and the model used in each of the three modules.&lt;/p&gt;

&lt;h2 id=&#34;module-1-modelling-the-scoring-intensity&#34;&gt;Module 1: Modelling the Scoring Intensity&lt;/h2&gt;

&lt;p&gt;In the first module of the framework, we model the number of points scored by the home and away team in the $i$-th match of the season $\boldsymbol y=(y&lt;em&gt;{hi},y&lt;/em&gt;{ai})$
using two independent Poisson distributions&lt;/p&gt;

&lt;p&gt;\[
y_{hi} \sim Poisson(\theta_{hi}),
\]&lt;/p&gt;

&lt;p&gt;\[
y_{ai} \sim Poisson(\theta_{ai}),
\]&lt;/p&gt;

&lt;p&gt;conditionally on the set of parameters $\boldsymbol \theta=(\theta_{hi},\theta_{ai})$, representing the scoring intensity in the $i$-th match for the home and away team, respectively.
These parameters are then modelled using the log-linear regressions&lt;/p&gt;

&lt;p&gt;\[
log(\theta_{hi}) =\mu + \lambda + att_{h(i)} + def_{a(i)},
\]&lt;/p&gt;

&lt;p&gt;\[
log(\theta_{ai}) =\mu + att_{a(i)} + def_{h(i)},
\]&lt;/p&gt;

&lt;p&gt;which corresponds to a Poisson log-linear model. Within these formulae, $\mu$ is a constant, while $\lambda$ can be identified as the home effect and represents the advantage
for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season.
The overall offensive and defensive performances of the $k$-th team is captured by the parameters $att$ and $def$, whose nested indexes $h(i), a(i)=1,\ldots,K$ identify
the home and away team in the $i$-th game of the season, where $K$ denotes the total number of the teams.&lt;/p&gt;

&lt;p&gt;We then expand the modelling framework to incorporate match-specific statistics related to the offensive and defensive performances of the home and away teams.
More specifically, the effects associated with the attack intensity of the home teams and the defence effect of the away teams are:&lt;/p&gt;

&lt;p&gt;\[
att_{h(i)} =\alpha_{0h(i)} + \alpha_{1h(i)}att^{eff}_{hi} + \alpha_{2h(i)}ser^{eff}_{hi},
\]&lt;/p&gt;

&lt;p&gt;\[
def_{a(i)} =\beta_{0a(i)} + \beta_{1a(i)}def^{eff}_{ai} + \beta_{2a(i)}blo^{eff}_{ai}.
\]&lt;/p&gt;

&lt;p&gt;We omit the index $i$ from the terms to the left-hand side of the above formulae to ease notation, i.e. $att_{h(i)}=att_{h(i)i}$ and $def_{a(i)}=def_{a(i)i}$.
The overall offensive effect of the home teams is a function of a baseline team specific parameter $\alpha_{0h(i)}$, and the attack and serve efficiencies of the home team,
whose impact is captured by the parameters $\alpha_{1h(i)}$ and $\alpha_{2h(i)}$. The overall defensive effect of the away team is a function of a baseline team-specific
parameter $\beta_{0a(i)}$, and the defence and block efficiencies of the away team, whose impact is captured by the parameters $\beta_{1a(i)}$ and $\beta_{2a(i)}$, respectively.
Similarly, the effects associated with the attack intensity of the away teams and the defence effect of the home teams are:&lt;/p&gt;

&lt;p&gt;\[
att_{a(i)} =\alpha_{0a(i)} + \alpha_{1a(i)}att^{eff}_{ai}+ \alpha_{2a(i)}ser^{eff}_{ai},
\]&lt;/p&gt;

&lt;p&gt;\[
def_{h(i)} =\beta_{0h(i)} + \beta_{1h(i)}def^{eff}_{hi}+ \beta_{2h(i)}blo^{eff}_{hi},
\]&lt;/p&gt;

&lt;p&gt;To achieve identifiability of the model, a set of parametric constraints needs to be imposed. We impose &lt;em&gt;sum-to-zero&lt;/em&gt; constraints on the team-specific parameters, i.e. we set $\sum_{k=1}^{K}\alpha_{jk}=0$ and $\sum_{k=1}^{K}\beta_{jk}=0$, for $k=1,\ldots,K$ and $j=(0,1,2)$.
Under this set of constraints, the overall offensive and defensive effects of the teams are expressed as departures from a team of average offensive and defensive performance.
Within a Bayesian framework, prior distributions need to be specified for all random parameters in the model. Weakly informative Normal distributions centred at $0$ with a relatively large variances are specified for the fixed effect parameters.&lt;/p&gt;

&lt;h2 id=&#34;module-2-modelling-the-probability-of-playing-5-sets&#34;&gt;Module 2: Modelling the Probability of Playing 5 Sets&lt;/h2&gt;

&lt;p&gt;In the second module, we explicitly model the chance of playing $5$ sets in the $i$-th match of the season, i.e. the sum of the sets won by the home ($s_{hi}$)
and away ($s_{ai}$) team is equal to $5$. This is necessary when generating predictions in order to correctly assign the points to the winning/losing teams throughout
the season and evaluate the rankings of the teams at the end of the season.
We model the indicator variable $d^s_{i}$, taking value $1$ if $5$ sets were played in the $i-$th match and $0$ otherwise, using a Bernoulli distribution&lt;/p&gt;

&lt;p&gt;\[
d^s_{i}:=\mathbb{I}(s_{hi}+s_{ai}=5)\sim\mbox{Bernoulli}(\pi^s_{i}),
\]&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;\[
logit(\pi^s_{i})= \gamma_0 + \gamma_1y_{hi} + \gamma_2y_{ai}.&lt;br /&gt;
\]&lt;/p&gt;

&lt;h2 id=&#34;module-3-modelling-the-probability-of-winning-the-match&#34;&gt;Module 3: Modelling the Probability of Winning the Match&lt;/h2&gt;

&lt;p&gt;The last module deals with the chance of the home team to win the $i$-th match, i.e. the total number of sets won by the home team  ($s_{hi}$)
is larger than that of the away team ($s_{ai}$) &amp;ndash; we note that we could have also equivalently decided to model the chance of the away team to win the $i$-th match.
This part of the model is again necessary when predicting the results for future matches, since the team associated with the higher number of points scored in the $i$-th
match may not correspond to the winning team.
We model the indicator variable $d^m_{i}$, taking value $1$ if the home team won the $i-$th match and $0$ otherwise, using another Bernoulli distribution&lt;/p&gt;

&lt;p&gt;\[
d^m_{i}:=\mathbb{I}(s_{hi}&amp;gt;s_{ai}) \sim\mbox{Bernoulli}(\pi^m_{i}),
\]&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;\[
logit(\pi^m_{i})= \eta_0 + \eta_1y_{hi} + \eta_2y_{ai} + \eta_3 d^s_i.
\]&lt;/p&gt;

&lt;p&gt;The next figure shows a graphical representation of the modelling framework proposed.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/framework_volley.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Graphical representation of the modelling framework.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;The framework corresponds to a joint distribution for all the observed quantities which are explicitly modelled.
This is factored into the product of the marginal distribution of the total number of points scored by the two teams in each match, Module 1 &amp;ndash; $p(\boldsymbol y)$,
the conditional distribution of the probability of playing $5$ sets in a match given $\boldsymbol y$, Module 2 &amp;ndash; $p(d^s_i \mid \boldsymbol y)$,
and the conditional probability of winning the match given $\boldsymbol y$ and $d^s_i$, Module 3 &amp;ndash; $p(d^m_i\mid \boldsymbol y, d^s_i)$.
Module 1 also includes the different in-game statistics as covariates in the model. These are related to the either the offensive (serve and attack efficiency)
or defensive (defence and block efficiency) effects of the home and away teams in each match of the season, and are respectively denoted in the graph as
$\boldsymbol x^{att}_{ti}=(ser^{eff}_{ti}, att^{eff}_{ti})$ and $\boldsymbol x^{def}_{ti}=(def^{eff}_{ti}, blo^{eff}_{ti})$ to ease notation, for $t=(h,a)$.&lt;/p&gt;

&lt;h2 id=&#34;accounting-for-the-multilevel-correlation&#34;&gt;Accounting for the multilevel correlation&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;
Although the individual-level correlation between the observable variables $y_{hi}$ and $y_{ai}$ is taken into account through the hierarchical structure of the framework,
a potential limitation of the model is that it ignores the possible multilevel correlation between the team-specific offensive $\alpha_{jk}$ and defensive $\beta_{jk}$ coefficients, for $j=(0,1,2)$ and $k=1,\ldots,K$.
In an alternative analysis, we account for the multilevel correlation using Inverse-Wishart distributions on the covariance matrix of the team specific parameters $ \boldsymbol \Sigma_{\boldsymbol \alpha}$ and $ \boldsymbol \Sigma_{\boldsymbol \beta}$,
which are scaled in order to facilitate the specification of the priors.&lt;/p&gt;

&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;

&lt;p&gt;Overall, the predicted results from both the basic and the scaled IW model seem to replicate the observed data relatively well for most of the teams.
The total number of points scored and conceded are similar between the observed and replicated data, with the teams scoring (conceding) the most being also associated
with the highest replicated points scored (conceded) and vice versa. Relatively small discrepancies are observed between the results of the two models for some of the teams.
The total number of wins and league points are almost identical between the observed and replicated data, with the scaled IW model being associated with slightly m
ore accurate predictions compared with the basic model.&lt;/p&gt;

&lt;p&gt;The following figure compares the cumulative points derived from the observed results throughout the season (the black line) and the predictions from both the basic model (in red), and the scaled Inverse-Wishart model (in blue).&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/plot_cumul_points_volley.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Posterior predictive validation of the basic (red) and IW (blue) model with respect to the observed data (black).&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;For almost all teams the predicted results are relatively close to the observed data and suggest a good performance of both models.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;To our knowledge, this is the first modelling framework which jointly allows to predict team rankings and the outcomes of the matches during a season in volleyball.
The two alternative specifications implemented in our analysis show generally good predictive performances; between the two models, the scaled IW model seems to be
slightly more accurate compared with the basic model, but is also associated with a higher level of complexity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Full Bayesian Model to Handle Structural Ones and Missingness in Health Economic Evaluations from Individual-Level Data</title>
      <link>/talk/hesymposium2018/</link>
      <pubDate>Mon, 05 Feb 2018 10:00:00 +0000</pubDate>
      
      <guid>/talk/hesymposium2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>missingHE</title>
      <link>/project/missinghe/</link>
      <pubDate>Mon, 05 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/missinghe/</guid>
      <description>&lt;p&gt;&lt;code&gt;missingHE&lt;/code&gt; is a &lt;code&gt;R&lt;/code&gt; package, available on &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE/&#34; target=&#34;_blank&#34;&gt;CRAN&lt;/a&gt; which is aimed at providing some useful tools to analysts in order to handle missing outcome data under a full Bayesian framework in economic evaluations.
The package relies on the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;R2jags&lt;/code&gt; to implement Bayesian methods via the statistical software &lt;code&gt;JAGS&lt;/code&gt; to obtain inferences using Markov Chain Monte Carlo (MCMC) methods.
Different types of missing data models are implemented in the package, including selection models, pattern mixture models and hurdle models. A range of parametric distributions can be specified when modelling the typical
outcomes in an trial-based economic evaluations, namely the effectiveness and cost variabels, while simultaneously incorporating different assumptions about the missingness mechanism, which allows to easily perform
sensitvity analysis to a range of alternative missing data assumptions according to the modelling choices selected by the user.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;missingHE&lt;/code&gt; also provides functions, taken and adapted from other &lt;code&gt;R&lt;/code&gt; packages, to assess the results of each type of model, including summaries of the posterior distributions of each model parameter,
range and imputations of the missing values, different types of model diagnostics to assess convergence of the algorithm, posterior predictive checks, model assessment measures based on the fit to the observed data,
and a general summary of the economic evaluations, including the results from probabilistic sensitivity analyses which are automatically performed within a Bayesian modelling framework.&lt;/p&gt;

&lt;p&gt;For example, the function &lt;code&gt;plot&lt;/code&gt;, when applied to the output of a model fitted using &lt;code&gt;missingHE&lt;/code&gt;, produces graphs which compare the observed and imputed values for both cost and benefit measures in each treatment group to detect possible concerns about the plausibility of the imputations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;imputed.jpg&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;More information, including new updates, about &amp;lsquo;missingHE&amp;rsquo; can be found at this &lt;a href=&#34;http://127.0.0.1:4321/missingHE/&#34; target=&#34;_blank&#34;&gt;dedicated page&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Handling Missing Data in Within-Trial Cost-Effectiveness Analysis: A Review with Future Recommendations</title>
      <link>/publication/gabrio2017/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Missingness Methods in trial-based CEA</title>
      <link>/project/missing-data-review/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/missing-data-review/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;We performed a systematic literature review that assesses the quality of the information reported and type of methods used to handle missing outcome data in trial-based economic evaluations. The purpose of this review is to critically appraise the current literature in within-trial CEAs with respect to the quality of the information reported and the methods used to deal with missingness for both effectiveness and costs. The review complements previous work, covering 2003-2009 (88 articles) with a new systematic review, covering 2009-2015 (81 articles) and focuses on two~perspectives.&lt;/p&gt;

&lt;p&gt;First, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. We propose to address this issue by means of a &lt;em&gt;quality evaluation scheme&lt;/em&gt;, providing a structured approach that can be used to guide the collection of information, formulation of the assumptions, choice of methods, and considerations of possible limitations for the given missingness problem. Second, we review the description of the missing data, the statistical methods used to deal with them and the quality of the judgement underpinning the choice of these methods.&lt;/p&gt;

&lt;h1 id=&#34;quality-evaluation-scheme&#34;&gt;Quality Evaluation Scheme&lt;/h1&gt;

&lt;p&gt;In order to judge whether missing data in CEAs have been adequately handled, we assembled guidelines from previous review articles on how information relating to the missing data should be reported. In particular, we defined three broad components of the analysis that are related to the description of the missingness problem (Description), details of the methods used to address it (Methods) and a discussion on the uncertainty in the conclusions resulting from the missingness (Limitations). For each component, information that is considered to be vital for transparency is listed under &lt;em&gt;key considerations&lt;/em&gt;, while other details that could usefully be provided as supplementary material are suggested under &lt;em&gt;optimal considerations&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Using the list of key considerations, we determine whether null (all key considerations absent), partial (one or more key considerations absent) or full (all key considerations present) information has been provided for each component. The set of key considerations is defined to ensure a full assessment of the impact that missingness may have on the final conclusions of the analysis with respect to all three components. However, providing a certain level of information on one component (e.g.~full information on Description) typically has a different impact on the results with respect to providing the same level of information on another component (e.g.~full information on Limitations). Based on this, we suggest computing a numerical score that weights each component by the impact that it may have on the final results to summarise the overall information provided on missingness.&lt;/p&gt;

&lt;p&gt;Different score values are calculated based on whether full, partial or null information content is provided in each component and by weighting the three components in a ratio of 3:2:1 (Description: Method: Limitations). This weighting scheme has been chosen according to the impact that each component is likely to have on the final conclusions based on assumptions that we deemed to be~reasonable. Specifically, the Limitations component typically has the least importance among the three because of its limited impact on the conclusions. In the same way, the Description component has potentially a higher impact on the results than the Method component as it generally drives the choice for the initial assumptions about the missingness.&lt;/p&gt;

&lt;p&gt;Finally, the relevance of the scores in terms of decision analysis is mainly associated with a qualitative assessment of the articles. Therefore, we suggest converting the scores into ordered grades (A-E) to evaluate the studies based on the overall information reported on the handling of the missing data. Studies that are graded in the top categories should be associated with a higher degree of confidence in their results, whereas more caution should be given in the consideration of results coming from studies that are graded in the bottom categories. When qualitatively assessing the articles, the different grading assigned to each of them could be an indication of a lack in the robustness of the conclusions provided due to missingness uncertainty.
With respect to the quality assessment of the studies, the aggregation of the quality scores on the components of the analysis (Description, Method and Limitations) into ordered grades could lead to some loss of information compared with the direct use of the quality scores on each component. However, merging the scores into a fewer number of categories ensures a relatively easy comparison of the quality of the information provided across the three analysis components and provides a useful indication about the different degree of confidence to assign to the results obtained by each study.&lt;/p&gt;

&lt;p&gt;The Figure below shows a visual representation of the grade (and score) assignment in the quality evaluation scheme. Although the importance between the different components is subjective, the chosen structure represents a reasonable and relatively straightforward assessment scheme.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/diagram.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Quality Evaluation Scheme.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;The articles reviewed for the two periods are presented and compared by type of analysis performed. First, the base-case methods are considered, i.e.~those used in the main analysis. Second, any alternative methods in these analyses are discussed; when present, these assess the robustness of the results obtained in the main analysis against departures from the initial assumptions on missingness.&lt;/p&gt;

&lt;h1 id=&#34;summary-of-the-findings&#34;&gt;Summary of the findings&lt;/h1&gt;

&lt;p&gt;Our review is based on a sample of recently published studies and should therefore provide a picture of current missing data handling in within-trial CEAs. However, the quality assessment of the articles is based on the information reported in the articles. It is possible that authors had assessed the robustness of their conclusions to the missing data using alternative approaches that were not reported in the published version because of space limitations in journals. In these cases, it is important that on-line appendices and supplementary material are used to report these~alternatives.
In our literature review, information about missing data information and methods was available from $4$ and $9$ on-line supplementary materials for the period 2003-2009 and 2009-2015, respectively. Both the larger number of on-line materials and more detailed information reported about missingness handling in the analyses indicate an increased use of this tool in the later period (2009-2015) compared to the first period (2003-2009).&lt;/p&gt;

&lt;h2 id=&#34;descriptive-review&#34;&gt;Descriptive Review&lt;/h2&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/res_methods.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Missingness methods by outcome and period.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;From the comparison of the base-case methods used for the costs and effects between 2009 and 2015, the Figure above shows a marked reduction in the number of methods not clearly described for the effects, compared to those for the costs. A possible reason for this is that, while clinical effectiveness measures are often collected through self-reported questionnaires, which are naturally prone to missingness, cost measures rely more on clinical patient files which may ensure a higher completeness rate. It was not possible to confirm this interpretation in the reviewed studies due to the high proportions of articles not clearly reporting the missing rates in both 2003-2009 and 2009-2015 periods, for effects
($\approx 45\%$ and $\approx 38\%$) and costs ( $\approx 50\%$ and $\approx 62\%$). In addition, clinical outcomes are almost invariably the main objective of RCTs and are usually subject to more advanced and standardised analyses. Arguably, costs are often considered as an add-on to the standard trial: for instance, sample size calculations are almost always performed with the effectiveness measure as the only outcome of interest. Consequently, missing data methods are less frequently well thought through for the analysis of the costs. However, this situation is likely to change as cost data from different perspectives (e.g. caregivers, patients, society, etc.) are being increasingly used in trials, leading to the more frequent adoption of self-report cost data which may start to exhibit similar missingness characteristics to effect data.&lt;/p&gt;

&lt;p&gt;The review identified only a few articles using more than one alternative method. In addition, these analyses are typically conducted without any clear justification about their underlying missing data assumptions and may therefore not provide a concrete assessment of the impact of missingness uncertainty. This situation indicates a gap in the literature associated with an under-implementation of sensitivity analysis, which may significantly affect the whole decision-making process outcome, under the perspective of a body who is responsible for providing recommendations about the implementation of alternative interventions for health care matters.&lt;/p&gt;

&lt;p&gt;Limiting the assessment of missingness assumptions to a single case is unlikely to provide a reliable picture of the underlying mechanism. This, in turn, may have a significant impact on the CEA and mislead its conclusions, suggesting the implementation of non-cost-effective treatments. Robustness analyses assess the sensitivity of the results to alternative missing data methods but do not justify the choice of these methods and their underlying assumptions about missingness which may therefore be inappropriate in the specific context analysed. By contrast, sensitivity analyses, which rely on external information to explore plausible alternative methods and missingness assumptions, represent an important and more appropriate tool to provide realistic assessments of the impact of missing data uncertainty on the final conclusions.&lt;/p&gt;

&lt;h2 id=&#34;quality-assessment&#34;&gt;Quality assessment&lt;/h2&gt;

&lt;p&gt;Generally speaking, most of the reviewed papers achieved an unsatisfactory quality score under the Quality Evaluation Scheme. Indeed, the benchmark area on the top-right corner of the graphs is barely reached by less than $7\%$ of the articles, both for cost and effect data.&lt;/p&gt;

&lt;p&gt;Overall, the proportions of the studies associated with the lowest category (E) prevails in the majority of the years, with a similar pattern over time between missing costs and effects. All the articles that are associated with the top category (A) belong to the period 2013-2015, with the highest proportions of articles falling in this category being observed in 2015 for both outcomes.
The opportunity of reaching such a target might be precluded by the choice of the method adopted, which may not be able to support less restrictive assumptions about missingness, even when this would be desirable. As a result, when simple methods cannot be fully justified it is necessary to replace them with more flexible ones that can relax assumptions and incorporate more alternatives. In settings such as those involving MNAR, sensitivity analysis might represent the only possible approach to account for the uncertainty due to the missingness in a principled way. However, due to the lack of studies either performing a sensitivity analysis or providing high quality scores on the assumptions, missingness is not adequately addressed in most studies. This could have the serious consequence of imposing too restrictive assumptions about missingness and affect the outcome of decision making.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;Our review shows, over time, a significant change from more to less restrictive methods in terms of the assumptions on the missingness mechanism. This is an encouraging movement towards a more suitable and careful missing data analysis. The results from the disaggregated analysis by year of publication in the later period (2009-2015) indicates the rise of a better and more transparent approach to handle missingness in the latest years of the review, especially in 2015. In particular, compared to the previous years, the articles reviewed from 2015 are associated with a higher proportion of MI methods used in the base-case analysis, a substantial increase in the number of robustness methods implemented, and a better quality score assignment.&lt;/p&gt;

&lt;p&gt;Nevertheless, improvements are still needed as, overall, only a small number of articles provide transparent information about the missing data and almost no study performs a sensitivity~analysis. These failings are probably due to the fact that the implications of using methods that do not handle missingness in a principled way are not well-known among practitioners. In addition, the choice of the missing data methods may also be guided by their ease of implementation in standard software packages rather than methodological reasons. This is a potentially serious issue for bodies such as the NICE who use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new treatment options.&lt;/p&gt;

&lt;p&gt;The Quality Evaluation Scheme represents a valuable tool to improve missing data handling. By carefully thinking about each component in the analysis we are forced to explicitly consider all the assumptions we make about missingness and assess the impact of their variation on final conclusions. The main advantage is a more comparable formalisation of the uncertainty as well as a better indication of possible issues in assessing the cost-effectiveness of new treatments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Handling Missing Data in Within-Trial Cost-Effectiveness Analysis: a Review with Future Recommendations</title>
      <link>/talk/euhea2016/</link>
      <pubDate>Thu, 08 Sep 2016 15:00:00 +0000</pubDate>
      
      <guid>/talk/euhea2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Augmented Inverse Probability Weighting</title>
      <link>/missmethods/augmented-inverse-probability-weighting/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/augmented-inverse-probability-weighting/</guid>
      <description>


&lt;p&gt;A general problem associated with the implementatio of &lt;em&gt;Inverse Probability Weighting&lt;/em&gt; (IPW) methods is that information in some available data is ignored by focussing only on the complete cases (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;). This has provided room to extend these methods to make a more efficient use of the available information through the incorporation of an “augmentation” term, which lead to the development of the so called &lt;em&gt;Augmented Inverse Probability Weighting&lt;/em&gt; (AIPW) methods. These approaches extend IPW methods by creating predictions from a model to recove the information in the incomplete units and applying IPW to the residuals from the model (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Considering the IPW &lt;em&gt;Generalised Estimating Equation&lt;/em&gt; (GEE)&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^{n_r} = w_i(\hat{\alpha})D_i(x_i,\beta)(y_i-g(x_i,\beta))=0,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(w_i(\hat{\alpha})=\frac{1}{p(x_i,z_i \mid \hat{\alpha})}\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(p(x_i,z_i \mid \hat{\alpha})\)&lt;/span&gt; an estimate of the probability of being a complete unit estimated for example using logistic regressions of the missingness indicator &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; on the vectors of the covariate and auxiliary variables &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;, respectively. A problem of this IPW estimator is that it has poor small sample properties when the propensity score gets close to zero or one for some observations, which will lead to high variance in the estimator. AIPW methods can provide estimators of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; which are more efficient than their nonaugmented IPW versions. In general, AIPW estimating functions provide a method for constructing estimators of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; based on two terms:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The usual IPW term &lt;span class=&#34;math inline&#34;&gt;\(p(x_i,z_i \mid \hat{\alpha})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An augmentation term &lt;span class=&#34;math inline&#34;&gt;\(g^\star(x_i,\beta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The basis for the first term is a complete data unbiased estimating function for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, whereas the basis for the second term is some function of the observed data chosen so it has conditional mean of zero given the complete data (&lt;span class=&#34;citation&#34;&gt;Molenberghs et al. (2014)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;doubly-robust-estimators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Doubly Robust Estimators&lt;/h2&gt;
&lt;p&gt;An important class of AIPW methods is known as &lt;em&gt;doubly robust&lt;/em&gt; estimators, which have desirable robustness properties (&lt;span class=&#34;citation&#34;&gt;Robins, Rotnitzky, and Laan (2000)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Robins and Rotnitzky (2001)&lt;/span&gt;). The key feature of these estimators is that they relax the assumption that the model of the missingness probabilities is correctly specified, although requiring additional assumptions on the model for &lt;span class=&#34;math inline&#34;&gt;\(y_i \mid x_i\)&lt;/span&gt;. For example, doubly robust estimators for a population mean parameter &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; could be obtained as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Fit a logistic regression model for the probability of observing &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; to derive the individual weights &lt;span class=&#34;math inline&#34;&gt;\(w_i(\hat{\alpha})\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fit a generalized linear model for the outcome of responders in function of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; using weights &lt;span class=&#34;math inline&#34;&gt;\(w_i(\hat{\alpha})\)&lt;/span&gt; and let &lt;span class=&#34;math inline&#34;&gt;\(g^\star(x_i,\beta)\)&lt;/span&gt; denote the fitted values for subject &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take the sample average of the fitted values &lt;span class=&#34;math inline&#34;&gt;\(g^\star(x_i,\beta)\)&lt;/span&gt; of both respondents and nonrespondents as an estimate of the population mean &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Doubly robust estimators require the specification of two models: one for the missingness probability and another for the distribution of the incomplete data. When the augmentation term &lt;span class=&#34;math inline&#34;&gt;\(g^\star(x_i,\beta)\)&lt;/span&gt; is selected and modelled correctly according to the distribution of the complete data, the resulting estimator of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is consistent even if the model of missingness is misspecified. On the other hand, if the model of missingness is correctly specified, the augmentation term no longer needs to be correctly specified to yield consistent estimators of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Scharfstein, Daniels, and Robins (2003)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Bang and Robins (2005)&lt;/span&gt;). Doubly robust estimators therefore allow to obtain an unbiased estimating function for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; if either the model for the incomplete data or the model for the missingness mechanism has been correctly specified.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose the full data consists of a single outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and an additional variable &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; and that the objective is to estimate the population outcome mean &lt;span class=&#34;math inline&#34;&gt;\(\mu=\text{E}[y]\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is partially observed (while &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is always fully observed), individuals may fall into one of two missingness patterns &lt;span class=&#34;math inline&#34;&gt;\(r=(r_{y},r_{z})\)&lt;/span&gt;, namely &lt;span class=&#34;math inline&#34;&gt;\(r=(1,1)\)&lt;/span&gt; if both variables are observed or &lt;span class=&#34;math inline&#34;&gt;\(r=(1,0)\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is missing. Let &lt;span class=&#34;math inline&#34;&gt;\(c=1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(r=(1,1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c=0\)&lt;/span&gt; otherwise, so that the observed data can be summarised as &lt;span class=&#34;math inline&#34;&gt;\((c,cy,z)\)&lt;/span&gt;. Assuming that missingness only depends on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p(c=1 \mid y,z)=p(c=1 \mid z)=\pi(z),
\]&lt;/p&gt;
&lt;p&gt;then the missing data mechanism is &lt;em&gt;Missing At Random&lt;/em&gt; (MAR). Under these conditions, consider the consistent IPW complete case estimating equation&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n\frac{c_i}{\pi(z_i \mid \hat{\alpha})}(y_i-\mu)=0,
\]&lt;/p&gt;
&lt;p&gt;which can be used to weight the contribution of each complete case by the inverse of &lt;span class=&#34;math inline&#34;&gt;\(\pi(z_i \mid \hat{\alpha})\)&lt;/span&gt;, typically estimated via logistic regressions. A general problem of this type of estimators is that they discard all the available data among the non-completers and are therefore inefficient. However, it is possible to augment the simple IPW complete case estimating equation to improve efficiency. The optimal estimator for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; within this class is the solution to the estimating equation&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n \left(\frac{c_i}{\pi(z_i \mid \hat{\alpha})}(y_i-\mu) - \frac{c_i-\pi(z_i \mid \hat{\alpha})}{\pi(z_i \mid \hat{\alpha})}\text{E}[(y_i-\mu)\mid z_i] \right),
\]&lt;/p&gt;
&lt;p&gt;which leads to the estimator&lt;/p&gt;
&lt;p&gt;\[
\mu_{aipw}=\frac{1}{n}\sum_{i=1}^n \left(\frac{c_iy_i}{\pi(z_i\mid \hat{\alpha})} - \frac{c_i - \pi(z_i\mid \hat{\alpha})}{\pi(z_i\mid \hat{\alpha})} \text{E}[y_i \mid z_i] \right).
\]&lt;/p&gt;
&lt;p&gt;The conditional expectation &lt;span class=&#34;math inline&#34;&gt;\(\text{E}[y_i \mid z_i]\)&lt;/span&gt; is not known and must be estimated from the data. Under a &lt;em&gt;Missing At Random&lt;/em&gt; (MAR) assumption we have that &lt;span class=&#34;math inline&#34;&gt;\(\text{E}[y \mid z]=\text{E}[y \mid z, c=1]\)&lt;/span&gt;, that is the conditional expecation of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is the same as that among the completers. Thus, we can specify a model &lt;span class=&#34;math inline&#34;&gt;\(m(z,\xi)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\text{E}[y \mid z]\)&lt;/span&gt;, indexed by the parameter &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;, that can be estimated from the completers. If &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is continuous, a simple choice is to estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{\xi}\)&lt;/span&gt; by OLS from the completers. The AIPW estimator for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; then becomes&lt;/p&gt;
&lt;p&gt;\[
\mu_{aipw}=\frac{1}{n}\sum_{i=1}^n \left(\frac{c_iy_i}{\pi(z_i\mid \hat{\alpha})} - \frac{c_i - \pi(z_i\mid \hat{\alpha})}{\pi(z_i\mid \hat{\alpha})} m(z_i\mid \hat{\xi}) \right).
\]&lt;/p&gt;
&lt;p&gt;It can be shown that this estimator is more efficient that the simple IPW complete case estimator for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and that it has a double robustness property. This ensures that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{aipw}\)&lt;/span&gt; is a consitent estimator of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; if &lt;strong&gt;either&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the model &lt;span class=&#34;math inline&#34;&gt;\(\pi(z\mid\alpha)\)&lt;/span&gt; is correctly specified, &lt;strong&gt;or&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the model &lt;span class=&#34;math inline&#34;&gt;\(m(z\mid \xi)\)&lt;/span&gt; is correctly specified.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To see a derivation of the double robustness property I put here a link to some nice &lt;a href=&#34;https://www4.stat.ncsu.edu/~davidian/st790/notes/chap5.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conlcusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conlcusions&lt;/h2&gt;
&lt;p&gt;As all weighting methods, such as IPW, AIPW methods are &lt;em&gt;semiparametric&lt;/em&gt; methods that aim to achieve robustness and good performance over more general classes of population distributions. However, semiparametric estimators can be less efficient and less powerful than &lt;em&gt;Maximum Likelihood&lt;/em&gt; or &lt;em&gt;Bayesian&lt;/em&gt; estimators under a well specified parametric model. With missing data, &lt;span class=&#34;citation&#34;&gt;Rubin (1976)&lt;/span&gt; results show that likelihood-based methods perform uniformly well over any &lt;em&gt;Missing At Random&lt;/em&gt; (MAR) missingness distribution, and the user does not need to specify that distribution. However, semiparametric methods that relax assumptions about the data must in turn assume a specific form for the distribution of missingness. It has been argued that, for these semiparametric methods to gain a substantial advantage over well-specified likelihood methods, the parametric model has to be grossly misspecified (&lt;span class=&#34;citation&#34;&gt;Meng (2000)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-bang2005doubly&#34;&gt;
&lt;p&gt;Bang, Heejung, and James M Robins. 2005. “Doubly Robust Estimation in Missing Data and Causal Inference Models.” &lt;em&gt;Biometrics&lt;/em&gt; 61 (4): 962–73.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-meng2000missing&#34;&gt;
&lt;p&gt;Meng, Xiao-Li. 2000. “Missing Data: Dial M for???” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 95 (452): 1325–30.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-molenberghs2014handbook&#34;&gt;
&lt;p&gt;Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. &lt;em&gt;Handbook of Missing Data Methodology&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins2001comment&#34;&gt;
&lt;p&gt;Robins, James M, and Andrea Rotnitzky. 2001. “Comment on the Bickel and Kwon Article,‘Inference for Semiparametric Models: Some Questions and an Answer’.” &lt;em&gt;Statistica Sinica&lt;/em&gt; 11 (4): 920–36.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins2000profile&#34;&gt;
&lt;p&gt;Robins, James M, Andrea Rotnitzky, and Mark van der Laan. 2000. “On Profile Likelihood: Comment.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 95 (450): 477–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1976inference&#34;&gt;
&lt;p&gt;Rubin, Donald B. 1976. “Inference and Missing Data.” &lt;em&gt;Biometrika&lt;/em&gt; 63 (3): 581–92.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-scharfstein2003incorporating&#34;&gt;
&lt;p&gt;Scharfstein, Daniel O, Michael J Daniels, and James M Robins. 2003. “Incorporating Prior Beliefs About Selection Bias into the Analysis of Randomized Trials with Missing Outcomes.” &lt;em&gt;Biostatistics&lt;/em&gt; 4 (4): 495–512.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Available Case Analysis</title>
      <link>/missmethods/available-case-analysis/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/available-case-analysis/</guid>
      <description>


&lt;p&gt;Complete case analysis (CCA) can be particularly inefficient for data sets with a large number of variables which are partially observed. An alternative approach that can be used to conduct univariate analyses in known as &lt;em&gt;Available Case Analysis&lt;/em&gt; (ACA), which uses all the available cases, separately for each variable under examination, to estimate the quantities of interest.&lt;/p&gt;
&lt;p&gt;The main drawback of ACA is that the sample used to perform the analysis varies from variable to variable according to the patterns of missing data, which generates problems of comparability across variables if the missingness mechanism is not &lt;em&gt;missing completely at random&lt;/em&gt; (MCAR), i.e. the missing data probabilities depend on the variables under study. While estimates of means and variances can be easily computed, measures of covariation need to be adjusted. In particular, for estimating sample covariances, this approach is known as &lt;em&gt;pairwise deletion&lt;/em&gt; or &lt;em&gt;pairwise inclusion&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;pairwise-measures-of-covariation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pairwise measures of covariation&lt;/h2&gt;
&lt;p&gt;One possible approach to estimate pairwise measures of covariation for &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_k\)&lt;/span&gt; is to use only those units &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n_{ac}\)&lt;/span&gt; for which both variables are observed (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;). For example, one can compute pairwise sample covariances as:&lt;/p&gt;
&lt;p&gt;\[
s^{ac}_{jk} = \frac{\sum_{i \in I_{ac}}(y_{ij}-\bar{y}_{j}^{ac})(y_{ik}-\bar{y}_{k}^{ac})}{(n_{ac}-1)},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(I_{ac}\)&lt;/span&gt; is the set of &lt;span class=&#34;math inline&#34;&gt;\(n_{ac}\)&lt;/span&gt; with both &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_k\)&lt;/span&gt; observed, while the sample means &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{ac}_{j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{ac}_{k}\)&lt;/span&gt; are calculated over this set of units. We can also estimate the sample correlation&lt;/p&gt;
&lt;p&gt;\[
r^{\star}_{jk} = \frac{s^{ac}_{jk}}{\sqrt{s^2_{j}s^{2}_{k}}},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s^2_{j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s^2_{k}\)&lt;/span&gt; are the sample variances computed over the sets of observed units &lt;span class=&#34;math inline&#34;&gt;\(I_{j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(I_{k}\)&lt;/span&gt;, respectively. A problem of this type of correlation estimate is that it can lie outside the range &lt;span class=&#34;math inline&#34;&gt;\((-1,1)\)&lt;/span&gt;, which is typically addressed by computing &lt;em&gt;pairwise correlations&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Wilks (1932)&lt;/span&gt;), where variances are estimated from the set of units with both variables observed &lt;span class=&#34;math inline&#34;&gt;\(I_{jk}\)&lt;/span&gt;, i.e. &lt;/p&gt;
&lt;p&gt;\[
r^{ac}_{jk} = \frac{s^{ac}_{jk}}{\sqrt{s^{2,ac}_{j}s^{2,ac}_{k}}}.
\]&lt;/p&gt;
&lt;p&gt;In addition, we could also replace the sample means &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{ac}_{j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{ac}_{k}\)&lt;/span&gt;, evaluated on the common set of units &lt;span class=&#34;math inline&#34;&gt;\(I_{jk}\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{k}\)&lt;/span&gt;, which are evaluated on the sets of units &lt;span class=&#34;math inline&#34;&gt;\(I_{j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(I_{k}\)&lt;/span&gt;, respectively. This leads to the following estimates for the sample covariances (&lt;span class=&#34;citation&#34;&gt;Matthai (1951)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;\[
s^{\star}_{jk} = \frac{\sum_{i \in I_{ac}}(y_{ij}-\bar{y}_{j})(y_{ik}-\bar{y}_{k})}{(n_{ac}-1)},
\]&lt;/p&gt;
&lt;p&gt;Pairwise AC estimates aim at recovering information from partially-observed units that are lost by CCA. However, when considered together, the estimates suffer from inconsistencies that undermine the validity of these methods. For example, pairwise correlation matrices may be not positive definite. Because parameters are estimated from different sets of units, different approaches can be used to obtain estimate of the measures of uncertainty (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;AC estimates allow to make use of all the available evidence in the data and may be more efficient that CCA when the missingness mechanism is MCAR and correlations are modest (&lt;span class=&#34;citation&#34;&gt;Kim and Curry (1977)&lt;/span&gt;). However, when correlations are more substantial, ACA may become even less efficient than CCA (&lt;span class=&#34;citation&#34;&gt;Haitovsky (1968)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Azen and Van Guilder (1981)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-azen1981conclusions&#34;&gt;
&lt;p&gt;Azen, S, and M Van Guilder. 1981. “Conclusions Regarding Algorithms for Handling Incomplete Data.” &lt;em&gt;1981 Proceedings of the Statistical Computing Section&lt;/em&gt;, 53–56.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-haitovsky1968missing&#34;&gt;
&lt;p&gt;Haitovsky, Yoel. 1968. “Missing Data in Regression Analysis.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Methodological)&lt;/em&gt; 30 (1): 67–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kim1977treatment&#34;&gt;
&lt;p&gt;Kim, Jae-On, and James Curry. 1977. “The Treatment of Missing Data in Multivariate Analysis.” &lt;em&gt;Sociological Methods &amp;amp; Research&lt;/em&gt; 6 (2): 215–40.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-matthai1951estimation&#34;&gt;
&lt;p&gt;Matthai, Abraham. 1951. “Estimation of Parameters from Incomplete Data with Application to Design of Sample Surveys.” &lt;em&gt;Sankhyā: The Indian Journal of Statistics&lt;/em&gt;, 145–52.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wilks1932moments&#34;&gt;
&lt;p&gt;Wilks, Samuel S. 1932. “Moments and Distributions of Estimates of Population Parameters from Fragmentary Samples.” &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt; 3 (3): 163–95.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Iterative Simulation Methods</title>
      <link>/missmethods/bayesian-methods/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/bayesian-methods/</guid>
      <description>


&lt;p&gt;A useful alternative approach to &lt;em&gt;Maximum Likelihood&lt;/em&gt;(ML) methods, particularly when the sample size is small, is to include a reasonable prior distribution for the parameters and compute the posterior distribution of the parameters of interest. The posterior distribution for a model with ignorable missingness is&lt;/p&gt;
&lt;p&gt;\[
p(\theta \mid Y_0, M) \equiv p(\theta \mid Y_0) \propto p(\theta)f(Y_0 \mid \theta),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; is the prior and &lt;span class=&#34;math inline&#34;&gt;\(f(Y_0 \mid \theta)\)&lt;/span&gt; is the density of the observed data &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt;. Simulation from the posterior without iteration can be accomplished if the likelihood can be factored into complete data components, while for general patterns of missing data, Bayesian simulation requires iteration.&lt;/p&gt;
&lt;div id=&#34;data-augmentation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Augmentation&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Data Augmentation&lt;/em&gt;(&lt;span class=&#34;citation&#34;&gt;Tanner and Wong (1987)&lt;/span&gt;), or DA, is an iterative method of simulating the posteiror distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that combines features of the &lt;em&gt;Expecation Maximisation&lt;/em&gt;(EM) algorithm and &lt;em&gt;Multiple Imputation&lt;/em&gt;(MI). Starting with an initial draw &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; from an approximation to the posterior, then given the value &lt;span class=&#34;math inline&#34;&gt;\(\theta_t\)&lt;/span&gt; at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Draw &lt;span class=&#34;math inline&#34;&gt;\(Y_{1,t+1}\)&lt;/span&gt; with density &lt;span class=&#34;math inline&#34;&gt;\(p(Y_1 \mid Y_0, \theta_t)\)&lt;/span&gt; (I step).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Draw &lt;span class=&#34;math inline&#34;&gt;\(\theta_{t+1}\)&lt;/span&gt; with density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid Y_0, Y_{1,t+1})\)&lt;/span&gt; (P step).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The procedure is motivated by the fact that the distributions in these two steps are often much easier to draw from than either of the posteriors &lt;span class=&#34;math inline&#34;&gt;\(p(Y_1 \mid Y_0)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid Y_0)\)&lt;/span&gt;, or the joint posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\theta, Y_1 \mid Y_0)\)&lt;/span&gt;. The procedure can be shown to eventually yield a draw from the joint posterior of &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt;, in the sense that as &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; tends to infinity this sequence converges to a draw from the joint distribution.&lt;/p&gt;
&lt;div id=&#34;bivariate-normal-data-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bivariate Normal Data Example&lt;/h3&gt;
&lt;p&gt;Suppose having a sample &lt;span class=&#34;math inline&#34;&gt;\(y_i=(y_{1i},y_{2i})\)&lt;/span&gt; from a Bivariate Normal distribution for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; units, with mean vector &lt;span class=&#34;math inline&#34;&gt;\(\mu=(\mu_1,\mu_2)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(2\times2\)&lt;/span&gt; covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;. Assume that one group of units has &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; observed and &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt; missing, while a second group of units has both variables observed and a third group of units has &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; missing and &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt; observed. Under DA methods, each iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; consists of an I step and a P step. In the first, missing data are replaced with draws from its conditional distribution given the observed data and current values of the parameters (rather then its conditional mean as in the EM algorithm). Because units are conditionally independent given the parameters, each missing &lt;span class=&#34;math inline&#34;&gt;\(y_{2i}\)&lt;/span&gt; is drawn independently as&lt;/p&gt;
&lt;p&gt;\[
y_{2i,t+1} \sim N\left(\beta_{20t} + \beta_{21t}y_{1i}, \sigma^2_{2t} \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_{20t},\beta_{21t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{2t}\)&lt;/span&gt; are the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-th iterates of the regression parameters of &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt;. Analogously, each missing &lt;span class=&#34;math inline&#34;&gt;\(y_{1i}\)&lt;/span&gt; is drawn independently as&lt;/p&gt;
&lt;p&gt;\[
y_{1i,t+1} \sim N\left(\beta_{10t} + \beta_{11t}y_{2i}, \sigma^2_{1t} \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_{10t},\beta_{11t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{1t}\)&lt;/span&gt; are the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-th iterates of the regression parameters of &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt;. In the second step, these drawn values are treated as if they were the observed values and one draw of the bivariate Normal parameters is made from the complete data posterior. In the limit, the draws are from the joint posterior of the missing values and the parameters. Thus, a run of DA generates both a draw from the posterior predictive distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; and a draw from the posterior of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, and the procedure can be run &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; times to obtain &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; iid draws from the joint posterior of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt;. Unlike the EM, estimates of the sampling covariance matrix from the filled-in data can be computed without any corrections to the estimated variances because draws from the posterior predictive distribution of the missing values are imputed in the I step of DA, rather than the conditional means as in the E step of EM. The loss of efficiency from imputing draws is limited when the posterior mean from DA is computed over many draws from the posterior.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-gibbs-sampler&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Gibbs’ Sampler&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;Gibbs’s sampler&lt;/em&gt; is an iterative simulation method that is designed to yield draws from the joint posterior distribution in the case of a general pattern of missingness and provides a Bayesian analogous to the &lt;em&gt;Expectation Conditonal Maximisation &lt;/em&gt;(ECM) algorithm for ML estimation. The Gibbs’ sampler eventually generates a draw from the distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x_1,\ldots,x_J)\)&lt;/span&gt; of a set of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; random variables &lt;span class=&#34;math inline&#34;&gt;\(X_1,\ldots,X_J\)&lt;/span&gt; in settings where draws from the joint distribution are hard to compute but draws from the conditional distributions &lt;span class=&#34;math inline&#34;&gt;\(p(x_j \mid x_1,\ldots,x_{j-1},x_{j+1},\ldots, x_J)\)&lt;/span&gt; are relatively easy to compute. Initial values &lt;span class=&#34;math inline&#34;&gt;\(x_{10},\ldots,x_{J0}\)&lt;/span&gt; are chosen in some way and then, given current values of &lt;span class=&#34;math inline&#34;&gt;\(x_{1t},\ldots,x_{Jt}\)&lt;/span&gt; at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, new values are found by drawing from the following sequence of conditional distributions:&lt;/p&gt;
&lt;p&gt;\[
x_{1t+1} \sim p\left(x_1 \mid x_{2t},\ldots,x_{Jt} \right),
\]&lt;/p&gt;
&lt;p&gt;\[
x_{2t+1} \sim p\left(x_2 \mid x_{1t+1},\ldots,x_{Jt} \right),
\]&lt;/p&gt;
&lt;p&gt;up to&lt;/p&gt;
&lt;p&gt;\[
x_{Jt+1} \sim p\left(x_J \mid x_{2t+1},\ldots,x_{J-1t+1} \right).
\]&lt;/p&gt;
&lt;p&gt;It can be shown that, under general conditions, the sequence of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; iterates converges to a draw from the joint posterior of the variables. When &lt;span class=&#34;math inline&#34;&gt;\(J=2\)&lt;/span&gt;, the Gibbs’ sampler is the same as DA if &lt;span class=&#34;math inline&#34;&gt;\(x_1=Y_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2=\theta\)&lt;/span&gt; and the distributions condition on &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt;. We can then obtain a draw from the joint posterior of &lt;span class=&#34;math inline&#34;&gt;\(Y_1,\theta \mid Y_0\)&lt;/span&gt; by applying the Gibbs’ sampler, where at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-th imputed data set:&lt;/p&gt;
&lt;p&gt;\[
Y^d_{1t+1} \sim p\left(Y_1 \mid Y_0, \theta^d_{t}\right) \;\;\; \text{and} \;\;\; \theta^d_{t+1} \sim p\left(\theta \mid Y^d_{1t+1}, Y_0\right),
\]&lt;/p&gt;
&lt;p&gt;such that one run of the sampler converges to a draw from the posterior predictive distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; and a draw from the posterior of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. The sampler can be run independently &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; times to generate &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; iid draws from the approximate joint posterior of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt;. The values of &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; are multiple imputations of the missing values, drawn from their posterior predictive distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assessing-convergence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assessing Convergence&lt;/h2&gt;
&lt;p&gt;Assessing convergence of the sequence of draws to the target distribution is more difficult than assessing convergence of an EM-type algorithm because there is no single target quantity to monitor like the maximum value of the likelihood. Methods have been proposed to assess convergence of a single sequence (&lt;span class=&#34;citation&#34;&gt;Geyer (1992)&lt;/span&gt;), but a more reliable approach is to simulate &lt;span class=&#34;math inline&#34;&gt;\(D&amp;gt;1\)&lt;/span&gt; sequences with starting values dispersed throughout the parameter space, and the convergence of all quantities of interest can then be monitored by comparing variation between and within simulated sequences, until the “within” variation roughly equals the “between” variation. The idea is that when the distribution of each simulated sequence is close enough to the distribution of all the sequences mixed together, they can all be approximating the target distribution. &lt;span class=&#34;citation&#34;&gt;Gelman and Rubin (1992)&lt;/span&gt; developed an explicit monitoring statistic based on the following idea. For each scalar estimand &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;, label the draws from &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; parallel sequences as &lt;span class=&#34;math inline&#34;&gt;\(\psi^d_{t}\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(t=1,\ldots,T\)&lt;/span&gt; iterations and &lt;span class=&#34;math inline&#34;&gt;\(d=1,\ldots,D\)&lt;/span&gt; sequences, and compute the between &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and within &lt;span class=&#34;math inline&#34;&gt;\(\bar{V}\)&lt;/span&gt; sequence variances as:&lt;/p&gt;
&lt;p&gt;\[
B=\frac{T}{D-1}\sum_{d=1}^D(\bar{\psi}_{d.} - \bar{\psi}_{..})^2, \;\;\; \text{and} \;\;\; \bar{V}=\frac{1}{D}\sum_{d=1}^D s^2_{d},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\bar{\psi}_{d.}=\frac{1}{T}\sum_{t=1}^T \psi_{dt}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bar{\psi}_{..}=\frac{1}{D}\sum_{d=1}^D \bar{\psi}_{d}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(s^2_{d}=\frac{1}{T-1}\sum_{t=1}^T(\psi_{dt} - \bar{\psi}_{d.})^2\)&lt;/span&gt;. We can then estimate the marginal posterior variance of the estimand as&lt;/p&gt;
&lt;p&gt;\[
\widehat{Var}(\psi \mid Y_0) = \frac{T-1}{T}\hat{V} + \frac{1}{T} B,
\]&lt;/p&gt;
&lt;p&gt;which will &lt;em&gt;overestimate&lt;/em&gt; the marginal posterior variance assuming the starting distribution is appropriately over-dispersed but is &lt;em&gt;unbiased&lt;/em&gt; under stationarity (starting distribution equals the target distribution). For any finte &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, the within variance &lt;span class=&#34;math inline&#34;&gt;\(\hat{V}\)&lt;/span&gt; will &lt;em&gt;underestimate&lt;/em&gt; the marginal variance because individual sequences have not had time to range over all the target distribution and should have smaller variance then B. In the limit as &lt;span class=&#34;math inline&#34;&gt;\(T \rightarrow \infty\)&lt;/span&gt; the expecation of &lt;span class=&#34;math inline&#34;&gt;\(\hat{V}\)&lt;/span&gt; approaches the marginal variance. These facts suggest monitoring convergence by estimating the factor by which the scale of the current distribution for &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; might be reduced if the simulations were continued. This is the &lt;em&gt;potential scale reduction factor&lt;/em&gt; and is estimated by&lt;/p&gt;
&lt;p&gt;\[
\sqrt{\hat{R}} = \sqrt{\frac{\widehat{Var}(\psi \mid Y_0)}{\hat{V}}},
\]&lt;/p&gt;
&lt;p&gt;which declines to 1 as &lt;span class=&#34;math inline&#34;&gt;\(T \rightarrow \infty\)&lt;/span&gt;. When this quantity is high, there is evidence to proceed the simulations further to improve our inference about the target distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-simulation-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other Simulation Methods&lt;/h2&gt;
&lt;p&gt;When draws from the sequence of conditional distributions forming the Gibbs’ sampler are not easy to obtain, other simulation approaches are needed. Among these there are the &lt;em&gt;Sequential Imputation&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Kong, Liu, and Wong (1994)&lt;/span&gt;), &lt;em&gt;Sampling Imprtance Resampling&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Gelfand and Smith (1990)&lt;/span&gt;), &lt;em&gt;Rejection Sampling&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Von Neumann and others (1951)&lt;/span&gt;). One of these alternatives are the &lt;em&gt;Metropolis-Hastings&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Metropolis et al. (1953)&lt;/span&gt;) algorithms, of which the Gibbs’ sampler is a particular case, which constitute the so-called &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; (MCMC) algorithms as the sequence of iterates forms a Markov Chain (&lt;span class=&#34;citation&#34;&gt;Gelman et al. (2013)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-gelfand1990sampling&#34;&gt;
&lt;p&gt;Gelfand, Alan E, and Adrian FM Smith. 1990. “Sampling-Based Approaches to Calculating Marginal Densities.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 85 (410): 398–409.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34;&gt;
&lt;p&gt;Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. &lt;em&gt;Bayesian Data Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman1992single&#34;&gt;
&lt;p&gt;Gelman, Andrew, and Donald B Rubin. 1992. “A Single Series from the Gibbs Sampler Provides a False Sense of Security.” &lt;em&gt;Bayesian Statistics&lt;/em&gt; 4: 625–31.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-geyer1992practical&#34;&gt;
&lt;p&gt;Geyer, Charles J. 1992. “Practical Markov Chain Monte Carlo.” &lt;em&gt;Statistical Science&lt;/em&gt;, 473–83.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kong1994sequential&#34;&gt;
&lt;p&gt;Kong, Augustine, Jun S Liu, and Wing Hung Wong. 1994. “Sequential Imputations and Bayesian Missing Data Problems.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 89 (425): 278–88.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-metropolis1953equation&#34;&gt;
&lt;p&gt;Metropolis, Nicholas, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. 1953. “Equation of State Calculations by Fast Computing Machines.” &lt;em&gt;The Journal of Chemical Physics&lt;/em&gt; 21 (6): 1087–92.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tanner1987calculation&#34;&gt;
&lt;p&gt;Tanner, Martin A, and Wing Hung Wong. 1987. “The Calculation of Posterior Distributions by Data Augmentation.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 82 (398): 528–40.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-von1951general&#34;&gt;
&lt;p&gt;Von Neumann, John, and others. 1951. “The General and Logical Theory of Automata.” &lt;em&gt;1951&lt;/em&gt;, 1–41.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Methods for Health Technology Assessment</title>
      <link>/project/health-economics/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/health-economics/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;The type of data used in economic evaluations typically come from a range of sources, whose evidence is combined to inform HTA decision-making. Traditionally, relative effectiveness data are derived from &lt;em&gt;randomised controlled clinical trials&lt;/em&gt; (RCTs), while healthcare resource utilisation, costs and preference-based quality of life data may come from the same study that estimated the clinical effectiveness or not. A number of HTA agencies have developed their own methodological guidelines to support the generation of the evidence required to inform their decisions. In this context, the primary role of economic evaluation for HTA is not the estimation of the quantities of interest (e.g. the computation of point or interval estimation, or hypothesis testing), but to aid decision making. The implication of this is that the standard frequentist analyses that rely on power calculations and $P$-values to estimate  statistical and clinical significance, typically used in RCTs, are not well-suited for addressing these HTA requirements.&lt;/p&gt;

&lt;p&gt;It has been argued that, to be consistent with its intended role in HTA, economic evaluation should embrace a decision-theoretic paradigm and develop ideally within a Bayesian statistical framework to inform two decisions&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;whether the treatments under evaluation are cost-effective given the available evidence and&lt;/li&gt;
&lt;li&gt;whether the level of uncertainty surrounding the decision is acceptable (i.e. the potential benefits are worth the costs of making the wrong decision).&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This corresponds to quantify the impact of the uncertainty in the evidence on the entire decision-making process (e.g. to what extent the uncertainty in the estimation of the effectiveness of a new intervention affects the decision about whether it is paid for by the public provider).&lt;/p&gt;

&lt;h1 id=&#34;bayesian-methods-in-hta&#34;&gt;Bayesian methods in HTA&lt;/h1&gt;

&lt;p&gt;There are several reasons that make the use of Bayesian methods in economic evaluations particularly appealing. First, Bayesian modelling is naturally embedded in the wider scheme of decision theory; by taking a probabilistic approach, based on decision rules and available information, it is possible to explicitly account for relevant sources of uncertainty in the decision process and obtain an &lt;em&gt;optimal&lt;/em&gt; course of action. Second, Bayesian methods allow extreme flexibility in modelling using computational algorithms such as &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; (MCMC) methods; this allows to handle in a relatively easy way the generally sophisticated structure of the relationships and complexities that characterise effectiveness, quality of life and cost data. Third, through the use of prior distributions, the Bayesian approach naturally allows the incorporation of evidence from different sources in the analysis (e.g. expert opinion or multiple studies), which may improve the estimation of the quantities of interest; the process is generally referred to as evidence synthesis and finds its most common application in the use of meta-analytic tools. This may be extremely important when, as it often happens, there is only some partial (imperfect) information to identify the model parameters. In this case analysts are required to develop chain-of-evidence models. When required by the limitations in the evidence base, subjective prior distributions can be specified based on the synthesis and elicitation of expert opinion to identify the model, and their impact on the results can be assessed by presenting or combining the results across a range of plausible alternatives. Finally, under a Bayesian approach, it is straightforward to conduct &lt;em&gt;sensitivity analysis&lt;/em&gt; to properly account for the impact of uncertainty in all inputs of the decision process; this is a required component in the approval or reimbursement of a new intervention for many decision-making bodies, such as NICE in the UK.&lt;/p&gt;

&lt;p&gt;The general process of conducting a Bayesian analysis (with a view of using the results of the model to perform an economic evaluation) can be broken down in several steps, which are graphically summarized in the Figure below.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/HTA.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Process of health economic evaluation.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;The starting point is the identification of the decision problem, which defines the objective of the economic evaluation (e.g. the interventions being compared, the target population, the relevant time horizon). In line with the decision problem, a statistical model is constructed to describe the (by necessity, limited) knowledge of the underlying clinical pathways. This implies, for example, the definition of suitable models to describe variability in potentially observed data (e.g. the number of patients recovering from the disease because of a given treatment), as well as the epistemic uncertainty in the population parameters (e.g.~the underlying probability that a random individual in the target population is cured, if given the treatment under study).  At this point, all the relevant data are identified, collected and quantitatively sytnthesised to derive the estimates of the input parameters of interest for the model.&lt;/p&gt;

&lt;p&gt;These parameter estimates (and associated uncertainties) are then fed to the economic model, with the objective of obtaining some relevant summaries indicating the benefits and costs for each intervention under evaluation. Uncertainty analysis represents some sort of &lt;em&gt;detour&lt;/em&gt; from the straight path going from the statistical model to the decision analysis: if the output of the statistical model allowed us to know with perfect certainty the &lt;em&gt;true&lt;/em&gt; value of the model parameters, then it would be possible to simply run the decision analysis and make the decision.  Of course, even if the statistical model were the &lt;em&gt;true&lt;/em&gt; representation of the underlying data generating process (which it most certainly is not), because the data may be limited in terms of length of follow up, or sample size, the uncertainty in the value of the model parameters would still remain. This &lt;em&gt;parameter&lt;/em&gt; (and &lt;em&gt;structural&lt;/em&gt;) uncertainty is propagated throughout the whole process to evaluate its impact on the decision-making. In some cases, although there might be substantial uncertainty in the model inputs, this may not turn out to modify substantially the output of the decision analysis, i.e. the new treatment would be deemed as optimal irrespectively.  In other cases, however, even a small amount of uncertainty in the inputs could be associated with very serious consequences. In such circumstances, the decision-maker may conclude that the availbale evidence is not sufficient to decide on which intervention to select and require more information before a decision can be made.&lt;/p&gt;

&lt;p&gt;The results of the above analysis can be used to inform policy makers about two related decisions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;whether the new intervention is to be considered (on average) &lt;em&gt;value for money&lt;/em&gt;, given the evidence base available at the time of decision, and&lt;/li&gt;
&lt;li&gt;whether the consequences (in terms of net health loss) of making the wrong decision would warrant further research to reduce this &lt;em&gt;decision uncertaint&lt;/em&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the type and specification of the statistical and economic models vary with the nature of the underlying data (e.g. individual (ILD) level versus aggregated (ALD) data, the decision and uncertainty analyses have a more standardised set up.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;HTA has been slow to adopt Bayesian methods; this could be due to a reluctance to use prior opinions, unfamiliarity, mathematical complexity, lack of software, or conservatism of the healthcare establishment and, in particular, the regulatory~authorities. However, the use of Bayesian approach has been increasingly advocated as an efficient tool to integrate statistical evidence synthesis and parameter estimation with probabilistic decision analysis in an unified framework for HTA. This enables a transparent &lt;em&gt;evidence-based&lt;/em&gt; decision modelling, reflecting the uncertainty and the structural relationships in all the available~data.&lt;/p&gt;

&lt;p&gt;With respect to trial-based analyses, the flexibility and modularity of the Bayesian modelling structure are well-suited to jointly account for the typical complexities that affect ILD. In addition, prior distributions can be used as convenient means to incorporate external information into the model when the evidence from the data is limited or absent (e.g. for missing values). In the context of evidence synthesis, the Bayesian approach is particularly appealing in that it allows for all the uncertainty and correlation induced by the often heterogeneous nature of the evidence (either ALD only or both ALD and ILD) to be synthesised in a way that can be easily integrated within a decision modelling framework.&lt;/p&gt;

&lt;p&gt;The availability and spread of Bayesian software among practitioners since the late 1990s, such as &lt;code&gt;OpenBUGS&lt;/code&gt; or &lt;code&gt;JAGS&lt;/code&gt;, has greatly improved the applicability and reduced the computational costs of these models. Thus, analysts are provided with a powerful framework, which has been termed &lt;em&gt;comprehensive decision modelling&lt;/em&gt;, for simultaneously estimating posterior distributions for parameters based on specified prior knowledge and data evidence, and for translating this into the ultimate measures used in the decision analysis to inform cost-effectiveness conclusions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Modelling for Health Economic Evaluations</title>
      <link>/project/bayesian-modelling/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/bayesian-modelling/</guid>
      <description>

&lt;h1 id=&#34;modelling-framework&#34;&gt;Modelling Framework&lt;/h1&gt;

&lt;p&gt;We propose a unified Bayesian framework that jointly accounts for the typical complexities of the data (e.g. correlation, skewness, spikes at the boundaries  and missingness), and that can be implemented in a relatively easy way.&lt;/p&gt;

&lt;p&gt;Consider the usual cross-sectional bivariate outcome formed by the QALYs and total cost variables $(e_{it}, c_{it})$ calculated for the $i-$th person in group $t$ of the trial. To simplify the notation, unless necessary, we suppress the treatment indicator $t$.
We specify the joint distribution $p(e_i,c_i)$ as&lt;/p&gt;

&lt;p&gt;\[
p(e_i,c_i) = p(c_i)p(e_i\mid c_i) = p(e_i)p(c_i\mid e_i)
\]&lt;/p&gt;

&lt;p&gt;where, for example, $p(e_i)$ is the &lt;em&gt;marginal&lt;/em&gt; distribution of the QALYs and $p(c_i\mid e_i)$ is the &lt;em&gt;conditional&lt;/em&gt; distribution of the costs given the QALYs. Note that, although the two factorisations are mathematically equivalent, the choice of which to use has different practical implications. From a statistical point of view, the factorisations require the specifications of different statistical models, e.g. $p(e_i)$ or $p(e_i\mid c_i)$, which may have different approximation errors. From a clinical point of view, the two versions make different assumptions about the casual relationships between the outcomes, i.e. either $e_i$ determines $c_i$ or vice versa. We describe our analysis under the assumption that the costs are determined by the effectiveness measures and therefore we specify the joint distribution $p(e_i,c_i)$ in terms of a marginal distribution for the QALYs and a conditional distribution for the costs.&lt;/p&gt;

&lt;p&gt;For each individual we consider a marginal distribution $p(e_i \mid \boldsymbol \theta_e)$ indexed by a set of parameters $\boldsymbol \theta_e$ comprising a &lt;em&gt;location&lt;/em&gt; $\boldsymbol \phi_{ie}$ and a set of &lt;em&gt;ancillary&lt;/em&gt; parameters $\boldsymbol\psi_e$ typically including some measure of &lt;em&gt;marginal&lt;/em&gt; variance $\sigma^2_e$. We can model the location parameter using a generalised linear structure, e.g.&lt;/p&gt;

&lt;p&gt;\[
g_e(\phi_{ie})= \alpha_0 \,\,[+ \ldots]
\]&lt;/p&gt;

&lt;p&gt;where $\alpha_0$ is the intercept and the notation $[+\ldots]$ indicates that other terms (e.g. quantifying the effect of relevant covariates) may or may not be included. In the absence of covariates or assuming that a centered version $x_i^{\star} = (x_i - \bar{x})$ is used, the parameter $\mu_e = g_e^{-1}(\alpha_0)$ represents the population average QALYs. For the costs, we consider a conditional model $p(c_i\mid e_i,\boldsymbol\theta_c)$, which explicitly depends on the QALYs, as well as on a set of quantities $\boldsymbol\theta_c$, again comprising a location $\phi_{ic}$ and ancillary parameters $\boldsymbol \psi_{c}$. For example, when normal distributions are assumed for both $p(e_i \mid \boldsymbol \theta_e)$ and $p(c_i \mid e_i, \boldsymbol \theta_c)$, i.e. bivariate normal on both outcomes, the ancillary parameters $\boldsymbol\psi_c$ include a &lt;em&gt;conditional&lt;/em&gt; variance $\tau^2_c$, which can be expressed as a function of the marginal variance $\sigma^2_c$. More specifically, the conditional variance of $p(c_i \mid e_i, \boldsymbol \theta_c)$ is a function of the marginal effectiveness and cost variances and has the closed form $\tau^2_c=\sigma^2_c - \sigma^2_e \beta^2$, where $\beta=\rho \frac{\sigma_c}{\sigma_e}$ and $\rho$ is the parameter capturing the correlation between the variables.&lt;/p&gt;

&lt;p&gt;The location can be modelled as a function of the QALYs as&lt;/p&gt;

&lt;p&gt;\[
g_c(\phi_{ic}) = \beta_{0} + \beta_{1}(e_{i}-\mu_{e})\,\,[+\ldots]
\]&lt;/p&gt;

&lt;p&gt;Here, $(e_i-\mu_e)$ is the centered version of the QALYs, while $\beta_{1}$ quantifies the correlation between costs and QALYs. Assuming other covariates are either also centered or absent, $\mu_c = g_c^{-1}(\beta_{0})$ is the estimated population average cost. The Figure below shows a graphical representation of the general modelling framework.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/framework.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Modelling framework.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;The QALYs and cost distributions are represented in terms of combined &lt;em&gt;modules&lt;/em&gt;, the blue and the red boxes, in which the random quantities are linked through logical relationships. This ensures the full characterisation of the uncertainty for each variable in the model. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in either or both the modules.&lt;/p&gt;

&lt;p&gt;The proposed framework allows jointly tackling of the different complexities that affect the data in a relatively easy way by means of its modular structure and flexible choice for the distributions of the QALYs and cost variables. Using the MenSS trial as motivating example, we start from the original analysis and expand the model using alternative specifications that progressively account for an increasing number of complexities in the outcomes. We specifically focus on appropriately modelling spikes at the boundary and missingness, as they have substantial implications in terms of inferences and, crucially, cost-effectiveness results.&lt;/p&gt;

&lt;h1 id=&#34;example&#34;&gt;Example&lt;/h1&gt;

&lt;p&gt;Three model specifications are considered and applied to QALY data from a RCT case study: 1) Normal marginal for the QALYs and Normal conditional for the costs (which is identical to a Bivariate Normal distribution for the two outcomes); 2) Beta marginal for the QALYs and Gamma conditional for the costs; and 3) Hurdle Model. The following Figure shows the observed QALYs in both treatment groups (indicated with black crosses) as well as summaries of the posterior distributions for the imputed values, obtained from each model. Imputations are distinguished based on whether the corresponding baseline utility value is observed or missing (blue or red lines and dots, respectively) and are summarised in terms of posterior mean and $90\%$ HPD intervals.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/imputations.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Imputed QALYs under alternative model specifications.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;There are clear differences in the imputed values and corresponding credible intervals between the three models in both treatment groups. Neither the Bivariate Normal nor the Beta-Gamma models  produce imputed values that capture the structural one component in the data. In addition, as to be expected, the Bivariate Normal fails to respect the natural support for the observed QALYs, with many of the imputations exceeding the unit threshold bound. These unrealistic imputed values highlight the inadequacy of the Normal distribution for the data and may lead to distorted inferences. Conversely, imputations under the Hurdle Model are more realistic, as they can replicate values in the whole range of the observed data, including the structural ones. Imputed unit QALYs with no discernible interval are only observed in the intervention group due to the original data composition, i.e. individuals associated with a unit baseline utility and missing QALYs are almost exclusively present in the intervention group.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;We have presented a flexible Bayesian framework that can handle the typical complexities affecting outcome data in CEA, while also being relatively easy to implement using freely available Bayesian software.  This is a key advantage that can encourage practitioners to move away from likely biased methods and promote the use of our framework in routine analyses. In conclusion, the proposed framework can:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Jointly model costs and QALYs;&lt;/li&gt;
&lt;li&gt;Account for skewness and structural values;&lt;/li&gt;
&lt;li&gt;Assess the robustness of the results under a set of differing missingness assumptions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The original contribution of this work consists in the joint implementation of methods that account for the complexities of the data within a unique and flexible framework that is relatively easy to apply. In the next chapter we will take a step forward in the analysis and present a longitudinal model that can use all observed utility and cost data in the analysis, explore alternative nonignorable missing data assumptions, while simultaneously handling the complexities that affect the data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Complete Case Analysis</title>
      <link>/missmethods/complete-case-analysis/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/complete-case-analysis/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Complete case analysis&lt;/em&gt; (CCA), also known as &lt;em&gt;case&lt;/em&gt; or &lt;em&gt;listwise deletion&lt;/em&gt; (LD), is one of the oldest methods to handle missing data and consists in discarding any unit or case whose information is incomplete. Only the cases with observed values for all the variables under consideration are used in the analysis. For example, suppose we have a data set formed by &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; individuals and that we want to fit a linear regression on some outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; using some other variables &lt;span class=&#34;math inline&#34;&gt;\(x_{i1},\ldots,x_{ik}\)&lt;/span&gt; as covariates. CCA uses only the subset of cases with observed values on all the variables included in the analysis (&lt;em&gt;completers&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;CCA has been a quite popular approach to deal with missingness, mainly because it is very easy to implement (used by default in many statistical programs) and it allows the comparison of different univariate statistics in a straightforward way (calculated on a common set of cases). However, there are a number of potential disadvantages which threatens the validity of this method:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Bias, when the missing data mechanism is not &lt;em&gt;missing completely at random&lt;/em&gt; (MCAR) and the completers are not a random samples of all the cases&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Loss of efficiency, due to the potential loss of information in discarding the incomplete cases.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;CCA may be justified when the loss of precision and bias are minimal, which is more likley to occur when the proportion of completers is high, although it is difficult to formulate rules that apply in general circumstances. Indeed, both the degree of loss of precision and bias depend not only on the fraction of completers and missingness patterns, but also on the extent to which complete and incomplete cases differ and the parameters of interest.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_{cc}\)&lt;/span&gt; be an estimate of a parameter of interest from the completers. One might measure the increase in variance of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_{cc}\)&lt;/span&gt; with respect to the estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; that would be obtained in the absence of missing values. Using the notation of &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;\[
\text{Var}(\hat{\theta}_{cc}) = \text{Var}(\hat{\theta})(1 + \Delta^{\star}_{cc}),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Delta^{\star}_{cc}\)&lt;/span&gt; is the proportional increase in variance from the loss of information. A more practical measure of the loss of inofrmation is &lt;span class=&#34;math inline&#34;&gt;\(\Delta_{cc}\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;\[
\text{Var}(\hat{\theta}_{cc}) = \text{Var}(\hat{\theta}_{eff})(1 + \Delta_{cc}),
\]&lt;/p&gt;
&lt;p&gt;and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_{eff}\)&lt;/span&gt; is an efficient estimate based on all the available data.&lt;/p&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;p&gt;Consider bivariate normal monotone data &lt;span class=&#34;math inline&#34;&gt;\(\bf y = (y_1,y_2)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(n_{cc}\)&lt;/span&gt; out of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; cases are complete and &lt;span class=&#34;math inline&#34;&gt;\(n - n_{cc}\)&lt;/span&gt; cases have observed values only on &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;. Assume for simplicity that the missingness mechanism is MCAR and that the mean of &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; is estimated by the empirical mean from the complete cases &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{cc}_j\)&lt;/span&gt;. Then, the loss in sample size for estimating the mean of &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;\[
\Delta_{cc}(\bar{y}_1) = \frac{n - n_{cc}}{n_{cc}},
\]&lt;/p&gt;
&lt;p&gt;so that if half the cases are missing, the variance is doubled. For the mean of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt;, the loss of information alos depends on the squared correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}\)&lt;/span&gt; between the variables: (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;\[
\Delta_{cc}(\bar{y}_2) \approx \frac{(n - n_{cc})\rho^{2}}{n_{cc}(1 - \rho^{2}) + n_{cc}\rho^{2}}.
\]&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Delta_{cc}(\bar{y}_2)\)&lt;/span&gt; varies from zero (when &lt;span class=&#34;math inline&#34;&gt;\(\rho=0\)&lt;/span&gt;) to &lt;span class=&#34;math inline&#34;&gt;\(\Delta_{cc}(\bar{y}_1)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2} \rightarrow 1\)&lt;/span&gt;. However, for the regression coefficients of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; we have that &lt;span class=&#34;math inline&#34;&gt;\(\Delta_{cc}=0\)&lt;/span&gt; since the incomplete observations of &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; provide no information for estimating the parameters of the regression of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2&lt;/h2&gt;
&lt;p&gt;For inference about the population mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, the bias of CCA depends on the proportion of the completers &lt;span class=&#34;math inline&#34;&gt;\(\pi_{cc}\)&lt;/span&gt; and the extent to which complete and incomplete cases differ on the variables of interest. Suppose a variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is partially-observed and that we partition the data into the subset of the completers &lt;span class=&#34;math inline&#34;&gt;\(y_{cc}\)&lt;/span&gt; and incompleters &lt;span class=&#34;math inline&#34;&gt;\(y_{ic}\)&lt;/span&gt;, with associated population means &lt;span class=&#34;math inline&#34;&gt;\(\mu_{cc}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ic}\)&lt;/span&gt;, respectively. The overall mean can be written as a weighted average of the means of the two subsets&lt;/p&gt;
&lt;p&gt;\[
\mu = \pi_{cc}\mu_{cc} + (1 - \pi_{cc})\mu_{ic}.
\]&lt;/p&gt;
&lt;p&gt;The bias of CCA is then equal to the expected fraction of incomplete cases multiplied by the differences in the means for complete and incomplete cases&lt;/p&gt;
&lt;p&gt;\[
\mu_{cc} - \mu = (1 - \pi_{cc})(\mu_{cc} - \mu_{ic}).&lt;br /&gt;
\]&lt;/p&gt;
&lt;p&gt;Under MCAR, we have that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{cc} = \mu_{ic}\)&lt;/span&gt; and therefore the bias is zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 3&lt;/h2&gt;
&lt;p&gt;Consider the estimation of the regression of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x_1,\ldots,x_K\)&lt;/span&gt; from data with potential missing values on all variables and with the regression function correctly specified. The bias of CCA for estimating the regression coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_1,\ldots,\beta_K\)&lt;/span&gt; associated with the covariates is null if the probbaility of being a completer depends on the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;s but not &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, since the analysis conditions on the values of the covariates (&lt;span class=&#34;citation&#34;&gt;Glynn and Laird (1986)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;White and Carlin (2010)&lt;/span&gt;). This class of missing data mechanisms includes &lt;em&gt;missing not at random&lt;/em&gt; (MNAR), where the probability that a covariate is missing depends on the value of that covariate. However, CCA is biased if the probability of being a completer depends on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; after conditioning on the covariates. A nice example of this particular topic and its implications for the analysis has been provided by professor &lt;a href=&#34;https://thestatsgeek.com/about-thestatsgeek-com/&#34;&gt;Bartlett&lt;/a&gt; using some nice &lt;a href=&#34;http://thestatsgeek.com/wp-content/uploads/2016/08/Jonathan-Bartlett-28-06-2013.pdf&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The main virtue of case deletion is simplicity. If a missing data problem can be resolved by discarding only a small part of the sample, then the method can be quite effective. However, even in that situation, one should explore the data (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;). The discarded information from incomplete cases can be used to study whether the complete cases are plausibly a random subsample of the original sample, that is, whether MCAR is a reasonable assumption. A simple procedure is to compare the distribution of a particular variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; based on complete cases with the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; based on incomplete cases for which &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is recorded. Significant differences indicate that the MCAR assumption is invalid, and the complete-case analysis yields potentially biased estimates. Such tests are useful but have limited power when the sample of incomplete cases is small. Also the tests can offer no direct evidence on the validity of the &lt;em&gt;missing at random&lt;/em&gt; (MAR) assumption.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-glynn1986regression&#34;&gt;
&lt;p&gt;Glynn, RJ, and NM Laird. 1986. “Regression Estimates and Missing Data: Complete Case Analysis.” &lt;em&gt;Cambridge MA: Harvard School of Public Health, Department of Biostatistics&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-white2010bias&#34;&gt;
&lt;p&gt;White, Ian R, and John B Carlin. 2010. “Bias and Efficiency of Multiple Imputation Compared with Complete-Case Analysis for Missing Covariate Values.” &lt;em&gt;Statistics in Medicine&lt;/em&gt; 29 (28): 2920–31.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Expectation Maximisation Algorithm</title>
      <link>/missmethods/em-algorithm/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/em-algorithm/</guid>
      <description>


&lt;p&gt;Patterns of incomplete data in practice often do not have the forms that allow explicit &lt;em&gt;Maximum Likelihood&lt;/em&gt;(ML) estimates to be calculated. Suppose we have a model for the complete data &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, with density &lt;span class=&#34;math inline&#34;&gt;\(f(Y\mid \theta)\)&lt;/span&gt;, indexed by the set of unknown parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Writing &lt;span class=&#34;math inline&#34;&gt;\(Y=(Y_0,Y_1)\)&lt;/span&gt; in terms of the observed &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt; and missing &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; components, and assuming that the missingness mechanism is &lt;em&gt;Missing At Random&lt;/em&gt;(MAR), we want to maximise the likelihood&lt;/p&gt;
&lt;p&gt;\[
L\left(\theta \mid Y_0 \right) = \int f\left(Y_0, Y_1 \mid \theta \right)dY_1
\]&lt;/p&gt;
&lt;p&gt;with respect to &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. When the likelihood is differentiable and unimodal, ML estimates can be found by solving the likelihood equation&lt;/p&gt;
&lt;p&gt;\[
D_l\left(\theta \mid Y_0 \right) \equiv \frac{\partial ln L\left(\theta \mid Y_0 \right)}{\partial \theta} = 0,
\]&lt;/p&gt;
&lt;p&gt;while, if a closed-form solution cannot be found, iterative methods can be applied. One of these methods is the popular &lt;em&gt;Expectation Maximisation&lt;/em&gt;(EM) algorithm (&lt;span class=&#34;citation&#34;&gt;Dempster, Laird, and Rubin (1977)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The EM algorithm is a general iterative method for ML estimation in incomplete data problems. The basic idea behind it is based on a sequence of steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Replace missing values by estimated values&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Estimate the parameters&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Re-estimate the missing values assuming the new parameter estimates are correct&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Re-estimate parameters&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The procedure is then iterated until apparent convergence. Each iteration of EM consists of an &lt;em&gt;expectation step&lt;/em&gt; (E step) and a &lt;em&gt;maximisation step&lt;/em&gt; (M step) which ensure that, under general conditions, each iteration increases the loglikelihood &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid Y_0)\)&lt;/span&gt;. In addition, if the loglikelihood is bounded, the sequence &lt;span class=&#34;math inline&#34;&gt;\(\{l(\theta_t \mid Y_0), t=(0,1,\ldots)\}\)&lt;/span&gt; converges to a stationary value of &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid Y_0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;the-e-step-and-the-m-step&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The E step and the M step&lt;/h2&gt;
&lt;p&gt;The M step simply consists of performing ML estimation of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; as if there were no missing data, that is, after they had been filled in. The E step finds the conditional expectation of the missing values given the observed data and current estimated parameters. In practice, EM does not necessarily substitute the missing values themselves but its key idea is that they are generally not &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt; but the functions of &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt; appearing in the complete data loglikelihood &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid Y)\)&lt;/span&gt;. Specifically, let &lt;span class=&#34;math inline&#34;&gt;\(\theta_t\)&lt;/span&gt; be the current estimate of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, then the E step finds the expected complete data loglikelihood if &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; were &lt;span class=&#34;math inline&#34;&gt;\(\theta_t\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;\[
Q\left(\theta \mid \theta_t \right) = \int l\left(\theta \mid Y \right)f\left(Y_0 \mid Y_1 , \theta = \theta_t \right)dY_0.
\]&lt;/p&gt;
&lt;p&gt;The M step determines &lt;span class=&#34;math inline&#34;&gt;\(\theta_{t+1}\)&lt;/span&gt; by maximising this expected complete data loglikelihood:&lt;/p&gt;
&lt;p&gt;\[
Q\left(\theta_{t+1} \mid \theta_t \right) \geq Q\left(\theta \mid \theta_t \right),
\]&lt;/p&gt;
&lt;p&gt;for all &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;univariate-normal-data-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Normal Data Example&lt;/h3&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; form a an iid sample from a Normal distribution with population mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n_{cc}\)&lt;/span&gt; observed units and &lt;span class=&#34;math inline&#34;&gt;\(i=n_{cc}+1,\ldots,n\)&lt;/span&gt; missing units. Under the assumption that the missingness mechanism is ignorable, the expectation of each missing &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\sigma^2)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. Since the loglikelihood based on all &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is linear in the sufficient statistics &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^n y_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^n y^2_i\)&lt;/span&gt;, the E step of the algorithm calculates&lt;/p&gt;
&lt;p&gt;\[
E\left(\sum_{i=1}^{n}y_i \mid \theta_t, Y_0 \right) = \sum_{i=1}^{n_{cc}}y_i + (n-n_{cc})\mu_t
\]&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;\[
E\left(\sum_{i=1}^{n}y^2_i \mid \theta_t, Y_0 \right) = \sum_{i=1}^{n_{cc}}y^2_i + (n-n_{cc})\left(\mu^2_t + \sigma^2_t \right)
\]&lt;/p&gt;
&lt;p&gt;for current estimates &lt;span class=&#34;math inline&#34;&gt;\(\theta_t=(\mu_t,\sigma_t)\)&lt;/span&gt; of the parameters. Note that simply substituting &lt;span class=&#34;math inline&#34;&gt;\(\mu_t\)&lt;/span&gt; for the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{n_{cc}+1},\ldots,y_n\)&lt;/span&gt; is not correct since the term &lt;span class=&#34;math inline&#34;&gt;\((n-n_{cc})(\sigma_t^2)\)&lt;/span&gt; is omitted. Without missing data, the ML estimate of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sum_{i=1}^ny_i}{n}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sum_{i=1}^ny^2_i}{n}-\left(\frac{\sum_{i=1}^ny_i}{n}\right)^2\)&lt;/span&gt;, respectively. The M step uses the same expressions based on the current expectations of the sufficient statistics calculated in the E step. Thus, the M step calculates&lt;/p&gt;
&lt;p&gt;\[
\mu_{t+1} = \frac{E\left(\sum_{i=1}^n y_i \mid \theta_t, Y_0 \right)}{n}
\]&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;\[
\sigma^2_{t+1} = \frac{E\left(\sum_{i=1}^n y^2_i \mid \theta_t, Y_0 \right)}{n} - \mu^2_{t+1}.
\]&lt;/p&gt;
&lt;p&gt;Setting &lt;span class=&#34;math inline&#34;&gt;\(\mu_t=\mu_{t+1}=\hat{\mu}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_t=\sigma_{t+1}=\hat{\sigma}\)&lt;/span&gt; in these equations shows that a fixed point of these iterations is &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=\frac{\sum_{i=1}^{n_{cc}}y_i}{n_{cc}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2=\frac{\sum_{i=1}^{n_{cc}}y^2_i}{n_{cc}} - \hat{\mu}^2\)&lt;/span&gt;, which are the ML estimates of the parameters from &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt; assuming MAR and distinctness of the parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;extensions-of-em&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extensions of EM&lt;/h2&gt;
&lt;p&gt;There are a variety of applications where the M step does not have a simple computational form. In such cases, one way to avoid an iterative M step is to increase the Q function, rather than maximising it at each iteration, which corresponds to a &lt;em&gt;Generalised Expectation Maximisation&lt;/em&gt;(GEM) algorithm. GEM inceases the likelihood at each iteration but appropriate convergence is not guaranteed without further specification of the process of increasing the Q function. One specific case of GEM is the &lt;em&gt;Expectation Conditional Maximisation&lt;/em&gt;(ECM) algorithm (&lt;span class=&#34;citation&#34;&gt;Meng and Rubin (1993)&lt;/span&gt;), which replaces the M step with a sequence of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; conditional maximisation (CM) steps, each of which maximises the Q function over &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; but with some vector function of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, say &lt;span class=&#34;math inline&#34;&gt;\(g_s(\theta)\)&lt;/span&gt;, fixed at its previous values for &lt;span class=&#34;math inline&#34;&gt;\(s=1,\ldots,S\)&lt;/span&gt;. Very briefly, assume that we have a parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that can be partitioned into subvectors &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\theta_1,\ldots,\theta_S)\)&lt;/span&gt;, then we can take the &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;-th of the CM steps to be maximisation with respect to &lt;span class=&#34;math inline&#34;&gt;\(\theta_s\)&lt;/span&gt; with all other parameters held fixed. Alternatively, it may be useful to take the &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;-th of the CM steps to be simultaneous maximisation over all of the subvectors expect &lt;span class=&#34;math inline&#34;&gt;\(\theta_s\)&lt;/span&gt;, which is fixed. Because the ECM increases Q, it belongs to the class of GEM algorithms and therefore monotonically increases the likelihood of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. When the set of functions &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is “space filling” in the sense that it allows unconstrained maximisation over &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in its parameter space, ECM converges to a stationary point under the same conditions ensuring convergence of EM.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Expectation Conditional Maximisation Either&lt;/em&gt;(ECME) algorithm (&lt;span class=&#34;citation&#34;&gt;Liu and Rubin (1994)&lt;/span&gt;) is another version of GEM, which replaces some of the CM steps of ECM, maximising the constrained expected complete data loglikelihood function, with steps that maximise the correspondingly constrained actual likelihood function. The algorithm has stable monotone convergence and basic simplicity implementation relative to competing faster converging methods, and can have faster convergence rate than EM or ECM, measured using either the number of iterations or actual computer time. The The &lt;em&gt;Alternative Expectation Conditional Maximisation&lt;/em&gt;(AECM) algorithm (&lt;span class=&#34;citation&#34;&gt;Meng and Van Dyk (1997)&lt;/span&gt;) builds on the ECME idea by maximising functions other than Q or L in particular CM steps, corresponding to varying definitions of what constitutes missing data. An iteration of AECM consists of cycles, each consisting of an E step with a particular definition of complete and missing data, followed by CM steps, which can result in enhanced computational efficiency.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-dempster1977maximum&#34;&gt;
&lt;p&gt;Dempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the Em Algorithm.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Methodological)&lt;/em&gt; 39 (1): 1–22.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-liu1994ecme&#34;&gt;
&lt;p&gt;Liu, Chuanhai, and Donald B Rubin. 1994. “The Ecme Algorithm: A Simple Extension of Em and Ecm with Faster Monotone Convergence.” &lt;em&gt;Biometrika&lt;/em&gt; 81 (4): 633–48.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-meng1993maximum&#34;&gt;
&lt;p&gt;Meng, Xiao-Li, and Donald B Rubin. 1993. “Maximum Likelihood Estimation via the Ecm Algorithm: A General Framework.” &lt;em&gt;Biometrika&lt;/em&gt; 80 (2): 267–78.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-meng1997algorithm&#34;&gt;
&lt;p&gt;Meng, Xiao-Li, and David Van Dyk. 1997. “The Em Algorithm—an Old Folk-Song Sung to a Fast New Tune.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 59 (3): 511–67.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Explicit Single Imputation</title>
      <link>/missmethods/mean-imputation/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/mean-imputation/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;All case deletion methods, such as &lt;em&gt;Complete Case Analysis&lt;/em&gt;(CCA) or &lt;em&gt;Available Case Analysis&lt;/em&gt;(ACA) make no use of units with partially observed data, when estimating the marginal distribution of the variables under study or the covariation between variables. Clearly, this is inefficient and a tempting alternative would be to &lt;em&gt;impute&lt;/em&gt; or “fill in” the unobserved data with some plausible values. When a single value is used to replace each missing data, we talk about &lt;em&gt;Single Imputation&lt;/em&gt;(SI) methods and, according to the precedure used to generate these imputations, different SI methods can be used. In general, the idea of imputing the missing values is really appealing as it allows to recover the full sample on which standard complete data methods can be applied to derive the estimates of interest.&lt;/p&gt;
&lt;p&gt;However, it is important to be aware of the potential problems of imputing missing data without a clear understanding about the process underlying the values we want to impute, which is the key factor to determine whether the selected approach would be plausible in the context considered. Indeed, imputation should be conceptualised as draws from a predictive distribution of the missing values and require methods for creating a predictive distribution for the imputation based on the observed data. According to &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;, these predictive distributions can be created using&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Explicit modelling&lt;/em&gt;, when the distribution is based on formal statistical models which make the underlying assumptions explicit.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Implicit modelling&lt;/em&gt;, when the distribution is based on an algorithm which implicitly relies on some underlying model assumptions.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this part, we focus on some of the most popular &lt;em&gt;Explicit Single Imputation&lt;/em&gt; methods. These include: &lt;em&gt;Mean Imputation&lt;/em&gt;(SI-M), where means from the observed data are used as imputed values; &lt;em&gt;Regression Imputation&lt;/em&gt;(SI-R), where missing values are replaced with values predicited from a regression of the missing variable on some other observed variables; and &lt;em&gt;Stochastic Regression Imputation&lt;/em&gt;(SI-SR), where unobserved data are substituted with the predicted values from a regression imputation plus a randomly selected residual drawn to reflect uncertainty in the predicted values.&lt;/p&gt;
&lt;div id=&#34;mean-imputation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean Imputation&lt;/h2&gt;
&lt;p&gt;The simplest type of SI-M consists in replacing the missing values in a variable with the mean of the observed units from the same variable, a method known as &lt;em&gt;Unconditional Mean Imputation&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;). Let &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt; be the value of variable &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, such that the unconditional mean based on the observed values of &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; is given by &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_j\)&lt;/span&gt;. The sample mean of the observed and imputed values is then &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{m}_j=\bar{y}^{ac}_j\)&lt;/span&gt;, i.e. the estimate from ACA, while the sample variance is given by&lt;/p&gt;
&lt;p&gt;\[
s^{m}_{j}=s^{ac}_{j}\frac{(n^{ac}-1)}{(n-1)},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s^{ac}_j\)&lt;/span&gt; is the sample variance estimated from the &lt;span class=&#34;math inline&#34;&gt;\(n^{ac}\)&lt;/span&gt; available units. Under a &lt;em&gt;Missing Completely At Random&lt;/em&gt;(MCAR) assumption, &lt;span class=&#34;math inline&#34;&gt;\(s^{ac}_j\)&lt;/span&gt; is a consistent estimator of the tru variance so that the sample variance from the imputed data &lt;span class=&#34;math inline&#34;&gt;\(s^m_j\)&lt;/span&gt; systematically underestimates the true variance by a factor of &lt;span class=&#34;math inline&#34;&gt;\(\frac{(n^{ac}-1)}{(n-1)}\)&lt;/span&gt;, which clearly comes from the fact that missing data are imputed using values at the centre of the distribution. The imputation distorts theempirical distribution of the observed values as well as any quantities that are not linear in the data (e.g. variances, percentiles, measures of shape). The sampel covariance of &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_k\)&lt;/span&gt; from the imputed data is&lt;/p&gt;
&lt;p&gt;\[
s^{m}_{jk}=s^{ac}_{jk}\frac{(n^{as}_{jk}-1)}{(n-1)},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(n^{ac}_{jk}\)&lt;/span&gt; is the number of units with both variables observed and &lt;span class=&#34;math inline&#34;&gt;\(s^{ac}_{jk}\)&lt;/span&gt; is the corresponding covariance estimate from ACA. Under MCAR &lt;span class=&#34;math inline&#34;&gt;\(s^{ac}_{jk}\)&lt;/span&gt; is a consistent estimator of the true covariance, so that &lt;span class=&#34;math inline&#34;&gt;\(s^{m}_{jk}\)&lt;/span&gt; underestimates the magnitude of the covariance by a factor of &lt;span class=&#34;math inline&#34;&gt;\(\frac{(n^{ac}_{jk}-1)}{(n-1)}\)&lt;/span&gt;. Obvious adjustments for the variance (&lt;span class=&#34;math inline&#34;&gt;\(\frac{(n-1)}{(n^{ac}_j-1)}\)&lt;/span&gt;) and the covariance (&lt;span class=&#34;math inline&#34;&gt;\(\frac{(n-1)}{(n^{ac}_{jk}-1)}\)&lt;/span&gt;) yield ACA estimates, which could lead to covariance matrices that are not positive definite.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-imputation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression Imputation&lt;/h2&gt;
&lt;p&gt;An improvement over SI-M is to impute each missing data using the conditional means given the observed values, a method known SI-R or &lt;em&gt;Conditional Mean Imputation&lt;/em&gt;. To be precise, it would also be possible to impute conditional means without using a regression approach, for example by grouping individuals into adjustment classes (analogous to weighting methods) based on the observed data and then impute the missing values using the observed means in each adjustment class (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;). However, for the sake of simplicity, here we will assume that SI-R and conditional mean imputation are the same.&lt;/p&gt;
&lt;p&gt;To generate imputations under SI-R, consider a set of &lt;span class=&#34;math inline&#34;&gt;\(J-1\)&lt;/span&gt; fully observed response variables &lt;span class=&#34;math inline&#34;&gt;\(y_1,\ldots,y_{J-1}\)&lt;/span&gt; and a partially observed response variable &lt;span class=&#34;math inline&#34;&gt;\(y_J\)&lt;/span&gt; which has the first &lt;span class=&#34;math inline&#34;&gt;\(n_{cc}\)&lt;/span&gt; units observed and the remaiing &lt;span class=&#34;math inline&#34;&gt;\(n-n_{cc}\)&lt;/span&gt; units missing. SI-R computes the regression of &lt;span class=&#34;math inline&#34;&gt;\(y_J\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1,\ldots,y_{J-1}\)&lt;/span&gt; based on the &lt;span class=&#34;math inline&#34;&gt;\(n_{cc}\)&lt;/span&gt; complete units and then fills in the missing values as predictions from the regression. For example, for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the missing value &lt;span class=&#34;math inline&#34;&gt;\(y_{iJ}\)&lt;/span&gt; is imputed using&lt;/p&gt;
&lt;p&gt;\[
\hat{y}_{iJ}=\hat{\beta}_{J0}+\sum_{j=1}^{J-1}\hat{\beta}_{Jj}y_{ij},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{J0}\)&lt;/span&gt; is the intercept and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{Jj}\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; coefficient of of the regression of &lt;span class=&#34;math inline&#34;&gt;\(y_J\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1,\ldots,y_{J-1}\)&lt;/span&gt; based on the &lt;span class=&#34;math inline&#34;&gt;\(n_{cc}\)&lt;/span&gt; units.&lt;/p&gt;
&lt;p&gt;An extension of regression imputation to a general pattern of missing data is known as &lt;em&gt;Buck’s method&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Buck (1960)&lt;/span&gt;). This approach first estimates the population mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; from the sample mean and covariance matrix of the complete units and then uses these estimates to calculate the OLS regressions of the missing variables on the observed variables for each missing data pattern. Predictions of the missing data for each observation are obtained by replacing the values of the present variables in the regressions. The average of the observed and imputed values from this method are consistent estimates of the means and MCAR and mild assumptions about the moments of the distribution (&lt;span class=&#34;citation&#34;&gt;Buck (1960)&lt;/span&gt;). They are also consistent when the missingness mechanism depends on observed variables, i.e. under a &lt;em&gt;Missing At Random&lt;/em&gt;(MAR) assumption, although addtional assumptions are required in this case (e.g. using linear regressions it assumes that the “true” regression of the missing varables on the observed variables is linear).&lt;/p&gt;
&lt;p&gt;The filled in data from Buck’s method typically yield reasonable estimates of means, while the sample variances and covariances are biased, although the bias is less than the one associated with unconditional mean imputation. Specifically, the sample variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2,SI-R}_j\)&lt;/span&gt; from the imputed data underestimates the true variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_j\)&lt;/span&gt; by a factor of &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n-1}\sum_{i=1}^n\sigma^{2}_{ji}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_{ji}\)&lt;/span&gt; is the residual variance from regressing &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; on the variables observed in unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt; is missing and zero if &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt; is observed. The sample covariance of &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_k\)&lt;/span&gt; has a bias of &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n-1}\sum_{i=1}^n\sigma_{jki}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{jki}\)&lt;/span&gt; is the residual covariance from the multivariate regression of &lt;span class=&#34;math inline&#34;&gt;\((y_{ij},y_{ik})\)&lt;/span&gt; on the variables observed in unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; if both variables are missing and zero otherwise. A consistent estimator of &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; can be constructed under MCAR by replacing consistent estimates of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_{ji}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{jki}\)&lt;/span&gt; in the expressions for bias and then adding the resulting quantities to the sample covariance matrix of the filled-in data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stochastic-regression-imputation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stochastic Regression Imputation&lt;/h2&gt;
&lt;p&gt;Any type of mean or regression imputation will lead to bias when the interest is in the tails of the distributions because “best prediction” imputation systematically underestimates variability and standard errors calculated from the imputed data are typically too small. These considerations suggest an alternative imputation strategy, where imputed values are drawn from a predictive distribution of a plausible set of values rather than from the centre of the distribution. This is the idea behind SI-SR, which imputes a conditional draw&lt;/p&gt;
&lt;p&gt;\[
\hat{y}_{iJ}=\hat{\beta}_{J0}+\sum_{j=1}^{J-1}\hat{\beta}_{Jj}y_{ij}+z_{iJ},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(z_{iJ}\)&lt;/span&gt; is a random normal deviate with mean 0 and variance &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2_J\)&lt;/span&gt;, the residual variance from the regression of &lt;span class=&#34;math inline&#34;&gt;\(y_J\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1,\ldots,y_{J-1}\)&lt;/span&gt; based on the complete units. The addition of the random deviate makes the imputation a random draw from the predictive distribution of the missing values, rather than the mean, which is likely to ameliorate the distortion of the predictive distributions (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;Consider a bivariate normal monotone missing data with &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; fully observed and &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt; missing for a fraction &lt;span class=&#34;math inline&#34;&gt;\(\lambda=\frac{(n-n_{cc})}{n}\)&lt;/span&gt; and a MCAR mechanism. The following table shows the large sample bias of standard OLS estimates obtained from the filled-in data about the mean, the variance of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt;, the regression coefficient of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;, and the regression coefficient of &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt;, using four different single imputation methods: uncondtional mean (UM), unconditional draw (UD), conditional mean (CM), and conditional draw (CD).&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Bivariate normal monotone MCAR data; large sample bias of four imputation methods.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
mu_2
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
sigma_2
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
beta_21
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
beta_12
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
UM
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
-lambda * sigma_2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
-lambda * beta_21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
UD
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
-lambda * beta_21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
-lambda * beta_21
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CM
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
-lambda * (1-rho^2) * sigma_2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
((lambda * (1-rho^2)) / (1-lambda * (1-rho^2)) ) * beta_12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CD
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Under MCAR, all four methods yield consistent estimates of &lt;span class=&#34;math inline&#34;&gt;\(\mu_2\)&lt;/span&gt; but both UM and CM underestimate the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma_2\)&lt;/span&gt;, UD leads to attenuation of the regression coefficients, while CD yields consistent estimates of all four parameters. However, CD has some important drawbacks. First, adding random draws to the conditional mean imputations is inefficient as the large sample variance of the CD estimates of &lt;span class=&#34;math inline&#34;&gt;\(\mu_2\)&lt;/span&gt; can be shown (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;) to be&lt;/p&gt;
&lt;p&gt;\[
\frac{[1-\lambda\rho^2+(1-\rho^2)\lambda(1-\lambda)]\sigma_2}{n_{cc}},
\]&lt;/p&gt;
&lt;p&gt;which is larger than the large sample sampling variance of the CM estimate of &lt;span class=&#34;math inline&#34;&gt;\(\mu_2\)&lt;/span&gt;, namely &lt;span class=&#34;math inline&#34;&gt;\(\frac{[1-\lambda\rho^2]\sigma_2}{n_{cc}}\)&lt;/span&gt;. Second, the standard errors of the CD estimates from the imputed data are too small because they do not incorporate imputation uncertainty.&lt;/p&gt;
&lt;p&gt;When the analysis involves units with some covariates missing and other observed, it is common practice to condition on the observed covariates when generating the imputations for the missing covariates. It is also possible to condition on the outcome &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to impute missing covariates, even if the final objective is to regress &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; on the full set of covariates and conditioning on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; will lead to bias when conditional means are imputed. However, if predictive draws are imputed, this approach will yield consistent estimates of the regression coefficients. Imputing missing covariates using the means by conditioning only the observed covariates (and not also on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) also yields consistent estimates of the regression coefficients under certain conditions, although these are typically less efficient then those from CCA, but yields inconsistent estimates of other parameters such as variances and correlations (&lt;span class=&#34;citation&#34;&gt;Little (1992)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;According to &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;, imputation should generally be&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Conditional&lt;/strong&gt; on observed variables, to reduce bias, improve precision and preserve association between variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Multivariate&lt;/strong&gt;, to preserve association between missing variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Draws&lt;/strong&gt; from the predictive distributions rather than means, to provide valid estimates of a wide range of estimands.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Nevertheless, a main problem of SI methods is that inferences based on the imputed data do not account for imputation uncertainty and standard errors are therefore systematically underestimated, p-values of tests are too significant and confidence intervals are too narrow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-buck1960method&#34;&gt;
&lt;p&gt;Buck, Samuel F. 1960. “A Method of Estimation of Missing Values in Multivariate Data Suitable for Use with an Electronic Computer.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Methodological)&lt;/em&gt; 22 (2): 302–6.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little1992regression&#34;&gt;
&lt;p&gt;Little, Roderick JA. 1992. “Regression with Missing X’s: A Review.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 87 (420): 1227–37.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Implicit Single Imputation</title>
      <link>/missmethods/last-value-carried-forward/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/last-value-carried-forward/</guid>
      <description>


&lt;p&gt;All case deletion methods, such as &lt;em&gt;Complete Case Analysis&lt;/em&gt;(CCA) or &lt;em&gt;Available Case Analysis&lt;/em&gt;(ACA) make no use of units with partially observed data, when estimating the marginal distribution of the variables under study or the covariation between variables. Clearly, this is inefficient and a tempting alternative would be to &lt;em&gt;impute&lt;/em&gt; or “fill in” the unobserved data with some plausible values. When a single value is used to replace each missing data, we talk about &lt;em&gt;Single Imputation&lt;/em&gt;(SI) methods and, according to the precedure used to generate these imputations, different SI methods can be used. In general, the idea of imputing the missing values is really appealing as it allows to recover the full sample on which standard complete data methods can be applied to derive the estimates of interest.&lt;/p&gt;
&lt;p&gt;However, it is important to be aware of the potential problems of imputing missing data without a clear understanding about the process underlying the values we want to impute, which is the key factor to determine whether the selected approach would be plausible in the context considered. Indeed, imputation should be conceptualised as draws from a predictive distribution of the missing values and require methods for creating a predictive distribution for the imputation based on the observed data. According to &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;, these predictive distributions can be created using&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Explicit modelling&lt;/em&gt;, when the distribution is based on formal statistical models which make the underlying assumptions explicit.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Implicit modelling&lt;/em&gt;, when the distribution is based on an algorithm which implicitly relies on some underlying model assumptions.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this part, we focus on some of the most popular &lt;em&gt;Implicit Single Imputation&lt;/em&gt; methods. These include: &lt;em&gt;Hot Deck Imputation&lt;/em&gt;(SI-HD), where missing values are imputed using observed values from similar responding units in the sample; &lt;em&gt;Substitution&lt;/em&gt;(SI-S), where nonresponding units are replaced with alternative units not yet selected into the sample; &lt;em&gt;Cold Deck Imputation&lt;/em&gt;(SI-CD), where missing values are replaced with a constant value from an external source; &lt;em&gt;Composite Methods&lt;/em&gt;, which combine procedures from the previous approaches. We will specifically focus on SI-HD methods, which are the most popular among these.&lt;/p&gt;
&lt;div id=&#34;hot-deck-imputation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hot Deck Imputation&lt;/h2&gt;
&lt;p&gt;SI-HD procedures refer to the deck of match &lt;a href=&#34;https://en.wikipedia.org/wiki/Punched_card#Hollerith&amp;#39;s_early_punched_card_formats&#34;&gt;Hollerith cards&lt;/a&gt; for the donors available for a nonrespondent. Suppose that a sample of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; out of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; units is selected and that &lt;span class=&#34;math inline&#34;&gt;\(n_{cc}\)&lt;/span&gt; out of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; are recorded. Given an equal probability sampling scheme, the mean of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; can be estimated from the filled-in data as the mean of the responding and the imputed units&lt;/p&gt;
&lt;p&gt;\[
\bar{y}_{HD}=\frac{(n_{cc}\bar{y}_{cc}+(n-n_{cc})\bar{y}^{\star})}{n},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{cc}\)&lt;/span&gt; is the mean of the responding units, and &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^\star=\sum_{i=1}^{n_{cc}}\frac{H_iy_i}{n-n_{cc}}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(H_i\)&lt;/span&gt; is the number of times &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is used as substitute for a missing value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^{n_{cc}}H_i=n-n_{cc}\)&lt;/span&gt; being the number of missing units. The proprties of &lt;span class=&#34;math inline&#34;&gt;\(bar{y}_{HD}\)&lt;/span&gt; depend on the procedure used to generate the numbers &lt;span class=&#34;math inline&#34;&gt;\(H_i\)&lt;/span&gt; and in general the mean and sampling variance of this estimator can be written as&lt;/p&gt;
&lt;p&gt;\[
E[\bar{y}_{HD}]=E[E[\bar{y}_{HD}\mid y_{obs}]] ;;; \text{and} ;;; Var(\bar{y}_{HD})=Var(E[\bar{y}_{HD} \mid y_{obs}]) + E[Var(\bar{y}_{HD} \mid y_{obs})],
\]&lt;/p&gt;
&lt;p&gt;where the inner expectations and variances are taken over the distribution of &lt;span class=&#34;math inline&#34;&gt;\(H_i\)&lt;/span&gt; given the observed data &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}\)&lt;/span&gt;, and the outer expectations and variances are taken over the model distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. The term &lt;span class=&#34;math inline&#34;&gt;\(E[Var(\bar{y}_{HD} \mid y_{obs})]\)&lt;/span&gt; represents the additional sampling variance from the stochastic imputation procedure. Examples of these procedures include &lt;em&gt;predictive mean matching&lt;/em&gt; or PMM(&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;) and &lt;em&gt;last value carried forward&lt;/em&gt; or LVCF(&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;predictive-mean-matching&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Predictive Mean Matching&lt;/h3&gt;
&lt;p&gt;A general approach to hot-deck imputation is to define a metric &lt;span class=&#34;math inline&#34;&gt;\(d(i,j)\)&lt;/span&gt; measuring the distance between units based on observed variables &lt;span class=&#34;math inline&#34;&gt;\(x_{i1},\ldots,x_{iJ}\)&lt;/span&gt; and then choose the imputed values that come from responding units close to the unit with the missing value, i.e. we choose the imputed value for &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; from a &lt;em&gt;donor pool&lt;/em&gt; of units &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; that are such that &lt;span class=&#34;math inline&#34;&gt;\(y_j,x_1,\ldots,x_J\)&lt;/span&gt; are observed and &lt;span class=&#34;math inline&#34;&gt;\(d(i,j)\)&lt;/span&gt; is less than some value &lt;span class=&#34;math inline&#34;&gt;\(d_0\)&lt;/span&gt;. Varying the value for &lt;span class=&#34;math inline&#34;&gt;\(d_0\)&lt;/span&gt; can control the number of available donors &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. When the choice of the metric has the form&lt;/p&gt;
&lt;p&gt;\[
d(i,j)=(\hat{y}(x_i)-\hat{y}(x_j))^2,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}(x_i)\)&lt;/span&gt; is the predicted value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; from the regression of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; from the complete units, then the procedure is known as PMM. A powerful aspect of this metric is that it weights predictors according to their ability to predict the missing variable, which allows to have some protection against misspecification of the regression of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, even though better approaches are available when good matches to donor units cannot be found or the sample size is small.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;last-value-carried-forward&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Last Value Carried Forward&lt;/h3&gt;
&lt;p&gt;Longitudinal data are often subject to attrition when units leave the study prematurely. Let &lt;span class=&#34;math inline&#34;&gt;\(y_i=(y_{i1},\ldots,y_{iJ})\)&lt;/span&gt; be a &lt;span class=&#34;math inline&#34;&gt;\((J\times1)\)&lt;/span&gt; vector of partially-observed outcomes for subject &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, and denote with &lt;span class=&#34;math inline&#34;&gt;\(y_{i,obs}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{i,mis}\)&lt;/span&gt; the observed and missing components of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(y=(y_{i,obs},y_{i,mis})\)&lt;/span&gt;. Define the indicator variable &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; taking value 0 for complete units and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; if subject &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; drops out between &lt;span class=&#34;math inline&#34;&gt;\(j-1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; time points. LVCF, also called &lt;em&gt;last observation carried forward&lt;/em&gt;(&lt;span class=&#34;citation&#34;&gt;Pocock (2013)&lt;/span&gt;), imputes all missing values for individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (for whom &lt;span class=&#34;math inline&#34;&gt;\(m_i=j\)&lt;/span&gt;) using the last recorded value for that unit, that is&lt;/p&gt;
&lt;p&gt;\[
\hat{y}_{it}=y_{i,j-1},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(t=j,\ldots,J\)&lt;/span&gt;. Although simple, this approach makes the often unrealistic assumption that the value of the outcome remains unchanged after dropout.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;According to &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;, imputation should generally be&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Conditional&lt;/strong&gt; on observed variables, to reduce bias, improve precision and preserve association between variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Multivariate&lt;/strong&gt;, to preserve association between missing variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Draws&lt;/strong&gt; from the predictive distributions rather than means, to provide valid estimates of a wide range of estimands.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Nevertheless, a main problem of SI methods is that inferences based on the imputed data do not account for imputation uncertainty and standard errors are therefore systematically underestimated, p-values of tests are too significant and confidence intervals are too narrow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pocock2013clinical&#34;&gt;
&lt;p&gt;Pocock, Stuart J. 2013. &lt;em&gt;Clinical Trials: A Practical Approach&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Bayesian Inference</title>
      <link>/missmethods/likelihood-based-methods-ignorable2/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/likelihood-based-methods-ignorable2/</guid>
      <description>


&lt;p&gt;Bayesian inference offers a convenient framework to analyse missing data as it draws no distinction between missing values and parameters, both interprted as unobserved quantities who are associated with a joint posterior distribution conditional on the observed data. In this section, I review basic concepts of Bayesian inference based on fully observed data, with notation and structure mostly taken from &lt;span class=&#34;citation&#34;&gt;Gelman et al. (2013)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;bayesian-inference-for-complete-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian Inference for Complete Data&lt;/h2&gt;
&lt;p&gt;Bayesian inference is the process of fitting a probability model to a set of data &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and summarising the results by a probability distribution on the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; of the model and on unobserved quantities &lt;span class=&#34;math inline&#34;&gt;\(\tilde{Y}\)&lt;/span&gt; (e.g. predictions). Indeed, Bayesian statistical conclusions about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(\tilde{Y}\)&lt;/span&gt;) are made in terms of probability statements, conditional on the observed data &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, typically indicated with the notation &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid y)\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde{y} \mid y)\)&lt;/span&gt;. Conditioning on the observed data is what makes Bayesian inference different from standard statistical approaches which are instead based on the retrospective evaluation of the procedures used to estimate &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y}\)&lt;/span&gt;) over the distribution of possible &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; values conditional on the “true” unknown value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;bayes-rule&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bayes’ Rule&lt;/h3&gt;
&lt;p&gt;In order to make probability statements about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, we start with a model providing a &lt;em&gt;joint probability distribution&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(p(\theta,y)\)&lt;/span&gt;. Thus, the joint probability mass or density function can be written as a product of two densities that are often referred to as the &lt;em&gt;prior distribution&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; and the &lt;em&gt;sampling distribution&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(p(y \mid \theta)\)&lt;/span&gt;, respectively:&lt;/p&gt;
&lt;p&gt;\[
p(\theta,y) = p(\theta)p(y \mid \theta),
\]&lt;/p&gt;
&lt;p&gt;and conditioning on the observed values of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, using the basic property of conditional probability known as &lt;em&gt;Bayes’ rule&lt;/em&gt;, yields the &lt;em&gt;posterior distribution&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;\[
p(\theta \mid y) = \frac{p(\theta,y)}{p(y)} = \frac{p(\theta)p(y \mid \theta)}{p(y)},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(p(y)=\sum_{\theta \in \Theta}p(\theta)p(y\mid \theta)\)&lt;/span&gt; is the sum (or integral in the case of continous &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) over all possible values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in the sample space &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;. We can approximate the above equation by omitting the factor &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; which does not depend on &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and, given &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, can be considered as fixed, yielding the &lt;em&gt;unnormalised posterior density&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;\[
p(\theta \mid y) \propto p(\theta) p(y \mid \theta),
\]&lt;/p&gt;
&lt;p&gt;with the purpose of the analysis being to develop the model &lt;span class=&#34;math inline&#34;&gt;\(p(\theta,y)\)&lt;/span&gt; and adequately summarise &lt;span class=&#34;math inline&#34;&gt;\(p(\theta \mid y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate-normal-example-known-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Normal Example (known variance)&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,\ldots,y_n)\)&lt;/span&gt; denote an independent and identially distributed sample of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units, which are assumed to come from a Normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, whose sampling density function is&lt;/p&gt;
&lt;p&gt;\[
p(y \mid \mu)=\frac{1}{\sqrt{\left(2\pi\sigma^2\right)^n}}\text{exp}\left(-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2} \right),
\]&lt;/p&gt;
&lt;p&gt;where for the moment we assume the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; to be known (i.e. constant). Consider now a prior probability distribution for the mean parameter &lt;span class=&#34;math inline&#34;&gt;\(p(\mu)\)&lt;/span&gt;, which belongs to the family of &lt;em&gt;conjugate prior densities&lt;/em&gt;, for example a Normal distribution, and parameterised in terms of a prior mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_0\)&lt;/span&gt;. Thus, its prior density function is&lt;/p&gt;
&lt;p&gt;\[
p(\mu) = \frac{1}{\sqrt{2\pi\sigma^2_0}}\text{exp}\left(-\frac{1}{2}\frac{(\mu -\mu_0)^2}{\sigma^2} \right),
\]&lt;/p&gt;
&lt;p&gt;under the assumption tha the hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_0\)&lt;/span&gt; are known. The conjugate prior density implies that the posterior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (with &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; assumed constant) belongs to the same family of distributions of the sampling function, that is Normal, but some algebra is required to reveal its specific form. In particular, the posterior density is&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y) = \frac{p(\mu)p(y\mid \mu)}{p(y)} \propto \frac{1}{\sqrt{2\pi\sigma^2_0}}\frac{1}{\sqrt{\left(2\pi\sigma^2\right)^n}}\text{exp}\left(-\frac{1}{2} \left[\frac{(\mu - \mu_0)^2}{\sigma^2_0} + \sum_{i=1}^n\frac{(y_i-\mu)^2}{\sigma^2} \right] \right).
\]&lt;/p&gt;
&lt;p&gt;Exapanding the components, collecting terms and completing the square in &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; gives&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y) \propto \text{exp}\left(-\frac{(\mu - \mu_1)}{2\tau^2_1} \right),
\]&lt;/p&gt;
&lt;p&gt;that is the posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is Normal with posterior mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_1\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\tau^2_1\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;\[
\mu_1 = \frac{\frac{1}{\tau^2_0}\mu_0 + \frac{n}{\sigma^2}\bar{y}}{\frac{1}{\tau^2_0} + \frac{n}{\sigma^2}} \;\;\; \text{and} \;\;\; \frac{1}{\tau^2_1}=\frac{1}{\tau^2_0} + \frac{n}{\sigma^2}.
\]&lt;/p&gt;
&lt;p&gt;We can see that the posterior distribution depends on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; only through the sample mean &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}=\sum_{i=1}^ny_i\)&lt;/span&gt;, which is a &lt;em&gt;sufficient statistic&lt;/em&gt; in this model. When working with Normal distributions, the inverse of the variance plays a prominent role and is called the &lt;em&gt;precision&lt;/em&gt; and, from the above expressions, it can be seen that for normal data and prior, the posterior precision &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\tau^2_1}\)&lt;/span&gt; equals the sum of the prior precision &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\tau^2_0}\)&lt;/span&gt; and the sampling precision &lt;span class=&#34;math inline&#34;&gt;\(\frac{n}{\sigma^2}\)&lt;/span&gt;. Thus, when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is large, the posterior precision is largely dominated by &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and the sample mean &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt; compared to the corresponding prior parameters. In the specific case where &lt;span class=&#34;math inline&#34;&gt;\(\tau^2_0=\sigma^2\)&lt;/span&gt;, the prior has the same weight as one extra observation with the value of &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; and, as &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow\infty\)&lt;/span&gt;, we have that &lt;span class=&#34;math inline&#34;&gt;\(p(\mu\mid y)\approx N\left(\mu \mid \bar{y},\frac{\sigma^2}{n}\right)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate-normal-example-unknown-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Normal Example (unknown variance)&lt;/h3&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(p(y \mid \mu,\sigma^2)=N(y \mid \mu, \sigma^2)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; known and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; unknown, the sampling distribution for a vector &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units is&lt;/p&gt;
&lt;p&gt;\[
p(y \mid \sigma^2)=\frac{1}{\sqrt{\left(2\pi\sigma^2\right)^n}}\text{exp}\left(-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2} \right),
\]&lt;/p&gt;
&lt;p&gt;with the corresponding conjugate prior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; being the Inverse-Gamma distribution &lt;span class=&#34;math inline&#34;&gt;\(\Gamma^{-1}(\alpha,\beta)\)&lt;/span&gt; with density function&lt;/p&gt;
&lt;p&gt;\[
p(\sigma^2) \propto (\sigma^2)^{-(\alpha+1)}\text{exp}\left(-\frac{\beta}{\sigma^2}\right),
\]&lt;/p&gt;
&lt;p&gt;indexed by the hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. A convenient parameterisation is as a Scaled Inverse-Chi Squared distribution &lt;span class=&#34;math inline&#34;&gt;\(\text{Inv-}\chi^2(\sigma^2_0,\nu_0)\)&lt;/span&gt; with scale and degrees of freedom parameters &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu_0\)&lt;/span&gt;, respectively. This means that the prior on &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; corresponds to the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma^2_0 \nu_0}{X}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(X\sim \chi^2_{\nu_0}\)&lt;/span&gt; random variable. After some calculations, the resulting posterior for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;\[
p(\sigma^2 \mid y) \propto (\sigma^2)^\left(\frac{n+\nu_0}{2}+1\right)\text{exp}\left(-\frac{\nu_0 \sigma^2_0 + n \nu}{2\sigma^2} \right)
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\nu=\frac{1}{n}\sum_{i=1}^n(y_i-\mu)^2\)&lt;/span&gt;. This corresponds to say that&lt;/p&gt;
&lt;p&gt;\[
\sigma^2 \mid y \sim \text{Inv-}\chi^2\left(\nu_0 +n, \frac{\nu_0\sigma^2_0+n\nu}{\nu_0 + n} \right),
\]&lt;/p&gt;
&lt;p&gt;with scale equal to the degrees of freedom-weighted average of the prior and data scales and degrees of freedom equal to the sum of the prior and data degrees of freedom.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate-normal-example-unknown-mean-and-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Normal Example (unknown mean and variance)&lt;/h3&gt;
&lt;p&gt;Suppose now that both the mean and variance parameters are unknown such that&lt;/p&gt;
&lt;p&gt;\[
p(y \mid \mu, \sigma^2) \sim N(\mu, \sigma^2),
\]&lt;/p&gt;
&lt;p&gt;and that the interest is centred on making inference about &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, that is we seek the conditional posterior distribution of the parameters of interest given the observed data &lt;span class=&#34;math inline&#34;&gt;\(p(\mu \mid y)\)&lt;/span&gt;. This can be derived from the joint posterior distribution density &lt;span class=&#34;math inline&#34;&gt;\(p(\mu, \sigma^2 \mid y)\)&lt;/span&gt; by averaging over all possible values of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y)=\int p(\mu, \sigma^2 \mid y)d\sigma^2,
\]&lt;/p&gt;
&lt;p&gt;or, alternatively, the joint posterior can be factored as the product of the marginal distribution of one parameter and the conditional distribution of the other given the former and then taking the average over the values of the “nuisance” parameter&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y)=\int p(\mu \mid \sigma^2, y)p(\sigma^2 \mid y)d\sigma^2.
\]&lt;/p&gt;
&lt;p&gt;The integral forms are rarely computed in practice but this expression helps us to understand that posterior distributions can be expressed in terms of the product of marginal and conditional densities, first drawing &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; from its marginal and then &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; from its conditional given the drawn value of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, so that the integration is indirectly performed. For example, consider the Normal model with both unknown mean and variance and assume a vague prior density &lt;span class=&#34;math inline&#34;&gt;\(p(\mu,\sigma^2)\propto (\sigma^2)^{-1}\)&lt;/span&gt; (corresponding to uniform prior on &lt;span class=&#34;math inline&#34;&gt;\((\mu, \log\sigma)\)&lt;/span&gt;), then the joint posterior distribution is proportional to the sampling distribution multiplied by the factor &lt;span class=&#34;math inline&#34;&gt;\((\sigma^2)^{-1}\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p(\mu,\sigma^2 \mid y)\propto \sigma^{-n-2}\text{exp}\left(-\frac{1}{2\sigma^2}\left[(n-1)s^2+n(\bar{y}-\mu)^2 \right] \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s^2=\frac{1}{n-1}\sum_{i=1}^n(y_i-\bar{y})^2\)&lt;/span&gt; is the sample variance. Next, the conditional posterior density &lt;span class=&#34;math inline&#34;&gt;\(p(\mu \mid \sigma^2)\)&lt;/span&gt; can be shown to be equal to&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid \sigma^2,y) \sim N(\bar{y},\frac{\sigma^2}{n}),
\]&lt;/p&gt;
&lt;p&gt;while the marginal posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2 \mid y)\)&lt;/span&gt; can be obtained by averaging the joint &lt;span class=&#34;math inline&#34;&gt;\(p(\mu,\sigma^2\mid y)\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p(\sigma^2 \mid y)\propto \int \left(\sigma^{-n-2}\text{exp}\left(-\frac{1}{2\sigma^2}\left[(n-1)s^2+n(\bar{y}-\mu)^2 \right] \right)\right)d\mu,
\]&lt;/p&gt;
&lt;p&gt;which leads to&lt;/p&gt;
&lt;p&gt;\[
p(\sigma^2 \mid ,y) \sim \text{Inv-}\chi^2(n-1,s^2).
\]&lt;/p&gt;
&lt;p&gt;Typically, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; represents the estimand of interest and the obejective of the analysis is therefore to make inference about the marginal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\mu \mid y)\)&lt;/span&gt;, which can be obtained by integrating &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; out of the joint posterior&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y)=\int_{0}^{\infty}p(\mu,\sigma^2\mid y)d\sigma^2 \propto \left[1+\frac{n(\mu-\bar{y})}{(n-1)s^2} \right]
\]&lt;/p&gt;
&lt;p&gt;which corresponds to a Student-&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; density with &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt; degrees of freedom&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y)\sim t_{n-1}\left(\bar{y},\frac{s^2}{n}\right)
\]&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-normal-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multivariate Normal Example&lt;/h3&gt;
&lt;p&gt;Similar considerations to those applied to the univariate case can be extended to the multivariate case when &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is formed by &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; components coming from the Multivariate Normal distribution&lt;/p&gt;
&lt;p&gt;\[
p(y\mid \mu, \Sigma) \sim N(\mu, \Sigma),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is a vector of length &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(J\times J\)&lt;/span&gt; covariance matrix, which is symmetric and positive definite. The sampling distribution for a sample of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units is&lt;/p&gt;
&lt;p&gt;\[
p(y\mid \mu, \Sigma) \propto \mid \Sigma \mid^{-n/2}\text{exp}\left(-\frac{1}{2}\sum_{i=1}^n(y_i-\mu)^{T}\Sigma^{-1}(y_i-\mu) \right),
\]&lt;/p&gt;
&lt;p&gt;As with the univariate normal model, we can derive the posterior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; according to the factorisation used of the joint posterior and the prior distributions specified. For example, using the conjugate normal prior for the mean &lt;span class=&#34;math inline&#34;&gt;\(p(\mu)\sim N(\mu_0,\Sigma_0)\)&lt;/span&gt;, given &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; known, the posterior can be shown to be&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid y) \sim N(\mu_1,\Sigma_1),
\]&lt;/p&gt;
&lt;p&gt;where the posterior mean is a weighted average of the data and prior mean with weights given by the data and prior precision matrices &lt;span class=&#34;math inline&#34;&gt;\(\mu_1=(\Sigma^{-1}_0+n\Sigma^{-1})^{-1} (\Sigma_0^{-1}\mu_0 + n\Sigma^{-1}\bar{y})\)&lt;/span&gt;, and the posterior precision is the sum of the data and prior precisions &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^{-1}_1=\Sigma^{-1}_0+n\Sigma^{-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the situation in which both &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; are unknown, convenient conjugate prior distributions which generalise those used in the univariate case are the Inverse-Wishart for the covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\sim \text{Inv-Wishart}(\Lambda_0,\nu_0)\)&lt;/span&gt; and the Multivariate Normal for the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\sim N(\mu_0, \Sigma_0)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\nu_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Lambda_0\)&lt;/span&gt; represent the degrees of freedom and the scale matrix for the Inverse-Wishart distribution, while &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_0=\frac{\Sigma}{\kappa_0}\)&lt;/span&gt; are the prior mean and covariance matrix for the Multivariate Normal. Woking out the form of the posterior, it can be shown that the joint posterior distribution has the same form of the sampling distribution with parameters&lt;/p&gt;
&lt;p&gt;\[
p(\mu \mid \Sigma, y) \sim N(\mu_1,\Sigma_1) \;\;\; \text{and} \;\;\; p(\Sigma \mid y) \sim \text{Inv-Wishart}(\Lambda_1,\nu_1),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_1=\frac{\Sigma}{\kappa_1}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu_1=\frac{1}{\kappa_0+n}\mu_0+\frac{n}{\kappa_0+n}\bar{y}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\kappa_1=\kappa_0+n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\nu_1=\nu_0+n\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\Lambda_1=\Lambda_0+\sum_{i=1}^n(y_i-\bar{y})(y_i-\bar{y})^T+\frac{\kappa_0 n}{\kappa_0+n}(\bar{y}-\mu_0)(\bar{y}-\mu_0)^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression Models&lt;/h2&gt;
&lt;p&gt;Suppose the data consist in &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units measured on an outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and a set of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; covariates &lt;span class=&#34;math inline&#34;&gt;\(X=(x_{1},\ldots,x_{J})\)&lt;/span&gt; and assume that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is Normal with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_i=\beta_0+\sum_{j=1}^J\beta_jx_{ij}\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;\[
p(y \mid \beta,\sigma^2,X) \sim N(X\beta,\sigma^2I),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta=(\beta_0,\ldots,\beta_J)\)&lt;/span&gt; is the set of regression coefficients and &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(n\times n\)&lt;/span&gt; identity matrix. Within the normal regression model, a convenient vague prior distribution is uniform on &lt;span class=&#34;math inline&#34;&gt;\((\beta,\log\sigma)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;\[
p(\beta,\sigma^2)\propto\sigma^{-2}.
\]&lt;/p&gt;
&lt;p&gt;As with normal distributions with unknown mean and variance we can first determine the marginal posterior of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and factor the joint posterior as &lt;span class=&#34;math inline&#34;&gt;\(p(\beta,\sigma^2)=p(\beta \mid \sigma^2, y)p(\sigma^2 \mid y)\)&lt;/span&gt; (omit X for simplicity). Then, the conditional distribtuion &lt;span class=&#34;math inline&#34;&gt;\(p(\beta \mid \sigma^2,y)\)&lt;/span&gt; is Normal&lt;/p&gt;
&lt;p&gt;\[
p(\beta \mid \sigma^2, y) \sim N(\hat{\beta},V_{\beta}\sigma^2),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}=(X^{T}X)^{-1}(X^{T}y)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_{\beta}=(X^{T}X)^{-1}\)&lt;/span&gt;. The marginal posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2 \mid y)\)&lt;/span&gt; has a scaled Inverse-&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; form&lt;/p&gt;
&lt;p&gt;\[
p(\sigma^2\mid y) \sim \text{Inv-}\chi^2(n-J,s^2),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s^2=\frac{1}{n-J}(y-X\hat{\beta})^{T}(y-X\hat{\beta})\)&lt;/span&gt;. Finally, the marginal posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\beta \mid y)\)&lt;/span&gt;, averaging over &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, is multivariate &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n-J\)&lt;/span&gt; degrees of freedom, even though in practice since we can characterise the joint posterior by drawing from &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2)\)&lt;/span&gt; and then from &lt;span class=&#34;math inline&#34;&gt;\(p(\beta \mid \sigma^2)\)&lt;/span&gt;. When the anaysis is based on improper priors (do not have finite integral), it is important to check tha the posterior is proper. In the case of the regression model, the posterior for &lt;span class=&#34;math inline&#34;&gt;\(\beta \mid \sigma^2\)&lt;/span&gt; is proper only if the number of observations is larger than the number of parameters &lt;span class=&#34;math inline&#34;&gt;\(n&amp;gt;J\)&lt;/span&gt;, and that the rank of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; equals &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; (i.e. the columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; are linearly independent) in order for all &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; coefficients to be uniquely identified from the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generalised-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generalised Linear Models&lt;/h2&gt;
&lt;p&gt;The purpose of &lt;em&gt;Generalised Linear Models&lt;/em&gt;(GLM) is to extend the idea of linear modelling to cases for which the linear relationship between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(E[y\mid X]\)&lt;/span&gt; or the Normal distribution is not appropriate. GLMs are specified in three stages&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Choose the linear predictor &lt;span class=&#34;math inline&#34;&gt;\(\eta=X\beta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose the &lt;em&gt;link fuction&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(g()\)&lt;/span&gt; that relates the linear predictor to the mean of the outcome variable &lt;span class=&#34;math inline&#34;&gt;\(\mu=g^{-1}(\eta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose the random component specifying the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; with mean &lt;span class=&#34;math inline&#34;&gt;\(E[y\mid X]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus, the mean of the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is determined as &lt;span class=&#34;math inline&#34;&gt;\(E[y\mid X]=g^{-1}(X\beta)\)&lt;/span&gt;. The Normal linear model can be thought as a special case of GLMs where the link function is the identity &lt;span class=&#34;math inline&#34;&gt;\(g(\mu)=\mu\)&lt;/span&gt; and the random component is normally distributed. Perhaps, the most commonly used GLMs are those based on Poisson and Binomial distributions to analyse count and binary data, respectively.&lt;/p&gt;
&lt;div id=&#34;poisson&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poisson&lt;/h3&gt;
&lt;p&gt;Counted data are often modelled using Poisson regression models which assume that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is distributed according to a Poisson distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. The link function is typically chosen to be the logarithm so that &lt;span class=&#34;math inline&#34;&gt;\(\log \mu = X\beta\)&lt;/span&gt; and the distribution of the data has density&lt;/p&gt;
&lt;p&gt;\[
p(y\mid \beta)=\prod_{i=1}^n \frac{1}{y_i}\text{exp}\left(-\text{e}^{(\eta_i)}(\text{exp}(\eta_i))^{y_i}\right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\eta_i=(X\beta)_i\)&lt;/span&gt; is the linear predictor for the &lt;span class=&#34;math inline&#34;&gt;\(i-\)&lt;/span&gt;th unit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Binomial&lt;/h3&gt;
&lt;p&gt;Suppose there are some binomial data &lt;span class=&#34;math inline&#34;&gt;\(y_i \sim \text{Bin}(n_i,\mu_i)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(n_i\)&lt;/span&gt; known. It is common to specify the model in terms of the mean of the proportions &lt;span class=&#34;math inline&#34;&gt;\(\frac{y_i}{n_i}\)&lt;/span&gt; rather than the mean of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;. Choosing the logit tranformation of the probability of success &lt;span class=&#34;math inline&#34;&gt;\(g(\mu_i)=\log\left(\frac{\mu_i}{1-\mu_i}\right)\)&lt;/span&gt; as the link function leads to the logistic regression where data have distribution&lt;/p&gt;
&lt;p&gt;\[
p(y \mid \beta)=\prod_{i=1}^n {n_i \choose y_i} {e^{\eta_i} \choose 1+e^{\eta_i}}^{y_i} {1 \choose 1+e^{\eta_i}}^{n_i-y_i}.
\]&lt;/p&gt;
&lt;p&gt;The link functions used in the previous models are known as the &lt;em&gt;canonical link&lt;/em&gt; functions for each family of distributions, which is the function of the mean parameter that appears in the exponent of the exponential family form of the probability density. However, it is also possible to use link functions which are not canonical.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34;&gt;
&lt;p&gt;Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. &lt;em&gt;Bayesian Data Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Maximum Likelihood Estimation</title>
      <link>/missmethods/likelihood-based-methods-ignorable/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/likelihood-based-methods-ignorable/</guid>
      <description>


&lt;p&gt;A possible approach to analyse missing data is to use methods based on the likelihood function under specific modelling assumptions. In this section, I review maximum likelihood methods based on fully observed data alone.&lt;/p&gt;
&lt;div id=&#34;maximum-likelihood-methods-for-complete-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximum Likelihood Methods for Complete Data&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; denote the set of data, which are assumed to be generated according to a certain probability density function &lt;span class=&#34;math inline&#34;&gt;\(f(Y= y,\mid \theta)=f(y \mid \theta)\)&lt;/span&gt; indexed by the set of parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, which lies on the parameter space &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; (i.e. set of values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; for which &lt;span class=&#34;math inline&#34;&gt;\(f(y\mid \theta)\)&lt;/span&gt; is a proper density function). The &lt;em&gt;Likelihood&lt;/em&gt; function, indicated with &lt;span class=&#34;math inline&#34;&gt;\(L(\theta \mid y)\)&lt;/span&gt;, is defined as any function of &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt; proportional that is to &lt;span class=&#34;math inline&#34;&gt;\(f(y \mid \theta)\)&lt;/span&gt;. Note that, in contrast to the density function which is defined as a function of the data &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given the values of the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, instead the likelihood is defined as a function of the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; for fixed data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. In addition, the &lt;em&gt;loglikelihood&lt;/em&gt; function, indicated with &lt;span class=&#34;math inline&#34;&gt;\(l(\theta\mid y)\)&lt;/span&gt; is defined as the natural logarithm of &lt;span class=&#34;math inline&#34;&gt;\(L(\theta \mid y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;univariate-normal-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Normal Example&lt;/h3&gt;
&lt;p&gt;The joint density function of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; independent and identially distributed units &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,\ldots,y_n)\)&lt;/span&gt; from a Normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, is&lt;/p&gt;
&lt;p&gt;\[
f(y \mid \mu, \sigma^2)=\frac{1}{\sqrt{\left(2\pi\sigma^2\right)^n}}\text{exp}\left(-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2} \right),
\]&lt;/p&gt;
&lt;p&gt;and therefore the loglikelihood is&lt;/p&gt;
&lt;p&gt;\[
l(\mu, \sigma^2 \mid y)= -\frac{n}{2}\text{ln}(2\pi)-\frac{n}{2}\text{ln}(\sigma^2)-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2},
\]&lt;/p&gt;
&lt;p&gt;which is considered as a function of &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\sigma^2)\)&lt;/span&gt; for fixed data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-normal-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multivariate Normal Example&lt;/h3&gt;
&lt;p&gt;If the sample considered has dimension &lt;span class=&#34;math inline&#34;&gt;\(J&amp;gt;1\)&lt;/span&gt;, e.g. we have a set of idependent and identically distributed variables &lt;span class=&#34;math inline&#34;&gt;\(y=(y_{ij})\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; units and &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots,J\)&lt;/span&gt; variables, which comes from a Multivariate Normal distribution with mean vector &lt;span class=&#34;math inline&#34;&gt;\(\mu=(\mu_1,\ldots\mu_J)\)&lt;/span&gt; and covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma=(\sigma_{jk})\)&lt;/span&gt; for $ j=1,,J, k=1,,K$ and &lt;span class=&#34;math inline&#34;&gt;\(J=K\)&lt;/span&gt;, then its density function is&lt;/p&gt;
&lt;p&gt;\[
f(y \mid \mu, \Sigma)=\frac{1}{\sqrt{\left(2\pi \right)^{nK}\left(\mid \Sigma \mid \right)^n}} \text{exp}\left(-\frac{1}{2}\sum_{i=1}^{n}(y_i-\mu)\Sigma^{-1}(y_i-\mu)^{T} \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(|\Sigma|\)&lt;/span&gt; denotes the determinant of the matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; and the superscript &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; denotes the transpose of a matrix or vector, while &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; denotes the row vector of observed values for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. The loglikelihood of &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\Sigma)\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;\[
l(\mu,\Sigma \mid y)= - \frac{n}{2}\text{ln}(2\pi) - \frac{n}{2}\text{ln}(|\Sigma|)-\frac{1}{2}\sum_{i=1}^{n}(y_i-\mu)\Sigma^{-1}(y_i-\mu)^T.
\]&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mle-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MLE estimation&lt;/h2&gt;
&lt;p&gt;Finding the maximum value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that is most likely to have generated the data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, corresponding to maximising the likelihood or &lt;em&gt;Maximum Likelihood Estimation&lt;/em&gt;(MLE), is a standard approach to make inference about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Suppose a specific value for the parameter &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(L(\hat{\theta}\mid y)\geq L(\theta \mid y)\)&lt;/span&gt; for any other value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. This implies that the observed data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is at least as likely under &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; as under any other value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is the value best supported by the data. More specifically, a maximum likelihood estimate of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is a value of &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Theta\)&lt;/span&gt; that maximises the likelihood &lt;span class=&#34;math inline&#34;&gt;\(L(\theta \mid y)\)&lt;/span&gt; or, equivalently, that maximises the loglikelihood &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid y)\)&lt;/span&gt;. In general, when the likelihood is differentiable and bounded from above, typically the MLE can be found by differentiating &lt;span class=&#34;math inline&#34;&gt;\(L(\theta \mid y)\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid y)\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, setting the result equal to zero, and solving for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. The resulting equation, &lt;span class=&#34;math inline&#34;&gt;\(D_l(\theta)=\frac{\partial l(\theta \mid y)}{\partial \theta}=0\)&lt;/span&gt;, is known as the &lt;em&gt;likelihood equation&lt;/em&gt; and the derivative of the loglikelihood as the &lt;em&gt;score function&lt;/em&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; consists in a set of &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots,J\)&lt;/span&gt; components, then the likelihood equation corresponds to a set of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; simultaneous equations, obtained by differentiating &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid y)\)&lt;/span&gt; with respect to each component of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;univariate-normal-example-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Normal Example&lt;/h3&gt;
&lt;p&gt;Recall that, for a Normal sample with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units, the loglikelihood is indexed by the set of parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\sigma^2)\)&lt;/span&gt; and has the form&lt;/p&gt;
&lt;p&gt;\[
l(\mu, \sigma^2 \mid y)= -\frac{n}{2}\text{ln}(2\pi)-\frac{n}{2}\text{ln}(\sigma^2)-\frac{1}{2}\sum_{i=1}^n \frac{(y_i-\mu)^2}{\sigma^2}.
\]&lt;/p&gt;
&lt;p&gt;Next, the MLE can be derived by first differentiating &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid y)\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and set the result equal to zero, that is&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial l(\theta \mid y)}{\partial \mu}= -\frac{2}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)(-1)=\frac{\sum_{i=1}^n y_i - n\mu}{\sigma^2}=0,
\]&lt;/p&gt;
&lt;p&gt;Next, after simplifying a bit, we can retrieve the solution&lt;/p&gt;
&lt;p&gt;\[
\hat{\mu}=\frac{1}{n}\sum_{i=1}^n y_i=\bar{y},
\]&lt;/p&gt;
&lt;p&gt;which corresponds to the sample mean of the observations. Next, we differentiate &lt;span class=&#34;math inline&#34;&gt;\(l(\theta \mid y)\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, that is we set&lt;/p&gt;
&lt;p&gt;\[
\frac{\partial l(\theta \mid y)}{\partial \sigma^2}= -\frac{n}{2\sigma^2}+\frac{1}{2(\sigma^2)^2}\sum_{i=1}^n (y_i-\mu)^2=0.
\]&lt;/p&gt;
&lt;p&gt;We then simplify and move things around to get&lt;/p&gt;
&lt;p&gt;\[
\frac{1}{\sigma^3}\sum_{i=1}^n(y_i-\mu)^2=\frac{n}{\sigma} \;\;\; \rightarrow \;\;\; \sigma^2=\frac{1}{n}\sum_{i=1}^n(y_i-\mu)^2.
\]&lt;/p&gt;
&lt;p&gt;Finally, we replace &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; in the expression above with the value &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=\bar{y}\)&lt;/span&gt; found before and obtain the solution&lt;/p&gt;
&lt;p&gt;\[
\hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^n(y_i-\bar{y})^2=s^2,
\]&lt;/p&gt;
&lt;p&gt;which, however, is a biased estimator of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and therefore is often replaced with the unbiased estimator &lt;span class=&#34;math inline&#34;&gt;\(\frac{s^2}{(n-1)}\)&lt;/span&gt;. In particular, given a population parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is said to be unbiased when &lt;span class=&#34;math inline&#34;&gt;\(E[\hat{\theta}]=\theta\)&lt;/span&gt;. This is the case, for example, of the sample mean &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=\bar{y}\)&lt;/span&gt; which is an unbiased estimator of the population mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;\[
E\left[\hat{\mu} \right]=E\left[\frac{1}{n}\sum_{i=1}^n y_i \right]=\frac{1}{n}\sum_{i=1}^n E\left[y_i \right]=\frac{1}{n} (n\mu)=\mu.
\]&lt;/p&gt;
&lt;p&gt;However, this is not true for the sample variance &lt;span class=&#34;math inline&#34;&gt;\(s^2\)&lt;/span&gt;. This can be seen by first rewriting the expression of the estimator as&lt;/p&gt;
&lt;p&gt;\[
\hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^n (y_i^2 -2y_i\bar{y}+\bar{y}^2)=\frac{1}{n}\sum_{i=1}^n y_i^2 -2\bar{y}\sum_{i=1}^n y_i + \frac{1}{n}n\bar{y}^2=\frac{1}{n}\sum_{i=1}^n y_i^2 - \bar{y}^2,
\]&lt;/p&gt;
&lt;p&gt;and then by computing the expectation of this quantity:&lt;/p&gt;
&lt;p&gt;\[
E\left[\hat{\sigma}^2 \right]=E\left[\frac{1}{n}\sum_{i=1}^n y_i^2 - \bar{y}^2 \right]=\frac{1}{n}\sum_{i=1}^n E\left[y_i^2 \right] - E\left[\bar{y}^2 \right]=\frac{1}{n}\sum_{i=1}^n (\sigma^2 + \mu^2) - (\frac{\sigma^2}{n}+\mu^2)=\frac{1}{n}\left(n\sigma^2+n\mu^2\right) - \frac{\sigma^2}{n}-\mu^2=\frac{(n-1)\sigma^2}{n}.
\]&lt;/p&gt;
&lt;p&gt;The above result is obtained by pluggin in the expression for the variance of a general variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and retrieving the expression for &lt;span class=&#34;math inline&#34;&gt;\(E[y^2]\)&lt;/span&gt; as a function of the variance and &lt;span class=&#34;math inline&#34;&gt;\(E[y]^2\)&lt;/span&gt;. More specifically, given that&lt;/p&gt;
&lt;p&gt;\[
Var(y)=\sigma^2=E\left[y^2 \right]-E\left[y \right]^2,
\]&lt;/p&gt;
&lt;p&gt;then we know that for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E\left[y^2 \right]=\sigma^2+E[y]^2\)&lt;/span&gt;, and similarly we can derive the same expression for &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt;. However, we can see that &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt; is biased by a factor of &lt;span class=&#34;math inline&#34;&gt;\((n-1)/n\)&lt;/span&gt;. Thus, an unbiased estimator for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is given by multiplying &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\frac{n}{(n-1)}\)&lt;/span&gt;, which gives the unbiased estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^{2\star}=\frac{s^2}{n-1}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(E\left[\hat{\sigma}^{2\star}\right]=\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-normal-example-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multivariate Normal Example&lt;/h3&gt;
&lt;p&gt;The same procedure can be applied to an independent and identically distributed multivariate sample &lt;span class=&#34;math inline&#34;&gt;\(y=(y_{ij})\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; units and &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots,J\)&lt;/span&gt; variables (&lt;span class=&#34;citation&#34;&gt;Anderson (1962)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Rao et al. (1973)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Gelman et al. (2013)&lt;/span&gt;). It can be shown that, maximising the loglikelihood with respect to &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; yields the MLEs&lt;/p&gt;
&lt;p&gt;\[
\hat{\mu}=\bar{y} \;\;\; \text{and} \;\;\; \Sigma=\frac{(n-1)\hat{\sigma}^{2\star}}{n},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}=(\bar{y}_1,\ldots,\bar{y}_{J})\)&lt;/span&gt; is the row vectors of sample means and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^{2\star}=(s^{\star_{jk}})\)&lt;/span&gt; is the sample covariance matrix with &lt;span class=&#34;math inline&#34;&gt;\(jk\)&lt;/span&gt;-th element &lt;span class=&#34;math inline&#34;&gt;\(s^\star_{jk}=\frac{\Sigma_{i=1}^n(y_{ij} - \bar{y}_j)}{(n-1)}\)&lt;/span&gt;. In addition, in general, given a function &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt; of the parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, if &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is a MLE of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(g(\hat{\theta})\)&lt;/span&gt; is a MLE of &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-distribution-of-a-bivariate-normal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional Distribution of a Bivariate Normal&lt;/h2&gt;
&lt;p&gt;Consider an indpendent and identically distributed sample formed by two variables &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,y_2)\)&lt;/span&gt;, each measured on &lt;span class=&#34;math inline&#34;&gt;\(i=1\ldots,n\)&lt;/span&gt; units, which come from a Bivariate Normal distribution with mean vector and covariance matrix&lt;/p&gt;
&lt;p&gt;\[
\mu=(\mu_1,\mu_2) \;\;\; \text{and} \;\;\; \Sigma = \begin{pmatrix} \sigma^2_1 &amp;amp; \rho\sigma_1\sigma_2 \\ \rho\sigma_2\sigma_1 &amp;amp; \sigma_2^2 \ \end{pmatrix},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is a correlation parameter between the two variables. Thus, intuitive MLEs for these parameters are&lt;/p&gt;
&lt;p&gt;\[
\hat{\mu}_j=\bar{y}_j \;\;\; \text{and} \;\;\; \hat{\sigma}_{jk}=\frac{(n-1)s_{jk}}{n},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_j=\sigma_{jj}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\rho\sigma_{j}\sigma_{k}=\sigma_{jk}\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(j,k=1,2\)&lt;/span&gt;. By properites of the Bivariate Normal distribution (&lt;span class=&#34;citation&#34;&gt;Ord and Stuart (1994)&lt;/span&gt;), the marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; and the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_2 \mid y_1\)&lt;/span&gt; are&lt;/p&gt;
&lt;p&gt;\[
y_1 \sim \text{Normal}\left(\mu_1,\sigma^2_1 \right) \;\;\; \text{and} \;\;\; y_2 \mid y_1 \sim \text{Normal}\left(\mu_2 + \beta(y_1-\mu_1 \right), \sigma^2_2 - \sigma^2_1\beta^2),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta=\rho\frac{\sigma_2}{\sigma_1}\)&lt;/span&gt; is the parameter that quantifies the linear dependence between the two variables. The MLEs of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_2\)&lt;/span&gt; can also be derived from the likelihood based on the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_2 \mid y_1\)&lt;/span&gt;, which have strong connections with the least squares estimates derived in a multiple linear regression framework.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-linear-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Linear Regression&lt;/h2&gt;
&lt;p&gt;Suppose the data consist in &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units measured on an outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and a set of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; covariates &lt;span class=&#34;math inline&#34;&gt;\(x=(x_{1},\ldots,x_{J})\)&lt;/span&gt; and assume that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is Normal with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_i=\beta_0+\sum_{j=1}^J\beta_jx_{ij}\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. The loglikelihood of &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\beta,\sigma^2)\)&lt;/span&gt; given the observed data &lt;span class=&#34;math inline&#34;&gt;\((y,x)\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;\[
l(\theta \mid y) = -\frac{n}{2}\text{ln}(2\pi) -\frac{n}{2}\text{ln}(\sigma^2) - \frac{\sum_{i=1}^n \left(y_i - \mu_i \right)^2}{2\sigma^2}.
\]&lt;/p&gt;
&lt;p&gt;Maximising this expression with respect to &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the MLEs are found to be equal to the least squares estimates of the intercept and regression coefficients. Using a matrix notation for the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-th vector of the outcome values &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and the &lt;span class=&#34;math inline&#34;&gt;\(n\times (J+1)\)&lt;/span&gt; matrix of the covariate values (including the constant term), then the MLEs are:&lt;/p&gt;
&lt;p&gt;\[
\hat{\beta}=(X^{T}X)^{-1}X^{T}Y \;\;\; \text{and} \;\;\; \hat{\sigma}^{2}=\frac{(Y-X\hat{\beta})(Y-X\hat{\beta})}{n},
\]&lt;/p&gt;
&lt;p&gt;where the numerator of the fraction is known as the &lt;em&gt;Residual Sum of Squares&lt;/em&gt;(RSS). Because the denominator of is equal to &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, the MLE of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; does not correct for the loss of degrees of freedom when estimating the &lt;span class=&#34;math inline&#34;&gt;\(J+1\)&lt;/span&gt; location parameters. Thus, the MLE should instead divide the RSS by &lt;span class=&#34;math inline&#34;&gt;\(n-(J+1)\)&lt;/span&gt; to obtain an unbiased estimator. An extension of standard multiple linear regression is the so called &lt;em&gt;weighted&lt;/em&gt; multiple linear regression, in which the regression variance is assumed to be equal to&lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma^2}{w_i}\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\((w_i) &amp;gt; 0\)&lt;/span&gt;. Thus, the variable &lt;span class=&#34;math inline&#34;&gt;\((y_i-\mu)\sqrt{w_i}\)&lt;/span&gt; is Normally distributed with mean &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, and the loglikelihood is&lt;/p&gt;
&lt;p&gt;\[
l(\theta \mid y)= - \frac{n}{2}\text{ln}(2\pi) - \frac{n}{2}\text{ln}(\sigma^2) - \frac{\sum_{i=1}^n w_i(y_i - \mu_i)^2}{2\sigma^2}.
\]&lt;/p&gt;
&lt;p&gt;Maximising this function yields MLEs given by the weighted least squares estimates&lt;/p&gt;
&lt;p&gt;\[
\hat{\beta}=\left(X^{T}WX\right)^{-1}\left(X^{T}WY \right) \;\;\; \text{and} \;\;\; \sigma^{2}=\frac{\left(Y-X\hat{\beta}\right)^{T}W\left(Y-X\hat{\beta}\right)}{n},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(W=\text{Diag}(w_1,\ldots,w_n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generalised-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generalised Linear Models&lt;/h2&gt;
&lt;p&gt;Consider the previous example where we had an outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and a set of &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; covariates, each measured on &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units. A more general class of models, compare with the Normal model, assumes that, given &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, the values of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are an independent sample from a regular exponential family distribution&lt;/p&gt;
&lt;p&gt;\[
f(y \mid x,\beta,\phi)=\text{exp}\left(\frac{\left(y\delta\left(x,\beta \right) - b\left(\delta\left(x,\beta\right)\right)\right)}{\phi} + c\left(y,\phi\right)\right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\delta()\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b()\)&lt;/span&gt; are known functions that determine the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(c()\)&lt;/span&gt; is a known function indexed by a scale parameter &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. The mean of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is assumed to linearly relate to the covariates via&lt;/p&gt;
&lt;p&gt;\[
E\left[y \mid x,\beta,\phi \right]=g^{-1}\left(\beta_0 + \sum_{j=1}^J\beta_jx_{j} \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(E\left[y \mid x,\beta,\phi \right]=\mu_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g()\)&lt;/span&gt; is a known one to one function which is called &lt;em&gt;link function&lt;/em&gt; because it “links” the expectation of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to a linear combination of the covariates. The canonical link function&lt;/p&gt;
&lt;p&gt;\[
g_c(\mu_i)=\delta(x_{i},\beta)=\beta_0+\sum_{j=1}^J\beta_jx_{ij},
\]&lt;/p&gt;
&lt;p&gt;which is obtained by setting &lt;span class=&#34;math inline&#34;&gt;\(g()\)&lt;/span&gt; equal to the inverse of the derivative of &lt;span class=&#34;math inline&#34;&gt;\(b()\)&lt;/span&gt; with respect to its argument. Examples of canonical links include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Normal linear regression: &lt;span class=&#34;math inline&#34;&gt;\(g_c=\text{identity}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b(\delta)=\frac{\delta^2}{2},\phi=\sigma^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Poisson regression: &lt;span class=&#34;math inline&#34;&gt;\(g_c=\log\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b(\delta)=\text{exp}(\delta),\phi=1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Logistic regression: &lt;span class=&#34;math inline&#34;&gt;\(g_c=\text{logit}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b(\delta)=\log(1+\text{exp}(\delta)),\phi=1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The loglikelihood of &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\beta,\phi)\)&lt;/span&gt; given the observed data &lt;span class=&#34;math inline&#34;&gt;\((y,x)\)&lt;/span&gt;, is&lt;/p&gt;
&lt;p&gt;\[
l(\theta \mid y,x)=\sum_{i=1}^n \left[\frac{\left(y_i\delta\left(x_i,\beta\right)-b\left(\delta\left(x_i,\beta\right)\right) \right)}{\phi}+c\left(y_i,\phi\right)\right],
\]&lt;/p&gt;
&lt;p&gt;which for non-normal cases does not have explicit maxima and numerical maximisation can be achieved using iterative algorithms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-anderson1962introduction&#34;&gt;
&lt;p&gt;Anderson, Theodore Wilbur. 1962. “An Introduction to Multivariate Statistical Analysis.” Wiley New York.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34;&gt;
&lt;p&gt;Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. &lt;em&gt;Bayesian Data Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ord1994kendall&#34;&gt;
&lt;p&gt;Ord, Keith, and Alan Stuart. 1994. “Kendall’s Advanced Theory of Statistics: Distribution Theory.” Edward Arnold.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rao1973linear&#34;&gt;
&lt;p&gt;Rao, Calyampudi Radhakrishna, Calyampudi Radhakrishna Rao, Mathematischer Statistiker, Calyampudi Radhakrishna Rao, and Calyampudi Radhakrishna Rao. 1973. &lt;em&gt;Linear Statistical Inference and Its Applications&lt;/em&gt;. Vol. 2. Wiley New York.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Inverse Probability Weighting</title>
      <link>/missmethods/inverse-probability-weighting/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/inverse-probability-weighting/</guid>
      <description>


&lt;p&gt;In certain cases, it is possible to reduce biases from case deletion by the application of weights. After incomplete cases are removed, the remaining complete cases can be weighted so that their distribution more closely resembles that of the full sample with respect to auxiliary variables. &lt;em&gt;Weighting methods&lt;/em&gt; can eliminate bias due to differential response related to the variables used to model the response probabilities, but it cannot correct for biases related to variables that are unused or unmeasured (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;). &lt;span class=&#34;citation&#34;&gt;Robins, Rotnitzky, and Zhao (1994)&lt;/span&gt; introduced &lt;em&gt;Inverse Probability Weighting&lt;/em&gt; (IPW) as a weighted regression approach that require an explicit model for the missingness but relaxes some of the parametric assumptions in the data model. Their method is an extension of &lt;em&gt;Generalized Estimating Equations&lt;/em&gt; (GEE), a popular technique for modeling marginal or populationaveraged relationships between a response variable and predictors (&lt;span class=&#34;citation&#34;&gt;Zeger, Liang, and Albert (1988)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(y_i=(y_{i1},\ldots,y_{iK})\)&lt;/span&gt; denote a vector of variables for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; subject to missing values with &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; being fully observed for &lt;span class=&#34;math inline&#34;&gt;\(i=1\ldots,n_r\)&lt;/span&gt; units and partially-observed for &lt;span class=&#34;math inline&#34;&gt;\(i=n_r+1,\ldots,n\)&lt;/span&gt; units. Define &lt;span class=&#34;math inline&#34;&gt;\(m_i=1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is incomplete and &lt;span class=&#34;math inline&#34;&gt;\(m_i=0\)&lt;/span&gt; if complete. Let &lt;span class=&#34;math inline&#34;&gt;\(x_i=(x_{i1},\ldots,x_{ip})\)&lt;/span&gt; denote a vector of fully observed covariates and suppose the interest is in estimating the mean of the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;, having the form &lt;span class=&#34;math inline&#34;&gt;\(g(x_i,\beta)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(g()\)&lt;/span&gt; is a possibly non-linear regression function indexed by a parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; of dimension &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Let also &lt;span class=&#34;math inline&#34;&gt;\(z_i=(z_{i1},\ldots,z_{iq})\)&lt;/span&gt; be a vector of fully observed auxiliary variables that potentially predictive of missingness but are not included in the model for &lt;span class=&#34;math inline&#34;&gt;\(y_i \mid x_i\)&lt;/span&gt;. When there are no missing data, a consistent estimate of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is given by the solution to the following GEE, under mild regularity conditions (&lt;span class=&#34;citation&#34;&gt;Liang and Zeger (1986)&lt;/span&gt;),&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n = D_i(x_i,\beta)(y_i-g(x_i,\beta))=0,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(D_i(x_i,\beta)\)&lt;/span&gt; is a suitably chosen &lt;span class=&#34;math inline&#34;&gt;\((d\times k)\)&lt;/span&gt; matrix of known functions of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. With missing data, the equation is applied only to the complete cases (&lt;span class=&#34;math inline&#34;&gt;\(n_{r}\)&lt;/span&gt;), which yields consistent estimates provided that&lt;/p&gt;
&lt;p&gt;\[
p(m_i=1 \mid x_i,y_i,z_i,\phi)=p(m_i=1\mid x_i,\phi),
\]&lt;/p&gt;
&lt;p&gt;that is, missingness does not depend on &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; after conditioning on &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. IPW GEE methods (&lt;span class=&#34;citation&#34;&gt;Robins and Rotnitzky (1995)&lt;/span&gt;) replace the equation with&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^{n_r} = w_i(\hat{\alpha})D_i(x_i,\beta)(y_i-g(x_i,\beta))=0,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(w_i(\hat{\alpha})=\frac{1}{p(x_i,z_i \mid \hat{\alpha})}\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(p(x_i,z_i \mid \hat{\alpha})\)&lt;/span&gt; being an estimate of the probability of being a complete unit, obtained for example via logistic regressions on &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;. If the logistic regression is correctly specified, IPW GEE yields a consistent estimator of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; provided that&lt;/p&gt;
&lt;p&gt;\[
p(m_i=1 \mid x_i,y_i,z_i,\phi)=p(m_i=1\mid x_i,z_i\phi).
\]&lt;/p&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Suppose the full data consists of a single outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and an additional variable &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; and that the objective is to estimate the population outcome mean &lt;span class=&#34;math inline&#34;&gt;\(\mu=\text{E}[y]\)&lt;/span&gt;. If data were fully observed for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; individuals, an obvious estimator of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; would be the sample outcome mean&lt;/p&gt;
&lt;p&gt;\[
\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i,
\]&lt;/p&gt;
&lt;p&gt;which is equivalent to the solution to the estimating equation &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^n(y_i-\mu)=0\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is partially observed (while &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is always fully observed), individuals may fall into one of two missingness patterns &lt;span class=&#34;math inline&#34;&gt;\(r=(r_{y},r_{z})\)&lt;/span&gt;, namely &lt;span class=&#34;math inline&#34;&gt;\(r=(1,1)\)&lt;/span&gt; if both variables are observed or &lt;span class=&#34;math inline&#34;&gt;\(r=(1,0)\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is missing. Let &lt;span class=&#34;math inline&#34;&gt;\(c=1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(r=(1,1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c=0\)&lt;/span&gt; otherwise, so that the observed data can be summarised as &lt;span class=&#34;math inline&#34;&gt;\((c,cy,z)\)&lt;/span&gt;. Assuming that missingness only depends on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p(c=1 \mid y,z)=p(c=1 \mid z)=\pi(z),
\]&lt;/p&gt;
&lt;p&gt;then the missing data mechanism is &lt;em&gt;Missing At Random&lt;/em&gt; (MAR). Under these conditions, the sample mean of the complete cases &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{cc}=\frac{\sum_{i=1}^nc_iy_i}{c_i}\)&lt;/span&gt;, i.e. the solution to the equation &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^nc_i(y_i-\mu)=0\)&lt;/span&gt;, is not a consistent estimator of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. To correct for this, the IPW complete case estimating equation&lt;/p&gt;
&lt;p&gt;\[
\sum_{i=1}^n\frac{c_i}{\pi(z_i)}(y_i-\mu)=0,
\]&lt;/p&gt;
&lt;p&gt;can be used to weight the contribution of each complete case by the inverse of &lt;span class=&#34;math inline&#34;&gt;\(\pi(z_i)\)&lt;/span&gt;. The solution of the equation corresponds to the IPW estimator&lt;/p&gt;
&lt;p&gt;\[
\mu_{ipw}=\left(\sum_{i=1}^n \frac{c_i}{\pi(z_i)} \right)^{-1} \sum_{i=1}^n \frac{c_iy_i}{\pi(z_i)},
\]&lt;/p&gt;
&lt;p&gt;which is unbiased under MAR and for &lt;span class=&#34;math inline&#34;&gt;\(\pi(z)&amp;gt;0\)&lt;/span&gt;. In case you want to have a look at the &lt;a href=&#34;https://www4.stat.ncsu.edu/~davidian/st790/notes/chap5.pdf&#34;&gt;proof&lt;/a&gt; of this I put here the link. In most situations &lt;span class=&#34;math inline&#34;&gt;\(\pi(z_i)\)&lt;/span&gt; is not known and must be estimated from the data, typically posing some model for &lt;span class=&#34;math inline&#34;&gt;\(p(c=1 \mid z, \hat{\alpha})\)&lt;/span&gt;, indexed by some parameter &lt;span class=&#34;math inline&#34;&gt;\(\hat{\alpha}\)&lt;/span&gt;, for example a logistic regression&lt;/p&gt;
&lt;p&gt;\[
\text{logit}(\pi)=\alpha_0 + \alpha_1z.
\]&lt;/p&gt;
&lt;p&gt;Of course, if the model for &lt;span class=&#34;math inline&#34;&gt;\(\pi(z)\)&lt;/span&gt; is misspecified, &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ipw}\)&lt;/span&gt; can be an inconsistent estimator. In addition, IPW methods typically used data only from the completers discarding all the partially observed values, which is clearly inefficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Thus, IPW estimators can correct for the bias of unweighted estimators due to the dependence of the missingness mechanism on &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;). The basic intuition of IPW methods is that each subject’s contribution to the weighted &lt;em&gt;Complete Case Analysis&lt;/em&gt; (CCA) is replicated &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; times in order to account once for herself and &lt;span class=&#34;math inline&#34;&gt;\((1-w_i)\)&lt;/span&gt; times for those subjects with the same responses and covariates who are missing. These models are called &lt;em&gt;semiparametric&lt;/em&gt; because they apart from requiring the regression equation to have a specific form, they do not specify any probability distribution for the response variable (&lt;span class=&#34;citation&#34;&gt;Molenberghs et al. (2014)&lt;/span&gt;). Older GEE methods can accommodate missing values only if they are &lt;em&gt;Missing Completely At Random&lt;/em&gt; (MCAR), while more recent methods allow them to be MAR or even &lt;em&gt;Missing Not At Random&lt;/em&gt; (MNAR), provided that a model for the missingness is correctly specified (&lt;span class=&#34;citation&#34;&gt;Robins, Rotnitzky, and Zhao (1995)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Rotnitzky, Robins, and Scharfstein (1998)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-liang1986longitudinal&#34;&gt;
&lt;p&gt;Liang, Kung-Yee, and Scott L Zeger. 1986. “Longitudinal Data Analysis Using Generalized Linear Models.” &lt;em&gt;Biometrika&lt;/em&gt; 73 (1): 13–22.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-molenberghs2014handbook&#34;&gt;
&lt;p&gt;Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. &lt;em&gt;Handbook of Missing Data Methodology&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins1995semiparametric&#34;&gt;
&lt;p&gt;Robins, James M, and Andrea Rotnitzky. 1995. “Semiparametric Efficiency in Multivariate Regression Models with Missing Data.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 90 (429): 122–29.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins1994estimation&#34;&gt;
&lt;p&gt;Robins, James M, Andrea Rotnitzky, and Lue Ping Zhao. 1994. “Estimation of Regression Coefficients When Some Regressors Are Not Always Observed.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 89 (427): 846–66.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-robins1995analysis&#34;&gt;
&lt;p&gt;———. 1995. “Analysis of Semiparametric Regression Models for Repeated Outcomes in the Presence of Missing Data.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 90 (429): 106–21.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rotnitzky1998semiparametric&#34;&gt;
&lt;p&gt;Rotnitzky, Andrea, James M Robins, and Daniel O Scharfstein. 1998. “Semiparametric Regression for Repeated Outcomes with Nonignorable Nonresponse.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 93 (444): 1321–39.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-zeger1988models&#34;&gt;
&lt;p&gt;Zeger, Scott L, Kung-Yee Liang, and Paul S Albert. 1988. “Models for Longitudinal Data: A Generalized Estimating Equation Approach.” &lt;em&gt;Biometrics&lt;/em&gt;, 1049–60.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Joint Multiple Imputation</title>
      <link>/missmethods/joint-multiple-imputation/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/joint-multiple-imputation/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Multiple Imputation&lt;/em&gt;(MI) refers to the procedure of replacing each missing value by a set of &lt;span class=&#34;math inline&#34;&gt;\(H\geq 2\)&lt;/span&gt; imputed values. These are ordered in the sense that &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets can be created from the sets of imputations, where the first imputed value replaces the missing value in the first completed data set, the second imputed value in the second completed data set, and so on. Next, standard complete data methods are used to analyse each completed data set. When the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; sets of imputations are repeated random draws from the predictive distribution of the missing data under a particular model of missingness, the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences can be combined to form one inference that properly reflects uncertainty due to missing values under that model. In general, MI procedures can be summarised in three main steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Specify an &lt;strong&gt;imputation model&lt;/strong&gt; to generate &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; imputed values, typically taken as random draws from the predictive distribution of the missing values given the observed values, and create &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets using these imputations and the observed data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Analyse each completed data sets using standard complete data methods based on an &lt;strong&gt;analysis model&lt;/strong&gt;, and derive &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pool together the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences into a single inference using standard MI formulas, which ensure that missing data uncertainty is taken into account&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mi was first proposed by Rubin (&lt;span class=&#34;citation&#34;&gt;Rubin (1978)&lt;/span&gt;) and has become more popular over time (&lt;span class=&#34;citation&#34;&gt;Rubin (1996)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;), as well as the focus of research for methodological and practical applications in a variety of fields (&lt;span class=&#34;citation&#34;&gt;Herzog and Rubin (1983)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Rubin and Schenker (1987)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Schafer (1999)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Carpenter and Kenward (2012)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Molenberghs et al. (2014)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Van Buuren (2018)&lt;/span&gt;). MI shares both advantages of &lt;em&gt;Single Imputaiton&lt;/em&gt; (SI) methods and solves both disadvantages. Indeed, like SI, MI methods allow the analyst to use familiar complete data methods when analysing the completed data sets. The only disadvantage of MI compared with SI methods is that it takes more time to generate the imputations and analyse the completed data sets. However, &lt;span class=&#34;citation&#34;&gt;Rubin (2004)&lt;/span&gt; showed that in order to obtain sufficiently precise estimates, a relatively small number of imputations (typically &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt;) is required. For example, considering a situation with &lt;span class=&#34;math inline&#34;&gt;\(\lambda=50\%\)&lt;/span&gt; missing information and &lt;span class=&#34;math inline&#34;&gt;\(H=10\)&lt;/span&gt; imputations, the efficiency of MI can be shown to be equal to &lt;span class=&#34;math inline&#34;&gt;\((1+\frac{\lambda}{H})^{-1}=95\%\)&lt;/span&gt;. In addition, in today’s computing environments, the work of analysing the completed data sets is quite modest since it involves performing the same task &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; times. Thus, once a precedure to combine multiple completed data sets is established, the additonal time and effort to handle &lt;span class=&#34;math inline&#34;&gt;\(50\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(20\)&lt;/span&gt;, or &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; imputations if often of little consequence.&lt;/p&gt;
&lt;p&gt;In the first step of MI, imputations should ideally be created as repeated draws from the &lt;em&gt;posterior predictive distribution&lt;/em&gt; of the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}\)&lt;/span&gt; given the observed values &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}\)&lt;/span&gt;, each repetition being an independent drawing of the parameters and missing values. In practice, implicit imputation models can also be used in place of explicit imputation models (&lt;span class=&#34;citation&#34;&gt;Herzog and Rubin (1983)&lt;/span&gt;). In the second step, each completed data set is analysed using the same complete data method that would be used in the absence of missingness. Finally, in the last step, standard procedures should be used to combine the compelted data inferences into a single one. The simplest and most popular method for combining the reuslts of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets is known as &lt;em&gt;Rubin’s rules&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Rubin (2004)&lt;/span&gt;), which can be explained with a simple example.&lt;/p&gt;
&lt;div id=&#34;rubins-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rubin’s rules&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_h\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(h=1,\ldots,H\)&lt;/span&gt;, be the completed data estimates and sampling variances for a scalar estimand &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, calculated from &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; repeated imputations under a given imputation model. Then, according to Rubin’s rules, the combined estimate is simply the average of the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data estimates, that is&lt;/p&gt;
&lt;p&gt;\[
\bar{\theta}_{H}=\frac{1}{H}\sum_{h=1}^{H}\hat{\theta}_{h}.
\]&lt;/p&gt;
&lt;p&gt;Because the imputations under MI are conditional draws, under a good imputaton model, they provide valid estimates for a wide range of estimands. In addition, the averaging over &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; imputed data sets increases the efficiency of estimation over that obtained from a single completed data set. The variability associated with the pooled estimate has two components: the &lt;em&gt;average within-imputation variance&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\bar{V}_H\)&lt;/span&gt; and the &lt;em&gt;between-imputation variance&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(B_H\)&lt;/span&gt;, defined as&lt;/p&gt;
&lt;p&gt;\[
\bar{V}_{H}=\frac{1}{H}\sum_{h=1}^{H}V_{h} \;\;\; \text{and} \;\;\; B_{H}=\frac{1}{H-1}\sum_{h=1}^{H}(\hat{\theta}_{h}-\bar{\theta}_{H})^2.
\]&lt;/p&gt;
&lt;p&gt;The total variability associated with &lt;span class=&#34;math inline&#34;&gt;\(\bar{\theta}_H\)&lt;/span&gt; is the computed as&lt;/p&gt;
&lt;p&gt;\[
T_{H}=\bar{V}_H + \frac{H+1}{H}B_{H},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\((1+\frac{1}{H})\)&lt;/span&gt; is an adjustment factor for finite due to estimating &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\bar{\theta}_H\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\lambda}_H=(1+\frac{1}{H})\frac{B_H}{T_H}\)&lt;/span&gt; is known as the &lt;em&gt;fraction of missing information&lt;/em&gt; and is an estimate of the fraction of information about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that is missing due to nonresponse. For large sample sizes and scalar quantities like &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the reference distribution for interval estimates and significance tests is a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution&lt;/p&gt;
&lt;p&gt;\[
(\theta - \bar{\theta}_H)\frac{1}{\sqrt{T^2_H}} \sim t_v,
\]&lt;/p&gt;
&lt;p&gt;where the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be approximated with the quantity &lt;span class=&#34;math inline&#34;&gt;\(v=(H-1)\left(1+\frac{1}{H+1}\frac{\bar{V}_H}{B_H} \right)^2\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Rubin and Schenker (1987)&lt;/span&gt;). In small data sets, an improved version of &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be obtained as &lt;span class=&#34;math inline&#34;&gt;\(v^\star=(\frac{1}{v}+\frac{1}{\hat{v}_{obs}})^{-1}\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;\[
\hat{v}_{obs}=(1-\hat{\lambda}_{H})\left(\frac{v_{com}+1}{v_{com}+3}\right)v_{com},
\]&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(v_{com}\)&lt;/span&gt; being the degrees of freedom for appropriate or exact &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; inferences about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; when there are no missing values (&lt;span class=&#34;citation&#34;&gt;Barnard and Rubin (1999)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The validity of MI rests on how the imputations are created and how that procedure relates to the model used to subsequently analyze the data. Creating MIs often requires special algorithms (&lt;span class=&#34;citation&#34;&gt;Schafer (1997)&lt;/span&gt;). In general, they should be drawn from a distribution for the missing data that reflects uncertainty about the parameters of the data model. Recall that with SI methods, it is desirable to impute from the conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(p(y_{mis}\mid y_{obs},\hat{\theta})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is an estimate derived from the observed data. MI extends this approach by first simulating &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; independent plausible values for the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_1,\ldots,\theta_H\)&lt;/span&gt; and then drawing the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}^h\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(p(y_{mis}\mid y_{obs}, \theta_h)\)&lt;/span&gt;. Treating parameters as random rather than fixed is
an essential part of MI. For this reason, it is natural (but not essential) to motivate MI from the Bayesian perspective, in which the state of knowledge about parameters is represented through a posterior distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;joint-multiple-imputation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Joint Multiple Imputation&lt;/h2&gt;
&lt;p&gt;Joint MI starts from the assumption that the data can be described by a multivariate distribution which in many cases, mostly for practical reasons, corresponds to assuming a multivariate Normal distribution. The general idea is that, for a general missing data pattern $ r$, missingness may occur anywhere in the multivariate outcome vector $ y=(y_1,,y_J)$, so that the distribution from which imputations should be drawn varies based on the observed variables in each pattern. For example, given $ r=(0,0,1,1)$, then imputations should be drawn from the bivariate distribution of the missing variables given the observed variables in that pattern, that is from &lt;span class=&#34;math inline&#34;&gt;\(f(y^{mis}_1,y^{mis}_2 \mid y^{obs}_3, y^{obs}_4, \phi_{12})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\phi_{12}\)&lt;/span&gt; is the probability of being in pattern $ r$ where the first two variables are missing.&lt;/p&gt;
&lt;p&gt;Consider the multivariate Normal distribution &lt;span class=&#34;math inline&#34;&gt;\(y \sim N(\mu,\Sigma)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\Sigma)\)&lt;/span&gt; represent the vector of the parameters of interest which need to be identified. Indeed, for non-monotone missing data, $ $ cannot be generally identified based on the observed data directly $ y^{obs}$, and the typical solution is to iterate imputation and parameter estimation using a general algorithm known as &lt;em&gt;data augmentation&lt;/em&gt;(&lt;span class=&#34;citation&#34;&gt;Tanner and Wong (1987)&lt;/span&gt;). Following &lt;span class=&#34;citation&#34;&gt;Van Buuren (2018)&lt;/span&gt;, the general procedure of the algorithm can be summarised as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Define some plausible starting values for all parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=(\mu_0,\Sigma_0)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At each iteration &lt;span class=&#34;math inline&#34;&gt;\(t=1,\ldots,T\)&lt;/span&gt;, draw &lt;span class=&#34;math inline&#34;&gt;\(h=1,\ldots,H\)&lt;/span&gt; imputations for each missing value from the predictive distribution of the missing data given the observed data and the current value of the parameters at &lt;span class=&#34;math inline&#34;&gt;\(t-1\)&lt;/span&gt;, that is&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\hat{y}^{mis}_{t} \sim p(y^{mis} \mid y^{obs},\theta_{t-1})
\]&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Re-estimate the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; using the observed and imputed data at &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; based on the multivariate Normal model, that is&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\hat{\theta}_{t} \sim p(\theta \mid y^{obs}, \hat{y}^{mis}_{t})
\]&lt;/p&gt;
&lt;p&gt;And reiterate the steps 2 and 3 until convergence, where the stopping rule typically consists in imposing that the change in the parameters between iterations &lt;span class=&#34;math inline&#34;&gt;\(t-1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; should be smaller than a predefined “small” threshold &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. &lt;span class=&#34;citation&#34;&gt;Schafer (1997)&lt;/span&gt; showed that imputations generated under the multivariate Normal model can be robust to non-normal data, even though it is generally more efficient to transform the data towards normality, especially when the parameters of interest are difficult to estimate, such as quantiles and variances.&lt;/p&gt;
&lt;p&gt;The multivariate Normal model is also often applied to categorical data, with different types of specifications that have been proposed in the literature (&lt;span class=&#34;citation&#34;&gt;Schafer (1997)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Horton, Lipsitz, and Parzen (2003)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Allison (2005)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Bernaards, Belin, and Schafer (2007)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Yucel, He, and Zaslavsky (2008)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Demirtas (2009)&lt;/span&gt;). For examples, missing data in contingency tables can be imputed using log-linear models (&lt;span class=&#34;citation&#34;&gt;Schafer (1997)&lt;/span&gt;); mixed continuous-categorical data can be imputed under the general location model which combines a log-linear and multivariate Normal model (&lt;span class=&#34;citation&#34;&gt;Olkin, Tate, and others (1961)&lt;/span&gt;); two-way imputation can be applied to missing test item responses by imputing missing categorical data by conditioning on the row and column sum scores of the multivariate data (&lt;span class=&#34;citation&#34;&gt;Van Ginkel et al. (2007)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-allison2005imputation&#34;&gt;
&lt;p&gt;Allison, Paul D. 2005. “Imputation of Categorical Variables with Proc Mi.” &lt;em&gt;SUGI 30 Proceedings&lt;/em&gt; 113 (30): 1–14.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-barnard1999miscellanea&#34;&gt;
&lt;p&gt;Barnard, John, and Donald B Rubin. 1999. “Miscellanea. Small-Sample Degrees of Freedom with Multiple Imputation.” &lt;em&gt;Biometrika&lt;/em&gt; 86 (4): 948–55.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bernaards2007robustness&#34;&gt;
&lt;p&gt;Bernaards, Coen A, Thomas R Belin, and Joseph L Schafer. 2007. “Robustness of a Multivariate Normal Approximation for Imputation of Incomplete Binary Data.” &lt;em&gt;Statistics in Medicine&lt;/em&gt; 26 (6): 1368–82.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-carpenter2012multiple&#34;&gt;
&lt;p&gt;Carpenter, James, and Michael Kenward. 2012. &lt;em&gt;Multiple Imputation and Its Application&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-demirtas2009rounding&#34;&gt;
&lt;p&gt;Demirtas, Hakan. 2009. “Rounding Strategies for Multiply Imputed Binary Data.” &lt;em&gt;Biometrical Journal: Journal of Mathematical Methods in Biosciences&lt;/em&gt; 51 (4): 677–88.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-herzog1983using&#34;&gt;
&lt;p&gt;Herzog, Thomas N, and Donald B Rubin. 1983. “Using Multiple Imputations to Handle Nonresponse in Sample Surveys.” &lt;em&gt;Incomplete Data in Sample Surveys&lt;/em&gt; 2: 209–45.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-horton2003potential&#34;&gt;
&lt;p&gt;Horton, Nicholas J, Stuart R Lipsitz, and Michael Parzen. 2003. “A Potential for Bias When Rounding in Multiple Imputation.” &lt;em&gt;The American Statistician&lt;/em&gt; 57 (4): 229–32.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-molenberghs2014handbook&#34;&gt;
&lt;p&gt;Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. &lt;em&gt;Handbook of Missing Data Methodology&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-olkin1961multivariate&#34;&gt;
&lt;p&gt;Olkin, Ingram, Robert Fleming Tate, and others. 1961. “Multivariate Correlation Models with Mixed Discrete and Continuous Variables.” &lt;em&gt;The Annals of Mathematical Statistics&lt;/em&gt; 32 (2): 448–65.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1978multiple&#34;&gt;
&lt;p&gt;Rubin, Donald B. 1978. “Multiple Imputations in Sample Surveys a Phenomenological Bayesian Approach to Nonresponse.” &lt;em&gt;Proceedings of the Survey Research Methods Section of the American Statistical Association&lt;/em&gt; 1: 20–34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1996multiple&#34;&gt;
&lt;p&gt;———. 1996. “Multiple Imputation After 18 Years.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 91 (434): 473–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin2004multiple&#34;&gt;
&lt;p&gt;———. 2004. &lt;em&gt;Multiple Imputation for Nonresponse in Surveys&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1987interval&#34;&gt;
&lt;p&gt;Rubin, Donald B, and Nathaniel Schenker. 1987. “Interval Estimation from Multiply Imputed Data: A Case Study Using Census Agriculture Industry Codes.” &lt;em&gt;Journal of Official Statistics&lt;/em&gt; 3 (4): 375.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer1997analysis&#34;&gt;
&lt;p&gt;Schafer, Joseph L. 1997. &lt;em&gt;Analysis of Incomplete Multivariate Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer1999multiple&#34;&gt;
&lt;p&gt;———. 1999. “Multiple Imputation: A Primer.” &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt; 8 (1): 3–15.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tanner1987calculation&#34;&gt;
&lt;p&gt;Tanner, Martin A, and Wing Hung Wong. 1987. “The Calculation of Posterior Distributions by Data Augmentation.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 82 (398): 528–40.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2018flexible&#34;&gt;
&lt;p&gt;Van Buuren, Stef. 2018. &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2007two&#34;&gt;
&lt;p&gt;Van Ginkel, Joost R, L Andries Van der Ark, Klaas Sijtsma, and Jeroen K Vermunt. 2007. “Two-Way Imputation: A Bayesian Method for Estimating Missing Scores in Tests and Questionnaires, and an Accurate Approximation.” &lt;em&gt;Computational Statistics &amp;amp; Data Analysis&lt;/em&gt; 51 (8): 4013–27.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-yucel2008using&#34;&gt;
&lt;p&gt;Yucel, Recai M, Yulei He, and Alan M Zaslavsky. 2008. “Using Calibration to Improve Rounding in Imputation.” &lt;em&gt;The American Statistician&lt;/em&gt; 62 (2): 125–29.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Likelihood Based Inference with Incomplete Data</title>
      <link>/missmethods/likelihood-based-methods-ignorable3/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/likelihood-based-methods-ignorable3/</guid>
      <description>


&lt;p&gt;As for the inference under complete data, inference under incomplete data consists in deriving the likelihood for the parameters based on the available data, either using a &lt;em&gt;Maximum Likelihood&lt;/em&gt; (ML) approach (solving the likelihood equation) or using the &lt;em&gt;Bayes’ rule&lt;/em&gt; incorporating a prior distribution (performing necessary integrations to obtain the posterior distribution). However, asymptotic standard errors obtained from the information matrix, are more questionable when dealing with missing data since the sample will not be typically iid and results that imply the large sample normality of the likelihood function do not immediately apply. More complications arise when dealing with the process that lead to some of the data to be missing. This can be explained with a simple example.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Y=(y_{ij})\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots,J\)&lt;/span&gt;, denote the complete dataset if there were no missing values, with a total of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; units and &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; variables. Let &lt;span class=&#34;math inline&#34;&gt;\(M=(m_{ij})\)&lt;/span&gt; denote the fully observed matrix of binary missing data indicators with &lt;span class=&#34;math inline&#34;&gt;\(m_{ij}=1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt; is missing and &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; otherwise. As an example, we can model the density of the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; using the &lt;em&gt;selection model factorisation&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;\[
p(Y=y,M=m \mid \theta, \psi) = f(y \mid \theta)f(m \mid y, \psi),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the parameter vector indexing the response model and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; is the parameter vector indexing the missingness mechanism. The observed values &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; effect a partition &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,y_0)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(y_0=[y_{ij} : m_{ij}=0]\)&lt;/span&gt; is the observed component and &lt;span class=&#34;math inline&#34;&gt;\(y_1=[y_{ij} : m_{ij}=1]\)&lt;/span&gt; is the missing component of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. The full likelihood based on the observed data and the assumed model is&lt;/p&gt;
&lt;p&gt;\[
L_{full}(\theta, \psi \mid y_{0},m) = \int f\left(y_{0},y_{1} \mid \theta \right) f\left(m \mid y_{0},y_{1}, \psi \right)dy_{1}
\]&lt;/p&gt;
&lt;p&gt;and is a function of the parameters &lt;span class=&#34;math inline&#34;&gt;\((\theta,\psi)\)&lt;/span&gt;. Next, we define the likelihood of ignoring the missingness mechanism or &lt;em&gt;ignorable likelihood&lt;/em&gt; as&lt;/p&gt;
&lt;p&gt;\[
L_{ign}\left(\theta \mid y_{0} \right) = \int f(y_{0},y_{1}\mid \theta)dy_{1},
\]&lt;/p&gt;
&lt;p&gt;which does not involve the model for &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;. In practice, modelling the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; is often challenging and, in fact, many approaches to missing data do not model &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and (explicitly or implicitly) base inference about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; on the ignorable likelihood. It is therefore important to assess under which conditions inferences about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; based on &lt;span class=&#34;math inline&#34;&gt;\(L_{ign}\)&lt;/span&gt; can be considered appropriate. More specifically, the missingness mechanism is said to be &lt;em&gt;ignorable&lt;/em&gt; if inferences about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; based on the ignorable likelihood equation evauluated at some realisations of &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; are the same as inferences about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; based on the full likelihood equation, evaluated at the same realisations of &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;. The conditions for ignoring the missingness mechanism depend on whether the inferences are direct likelihood, Bayesian or frequentist.&lt;/p&gt;
&lt;div id=&#34;direct-likelihood-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Direct Likelihood Inference&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Direct Likelihood Inference&lt;/em&gt; refers to inference based solely on likelihood ratios for pair of values of the parameters, with the data fixed at their observed values. The missingness mechanism can be ignored for direct likelihood if the likelihood ratio based on the ignorable likelihood is the same as the ratio based on the full likelihood. More precisely, the missingness mechanism is said to be ignorable for direct likelihood inference at some realisations of &lt;span class=&#34;math inline&#34;&gt;\((y_0,m)\)&lt;/span&gt; if the likelihood ratio for two values &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta^\star\)&lt;/span&gt; is the same whether based on the full or ignorable likelihood. That is&lt;/p&gt;
&lt;p&gt;\[
\frac{L_{full}\left( \theta, \psi \mid y_{0}, m \right)}{L_{full}\left( \theta^{\star}, \psi \mid y_{0}, m \right)}=\frac{L_{ign}\left( \theta \mid y_{0} \right)}{L_{ign}\left( \theta^{\star} \mid y_{0}\right)},
\]&lt;/p&gt;
&lt;p&gt;for all &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta^\star\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;. In general, the missingnes mechanism is ignorable for direct likelihood inference if the following two conditions hold:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Parameter distinctness. The parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; are distinct, in the sense that the joint parameter space &lt;span class=&#34;math inline&#34;&gt;\(\Omega_{\theta,\psi}\)&lt;/span&gt; is the product of the two parameter spaces &lt;span class=&#34;math inline&#34;&gt;\(\Omega_{\theta}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Omega_{\psi}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Factorisation of the full likelihood. The full likelihood factors as&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
L_{full}\left(\theta, \psi \mid y_{0},m \right) = L_{ign}\left(\theta \mid y_{0} \right) L_{rest}\left(\psi \mid y_{0},m \right)
\]&lt;/p&gt;
&lt;p&gt;for all values of &lt;span class=&#34;math inline&#34;&gt;\(\theta,\psi \in \Omega_{\theta,\psi}\)&lt;/span&gt;. The distinctness condition ensures that each value of &lt;span class=&#34;math inline&#34;&gt;\(\psi \in \Omega_{\psi}\)&lt;/span&gt; is compatible with different values of &lt;span class=&#34;math inline&#34;&gt;\(\theta \in \Omega_{\theta}\)&lt;/span&gt;. A sufficient condition for the factorisation of the full likelihood is that the missing data are &lt;em&gt;Missing At Random&lt;/em&gt;(MAR) at the specific realisations of &lt;span class=&#34;math inline&#34;&gt;\(y_{0},m\)&lt;/span&gt;. This means that the distribution function of &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;, evaluated at the given realisations &lt;span class=&#34;math inline&#34;&gt;\((y_{0},m)\)&lt;/span&gt;, does not depend on the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
f\left(m \mid y_{0}, y_{1}, \psi \right)=f\left(m \mid y_{0}, y^{\star}_{1} \psi \right),
\]&lt;/p&gt;
&lt;p&gt;for all &lt;span class=&#34;math inline&#34;&gt;\(y_{1},y^\star_{1},\psi\)&lt;/span&gt;. Thus, we have&lt;/p&gt;
&lt;p&gt;\[
f\left(y_{0}, m \mid \theta, \psi \right) = f\left(m \mid y_{0}, \psi \right) \int f\left(y_{0},y_{1} \mid \theta \right)dy_{1} = f\left(m \mid y_{0}, \psi \right) f\left( y_{0} \mid \theta \right).
\]&lt;/p&gt;
&lt;p&gt;From this it follows that, if the missing data are MAR at the given realisations of &lt;span class=&#34;math inline&#34;&gt;\((y_{0},m)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; are distinct, the missingnes mechanism is ignorable for likelihood inference.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian Inference&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Bayesian Inference&lt;/em&gt; under the full model for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; requires that the full likelihood is combined with a prior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\theta,\psi)\)&lt;/span&gt; for the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;\[
p\left(\theta, \psi \mid y_{0}, m \right) \propto p(\theta, \psi) L_{full}\left(\theta, \psi \mid y_{0}, m \right).
\]&lt;/p&gt;
&lt;p&gt;Bayesian inference ignoring the missingness mechanism combines the ignorable likelihood with a prior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; alone, that is&lt;/p&gt;
&lt;p&gt;\[
p(\theta \mid y_{0}) \propto p(\theta) L_{ign}\left(\theta \mid y_{0} \right).
\]&lt;/p&gt;
&lt;p&gt;More formally, the missingness mechanism is said to be ignorable for Bayesian inference at the given realisations of &lt;span class=&#34;math inline&#34;&gt;\((y_{0},m)\)&lt;/span&gt; if the posterior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; based on the posterior distribution for the full likelihood and prior distribution for &lt;span class=&#34;math inline&#34;&gt;\((\theta,\psi)\)&lt;/span&gt; is the same as the posterior distribution for the ignorable likelihood and the prior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; alone. This holds when the following conditions are satisfied:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; are &lt;em&gt;a priori&lt;/em&gt; independent, that is the prior distribution has the form&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
p(\theta , \psi) = p(\theta) p(\psi)
\]&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The full likelihood evaluated at the realisations of &lt;span class=&#34;math inline&#34;&gt;\((y_{0},m)\)&lt;/span&gt; factors as for direct likelihood inference&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Under these conditions:&lt;/p&gt;
&lt;p&gt;\[
p(\theta, \psi \mid y_{0}, m) \propto \left(p(\theta)L_{ign}\left( \theta \mid y_{0} \right) \right) \left(p(\psi)L_{rest}\left(\psi \mid y_{0},m \right) \right).
\]&lt;/p&gt;
&lt;p&gt;As for direct likelihood inference, MAR is a sufficient condition for the factorisation of the full likelihood. This means that, if the data are MAR at the given realisations of &lt;span class=&#34;math inline&#34;&gt;\((y_{0},m)\)&lt;/span&gt; and the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; are a prior independent, then the missingness mechanism is ignorable for Bayesian inference. We note that the a priori condition is more stringent than the distinctness condition because paramerers with distinct parameter spaces might have dependent prior distributions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;frequentist-asymptotic-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Frequentist Asymptotic Inference&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Frequentist Asymptotic Inference&lt;/em&gt; requires that, in order to ignore the missingness mechanism, the factorisation of the full likelihood needs to be valid for values of the observed data under repeated sampling. This means that we require&lt;/p&gt;
&lt;p&gt;\[
L_{full}\left(\theta,\psi \mid y_{0}, m \right) = L_{ign}\left(\theta \mid y_{0} \right) L_{rest}\left(\psi \mid y_{0}, m \right)
\]&lt;/p&gt;
&lt;p&gt;for all &lt;span class=&#34;math inline&#34;&gt;\(y_{0},m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta,\psi \in \Omega_{\theta,\psi}\)&lt;/span&gt;. For this form of inference, a sufficient condition for ignoring the missingness mechanism is given by the following conditions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Parameter distinctness as defined for direct likelihood inference.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Missing data are &lt;em&gt;Missing Always At Random&lt;/em&gt; (MAAR), that is&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
f\left(m \mid y_{0},y_{1},\psi \right) = f\left(m \mid y_{0}, y^{\star}_{1},\psi \right)
\]&lt;/p&gt;
&lt;p&gt;for all &lt;span class=&#34;math inline&#34;&gt;\(m,y_{0},y_{1},y^\star_{1},\psi\)&lt;/span&gt;. In the following example we discuss conditions for ignoring the missingness mechanism for direct likelihood and Bayesian inference, which can be extended to the case of frequentist asymptotic inference by requiring that they hold for for values of &lt;span class=&#34;math inline&#34;&gt;\(y_{0},m\)&lt;/span&gt; other than those observed that could arise in repeated sampling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-normal-sample-with-one-variable-subject-to-missingness&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bivariate Normal Sample with One Variable Subject to Missingness&lt;/h2&gt;
&lt;p&gt;Consider a bivariate normal sample &lt;span class=&#34;math inline&#34;&gt;\(y=(y_{i1},y_{i2})\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; units, but with the values of &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt; being missing for &lt;span class=&#34;math inline&#34;&gt;\(i=(n_{cc}+1),\ldots,n\)&lt;/span&gt;. This leads to a monotone missing data pattern with two variables. The loglikelihood of ignoring the missingness mechanism is&lt;/p&gt;
&lt;p&gt;\[
l_{ign}\left(\mu, \Sigma \mid y_{0} \right) = \log\left(L_{ign}\left(\mu,\Sigma \mid y_{0} \right) \right) = - \frac{1}{2}n_{cc}ln \mid \Sigma \mid - \frac{1}{2}\sum_{i=1}^{n_{cc}}(y_i - \mu ) \Sigma^{-1}(y_i - \mu)^{T} - \frac{1}{2}(n-n_{cc})ln\sigma_{1} - \frac{1}{2}\sum_{i=n_{cc}+1}^{n}\frac{(y_{i1}-\mu_1)^2}{\sigma_{1}}.
\]&lt;/p&gt;
&lt;p&gt;This loglikelihood is appropriate for inference provided the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; does not depend on the values of &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\Sigma)\)&lt;/span&gt; is distinct from &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;. Under these conditions, ML estimates of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; can be found by maximising this loglikelihood. For Bayesian inference, if these conditions hold and the prior distribution for &lt;span class=&#34;math inline&#34;&gt;\((\theta,\psi)\)&lt;/span&gt; has the form &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)p(\psi)\)&lt;/span&gt;, then the joint posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is proportional to the product of &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(L_{ign}(\theta \mid y_{0})\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Likelihood Based Inference with Incomplete Data (Nonignorable)</title>
      <link>/missmethods/likelihood-based-methods-nonignorable/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/likelihood-based-methods-nonignorable/</guid>
      <description>


&lt;p&gt;In many cases, analysis methods for missing data are based on the ignorable likelihood&lt;/p&gt;
&lt;p&gt;\[
L_{ign}\left(\theta \mid Y_0, X \right) \propto f\left(Y_0 \mid X, \theta \right),
\]&lt;/p&gt;
&lt;p&gt;regarded as a function of the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; for fixed observed data &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt; and some fully observed covariates &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The density &lt;span class=&#34;math inline&#34;&gt;\(f(Y_0 \mid X, \theta)\)&lt;/span&gt; is obtained by integrating out the missing data &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; from the joint density &lt;span class=&#34;math inline&#34;&gt;\(f(Y \mid X, \theta)=f(Y_0,Y_1\mid X, \theta)\)&lt;/span&gt;. Sufficient conditions for basing inference about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; on the ignorbale likelihood are that the missingness mechanism is &lt;em&gt;Missing At Random&lt;/em&gt;(MAR) and the parameters of the model of analysis &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and those of the missingness mechanism &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; are distinct. Here we focus our attention on the situations where the missingness mechanism is &lt;em&gt;Missing Not At Random&lt;/em&gt;(MNAR) and valid &lt;em&gt;Maximum Likelihood&lt;/em&gt;(ML), &lt;em&gt;Bayesian&lt;/em&gt; and &lt;em&gt;Multiple Imputation&lt;/em&gt;(MI) inferences generally need to be based on the full likelihood&lt;/p&gt;
&lt;p&gt;\[
L_{full}\left(\theta, \psi \mid Y_0, X, M \right) \propto f\left(Y_0, M \mid X, \theta, \psi \right),
\]&lt;/p&gt;
&lt;p&gt;regarded as a function of &lt;span class=&#34;math inline&#34;&gt;\((\theta,\psi)\)&lt;/span&gt; for fixed &lt;span class=&#34;math inline&#34;&gt;\((Y_0,M)\)&lt;/span&gt;. Here, &lt;span class=&#34;math inline&#34;&gt;\(f(Y_0,M\mid \theta, \psi)\)&lt;/span&gt; is obtained by integrating out &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; from the joint density &lt;span class=&#34;math inline&#34;&gt;\(f(Y,M \mid X, \theta, \psi)\)&lt;/span&gt;. Two main approaches for formulating MNAR models can be distinguished, namely &lt;em&gt;selection models&lt;/em&gt;(SM) and &lt;em&gt;pattern mixture models&lt;/em&gt;(PMM).&lt;/p&gt;
&lt;div id=&#34;selection-and-pattern-mixture-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selection and Pattern Mixture Models&lt;/h2&gt;
&lt;p&gt;SMs factor the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;\[
f(m_i,y_i \mid x_i, \theta, \psi) = f(y_i \mid x_i, \theta)f(m_i \mid x_i,y_i,\psi),
\]&lt;/p&gt;
&lt;p&gt;where the first factor is the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in the population while the second factor is the missingness mechanism, with &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; which are assumed to be distinct. Alternatively, PMMs factor the joint distribution as&lt;/p&gt;
&lt;p&gt;\[
f(m_i,y_i \mid x_i, \theta, \psi) = f(y_i \mid x_i, m_i,\xi)f(m_i \mid x_i),
\]&lt;/p&gt;
&lt;p&gt;where the first factor is the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in the strata defined by different patterns of missingness &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; while the second factor models the probabilities of the different patterns, with &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; which are assumed to be distinct (&lt;span class=&#34;citation&#34;&gt;Little (1993)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;). The distinction between the two factorisations becomes clearer when considering a specific example.&lt;/p&gt;
&lt;p&gt;Suppose thta missing values are confined to a single variable and let &lt;span class=&#34;math inline&#34;&gt;\(y_i=(y_{i,1},y_{i2})\)&lt;/span&gt; be a bivariate response outcome where &lt;span class=&#34;math inline&#34;&gt;\(y_{i1}\)&lt;/span&gt; is fully observed and &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt; is observed for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n_{cc}\)&lt;/span&gt; but missing for &lt;span class=&#34;math inline&#34;&gt;\(i=n_{cc}+1,\ldots,n\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(m_{i2}\)&lt;/span&gt; be the missingness indicator for &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt;, then a PMM factors the denisty of &lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;\[
f(y_0, M \mid X, \xi)=\prod_{i=1}^{n_{cc}}f(y_{i1},y_{i2}\mid x_i, m_{i2}=0,\xi)Pr(m_{i2}=0 \mid x_i, \omega) \times \prod_{i=n_{cc}+1}^{n}f(y_{i1} \mid x_i, m_{i2}=1,\xi)Pr(m_{i2}=1 \mid x_i, \omega).
\]&lt;/p&gt;
&lt;p&gt;This expression shows that there are no data with which to estimate directly the distribution &lt;span class=&#34;math inline&#34;&gt;\(f(y_{i2} \mid x_i, m_{i2}=1,\xi)\)&lt;/span&gt;, because all units with &lt;span class=&#34;math inline&#34;&gt;\(m_{i2}=1\)&lt;/span&gt; have &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt; missing. Under MAR, this is identified using the distribution of the observed data &lt;span class=&#34;math inline&#34;&gt;\(f(y_{i2} \mid x_i, m_{i2}=1,\xi)=f(y_{i2} \mid x_i, m_{i2}=0,\xi)\)&lt;/span&gt;, while under MNAR it must be identified using other assumptions. The SM formulation is&lt;/p&gt;
&lt;p&gt;\[
f(y_i, m_{i2} \mid \theta, \psi) = f(y_{i1} \mid x_i, \theta)f(y_{i2} \mid x_i, y_{i1},\theta)f(m_{i2}\mid x_i,y_{i1},y_{i2},\psi).
\]&lt;/p&gt;
&lt;p&gt;Typically, the missingness mechanism &lt;span class=&#34;math inline&#34;&gt;\(f(m_{i2} \mid x_i,y_{i1},y_{i2},\psi)\)&lt;/span&gt; is modelled using some additive probit or logit regression of &lt;span class=&#34;math inline&#34;&gt;\(m_{i2}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;,&lt;span class=&#34;math inline&#34;&gt;\(y_{i1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt;. However, the coefficient of &lt;span class=&#34;math inline&#34;&gt;\(y_{i2}\)&lt;/span&gt; in this regression is not directly estimable from the data and hence the model cannot be fully estimated without extra assumptions.&lt;/p&gt;
&lt;div id=&#34;normal-models-for-mnar-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Normal Models for MNAR data&lt;/h3&gt;
&lt;p&gt;Assume we have a complete sample &lt;span class=&#34;math inline&#34;&gt;\((y_i,x_i)\)&lt;/span&gt; on a continuous variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and a set of fully observed covariates &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt;. Suppose that &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n_{cc}\)&lt;/span&gt; units are observed while the remaining &lt;span class=&#34;math inline&#34;&gt;\(i=n_{cc}+1,\ldots,n\)&lt;/span&gt; units are missing, with &lt;span class=&#34;math inline&#34;&gt;\(m_i\)&lt;/span&gt; being the corresponding missingness indicator. Heckman (&lt;span class=&#34;citation&#34;&gt;Heckman (1976)&lt;/span&gt;) proposed the following selection model to handle missingness:&lt;/p&gt;
&lt;p&gt;\[
y_i \mid x_i, \theta, \psi \sim N(\beta_0 + \beta_1x_i, \sigma^2) \;\;\; \text{and} \;\;\; m_i \mid x_i,y_i,\theta,\psi \sim Bern\left(\Phi(\psi_0 + \psi_1x_i + \psi_2y_i) \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\beta_0,\beta_1,\sigma^2)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; denotes the probit (cumulative normal) distribution function. Note that if &lt;span class=&#34;math inline&#34;&gt;\(\psi_2=0\)&lt;/span&gt;, the missing data are MAR, while if &lt;span class=&#34;math inline&#34;&gt;\(\psi_2 \neq 0\)&lt;/span&gt; the missing data are MNAR since missingness in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; depends on the unobserved value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. This model can be estimated using either a two-step least squares method, ML in combination with an EM algorithm, or a Bayesian approach. The main issue is the lack of information about &lt;span class=&#34;math inline&#34;&gt;\(\psi_2\)&lt;/span&gt;, which can be partly identified through the specific assumptions about the distribution of the observed data of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. This, however, makes the implicit assumption that the assumed distribution can well described the distribution of the complete (observed and missing) data which can never be tested or checked. An alternative approach is to use a PMM factorisation and model:&lt;/p&gt;
&lt;p&gt;\[
y_i \mid m_i=m,x_i,\xi,\omega \sim N(\beta_0^m + \beta_1^mx_i, \sigma^{2m})\;\;\; \text{and} \;\;\; m_i \mid x_i,\xi,\omega \sim Bern\left(\Phi(\omega_0 + \omega_1x_i) \right),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\xi=(\beta_0^m,\beta_1^m,\sigma^{2m},\;\;\; m=0,1)\)&lt;/span&gt;. This model implies that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in the population is a mixture of two normal distributions with mean&lt;/p&gt;
&lt;p&gt;\[
\left[1 - \Phi(\omega_0 + \omega_1x_i) \right] \left[\beta_0^0 + \beta_1^0 x_i \right] + \left[\Phi(\omega_0 + \omega_1x_i) \right] \left[\beta_0^1 + \beta_1^1 x_i \right].
\]&lt;/p&gt;
&lt;p&gt;The parameters &lt;span class=&#34;math inline&#34;&gt;\((\beta_0^0,\beta_1^0,\sigma^{20},\omega)\)&lt;/span&gt; can be estimated from the data but the parameters &lt;span class=&#34;math inline&#34;&gt;\((\beta_0^1,\beta_1^1,\sigma^{21})\)&lt;/span&gt; are not estimable because &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is missing when &lt;span class=&#34;math inline&#34;&gt;\(m_i=1\)&lt;/span&gt;. Under MAR, the distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the same for units with &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; observed and missing, such that &lt;span class=&#34;math inline&#34;&gt;\(\beta_0^0=\beta_0^1=\beta_0\)&lt;/span&gt; (as well as for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;). Under MNAR, other assumptions are needed to esitmate the parameters indexed by &lt;span class=&#34;math inline&#34;&gt;\(m=1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Some final considerations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Both SM and PMM model the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The SM formulation is more natural when the substantive interest concerns the relationship between &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; in the population. However, these parameters can also be derived in PMM by averaging the patterns specific parameters over the missingness patterns.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The PMM factorisation is more transparent in terms of the underlying assumptions about the unidentified parameters of the model, while SM tends to impose some obscure constraints in order to identify these parameters, which are also difficult to interpret.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Given specific assumptions to identify all the parameters in the model, PMMs are often easier to fit than SMs. In addition, imputations of the missing values are based on the predictive distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M=0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These considerations seem to favour PMM over SM as MNAR approaches, especially when considering &lt;em&gt;sensitivity analysis&lt;/em&gt;. Bayesian approaches can also be used to identify these models, by assigning prior distributions which can be used to identify those parameters which cannot be estimated from the data. Justifications for the choice of these priors are therefore necessary to ensure the plausibility of the assumptions assessed and the impact of these assumptions on the posterior inference.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-heckman1976common&#34;&gt;
&lt;p&gt;Heckman, James J. 1976. “The Common Structure of Statistical Models of Truncation, Sample Selection and Limited Dependent Variables and a Simple Estimator for Such Models.” In &lt;em&gt;Annals of Economic and Social Measurement, Volume 5, Number 4&lt;/em&gt;, 475–92. NBER.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little1993pattern&#34;&gt;
&lt;p&gt;Little, Roderick JA. 1993. “Pattern-Mixture Models for Multivariate Incomplete Data.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 88 (421): 125–34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multiple Imputation by Chained Equations</title>
      <link>/missmethods/multiple-imputation-by-chained-equations/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/multiple-imputation-by-chained-equations/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Multiple Imputation&lt;/em&gt;(MI) refers to the procedure of replacing each missing value by a set of &lt;span class=&#34;math inline&#34;&gt;\(H\geq 2\)&lt;/span&gt; imputed values. These are ordered in the sense that &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets can be created from the sets of imputations, where the first imputed value replaces the missing value in the first completed data set, the second imputed value in the second completed data set, and so on. Next, standard complete data methods are used to analyse each completed data set. When the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; sets of imputations are repeated random draws from the predictive distribution of the missing data under a particular model of missingness, the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences can be combined to form one inference that properly reflects uncertainty due to missing values under that model. In general, MI procedures can be summarised in three main steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Specify an &lt;strong&gt;imputation model&lt;/strong&gt; to generate &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; imputed values, typically taken as random draws from the predictive distribution of the missing values given the observed values, and create &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets using these imputations and the observed data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Analyse each completed data sets using standard complete data methods based on an &lt;strong&gt;analysis model&lt;/strong&gt;, and derive &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pool together the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data inferences into a single inference using standard MI formulas, which ensure that missing data uncertainty is taken into account&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mi was first proposed by Rubin (&lt;span class=&#34;citation&#34;&gt;Rubin (1978)&lt;/span&gt;) and has become more popular over time (&lt;span class=&#34;citation&#34;&gt;Rubin (1996)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;), as well as the focus of research for methodological and practical applications in a variety of fields (&lt;span class=&#34;citation&#34;&gt;Herzog and Rubin (1983)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Rubin and Schenker (1987)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Schafer (1999)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Carpenter and Kenward (2012)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Molenberghs et al. (2014)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Van Buuren (2018)&lt;/span&gt;). MI shares both advantages of &lt;em&gt;Single Imputaiton&lt;/em&gt; (SI) methods and solves both disadvantages. Indeed, like SI, MI methods allow the analyst to use familiar complete data methods when analysing the completed data sets. The only disadvantage of MI compared with SI methods is that it takes more time to generate the imputations and analyse the completed data sets. However, &lt;span class=&#34;citation&#34;&gt;Rubin (2004)&lt;/span&gt; showed that in order to obtain sufficiently precise estimates, a relatively small number of imputations (typically &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt;) is required. For example, considering a situation with &lt;span class=&#34;math inline&#34;&gt;\(\lambda=50\%\)&lt;/span&gt; missing information and &lt;span class=&#34;math inline&#34;&gt;\(H=10\)&lt;/span&gt; imputations, the efficiency of MI can be shown to be equal to &lt;span class=&#34;math inline&#34;&gt;\((1+\frac{\lambda}{H})^{-1}=95\%\)&lt;/span&gt;. In addition, in today’s computing environments, the work of analysing the completed data sets is quite modest since it involves performing the same task &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; times. Thus, once a precedure to combine multiple completed data sets is established, the additonal time and effort to handle &lt;span class=&#34;math inline&#34;&gt;\(50\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(20\)&lt;/span&gt;, or &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt; imputations if often of little consequence.&lt;/p&gt;
&lt;p&gt;In the first step of MI, imputations should ideally be created as repeated draws from the &lt;em&gt;posterior predictive distribution&lt;/em&gt; of the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}\)&lt;/span&gt; given the observed values &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}\)&lt;/span&gt;, each repetition being an independent drawing of the parameters and missing values. In practice, implicit imputation models can also be used in place of explicit imputation models (&lt;span class=&#34;citation&#34;&gt;Herzog and Rubin (1983)&lt;/span&gt;). In the second step, each completed data set is analysed using the same complete data method that would be used in the absence of missingness. Finally, in the last step, standard procedures should be used to combine the compelted data inferences into a single one. The simplest and most popular method for combining the reuslts of &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data sets is known as &lt;em&gt;Rubin’s rules&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Rubin (2004)&lt;/span&gt;), which can be explained with a simple example.&lt;/p&gt;
&lt;div id=&#34;rubins-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rubin’s rules&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_h\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(h=1,\ldots,H\)&lt;/span&gt;, be the completed data estimates and sampling variances for a scalar estimand &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, calculated from &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; repeated imputations under a given imputation model. Then, according to Rubin’s rules, the combined estimate is simply the average of the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; completed data estimates, that is&lt;/p&gt;
&lt;p&gt;\[
\bar{\theta}_{H}=\frac{1}{H}\sum_{h=1}^{H}\hat{\theta}_{h}.
\]&lt;/p&gt;
&lt;p&gt;Because the imputations under MI are conditional draws, under a good imputaton model, they provide valid estimates for a wide range of estimands. In addition, the averaging over &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; imputed data sets increases the efficiency of estimation over that obtained from a single completed data set. The variability associated with the pooled estimate has two components: the &lt;em&gt;average within-imputation variance&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\bar{V}_H\)&lt;/span&gt; and the &lt;em&gt;between-imputation variance&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(B_H\)&lt;/span&gt;, defined as&lt;/p&gt;
&lt;p&gt;\[
\bar{V}_{H}=\frac{1}{H}\sum_{h=1}^{H}V_{h} \;\;\; \text{and} \;\;\; B_{H}=\frac{1}{H-1}\sum_{h=1}^{H}(\hat{\theta}_{h}-\bar{\theta}_{H})^2.
\]&lt;/p&gt;
&lt;p&gt;The total variability associated with &lt;span class=&#34;math inline&#34;&gt;\(\bar{\theta}_H\)&lt;/span&gt; is the computed as&lt;/p&gt;
&lt;p&gt;\[
T_{H}=\bar{V}_H + \frac{H+1}{H}B_{H},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\((1+\frac{1}{H})\)&lt;/span&gt; is an adjustment factor for finite due to estimating &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\bar{\theta}_H\)&lt;/span&gt;. Thus, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\lambda}_H=(1+\frac{1}{H})\frac{B_H}{T_H}\)&lt;/span&gt; is known as the &lt;em&gt;fraction of missing information&lt;/em&gt; and is an estimate of the fraction of information about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that is missing due to nonresponse. For large sample sizes and scalar quantities like &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the reference distribution for interval estimates and significance tests is a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution&lt;/p&gt;
&lt;p&gt;\[
(\theta - \bar{\theta}_H)\frac{1}{\sqrt{T^2_H}} \sim t_v,
\]&lt;/p&gt;
&lt;p&gt;where the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be approximated with the quantity &lt;span class=&#34;math inline&#34;&gt;\(v=(H-1)\left(1+\frac{1}{H+1}\frac{\bar{V}_H}{B_H} \right)^2\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Rubin and Schenker (1987)&lt;/span&gt;). In small data sets, an improved version of &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; can be obtained as &lt;span class=&#34;math inline&#34;&gt;\(v^\star=(\frac{1}{v}+\frac{1}{\hat{v}_{obs}})^{-1}\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;\[
\hat{v}_{obs}=(1-\hat{\lambda}_{H})\left(\frac{v_{com}+1}{v_{com}+3}\right)v_{com},
\]&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(v_{com}\)&lt;/span&gt; being the degrees of freedom for appropriate or exact &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; inferences about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; when there are no missing values (&lt;span class=&#34;citation&#34;&gt;Barnard and Rubin (1999)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The validity of MI rests on how the imputations are created and how that procedure relates to the model used to subsequently analyze the data. Creating MIs often requires special algorithms (&lt;span class=&#34;citation&#34;&gt;Schafer (1997)&lt;/span&gt;). In general, they should be drawn from a distribution for the missing data that reflects uncertainty about the parameters of the data model. Recall that with SI methods, it is desirable to impute from the conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(p(y_{mis}\mid y_{obs},\hat{\theta})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; is an estimate derived from the observed data. MI extends this approach by first simulating &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; independent plausible values for the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_1,\ldots,\theta_H\)&lt;/span&gt; and then drawing the missing values &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}^h\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(p(y_{mis}\mid y_{obs}, \theta_h)\)&lt;/span&gt;. Treating parameters as random rather than fixed is
an essential part of MI. For this reason, it is natural (but not essential) to motivate MI from the Bayesian perspective, in which the state of knowledge about parameters is represented through a posterior distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-imputation-by-chained-equations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Imputation by Chained Equations&lt;/h2&gt;
&lt;p&gt;MI by Chained Equations, also known as &lt;em&gt;Fully Conditional Specification&lt;/em&gt;(FCS), imputes multivariate missing data on a variable-by-variable basis, and therefore requires the specification of an imputation model for each incomplete variable to create imputations per variable in an iterative fashion (&lt;span class=&#34;citation&#34;&gt;Van Buuren (2007)&lt;/span&gt;). In contrast to Joint MI, MICE specifies the multivariate distribution for the outcome and missingness pattern &lt;span class=&#34;math inline&#34;&gt;\(p(y,r\mid \theta, \phi)\)&lt;/span&gt;, indexed by the parameter vectors of the outcome (&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) and missingness models (&lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;), through a set of conditional densities &lt;span class=&#34;math inline&#34;&gt;\(p(y_j \mid y_{-j},r,\theta_j, \phi_j)\)&lt;/span&gt;, which is used to impute &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; given the other variables. Starting from a random draw from the marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;, imputation is then carried out by iterating over the conditionally specified imputation models for each &lt;span class=&#34;math inline&#34;&gt;\(y_j=(y_2,\ldots,y_J)\)&lt;/span&gt; separately given the set of all the other variables &lt;span class=&#34;math inline&#34;&gt;\(y_{-j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Tha main idea of MICE is to directly draw the missing data from the predictive distribution of conditional densities, therefore avoiding the need to specify a joint multivariate model for all the data. Different approaches can be used to implement MICE. For example, a possible strategy is the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start at iteration &lt;span class=&#34;math inline&#34;&gt;\(t=0\)&lt;/span&gt; by drawing randomly from the the distribution of the missing data given the observed data and all other variables, according to some probability model for each variable &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;, that is&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\hat{y}^{mis}_{j,0} \sim p(y^{mis}_{j} \mid y^{obs}_{j}, y_{-j}, r)
\]&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;At each iteration &lt;span class=&#34;math inline&#34;&gt;\(t=1,\ldots,T\)&lt;/span&gt; and for each variable &lt;span class=&#34;math inline&#34;&gt;\(j=\ldots,J\)&lt;/span&gt;, set&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\hat{y}^{mis}_{-j,t}=\left(\hat{y}_{1,t},\ldots, \hat{y}_{j-1,t}, \hat{y}_{j+1,t}, \ldots, \hat{y}_{J,t} \right)
\]&lt;/p&gt;
&lt;p&gt;as the currently completed data except &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Draw &lt;span class=&#34;math inline&#34;&gt;\(h=1,\ldots,H\)&lt;/span&gt; imputations for each variable &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; from the predictive distribution of the missing data given the observed data and the currently imputed data at &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, that is&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\[
\hat{y}^{mis}_{j,t} \sim p(y^{mis}_{j} \mid y^{obs}_{j}, \hat{y}_{-j,t}, r)
\]&lt;/p&gt;
&lt;p&gt;and repeat the steps 2 and 3 until convergence. It is important to stress out that MICE is essentially a &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt;(MCMC) algorithm (&lt;span class=&#34;citation&#34;&gt;Brooks et al. (2011)&lt;/span&gt;), where the state space is the collection of all imputed values. More specifically, when the conditional distributions of all variables are compatible with a joint multivariate distribution, the algorithm corresponds to a Gibbs sampler, a Bayesian simulation method that samples from the conditional distributions in order to obtain samples from the joint multivariate distribution of all variables via some conditional factorisation of the latter (&lt;span class=&#34;citation&#34;&gt;Casella and George (1992)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Gilks, Richardson, and Spiegelhalter (1996)&lt;/span&gt;). A potential issue of MICE is that, since the conditional distributions are specified freely by the user, these may not be compatible with a joint distribution and therefore it is not clear from which distribution the algorithm is sampling from. However, a general advatage of MICE is that it gives freedom to the user for the specification of the univariate models for the variables, which can be tailored to handle different types of variabes (e.g. continuous and categorical) and different statistical issues for each variable (e.g. skewness and non-liner associations).&lt;/p&gt;
&lt;p&gt;Regardless of the theoretical implications of MICE, as a MCMC method, the algorithm converges to a stationary distribution when three conditions are satisfied (&lt;span class=&#34;citation&#34;&gt;Roberts (1996)&lt;/span&gt;,&lt;span class=&#34;citation&#34;&gt;Brooks et al. (2011)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The chain is &lt;em&gt;irreducible&lt;/em&gt;, i.e. must be able to reach any state from any state in the state space&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The chain is &lt;em&gt;aperiodic&lt;/em&gt;, i.e. must be able to return to each state after some unknown number of steps or transitions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The chain is &lt;em&gt;recurrent&lt;/em&gt;, i.e. there is probability of one of eventually returning to each state after some number of steps&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Typically periodicity and non-recurrence can be a problem in MICE when the imputation models are not compatible, possibly leading to different inferences based on the stopping point of the chain or to non-stationary behaviours of the chain.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-barnard1999miscellanea&#34;&gt;
&lt;p&gt;Barnard, John, and Donald B Rubin. 1999. “Miscellanea. Small-Sample Degrees of Freedom with Multiple Imputation.” &lt;em&gt;Biometrika&lt;/em&gt; 86 (4): 948–55.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brooks2011handbook&#34;&gt;
&lt;p&gt;Brooks, Steve, Andrew Gelman, Galin Jones, and Xiao-Li Meng. 2011. &lt;em&gt;Handbook of Markov Chain Monte Carlo&lt;/em&gt;. CRC press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-carpenter2012multiple&#34;&gt;
&lt;p&gt;Carpenter, James, and Michael Kenward. 2012. &lt;em&gt;Multiple Imputation and Its Application&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-casella1992explaining&#34;&gt;
&lt;p&gt;Casella, George, and Edward I George. 1992. “Explaining the Gibbs Sampler.” &lt;em&gt;The American Statistician&lt;/em&gt; 46 (3): 167–74.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gilks1996introducing&#34;&gt;
&lt;p&gt;Gilks, Walter R, Sylvia Richardson, and David J Spiegelhalter. 1996. “Introducing Markov Chain Monte Carlo.” &lt;em&gt;Markov Chain Monte Carlo in Practice&lt;/em&gt; 1: 19.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-herzog1983using&#34;&gt;
&lt;p&gt;Herzog, Thomas N, and Donald B Rubin. 1983. “Using Multiple Imputations to Handle Nonresponse in Sample Surveys.” &lt;em&gt;Incomplete Data in Sample Surveys&lt;/em&gt; 2: 209–45.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-molenberghs2014handbook&#34;&gt;
&lt;p&gt;Molenberghs, Geert, Garrett Fitzmaurice, Michael G Kenward, Anastasios Tsiatis, and Geert Verbeke. 2014. &lt;em&gt;Handbook of Missing Data Methodology&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-roberts1996markov&#34;&gt;
&lt;p&gt;Roberts, Gareth O. 1996. “Markov Chain Concepts Related to Sampling Algorithms.” &lt;em&gt;Markov Chain Monte Carlo in Practice&lt;/em&gt; 57.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1978multiple&#34;&gt;
&lt;p&gt;Rubin, Donald B. 1978. “Multiple Imputations in Sample Surveys a Phenomenological Bayesian Approach to Nonresponse.” &lt;em&gt;Proceedings of the Survey Research Methods Section of the American Statistical Association&lt;/em&gt; 1: 20–34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1996multiple&#34;&gt;
&lt;p&gt;———. 1996. “Multiple Imputation After 18 Years.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 91 (434): 473–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin2004multiple&#34;&gt;
&lt;p&gt;———. 2004. &lt;em&gt;Multiple Imputation for Nonresponse in Surveys&lt;/em&gt;. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1987interval&#34;&gt;
&lt;p&gt;Rubin, Donald B, and Nathaniel Schenker. 1987. “Interval Estimation from Multiply Imputed Data: A Case Study Using Census Agriculture Industry Codes.” &lt;em&gt;Journal of Official Statistics&lt;/em&gt; 3 (4): 375.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer1997analysis&#34;&gt;
&lt;p&gt;Schafer, Joseph L. 1997. &lt;em&gt;Analysis of Incomplete Multivariate Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer1999multiple&#34;&gt;
&lt;p&gt;———. 1999. “Multiple Imputation: A Primer.” &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt; 8 (1): 3–15.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2007multiple&#34;&gt;
&lt;p&gt;Van Buuren, Stef. 2007. “Multiple Imputation of Discrete and Continuous Data by Fully Conditional Specification.” &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt; 16 (3): 219–42.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2018flexible&#34;&gt;
&lt;p&gt;———. 2018. &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonignorable Missingness Models in HTA</title>
      <link>/project/missing-data/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/missing-data/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Economic evaluation alongside Randomised Clinical Trials (RCTs) is an important and increasingly popular component of the process of technology appraisal. The typical analysis of individual level data involves the comparison of two interventions for which suitable measures of clinical benefits and costs are observed on each patient enrolled in the trial at different time points throughout the follow up. Individual level data from RCTs are almost invariably affected by missingness. The recorded outcome process is often incomplete due to individuals who drop out or are observed intermittently throughout the study, causing some observations to be missing. In most applications, the economic evaluation is performed on the cross-sectional variables, computed using only the data from the individuals who are observed at each time point in the trial (completers), with at most limited sensitivity analysis to missingness assumptions. This, however, is an extremely inefficient approach as the information from the responses of all partially observed subjects is completely lost and it is also likely biased unless the completers are a random sample of the subjects on each arm. The problem of missingness is often embedded within a more complex framework, which makes the modelling task in economic evaluations particularly challenging. Specifically, the effectiveness and cost data typically present a series of complexities that need to be simultaneously addressed to avoid biased results.&lt;/p&gt;

&lt;p&gt;Using a recent randomised trial as our motivating example, we present a Bayesian parametric model for conducting inference on a bivariate health economic longitudinal response. We specify our model to account for the different types of complexities affecting the data while accommodating a sensitivity analysis to explore the impact of alternative missingness assumptions on the inferences and on the decision-making process for health technology assessment.&lt;/p&gt;

&lt;h1 id=&#34;standard-approach&#34;&gt;Standard approach&lt;/h1&gt;

&lt;p&gt;To perform the economic evaluation, aggregated measures for both utilities and costs are typically derived from the longitudinal responses recorded in the study. QALYs ($e_{it}$) and total costs ($c_{it}$) measures are computed as:&lt;/p&gt;

&lt;p&gt;\[
e_{it}=\sum_{j=1}^{J}(u_{ijt}+u_{ij-1t})\frac{\delta_{j}}{2} \;\;\; \text{and} \;\;\;\  c_{it}=\sum_{j=1}^{J}c_{ijt},
\]&lt;/p&gt;

&lt;p&gt;where $t$ denotes the treatment group, while $\delta_{j}=\frac{\text{Time}_{j}-\text{Time}_{j-1}}{\text{Unit of time}}$ is the percentage of the time unit (typically one year) which is covered between time $j-1$ and $j$ in the trial. The economic evaluation is then performed by applying some parametric model $p(e_{it},c_{it}\mid \boldsymbol \theta)$, indexed by a set of parameters $\boldsymbol \theta$, to these cross-sectional quantities, typically using linear regression methods to account for the imbalance in some baseline variables between treatments. We note that the term cross-sectional here refers to analyses based on variables derived from the combination of repeated measurements collected at different times over the trial duration and not on data collected at a single point in time.
Finally, QALYs and total costs population mean values are derived from the model:&lt;/p&gt;

&lt;p&gt;\[
\mu_{et} = \text{E}\left(e_{it} \mid \boldsymbol \theta\right) \;\;\; \text{and} \;\;\; \mu_{ct} = \text{E}\left(c_{it} \mid \boldsymbol \theta \right).
\]&lt;/p&gt;

&lt;p&gt;The differences in $\mu_{et}$ and $\mu_{ct}$ between the treatment groups represent the quantities of interest in the economic evaluation and are used in assessing the relative cost-effectiveness of the interventions. This modelling approach has the limitation that $\mu_{et}$ and $\mu_{ct}$ are derived based only on the completers in the study and does not assess the robustness of the results to a range of plausible missingness assumptions. The model also fails to account for the different complexities that affect the utility and cost data in the trial: from the correlation between variables to the skewness and the presence of structural values (zero for the costs and one for the utilities) in both outcomes.&lt;/p&gt;

&lt;h1 id=&#34;longitudinal-model-to-deal-with-missingness&#34;&gt;Longitudinal model to deal with missingness&lt;/h1&gt;

&lt;p&gt;We propose an alternative approach to deal with a missing bivariate outcome in economic evaluations, while simultaneously allowing for the different complexities that typically affect utility and cost data. Our approach includes a longitudinal model that improves the current practice by taking into account the information from all observed data as well as the time dependence between the responses.&lt;/p&gt;

&lt;p&gt;Let $\boldsymbol u_i=(u_{i0},\ldots,u_{iJ})$ and $\boldsymbol c_i=(c_{i0},\ldots,c_{iJ})$ denote the vectors of utilities and costs that were supposed to be observed for subject $i$ at time $j$ in the study, with $j \in {0,1,J}$. We denote with $\boldsymbol y_{ij}=(u_{ij},c_{ij})$ the bivariate outcome for subject $i$ formed by the utility and cost pair at time $j$. We group the individuals according to the missingness patterns and denote with $\boldsymbol r_{ij}=(r^u_{ij},r^c_{ij})$ a pair of indicator variables that take value $1$ if the corresponding outcome for subject $i$ at time $j$ is observed and $0$ otherwise. We denote with $\boldsymbol r_i = (\boldsymbol r_{i0}, \ldots, \boldsymbol r_{iJ})$ the missingness pattern to which subject $i$ belongs, where each pattern is associated with different values for $\boldsymbol r_{ij}$.&lt;/p&gt;

&lt;p&gt;We then define our modelling strategy and factor the joint distribution for the response and missingness as:&lt;/p&gt;

&lt;p&gt;\[
p(\boldsymbol y, \boldsymbol r \mid \boldsymbol \omega) = p(\boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r \mid  \boldsymbol \omega)p(\boldsymbol y^{\boldsymbol r}_{mis} \mid \boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r, \boldsymbol \omega)
\]&lt;/p&gt;

&lt;p&gt;where $\boldsymbol y^{\boldsymbol r}_{obs}$ and $\boldsymbol y^{\boldsymbol r}_{mis}$ indicate the observed and missing responses within pattern $\boldsymbol r$, respectively. This is the extrapolation &lt;em&gt;factorisation&lt;/em&gt; and factors the joint into two components, of which the extrapolation &lt;em&gt;distribution&lt;/em&gt; $p(\boldsymbol y^{\boldsymbol r}_{mis} \mid \boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r, \boldsymbol \omega)$ remains unidentified by the data in the absence of unverifiable assumptions about the full data.&lt;/p&gt;

&lt;p&gt;To specify the observed data distribution $p(\boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r \mid  \boldsymbol \omega)$ we use a working model $p^{\star}$ for the joint distribution of the response and missingness. Essentially, the idea is to use the working model $p^{\star}(\boldsymbol y, \boldsymbol{r} \mid  \boldsymbol \omega)$ to draw inferences about the distribution of the observed data $p(\boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r \mid \boldsymbol \omega)$ by integrating out the missing responses:&lt;/p&gt;

&lt;p&gt;\[
p(\boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r \mid \boldsymbol \omega) = \int p^{\star}(\boldsymbol y, \boldsymbol{r} \mid  \boldsymbol \omega)d \boldsymbol y^{\boldsymbol r}_{mis}.
\]&lt;/p&gt;

&lt;p&gt;This approach avoids direct specification of the joint distribution of the observed and missing data $p(\boldsymbol y, \boldsymbol r\mid \boldsymbol \omega)$, which has the undesirable consequence of identifying the extrapolation distribution with assumptions that are difficult to check. Indeed, since we use $p^{\star}(\boldsymbol y, \boldsymbol r \mid  \boldsymbol \omega)$ only to obtain a model for $p(\boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r \mid  \boldsymbol \omega)$ and not as a basis for inference, the extrapolation distribution is left unidentified. Any inference depending on the observed data distribution may be obtained using the working model as the true model, with the advantage that  it is often easier to specify a model for the the full data $p(\boldsymbol y,\boldsymbol r)$ compared with a model  for the observed data $p(\boldsymbol y^{\boldsymbol r}_{obs},\boldsymbol r)$.&lt;/p&gt;

&lt;p&gt;We specify $p^{\star}$ using a pattern mixture approach, factoring the joint $p(\boldsymbol y,\boldsymbol r \mid \boldsymbol \omega)$ as the product between the marginal distribution of the missingness patterns $p(\boldsymbol r\mid \boldsymbol \psi)$ and the distribution of the response conditional on the patterns $p(\boldsymbol y\mid \boldsymbol r,\boldsymbol \theta)$, respectively indexed by the distinct parameter vectors $\boldsymbol \psi$ and $\boldsymbol \theta$. If missingness is monotone it is possible to summarise the patterns by dropout time and directly model the dropout process. Unfortunately, as it often occurs in trial-based health economic data, missingness in the case study is mostly nonmonotone and the sparsity of the data in most patterns makes it infeasible to fit the response model within each pattern, with the exception of the completers ($\boldsymbol r = \boldsymbol 1$). Thus, we decided to collapse together all the non-completers patterns ($\boldsymbol r \neq \boldsymbol 1$) and fit the model separately to this aggregated pattern and to the completers. The joint distribution has three components. The first is given by the model for the patterns and the model for the completers ($\boldsymbol r = \boldsymbol 1$), where no missingness occurs. The second component is a model for the observed data in the collapsed patterns $\boldsymbol r \neq \boldsymbol 1$ that, together with the first component, form the observed data distribution. The last component is the extrapolation distribution.&lt;/p&gt;

&lt;p&gt;Because the targeted quantities of interest can be derived based on the marginal utility and cost means at each time $j$, in our analysis we do not require the full identification of $p(\boldsymbol y^{\boldsymbol r}_{mis} \mid \boldsymbol y^{\boldsymbol r}_{obs},  \boldsymbol r,\boldsymbol \xi)$. Instead, we only partially identify the extrapolation distribution using &lt;em&gt;partial identifying restrictions&lt;/em&gt;. Specifically, we only require the identification of the marginal means for the missing responses in each pattern. We identify the marginal mean of $\boldsymbol y^{\boldsymbol r}_{mis}$ using the observed values, averaged across $\boldsymbol r^\prime \neq \boldsymbol 1$, and some &lt;em&gt;sensitivity parameters&lt;/em&gt; $\boldsymbol \Delta = (\Delta_u,\Delta_c)$. Therefore, we compute the marginal means by averaging only across the observed components in pattern ${\boldsymbol r}^\prime$ and ignore the components that are missing.&lt;/p&gt;

&lt;p&gt;We start by setting a benchmark assumption with $\boldsymbol \Delta = \boldsymbol 0$, and then explore the sensitivity of the results to alternative scenarios by using different prior distributions on $\boldsymbol \Delta$, calibrated on the observed data. This provides a convenient benchmark scenario from which departures can be explored using alternative informative priors on $\boldsymbol \Delta$. Once the working model has been fitted to the observed data and the extrapolation distribution has been identified, the overall marginal mean for the response model can be computed by marginalising over $\boldsymbol r$, i.e. $\text{E}\left[\boldsymbol Y\right] = \sum_{\boldsymbol r} p(\boldsymbol r)\text{E}\left[\boldsymbol Y \mid \boldsymbol r \right]$.&lt;/p&gt;

&lt;h2 id=&#34;modelling-framework&#34;&gt;Modelling framework&lt;/h2&gt;

&lt;p&gt;The distribution of the observed responses $\boldsymbol y_{ijt}=(u_{ijv},c_{ijt})$ is specified in terms of a model for the utility and cost variables at time $j=0,1,2$, which are jointly modelled without using a multilevel approach and separately by treatment group. In particular, the joint distribution for $\boldsymbol y_{ijt}$ is specified as a series of conditional distributions that capture the dependence between utilities and costs as well as the time dependence.&lt;/p&gt;

&lt;p&gt;Following the recommendations from the published literature, we account for the skewness using Beta and Log-Normal distributions for the utilities and costs, respectively. Since the Beta distribution does not allow for negative values, we scaled the utilities on $[0,1]$ through the transformation $u^{\star}_{ij}=\frac{u_{ij}-\text{min}(\boldsymbol u_{j})}{\text{max}(\boldsymbol u_{j})-\text{min}(\boldsymbol u_{j})}$, and fit the model to these transformed variables. To account for the structural values $u_{ij} = 1$ and $c_{ij} = 0$ we use a hurdle approach by including in the model the indicator variables $d^u_{ij}:=\mathbb{I}(u_{ij}=1)$ and $d^c_{ij}:=\mathbb{I}(c_{ij}=0)$, which take value $1$ if subject $i$ is associated with a structural value at time $j$ and 0 otherwise. The probabilities of observing these values, as well as the mean of each variable, are then modelled conditionally on other variables via linear regressions defined on the logit or log scale. Specifically, at time $j=1,2$, the probability of observing a zero and the mean costs are modelled conditionally on the utilities and costs at the previous times, while the probability of observing a one and the mean utilities are modelled conditionally on the current costs (also at $j=0$) and the utilities at the previous times (only at $j=1,2$). The model is summarised by the following Figure.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/missing_model.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Longitudinal model for missingness.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;We use partial identifying restrictions to link the observed data distribution $p(\boldsymbol y_{obs},\boldsymbol r)$ to the extrapolation distribution $p(\boldsymbol y_{mis} \mid \boldsymbol y_{obs},\boldsymbol r)$ and consider interpretable deviations from a benchmark scenario to assess how inferences are driven by our assumptions. Specifically, we identify the marginal mean of the missing responses in each pattern $\boldsymbol y^{\boldsymbol r}_{mis}$ by averaging across the corresponding components that are observed and add the sensitivity parameters $\boldsymbol \Delta_j$.&lt;/p&gt;

&lt;p&gt;We define $\boldsymbol \Delta_j=(\Delta_{c_{j}},\Delta_{u_{j}})$ to be time-specific location shifts at the marginal mean in each pattern and set $\boldsymbol \Delta_j = \boldsymbol 0$ as the benchmark scenario. We then explore departures from this benchmark using alternative priors on $\boldsymbol \Delta_j$, which are calibrated using the observed standard deviations for costs and utilities at each time $j$ to define the amplitude of the departures from $\boldsymbol \Delta_j=\boldsymbol 0$.&lt;/p&gt;

&lt;h1 id=&#34;conlcusions&#34;&gt;Conlcusions&lt;/h1&gt;

&lt;p&gt;Missingness represents a threat to economic evaluations as, when dealing with partially-observed data, any analysis makes assumptions about the missing values that cannot be verified from the data at hand. Trial-based analyses are typically conducted on cross-sectional quantities, e.g. QALYs and total costs, which are derived based only on the observed data from the completers in the study. This is an inefficient approach which may discard a substantial proportion of the sample, especially when there is a relatively large number of time points, where individuals are more likely to have some missing value or to drop out from the study. In addition, when there are systematic differences between the responses of the completers and non-completers, which is typically the case when dealing with self-reported outcomes in trial-based analyses, the results based only on the former may be biased and mislead the final assessment. A further concern is that routine analyses typically rely on standard models that ignore or at best fail to properly account for potentially important features in the data such as correlation, skewness, and the presence of structural values.&lt;/p&gt;

&lt;p&gt;Our framework represents a considerable step forward for the handling of missingness in economic evaluations compared with the current practice, which typically relies on methods that assume an ignorable MAR and rarely conducts sensitivity analysis to MNAR departures. Nevertheless, further improvements are certainly possible. For example, a potential area for future work is to increase the flexibility of our approach through a semi-parametric or nonparametric specification for the observed data distribution, which would allow a weakening of the model assumptions and likely further improve the fit of the model to the observed data and address sparse patterns in an automated way. As for the extrapolation distribution, alternative identifying restrictions that introduce the sensitivity parameters via the conditional mean (rather than the marginal mean) could be considered, and their impact on the conclusions assessed in a sensitivity analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pattern Mixture Models</title>
      <link>/missmethods/pattern-mixture-models/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/pattern-mixture-models/</guid>
      <description>


&lt;p&gt;It is possible to summarise the steps involved in drawing inference from incomplete data as (&lt;span class=&#34;citation&#34;&gt;Daniels and Hogan (2008)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Specification of a full data model for the response and missingness indicators &lt;span class=&#34;math inline&#34;&gt;\(f(y,r)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Specification of the prior distribution (within a Bayesian approach)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sampling from the posterior distribution of full data parameters, given the observed data &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt; and the missingness indicators &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Identification of a full data model, particularly the part involving the missing data &lt;span class=&#34;math inline&#34;&gt;\(Y_{mis}\)&lt;/span&gt;, requires making unverifiable assumptions about the full data model &lt;span class=&#34;math inline&#34;&gt;\(f(y,r)\)&lt;/span&gt;. Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{mis}\)&lt;/span&gt;, given &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt;. Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Unverifiable parametric assumptions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Informative prior distributions (under a Bayesian approach)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We show some simple examples about how these &lt;em&gt;nonignorable&lt;/em&gt; models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as &lt;em&gt;Pattern Mixture Models&lt;/em&gt;(PMM).&lt;/p&gt;
&lt;div id=&#34;pattern-mixture-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pattern Mixture Models&lt;/h2&gt;
&lt;p&gt;The pattern mixture model approach factors the full data distribution as&lt;/p&gt;
&lt;p&gt;\[
f(y,r \mid \omega) = f(y \mid r, \phi) f(r \mid y,\chi),
\]&lt;/p&gt;
&lt;p&gt;where it is typically assumed that the set of full data parameters &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; can be decomposed as separate parameters for each factor &lt;span class=&#34;math inline&#34;&gt;\((\phi,\chi)\)&lt;/span&gt;. Thus, under the PMM approach, the &lt;em&gt;response model&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(y \mid \theta)\)&lt;/span&gt; can be retrieved as a mixture of the pattern specific distributions&lt;/p&gt;
&lt;p&gt;\[
f(y \mid \theta) = \sum_{r}f(y \mid r, \phi)f(r \mid \chi),
\]&lt;/p&gt;
&lt;p&gt;with weights given by the corresponding probabilities of the different patterns. The &lt;em&gt;missingness mechanism&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(r \mid y, \psi)\)&lt;/span&gt; can also be obtained using Bayes’ rule&lt;/p&gt;
&lt;p&gt;\[
f(y \mid r, \psi) = \frac{f(y \mid r, \phi)f(r\mid \chi)}{f(y \mid \theta)}.
\]&lt;/p&gt;
&lt;p&gt;The construction of PMMs requires the specification of the full data distribution conditional on different missingness patterns, which may be cumbersome when the number of patterns is large, but with the advantage of making explicit the parameters that cannot be identified by the observed data. In particular, PMMs are well suited to show that the distribution of the response within each pattern can be decomposed as&lt;/p&gt;
&lt;p&gt;\[
f(y_{obs},y_{mis} \mid r, \phi)= f(y_{mis} \mid y_{obs},r,\phi_{E})f(y_{obs}\mid r,\phi_{O}),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\phi_E = \lambda_1(\phi)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\phi_O=\lambda_2(\phi)\)&lt;/span&gt; are functions of the mixture component parameter &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. The former subset of parameters indexes the so called &lt;em&gt;extrapolation distribution&lt;/em&gt; and cannot be identified from the data, i.e. the distribution of the missing values given the observed values, while the latter indexes the &lt;em&gt;observed data distribution&lt;/em&gt; and is typically identifiable from the data. Assuming there exists a partition such that &lt;span class=&#34;math inline&#34;&gt;\(\phi_E=(\phi_{EI},\phi_{ENI})\)&lt;/span&gt; and the observed data distribution is a function of &lt;span class=&#34;math inline&#34;&gt;\(\phi_{EI}\)&lt;/span&gt; but not of &lt;span class=&#34;math inline&#34;&gt;\(\phi_{ENI}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\phi_{ENI}\)&lt;/span&gt; is a &lt;em&gt;senstivity parameter&lt;/em&gt; in that it can only be identified using information from sources other than the observed data and thus makes a suitable basis to formulate sensitivity analysis using informative priors.&lt;/p&gt;
&lt;div id=&#34;example-of-pmm-for-bivariate-normal-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example of PMM for bivariate normal data&lt;/h3&gt;
&lt;p&gt;Consider a sample of &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; units from a bivariate normal distribution &lt;span class=&#34;math inline&#34;&gt;\(Y=(Y_1,Y_2)\)&lt;/span&gt;. Assume also that &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; is always observed while &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt; may be missing, and let &lt;span class=&#34;math inline&#34;&gt;\(R=R_2\)&lt;/span&gt; be the missingness indicator for the partially-observed response &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt;. A PMM factors the full data distribution as&lt;/p&gt;
&lt;p&gt;\[
f(y_1,y_2,r \mid \omega) = f(y_1, y_2 \mid r, \phi)f(r \mid ,\chi),
\]&lt;/p&gt;
&lt;p&gt;where, for example, we may have &lt;span class=&#34;math inline&#34;&gt;\(Y \mid R=1 \sim N(\mu^1,\Sigma^1)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y \mid R=0 \sim N(\mu^0,\Sigma^0)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R \sim Bern(\chi)\)&lt;/span&gt;. We define &lt;span class=&#34;math inline&#34;&gt;\(\mu^r=(\mu^r_1)\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^r\)&lt;/span&gt; has elements &lt;span class=&#34;math inline&#34;&gt;\(\sigma^r=(\sigma^r_{11},\sigma^r_{12},\sigma^r_{22})\)&lt;/span&gt;. Similarly, we can define the parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta^r_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta^r_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^r_{2\mid 1}\)&lt;/span&gt; as the intercept, slope and residual variance of the regression of &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; for each pattern &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;. Under this reparameterisation, the full data model parameters are&lt;/p&gt;
&lt;p&gt;\[
\phi=\{\mu^r_1,\sigma^r_{11},\beta^r_0,\beta^1_1,\sigma^r_{2\mid 1}\}.
\]&lt;/p&gt;
&lt;p&gt;The extrapolation and observed data distributions, with associated parameters, are then&lt;/p&gt;
&lt;p&gt;\[
f(y_{mis}\mid y_{obs},\phi_{E}) \rightarrow \phi_{E}=(\beta^0_0, \beta^0_1,\sigma^0_{2\mid1})
\]&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;\[
f(y_{obs}\mid \phi_{O}) \rightarrow \phi_{O}=(\mu^1,\beta^1,\sigma^1_{11},\mu^0_0,\sigma^1_{11}).
\]&lt;/p&gt;
&lt;p&gt;It can be shown that, in this specific example, the observed data distribution does not depend on the parameters indexing the extrapolation distribtuon &lt;span class=&#34;math inline&#34;&gt;\(\phi_{ENI}=(\beta^0_0,\beta^0_1,\sigma^0_{2\mid 1})\)&lt;/span&gt;. It is possible to set &lt;span class=&#34;math inline&#34;&gt;\(\beta^0=\beta=1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^0_{2\mid1}=\sigma^1_{2\mid1}\)&lt;/span&gt; to yield a &lt;em&gt;Missing At Random&lt;/em&gt;(MAR) assumption. Hence, a function that maps identified parameters and sensitivity parameters &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt; to the space of unidentified parameters can be used to quantify departures from MAR. For example, assume we impose&lt;/p&gt;
&lt;p&gt;\[
\beta^0_0=\beta^1_0+\Delta,
\]&lt;/p&gt;
&lt;p&gt;then assigning a point mass prior at &lt;span class=&#34;math inline&#34;&gt;\(\Delta=0\)&lt;/span&gt; implies MAR, while fixing &lt;span class=&#34;math inline&#34;&gt;\(\Delta \neq 0\)&lt;/span&gt; or using any type of inofrmative prior on this parameter implies a &lt;em&gt;Missing Not At Random&lt;/em&gt;(MNAR) assumption.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conlcusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conlcusions&lt;/h2&gt;
&lt;p&gt;To summarise, PMMs have the advantage of being able to find full data parameters indexing the distribution of the missing data that are not identified from the observed data, making inference more transparent. A potential downside is the practical implementation of these models which becomes more difficult as the number of patterns and unidentified parameters grows.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-daniels2008missing&#34;&gt;
&lt;p&gt;Daniels, Michael J, and Joseph W Hogan. 2008. &lt;em&gt;Missing Data in Longitudinal Studies: Strategies for Bayesian Modeling and Sensitivity Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Selection Models</title>
      <link>/missmethods/selection-models/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/selection-models/</guid>
      <description>


&lt;p&gt;It is possible to summarise the steps involved in drawing inference from incomplete data as (&lt;span class=&#34;citation&#34;&gt;Daniels and Hogan (2008)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Specification of a full data model for the response and missingness indicators &lt;span class=&#34;math inline&#34;&gt;\(f(y,r)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Specification of the prior distribution (within a Bayesian approach)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sampling from the posterior distribution of full data parameters, given the observed data &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt; and the missingness indicators &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Identification of a full data model, particularly the part involving the missing data &lt;span class=&#34;math inline&#34;&gt;\(Y_{mis}\)&lt;/span&gt;, requires making unverifiable assumptions about the full data model &lt;span class=&#34;math inline&#34;&gt;\(f(y,r)\)&lt;/span&gt;. Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{mis}\)&lt;/span&gt;, given &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt;. Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Unverifiable parametric assumptions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Informative prior distributions (under a Bayesian approach)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We show some simple examples about how these &lt;em&gt;nonignorable&lt;/em&gt; models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as &lt;em&gt;Selection Models&lt;/em&gt;(SM).&lt;/p&gt;
&lt;div id=&#34;selection-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selection Models&lt;/h2&gt;
&lt;p&gt;The selection model approach factors the full data distribution as&lt;/p&gt;
&lt;p&gt;\[
f(y,r \mid \omega) = f(y \mid \theta) f(r \mid y,\psi),
\]&lt;/p&gt;
&lt;p&gt;where it is typically assumed that the set of full data parameters &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; can be decomposed as separate parameters for each factor &lt;span class=&#34;math inline&#34;&gt;\((\theta,\psi)\)&lt;/span&gt;. Thus, under the SM approach, the &lt;em&gt;response model&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(y \mid \theta)\)&lt;/span&gt; and the &lt;em&gt;missing data mechanism&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(r \mid y, \psi)\)&lt;/span&gt; must be specified by the analyst. SMs can be attractive for several reasons, including&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The possibility to directly specify the model of interest &lt;span class=&#34;math inline&#34;&gt;\(f(y \mid \theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The SM factorisation appeals to Rubin’s missing data taxonomy, enabling easy characterisation of the missing data mechanism&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When the missingness pattern is monotone, the missigness mechanism can be formulated as a hazard function, where the hazard of dropout at some time point &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; can depend on parts of the full data vector &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;example-of-sm-for-bivariate-normal-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example of SM for bivariate normal data&lt;/h3&gt;
&lt;p&gt;Consider a sample of &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; units from a bivariate normal distribution &lt;span class=&#34;math inline&#34;&gt;\(Y=(Y_1,Y_2)\)&lt;/span&gt;. Assume also that &lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt; is always observed while &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt; may be missing, and let &lt;span class=&#34;math inline&#34;&gt;\(R=R_2\)&lt;/span&gt; be the missingness indicator for the partially-observed response &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt;. A SM factors the full data distribution as&lt;/p&gt;
&lt;p&gt;\[
f(y_1,y_2,r \mid \omega) = f(y_1 \mid \theta)f(r \mid y_1,y_2,\psi),
\]&lt;/p&gt;
&lt;p&gt;where we assume &lt;span class=&#34;math inline&#34;&gt;\(\omega=(\theta,\psi)\)&lt;/span&gt;. Suppose we specify &lt;span class=&#34;math inline&#34;&gt;\(f(y_1,y_2 \mid \theta)\)&lt;/span&gt; as a bivariate normal density with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(2\times2\)&lt;/span&gt; covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;. The distribution of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is assumed to be distributed as a Bernoulli variable with probability &lt;span class=&#34;math inline&#34;&gt;\(\pi_i\)&lt;/span&gt;, such that&lt;/p&gt;
&lt;p&gt;\[
g(\pi_i) = \psi_0 + \psi_1y_{i1} + \psi_2y_{i2},
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(g()\)&lt;/span&gt; denotes a given &lt;em&gt;link function&lt;/em&gt; which relates the expected value of the response to the linear predictors in the model. When this is taken as the inverse normal cumulative distribution function &lt;span class=&#34;math inline&#34;&gt;\(\Phi^{-1}()\)&lt;/span&gt; the model corresponds to the Heckman probit selection model (&lt;span class=&#34;citation&#34;&gt;Heckman (1976)&lt;/span&gt;). In general, setting &lt;span class=&#34;math inline&#34;&gt;\(\psi_2=0\)&lt;/span&gt; leads to a &lt;em&gt;Missing At Random&lt;/em&gt;(MAR) assumption; if, in addition, we have distinctness of the parameters &lt;span class=&#34;math inline&#34;&gt;\(f(\mu,\Sigma,\psi)=f(\mu,\Sigma)f(\psi)\)&lt;/span&gt;, we have &lt;em&gt;ignorability&lt;/em&gt;. We note that, even though the parameter &lt;span class=&#34;math inline&#34;&gt;\(\psi_2\)&lt;/span&gt; characterises the association between &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt;, the parametric assumptions made in this example will identify &lt;span class=&#34;math inline&#34;&gt;\(\psi_2\)&lt;/span&gt; even in the absence of informative priors, that is the observed data likelihood is a function of &lt;span class=&#34;math inline&#34;&gt;\(\psi_2\)&lt;/span&gt;. Moreover, the parameter indexes the joint distribution of observables &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and in general can be identified from the observed data. This property of parametric SMs make them ill-suited to assessing sensitivity to assumptions about the missingness mechanism.&lt;/p&gt;
&lt;p&gt;The model can also be generalised to longitudinal data assuming a multivariate normal distribution for &lt;span class=&#34;math inline&#34;&gt;\(Y=(Y_1,\ldots,Y_J)\)&lt;/span&gt; and replacing &lt;span class=&#34;math inline&#34;&gt;\(\pi_i\)&lt;/span&gt; with a discrete time hazard function for dropout&lt;/p&gt;
&lt;p&gt;\[
h\left(t_j \mid \bar{Y}_{j}\right) = \text{Prob}\left(R_j = 0 \mid R_{j-1} = 1, Y_{1},\ldots,Y_{j} \right).
\]&lt;/p&gt;
&lt;p&gt;Using the logit function to model the discrete time hazard in terms of observed response history &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_{j-1}\)&lt;/span&gt; and the current but possibly unobserved &lt;span class=&#34;math inline&#34;&gt;\(Y_j\)&lt;/span&gt; corresponds to the model of &lt;span class=&#34;citation&#34;&gt;Diggle and Kenward (1994)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conlcusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conlcusions&lt;/h2&gt;
&lt;p&gt;To summarise, SMs allows to generalise ignorable models to handle nonignorable missingness by letting &lt;span class=&#34;math inline&#34;&gt;\(f(r \mid y_{obs},y_{mis})\)&lt;/span&gt; to depend on &lt;span class=&#34;math inline&#34;&gt;\(y_{mis}\)&lt;/span&gt; and their structure directly appeals to Rubin’s taxonomy. However, identification of the missing data distribution is accomplished through parametric assumptions about the full data response model &lt;span class=&#34;math inline&#34;&gt;\(f(y \mid \theta)\)&lt;/span&gt; and the explicit form of the missingness mechanism. This makes it difficult to disentagle the type of information that is used to identify the model, i.e. parametric modelling assumptions or information from the observed data, therefore complicating the task of assessing the robustness of the results to a range of transparent and plausible assumptions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-daniels2008missing&#34;&gt;
&lt;p&gt;Daniels, Michael J, and Joseph W Hogan. 2008. &lt;em&gt;Missing Data in Longitudinal Studies: Strategies for Bayesian Modeling and Sensitivity Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-diggle1994informative&#34;&gt;
&lt;p&gt;Diggle, Peter, and Michael G Kenward. 1994. “Informative Drop-Out in Longitudinal Data Analysis.” &lt;em&gt;Journal of the Royal Statistical Society: Series C (Applied Statistics)&lt;/em&gt; 43 (1): 49–73.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-heckman1976common&#34;&gt;
&lt;p&gt;Heckman, James J. 1976. “The Common Structure of Statistical Models of Truncation, Sample Selection and Limited Dependent Variables and a Simple Estimator for Such Models.” In &lt;em&gt;Annals of Economic and Social Measurement, Volume 5, Number 4&lt;/em&gt;, 475–92. NBER.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Shared Parameter Models</title>
      <link>/missmethods/shared-parameter-models/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/shared-parameter-models/</guid>
      <description>


&lt;p&gt;It is possible to summarise the steps involved in drawing inference from incomplete data as (&lt;span class=&#34;citation&#34;&gt;Daniels and Hogan (2008)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Specification of a full data model for the response and missingness indicators &lt;span class=&#34;math inline&#34;&gt;\(f(y,r)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Specification of the prior distribution (within a Bayesian approach)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sampling from the posterior distribution of full data parameters, given the observed data &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt; and the missingness indicators &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Identification of a full data model, particularly the part involving the missing data &lt;span class=&#34;math inline&#34;&gt;\(Y_{mis}\)&lt;/span&gt;, requires making unverifiable assumptions about the full data model &lt;span class=&#34;math inline&#34;&gt;\(f(y,r)\)&lt;/span&gt;. Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{mis}\)&lt;/span&gt;, given &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt;. Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Unverifiable parametric assumptions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Informative prior distributions (under a Bayesian approach)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We show some simple examples about how these &lt;em&gt;nonignorable&lt;/em&gt; models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as &lt;em&gt;Shared Parameter Models&lt;/em&gt;(SPM).&lt;/p&gt;
&lt;div id=&#34;shared-parameter-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shared Parameter Models&lt;/h2&gt;
&lt;p&gt;The shared parameter model approach consists in an explicit multilevel specification, where random effects &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; are modelled jointly with &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Wu and Carroll (1988)&lt;/span&gt;). The general form of the full data modelling using a SPM approach is&lt;/p&gt;
&lt;p&gt;\[
f(y,r \mid \omega) = \int f(y, r, b \mid \omega)db.
\]&lt;/p&gt;
&lt;p&gt;Next, specific SPMs are formulated by making assumptions about the joint distribution under the integral sign. Main advantages of this models is that they are quite easy to specify and that, through the use of random effects, high-dimensional or multilevel data modelling is relatively easy to accomplish. The main drawback is that the underlying missingness mechanism is often difficult to understand and may not have even a closed form.&lt;/p&gt;
&lt;div id=&#34;example-random-coefficients-selection-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example random coefficients selection model&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Wu and Carroll (1988)&lt;/span&gt; specified a SPM assuming the response follow a linear random effects model&lt;/p&gt;
&lt;p&gt;\[
Y_i \mid x_i,b_i \sim N(x_i\beta + w_ib_i, \Sigma_i(\phi)),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; are the random effects covariates with rows &lt;span class=&#34;math inline&#34;&gt;\(w_i=(1,t_{ij})\)&lt;/span&gt;, therefore implying that each individual has a random slope and intercept. The random effects &lt;span class=&#34;math inline&#34;&gt;\(b_i=(b_{i1},b_{i2})\)&lt;/span&gt; are assumed to follow a bivariate normal distribution&lt;/p&gt;
&lt;p&gt;\[
b_i \sim N(0,\Omega),
\]&lt;/p&gt;
&lt;p&gt;while the hazard of dropout is Bernoulli with&lt;/p&gt;
&lt;p&gt;\[
R_{ij} \mid R_{ij-1}=1,b_i \sim Bern(\pi_{ij}),
\]&lt;/p&gt;
&lt;p&gt;which depends on the random effects via&lt;/p&gt;
&lt;p&gt;\[
g(\pi_{ij}) = \psi_0 + \psi_1b_{i1} + \psi_2b_{i2}.
\]&lt;/p&gt;
&lt;p&gt;The model can be seen as a special case of the general SPM formulation by noticing that the joint distribution under the integral sign can be factored as&lt;/p&gt;
&lt;p&gt;\[
f(y,r,b \mid x, \omega) = f(r \mid b,x,\psi)f(y \mid b,x,\beta,\phi)f(b \mid \Omega)
\]&lt;/p&gt;
&lt;p&gt;under the assumption that &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is independent of both &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{mis}\)&lt;/span&gt;, conditionally on &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. However, integrating over the random effects, dependence between &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{mis}\)&lt;/span&gt;, given &lt;span class=&#34;math inline&#34;&gt;\(Y_{obs}\)&lt;/span&gt;, is induced and therefore the model characterises a &lt;em&gt;Missing Not At Random&lt;/em&gt;(MNAR) mechanism.&lt;/p&gt;
&lt;p&gt;The conditional linear model (&lt;span class=&#34;citation&#34;&gt;Wu and Bailey (1989)&lt;/span&gt;) can also be seen as a version of the SPM, which is formulated as&lt;/p&gt;
&lt;p&gt;\[
f(y,r,b \mid x) = f(y \mid r,b,x)f(b \mid r,x)f(r \mid x).
\]&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conlcusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conlcusions&lt;/h2&gt;
&lt;p&gt;To summarise, shared parameter models are very useful for characterizing joint distributions of repeated measures and event times, and can be particularly useful as a method of data reduction when the dimension of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is high. Nonetheless, their application to the problem of making full data inference from incomplete longitudinal data should be made with caution and with an eye toward justifying the required assumptions. Sensitivity analysis is an open area of research for these models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-daniels2008missing&#34;&gt;
&lt;p&gt;Daniels, Michael J, and Joseph W Hogan. 2008. &lt;em&gt;Missing Data in Longitudinal Studies: Strategies for Bayesian Modeling and Sensitivity Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wu1989estimation&#34;&gt;
&lt;p&gt;Wu, Margaret C, and Kent R Bailey. 1989. “Estimation and Comparison of Changes in the Presence of Informative Right Censoring: Conditional Linear Model.” &lt;em&gt;Biometrics&lt;/em&gt;, 939–55.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wu1988estimation&#34;&gt;
&lt;p&gt;Wu, Margaret C, and Raymond J Carroll. 1988. “Estimation and Comparison of Changes in the Presence of Informative Right Censoring by Modeling the Censoring Process.” &lt;em&gt;Biometrics&lt;/em&gt;, 175–88.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Weighting Adjustments</title>
      <link>/missmethods/weighting-adjustments/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/weighting-adjustments/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The notion of reducing bias due to missingness through &lt;em&gt;reweighting methods&lt;/em&gt; has its root in the survey literature and the basic idea is closely related to weighting in randomisation inference for finite population surveys (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;). In particular, in probability sampling, a unit selected from a target population with probability &lt;span class=&#34;math inline&#34;&gt;\(\pi_i\)&lt;/span&gt; can be thought as “representing” &lt;span class=&#34;math inline&#34;&gt;\(\pi^{-1}_i\)&lt;/span&gt; units in the population and hence should be given weight &lt;span class=&#34;math inline&#34;&gt;\(\pi^{-1}_i\)&lt;/span&gt; when estimating population quantities. For example, in a stratified random sample, a selected unit in stratum &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; represents &lt;span class=&#34;math inline&#34;&gt;\(\frac{N_j}{n_j}\)&lt;/span&gt; population units, where &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; indicates the units sampled from the &lt;span class=&#34;math inline&#34;&gt;\(N_j\)&lt;/span&gt; population units in stratum &lt;span class=&#34;math inline&#34;&gt;\(j=1,\ldots,J\)&lt;/span&gt;. The population total &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; can then be estimated by the weighted sum&lt;/p&gt;
&lt;p&gt;\[
T = \sum_{i=1}^{n}y_i\pi^{-1}_i,
\]&lt;/p&gt;
&lt;p&gt;known as the Horvitz-Thompson estimate (&lt;span class=&#34;citation&#34;&gt;Horvitz and Thompson (1952)&lt;/span&gt;), while the stratified mean can be written as&lt;/p&gt;
&lt;p&gt;\[
\bar{y}_{w} = \frac{1}{n}\sum_{i=1}^{n}w_iy_i,
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(w_i=\frac{n\pi^{-1}_i}{\sum_{k=1}^n\pi^{-1}_k}\)&lt;/span&gt; is the sampling weight attached to the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th unit scaled tosum up to the sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Weighting class estimators extend this approach to handle missing data such that, if the probabilities of response for unit &lt;span class=&#34;math inline&#34;&gt;\(\phi_i\)&lt;/span&gt; were known, then the probability of selection and response is &lt;span class=&#34;math inline&#34;&gt;\(\pi_i\phi_i\)&lt;/span&gt; and we have&lt;/p&gt;
&lt;p&gt;\[
\bar{y}_{w} = \frac{1}{n_r}\sum_{i=1}^{n_r}w_iy_i,
\]&lt;/p&gt;
&lt;p&gt;where the sum is now over responding units and &lt;span class=&#34;math inline&#34;&gt;\(w_i=\frac{n_r(\pi_i\phi_i)^{-1}}{\sum_{k=1}^{n_r}(\pi_k\phi_k)^{-1}}\)&lt;/span&gt;. In practice, the response probability &lt;span class=&#34;math inline&#34;&gt;\(\phi_i\)&lt;/span&gt; is not known and is typically estimated based on the information available for respondents and nonrespondents (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;weighting-class-estimator-of-the-mean&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weighting Class Estimator of the Mean&lt;/h2&gt;
&lt;p&gt;A simple reweighting approach is to partition the sample into &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; “weighting classes” according to the variables observed for respondents and nonrespondents. If &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; is the sample size, &lt;span class=&#34;math inline&#34;&gt;\(n_{rj}\)&lt;/span&gt; the number of respondents in class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(n_r=\sum_{j=1}^Jr_j\)&lt;/span&gt;, then a simple estimator of the response probability for units in class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is given by &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_{rj}}{n_j}\)&lt;/span&gt;. Thus, responding units in class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; receive weight &lt;span class=&#34;math inline&#34;&gt;\(w_i=\frac{n_r(\pi_i\hat{\phi}_i)^{-1}}{\sum_{k=1}^{n_r}(\pi_k\hat{\phi}_k)^{-1}}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\phi}_i=\frac{n_{rj}}{n_j}\)&lt;/span&gt; for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. The weighting class estimate of the mean is then&lt;/p&gt;
&lt;p&gt;\[
\bar{y}_{w} = \frac{1}{n_r}\sum_{i=1}^{n_r}w_iy_i,
\]&lt;/p&gt;
&lt;p&gt;which is unbiased under the &lt;em&gt;quasirandomisation&lt;/em&gt; assumption (&lt;span class=&#34;citation&#34;&gt;Oh and Scheuren (1983)&lt;/span&gt;), which requires respondents in weighting class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; to be a random sample of the sampled units, i.e. data are &lt;em&gt;Missing Completely At Random&lt;/em&gt; (MCAR) within adjustment class &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. Weighting class adjustments are simple because the same weights are obtained regardless of the outcome tp which they are applied, but these are inefficient and generally involves an increase in sampling variance for outcomes that are weakly related to the weighting class variable. Assuming random sampling within weighting classes, a constant variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; for an outcome &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and ignoring sampling variation in the weights, the increase in sampling variance of a sample mean is&lt;/p&gt;
&lt;p&gt;\[
\text{Var}\left(\frac{1}{n_{r}}\sum_{i=1}^{n_{r}}w_iy_i \right) = \frac{\sigma^2}{n_{r}^2}\left(\sum_{i=1}^{n_{r}}w_{i}^{2} \right) = \frac{\sigma^2}{n_{r}}(1+\text{cv}^2(w_i)),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{cv}(w_i)\)&lt;/span&gt; is the coefficient of variation of the weights (scaled to average one), which is a rough measure of the proportional increase in sampling variance due to weighting (&lt;span class=&#34;citation&#34;&gt;Kish (1992)&lt;/span&gt;). When the weighting class variable is predictive of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, weighting methods can lead to a reduction in sampling variance. &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt; summarise the effect of weighting on the bias and sampling variance of an estimated mean, according to whether the associations between the adjustment cells and the outcome &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and missing indicator &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; are high or low.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Effect of weighting adjustments on bias and sampling variance of a mean.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Low (y)
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
High (y)
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Low (m)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bias: /, var: /
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bias: /, var: -
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
High (m)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bias: /, var: +
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bias: -, var: -
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Thus, weighting is only effective when the outcome is associated with the adjustment cell variable because otherwise the sampling variance is increased with no bias reduction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;propensity-weighting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Propensity Weighting&lt;/h2&gt;
&lt;p&gt;In some settings, weighting class estimates cannot be feasibly derived by all recorded variables X because the number of classes become too large and some may include cells with nonrespondents but no respondents for which the nonresponse weight is infinite. The theory of propensity scores (&lt;span class=&#34;citation&#34;&gt;Rosenbaum and Rubin (1983)&lt;/span&gt;) provides a prescription for choosing the coarsest reduction of the variables to a weighting class variable &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. Suppose the data are &lt;em&gt;Missing At Random&lt;/em&gt; (MAR) such that&lt;/p&gt;
&lt;p&gt;\[
p(m\mid X,y,\phi)=p(m\mid X,\phi),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; are unknown parameters and define the nonresponse propensity for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;\[
\rho(x_i,\phi)=p(m_i=1 \mid \phi),
\]&lt;/p&gt;
&lt;p&gt;assuming that this is strictly positive for all values of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;. Then, it can be shown that&lt;/p&gt;
&lt;p&gt;\[
p(m\mid \rho(X,\phi),y,\phi)=p(m\mid \rho(X,\phi),\phi),
\]&lt;/p&gt;
&lt;p&gt;so that respondents are a random subsample within strata defined by the propensity score &lt;span class=&#34;math inline&#34;&gt;\(\rho(X,\phi)\)&lt;/span&gt;. In practice the parameter &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is unknown and must be estimated from sample data, for example via logistic, probit or robit regressions of the missingness indicator &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; based on respondent and nonrespondent data (&lt;span class=&#34;citation&#34;&gt;Liu (2004)&lt;/span&gt;). A variant of this procedure is to weight respondents &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; directly by the inverse of the estimated propensity score &lt;span class=&#34;math inline&#34;&gt;\(\rho(X,\hat{\phi})^{-1}\)&lt;/span&gt; (&lt;span class=&#34;citation&#34;&gt;Cassel, Sarndal, and Wretman (1983)&lt;/span&gt;), which allows to remove bias but may cause two problems: 1) estimates may be associated with very high sampling variances due to nonrespondents with low response propensity estimates receiving large nonresponse weights; 2) more reliance on correct model specification of the propensity score regression than response propensity stratification.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-cassel1983some&#34;&gt;
&lt;p&gt;Cassel, Claes M, Carl-Erik Sarndal, and Jan H Wretman. 1983. “Some Uses of Statistical Models in Connection with the Nonresponse Problem.” &lt;em&gt;Incomplete Data in Sample Surveys&lt;/em&gt; 3: 143–60.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-horvitz1952generalization&#34;&gt;
&lt;p&gt;Horvitz, Daniel G, and Donovan J Thompson. 1952. “A Generalization of Sampling Without Replacement from a Finite Universe.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 47 (260): 663–85.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kish1992weighting&#34;&gt;
&lt;p&gt;Kish, Leslie. 1992. “Weighting for Unequal Pi.” &lt;em&gt;Journal of Official Statistics&lt;/em&gt; 8 (2): 183.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-liu2004robit&#34;&gt;
&lt;p&gt;Liu, Chuanhai. 2004. “Robit Regression: A Simple Robust Alternative to Logistic and Probit Regression.” &lt;em&gt;Applied Bayesian Modeling and Casual Inference from Incomplete-Data Perspectives&lt;/em&gt;, 227–38.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-oh1983weighting&#34;&gt;
&lt;p&gt;Oh, H, and F Scheuren. 1983. “Weighting Adjustment for Unit Nonresponse. Chap. 13 in Vol. 2, Part 4 of Incomplete Data in Sample Surveys, Edited by William G. Madow, Harold Nisselson, and Ingram Olkin.” New York: Academic Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rosenbaum1983central&#34;&gt;
&lt;p&gt;Rosenbaum, Paul R, and Donald B Rubin. 1983. “The Central Role of the Propensity Score in Observational Studies for Causal Effects.” &lt;em&gt;Biometrika&lt;/em&gt; 70 (1): 41–55.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Missing Data</title>
      <link>/missingdata/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/missingdata/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/research/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
