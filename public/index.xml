<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Andrea Gabrio</title>
    <link>/</link>
    <description>Recent content on Andrea Gabrio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Nov 2019 13:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations</title>
      <link>/talk/isporeu2019/</link>
      <pubDate>Mon, 04 Nov 2019 13:00:00 +0000</pubDate>
      
      <guid>/talk/isporeu2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations</title>
      <link>/talk/ictmc2019/</link>
      <pubDate>Mon, 07 Oct 2019 11:00:00 +0000</pubDate>
      
      <guid>/talk/ictmc2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MissingHE 1.2.1</title>
      <link>/post/missinghe-121/missinghe-version121/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/missinghe-121/missinghe-version121/</guid>
      <description>&lt;p&gt;I have finally found some time to update the version for my R package &lt;a href=&#34;https://agabrioblog.onrender.com/missingHE/&#34; target=&#34;_blank&#34;&gt;missingHE&lt;/a&gt;, for which version 1.2.1 is now available on &lt;a href=&#34;https://cran.r-project.org/web/packages/missingHE/&#34; target=&#34;_blank&#34;&gt;CRAN&lt;/a&gt;.
I included two main features to the previous version of the package.&lt;/p&gt;

&lt;p&gt;First, I have added a new type of identifying restriction when fitting pattern mixture models through the function &amp;ldquo;pattern&amp;rdquo;. Before, only the complete case
restriction was available, which identifies the distributions of the missing data with those from the completers. Now the alternative available case restriction is can also be selected, which relies on the distributions that can be identified
among the non-completers to identify the distributions of the missing data. In this way, people can choose among at least two options for the type of restrictions and compare how this choice may affect the final estimates.&lt;/p&gt;

&lt;p&gt;Second, I added a new accessory function called &amp;ldquo;ppc&amp;rdquo;, which allows to perform posterior predictive checks using the conditional parameters saved from the fitted model to generate replications of the data at each posterior iteration of the model.
The function implements a relatively large number of checks, mostly taken from the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/bayesplot/&#34; target=&#34;_blank&#34;&gt;bayesplot&lt;/a&gt;, which allow to assess the fit of the model to the observed data by type of outcome (effects and costs) and treatment group (control and intervention).
For example, overalyed density plots can be generated to compare the empirical and replicated densities of the data to detect possible failures of the model.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/plotec.png&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Density plots for the observed and replicated data&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;I feel this is very important as when fitting a Bayesian model it is crucial to assess whether the model seems to adequately capture the different characteristics of the observed data (e.g. skewness, structural values, etc.).
A wide range of predictive checks are available, including histograms (see thumbnail pciture), scatterplots, error intervals, empirical cumulative distribution fucntions, statistcis of interest and many others. In addition ,
these checks can be performed for each type of missingness model and parametric distribution chosen within missingHE.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/NEvPzZ8bd1V4Y/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, it is important to remember that, when dealing with missing data the fit of the model can only be checked with respect to the observed values and therefore this
check is only partial since the fit to the unibserved values can never be checked. This is also why it is not meaningful to assess the fit of a model fitted under a missing not at random assumption
because this is based on information which is not directly available from the data at hand and thus impossible to check.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Discussing my thesis</title>
      <link>/post/update-interview/update-interview/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update-interview/update-interview/</guid>
      <description>&lt;p&gt;I have been kindly invited by the amazing person &lt;a href=&#34;https://www.ohe.org/about-us/meet-team/chris-sampson&#34; target=&#34;_blank&#34;&gt;Chris Sampson&lt;/a&gt; to talk about the work I inlcuded in my PhD thesis for his monthly rubric entitled  &amp;ldquo;Thesis Thursday&amp;rdquo; on the &lt;a href=&#34;https://aheblog.com/&#34; target=&#34;_blank&#34;&gt;The Academic Health Economists blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I happily accepted Chris&amp;rsquo;s invitation as I beleive this initiative is really interesting and represents a nice way for newly graduated PhD students to advertise their work while also giving the chance to people interested in health economics to read about some academic work which is typically freely available to everyone.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://aheblog.com/2019/09/19/thesis-thursday-andrea-gabrio/&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; you can find the full interview, which is not very long and resolves around 5 questions that Chris asked me about my work. I already new this blog but I have never had a proper chance to read through its posts carefully, which is a shame.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/13lTgtSUmqMrlu/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I shall promise myself to try to check it more often from now on, using this interview as a nice motivation to do so.
In fact, there are not many blogs around health economics matters (here a &lt;a href=&#34;https://blog.feedspot.com/health_economics_blogs/&#34; target=&#34;_blank&#34;&gt;non-comprehensive list&lt;/a&gt;), among which The Academic Health Economists and &lt;a href=&#34;http://www.statistica.it/gianluca/blog/&#34; target=&#34;_blank&#34;&gt;Gianluca&amp;rsquo;s blog&lt;/a&gt; are my favourites.&lt;/p&gt;

&lt;p&gt;I hope I will be able to find some time to write some nice posts about some health economic applications of my work in the next future as this is still the most interesting field for me at the moment. I am also the maintainer of another small blog called the &lt;a href=&#34;https://hearteam.blogspot.com/&#34; target=&#34;_blank&#34;&gt;Health Economics Analysis and Research Methods Team (HEART) blog&lt;/a&gt;, where I occasionally write some posts on health economics together with my colleagues from the UCL department of &lt;a href=&#34;https://www.ucl.ac.uk/epidemiology-health-care/research/pcph&#34; target=&#34;_blank&#34;&gt;Primary Care and Population Health&lt;/a&gt;. The blog is still new but I hope it can become more active in the next months.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some good news...</title>
      <link>/post/update-september/update-september/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/update-september/update-september/</guid>
      <description>&lt;p&gt;With the approaching of the new academic here I have received some good news for my most recently submitted paper on Bayesian parametric modelling in health economics for missing longitudinal data, which at the moment is only available on &lt;a href=&#34;https://arxiv.org/abs/1805.07147&#34; target=&#34;_blank&#34;&gt;arXiv&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am happy to announce that, after a couple of rounds of reviews, the paper has been finally accepted for publication in &lt;a href=&#34;https://rss.onlinelibrary.wiley.com/journal/1467985X&#34; target=&#34;_blank&#34;&gt;JSS: Series A&lt;/a&gt;. I believe that the reviewers provided a very nice feedback for improving the work and I am quite satisfied with the final version of the article which, I hope, will be of interest for anyone involved in the analsysi of partially-observed longitudinal data. I hope the pre-print of the paper will be available soon and I will &amp;ldquo;advertise&amp;rdquo; my work in two conferences in the next couple of months, where I will present the content of the paper, namely &lt;a href=&#34;https://ictmc2019.com/&#34; target=&#34;_blank&#34;&gt;ICTMC&lt;/a&gt; this October in Brighton, and &lt;a href=&#34;http://www.ispor.org/conferences-education/conferences/upcoming-conferences/ispor-europe-2019&#34; target=&#34;_blank&#34;&gt;ISPOR Europe&lt;/a&gt; this November in Copenhagen.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/vMEjhlxsBR7Fe/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I am really excited about this paper which represented the last part of my PhD thesis and on which I worked really hard in the last year of my studies. &lt;a href=&#34;https://agabrioblog.onrender.com/project/missing-data/&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; you can find a general summary of the content of the article, while &lt;a href=&#34;https://www.ucl.ac.uk/statistics/sites/statistics/files/presentation_priment_1.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; there are some slides that describe the main idea behind the proposed model.&lt;/p&gt;

&lt;p&gt;I just want to conlcude with some thanks with my co-authors of the paper, &lt;a href=&#34;http://users.stat.ufl.edu/~daniels/&#34; target=&#34;_blank&#34;&gt;Michael&lt;/a&gt; and &lt;a href=&#34;https://www.ucl.ac.uk/statistics/people/gianlucabaio&#34; target=&#34;_blank&#34;&gt;Ginaluca&lt;/a&gt;, without whom I would have not been able to write this paper. This was my first work with Mike, with whom I had a wonderful collaboration and I was able to visit the beatiful city of &lt;a href=&#34;https://en.wikipedia.org/wiki/Gainesville,_Florida&#34; target=&#34;_blank&#34;&gt;Gainesville&lt;/a&gt; (FL) during my first visiting period at the &lt;a href=&#34;https://www.ufl.edu/&#34; target=&#34;_blank&#34;&gt;University of Florida&lt;/a&gt; (see thumbnail picture). I hope this will be the first of many works together in the furture!.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The P value fallacy</title>
      <link>/post/p-value-fallacy/p-value-fallacy/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/p-value-fallacy/p-value-fallacy/</guid>
      <description>&lt;p&gt;Today, I would like to briefly comment an interesting research article written by &lt;a href=&#34;https://jhu.pure.elsevier.com/en/publications/toward-evidence-based-medical-statistics-1-the-p-value-fallacy-4&#34; target=&#34;_blank&#34;&gt;Goodman&lt;/a&gt;, who provided a clear and exemplary discussion about the typical incorrect interpretation of a standard frequentist analysis in the field of medical research. I will now briefly summarise the main argument of the paper and then add some personal comments.&lt;/p&gt;

&lt;p&gt;Essentially, the article describes the characteristics of the dominant school of medical statistics and highlights the logical fallacy at the heart of the typical frequentist analysis in clinical studies. This is based on a &lt;em&gt;deductive&lt;/em&gt; inferential approach, which starts with a given hypothesis and makes conclusions under the assumption that the hypothesis is true. This is in contrast with a &lt;em&gt;inductive&lt;/em&gt; approach, which uses the observed evidence to evaluate what hypothesis is most tenable. The two most popular methods of the frequentist paradigm are the &lt;em&gt;P value&lt;/em&gt; proposed by &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_6&#34; target=&#34;_blank&#34;&gt;Fisher&lt;/a&gt; and the &lt;em&gt;hypothesis testing&lt;/em&gt; developed by &lt;a href=&#34;https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1933.0009?casa_token=sbSkualIaPYAAAAA%3ACxPsFTFEUK7vaxMPi5dJwUr4HoUWjrkxNh7Hl2q0owjtcU2wJHnakG-Xug7y95v1Tyqbbc8Mymaq_Q&amp;amp;&#34; target=&#34;_blank&#34;&gt;Neyman and Pearson&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The P value is defined as the probability, under the assumption of no effect (null hypothesis), of obtaining a result equal to or more extreme than what was actually observed. Fisher proposed it as an informal index to be used as a measure of discrepancy between the data and the null hypothesis and therefore should not be interpreted as a formal inferential method. For example, since the P value can only be calculated on the assumption that the null hypothesis is true, it cannot be a direct measure of the probability that the null hypothesis is false. However, the main criticism to the P value is perhaps that it does not take into account the size of the observed effect, i.e. a small effect in a study with a large sample size can have the same P value as a large effect in a small study.&lt;/p&gt;

&lt;p&gt;Hypothesis testing was proposed by Neyman and Pearson as an alternative approach to the P value, which assumes the existence of a null hypothesis (e.g. no effect) and an alternative hypothesis (e.g. nonzero effect). The outcome of the test is then simply to reject one hypothesis in favour of the other, solely based on the data. This exposes the researcher to two types of errors: type I error or false-positive ($\alpha$) and type II error or false-negative ($\beta$) result. Rather than focussing on single experiments, like the P value, hypothesis testing is effectively based on a deductive approach to minimise the errors over a large number of experiments. However, the price to pay to obtain this &lt;em&gt;objectivity&lt;/em&gt; is the impossibility to make any inferential statement about a single experiment. The procedure only guarantees that in the long run, i.e. after considering many experiments, we shall not often be wrong.&lt;/p&gt;

&lt;p&gt;Over time a combination between the P value and hypothesis testing was developed under the assumption that the two approaches can be complementary. The idea was that the P value could be used to measure evidence in a single experiment while not violating the long run logic of hypothesis testing. The combined method is characterized by setting $\alpha$ and power $\beta$ before the experiment, then calculating a P value and rejecting the null hypothesis if the P value is less than the preset type I error rate. This means that the P value is considered a false-positive error rate specific to the data and also a measure of evidence against the null hypothesis. The &lt;strong&gt;P value fallacy&lt;/strong&gt; is born from this statement, which assumes that an event can be seen simultaneously from a long run perspective (where the observed results are put together with other results that might have occurred in hypothetical repetitions of the experiment) and from a short run perspective (where the observed results are interpreted only with respect to the single experiment). However, these views are not reconcilable since a result cannot be at the same time an interchangeable (long-run) and unique (short-run) member of a group of results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/JszzkKOlV6gTK/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I personally find this discussion fascinating and I believe that it is important to recognise the inconsistencies between the two alternative approaches to inference. The original authors of the two paradigms were well aware of the implications of their methods and never supported the combination of these. However, the combined approach has somehow become widely accepted in practice while its internal inconsistencies and conceptual limitations are hardly recognised.&lt;/p&gt;

&lt;p&gt;I feel that, since the two methods are perceived as &amp;ldquo;objective&amp;rdquo;, it is generally accepted that, if combined, they can produce reliable conclusions. This, however, is not necessarily true. Accepting at face value the significance result as a binary indicator of whether or not a relation is real is dangeroues and potentially misleading. This practice wants to show that conclusions are being drawn directly from the data, without any external influence, because direct inference from data to hypothesis is thought to result in mistaken conclusions only rarely and is therefore regarded as &amp;ldquo;scientific&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;This misguided approach has led to a much stronger emphasis towards the quantitative results alone (without any external input). In contrast, I believe that such perspective has the serious drawback of ignoring potentially useful information which is available (e.g. relevant medical knowledge or historical data) and which should be included in the analysis. Of course, I am aware of the potential issues that may arise from the selection and incorporation of external evidence, but I believe this should not be considered as &amp;ldquo;less reliable&amp;rdquo; or &amp;ldquo;more prone to mistakes&amp;rdquo; compared with the evidence from the available data. It is important that an agreement is reached about the selection of the type of evidence and methods to be used to perform the analysis solely based on their relevance with respect to the context analysed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adjusting for partially-observed utilities and costs in trial-based cost-effectiveness analysis: a comparison of different methods and their performance</title>
      <link>/talk/hesg2019/</link>
      <pubDate>Thu, 04 Jul 2019 16:00:00 +0000</pubDate>
      
      <guid>/talk/hesg2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Super basic introduction to STAN</title>
      <link>/stan/basic-introduction-to-stan/super-basic-introduction-to-stan/</link>
      <pubDate>Wed, 03 Jul 2019 21:13:14 -0500</pubDate>
      
      <guid>/stan/basic-introduction-to-stan/super-basic-introduction-to-stan/</guid>
      <description>


&lt;p&gt;The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using &lt;code&gt;STAN&lt;/code&gt; via &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Prerequisites:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latest version of &lt;code&gt;R&lt;/code&gt;, which can be downloaded and installed for Windows, Mac or Linux OS from the &lt;a href=&#34;https://www.r-project.org/%7D&#34;&gt;CRAN&lt;/a&gt; website&lt;/li&gt;
&lt;li&gt;I also &lt;strong&gt;strongly&lt;/strong&gt; recommend to download and install &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;Rstudio&lt;/a&gt;, an integrated development environment which provides an “user-friendly” interaction with &lt;code&gt;R&lt;/code&gt; (e.g. many drop-down menus, tabs, customisation options)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Preliminaries&lt;/h1&gt;
&lt;div id=&#34;what-is-stan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is STAN?&lt;/h2&gt;
&lt;p&gt;Stan provides full Bayesian inference for continuous-variable models through Markov Chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling&lt;/p&gt;
&lt;p&gt;&lt;code&gt;STAN&lt;/code&gt; is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (&lt;span class=&#34;citation&#34;&gt;Gelman, Lee, and Guo (2015)&lt;/span&gt;). &lt;code&gt;STAN&lt;/code&gt; is a free software and a probabilistic programming language for specifying statistical models using a specific class of MCMC algorithms known as &lt;strong&gt;H&lt;/strong&gt;amiltonian &lt;strong&gt;M&lt;/strong&gt;onte &lt;strong&gt;C&lt;/strong&gt;arlo methods (HMC). The latest version of &lt;code&gt;STAN&lt;/code&gt; can be dowloaded from the web &lt;a href=&#34;https://mc-stan.org/users/interfaces/&#34;&gt;repository&lt;/a&gt; and is available for different OS. There are different &lt;code&gt;R&lt;/code&gt; packages which function as frontends for &lt;code&gt;STAN&lt;/code&gt;. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the &lt;code&gt;rstan&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Stan Development Team (2018)&lt;/span&gt;) and show how to fit &lt;code&gt;STAN&lt;/code&gt; models using this package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-stan-and-rstan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing STAN and rstan&lt;/h2&gt;
&lt;p&gt;Unlike other Bayesian software, such as &lt;code&gt;JAGS&lt;/code&gt; or &lt;code&gt;OpenBUGS&lt;/code&gt;, it is not required to separately install the program and the corresponding frontend &lt;code&gt;R&lt;/code&gt; package. Indeed, installing the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;rstan&lt;/code&gt; will automatically install &lt;code&gt;STAN&lt;/code&gt; on your machine. However, you will also need to make sure to having installed on your pc a &lt;code&gt;C++&lt;/code&gt; compiler which is used by &lt;code&gt;rstan&lt;/code&gt; to fit the models. Under a Windows OS, for example, this can be done by installing &lt;code&gt;Rtools&lt;/code&gt;, a collection of resources for building packages for &lt;code&gt;R&lt;/code&gt;, which is freely available from the web &lt;a href=&#34;https://cran.r-project.org/bin/windows/Rtools/&#34;&gt;repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next, install the package &lt;code&gt;rstan&lt;/code&gt; from within &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;Rstudio&lt;/code&gt;, via the package installer or by typing in the command line&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;rstan&amp;quot;, dependencies = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;dependencies = TRUE&lt;/code&gt; option will automatically install all the packages on which the functions in the &lt;code&gt;rstan&lt;/code&gt; package rely.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic model&lt;/h1&gt;
&lt;div id=&#34;simulate-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulate data&lt;/h2&gt;
&lt;p&gt;For an example dataset, I simulate my own data in &lt;code&gt;R&lt;/code&gt;. I create a continuous outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as a function of one predictor &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and a disturbance term &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficients, i.e.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \beta_1 x + \epsilon  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;R&lt;/code&gt; commands which I use to simulate the data are the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; n_sim=100; set.seed(123)
&amp;gt; x=rnorm(n_sim, mean = 5, sd = 2)
&amp;gt; epsilon=rnorm(n_sim, mean = 0, sd = 1)
&amp;gt; beta0=1.5
&amp;gt; beta1=1.2
&amp;gt; y=beta0 + beta1 * x + epsilon
&amp;gt; n_sim=as.integer(n_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I define all the data for &lt;code&gt;STAN&lt;/code&gt; in a list object&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; datalist=list(&amp;quot;y&amp;quot;=y,&amp;quot;x&amp;quot;=x,&amp;quot;n_sim&amp;quot;=n_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model file&lt;/h2&gt;
&lt;p&gt;Now, I write the model for &lt;code&gt;STAN&lt;/code&gt; and save it as a stan file named &lt;code&gt;&#34;basic.mod.stan&#34;&lt;/code&gt; in the current working directory&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod= &amp;quot;
+ data {
+ int&amp;lt;lower=0&amp;gt; n_sim;
+ vector[n_sim] y;
+ vector[n_sim] x;
+ }
+ parameters {
+ real beta0;
+ real beta1;
+ real&amp;lt;lower=0&amp;gt; sigma;
+ }
+ transformed parameters {
+ vector[n_sim] mu;
+ mu=beta0 + beta1*x;
+ } 
+ model {
+ sigma~uniform(0,100);
+ beta0~normal(0,1000);
+ beta1~normal(0,1000);
+ y~normal(mu,sigma);
+ }
+ 
+ &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;STAN&lt;/code&gt; models are written using an imperative programming language, which means that the order in which you write the elements in your model file matters, i.e. you first need to define your variables (e.g. integers, vectors, matrices, etc.), the constraints which define the range of values your variable can take (e.g. only positive values for standard deviations), and finally define the relationship among the variables (e.g. one is a liner function of another).&lt;/p&gt;
&lt;p&gt;A Stan model is defined by six program blocks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data (required). The &lt;em&gt;data block&lt;/em&gt; reads external information – e.g. data vectors, matrices, integers, etc.&lt;/li&gt;
&lt;li&gt;Transformed data (optional). The &lt;em&gt;transformed data block&lt;/em&gt; allows for preprocessing of the data – e.g. transformation or rescaling of the data.&lt;/li&gt;
&lt;li&gt;Parameters (required). The &lt;em&gt;parameters block&lt;/em&gt; defines the sampling space – e.g. parameters to which prior distributions must be assigned.&lt;/li&gt;
&lt;li&gt;Transformed parameters (optional). The &lt;em&gt;transformed parameters block&lt;/em&gt; allows for parameter processing before the posterior is computed – e.g. tranformation or rescaling of the parameters.&lt;/li&gt;
&lt;li&gt;Model (required). In the &lt;em&gt;model block&lt;/em&gt; we define our posterior distributions – e.g. choice of distributions for all variables.&lt;/li&gt;
&lt;li&gt;Generated quantities (optional). The &lt;em&gt;generated quantities block&lt;/em&gt; allows for postprocessing – e.g. backtranformation of the parameters using the posterior samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this introduction I consider a very simple model which only requires the specification of four blocks in the &lt;code&gt;STAN&lt;/code&gt; model. In the data block, I first define the size of the sample &lt;code&gt;n_sim&lt;/code&gt; as a positive integer number using the expression &lt;code&gt;int&amp;lt;lower=0&amp;gt; n_sim&lt;/code&gt;; then I declare the two variables &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; as reals (or vectors) with length equal to N. In the parameters block, I define the coefficients for the linear regression &lt;code&gt;beta0&lt;/code&gt; and &lt;code&gt;beta1&lt;/code&gt; (as two real numbers) and the standard deviation parameter &lt;code&gt;sigma&lt;/code&gt; (as a positive real number). In the transformed parameters block, I define the conditional mean &lt;code&gt;mu&lt;/code&gt; (a real vector of length &lt;code&gt;N&lt;/code&gt;) as a linear function of the intercept &lt;code&gt;beta0&lt;/code&gt;, the slope &lt;code&gt;beta1&lt;/code&gt;, and the covariate &lt;code&gt;x&lt;/code&gt;. Finally, in the model block, I assign weakly informative priors to the regression coefficients and the standard deviation parameters, and I model the outcome data &lt;code&gt;y&lt;/code&gt; using a normal distribution indexed by the conditional mean &lt;code&gt;mu&lt;/code&gt; and the standard deviation &lt;code&gt;sigma&lt;/code&gt; parameters. In many cases, &lt;code&gt;STAN&lt;/code&gt; uses sampling statements which can be vectorised, i.e. you do not need to use for loop statements.&lt;/p&gt;
&lt;p&gt;To write and save the model as the text file “basic.mod.stan” in the current working directory, I use the &lt;code&gt;writeLines&lt;/code&gt; function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; writeLines(basic.mod, &amp;quot;basic.mod.stan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pre-processing&lt;/h2&gt;
&lt;p&gt;Define the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in &lt;code&gt;STAN&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params=c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;)
&amp;gt; inits=function(){list(&amp;quot;beta0&amp;quot;=rnorm(1), &amp;quot;beta1&amp;quot;=rnorm(1))}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters in the object &lt;code&gt;inits&lt;/code&gt; which is then passed to the &lt;code&gt;stan&lt;/code&gt; function in &lt;code&gt;rstan&lt;/code&gt;. However, for more complex models, this may not be immediate and a lot of trial and error may be required. However, &lt;code&gt;STAN&lt;/code&gt; can automatically select the initial values for all parameters randomly. This can be achieved by setting &lt;code&gt;inits=&#34;random&#34;&lt;/code&gt;, which is then passed to the &lt;code&gt;stan&lt;/code&gt; function in &lt;code&gt;rstan&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before using &lt;code&gt;rstan&lt;/code&gt; for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(rstan)
&amp;gt; set.seed(123)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit the model&lt;/h2&gt;
&lt;p&gt;Now, we can fit the model in &lt;code&gt;STAN&lt;/code&gt; using the &lt;code&gt;stan&lt;/code&gt; function in the &lt;code&gt;rstan&lt;/code&gt; package and save it in the object &lt;code&gt;basic.mod&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod&amp;lt;-stan(data = datalist, pars = params, iter = 9000, 
+   warmup = 1000, init = inits, chains = 2, file = &amp;quot;basic.mod.stan&amp;quot;)

SAMPLING FOR MODEL &amp;#39;basic&amp;#39; NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 9000 [  0%]  (Warmup)
Chain 1: Iteration:  900 / 9000 [ 10%]  (Warmup)
Chain 1: Iteration: 1001 / 9000 [ 11%]  (Sampling)
Chain 1: Iteration: 1900 / 9000 [ 21%]  (Sampling)
Chain 1: Iteration: 2800 / 9000 [ 31%]  (Sampling)
Chain 1: Iteration: 3700 / 9000 [ 41%]  (Sampling)
Chain 1: Iteration: 4600 / 9000 [ 51%]  (Sampling)
Chain 1: Iteration: 5500 / 9000 [ 61%]  (Sampling)
Chain 1: Iteration: 6400 / 9000 [ 71%]  (Sampling)
Chain 1: Iteration: 7300 / 9000 [ 81%]  (Sampling)
Chain 1: Iteration: 8200 / 9000 [ 91%]  (Sampling)
Chain 1: Iteration: 9000 / 9000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.078 seconds (Warm-up)
Chain 1:                0.593 seconds (Sampling)
Chain 1:                0.671 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL &amp;#39;basic&amp;#39; NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 9000 [  0%]  (Warmup)
Chain 2: Iteration:  900 / 9000 [ 10%]  (Warmup)
Chain 2: Iteration: 1001 / 9000 [ 11%]  (Sampling)
Chain 2: Iteration: 1900 / 9000 [ 21%]  (Sampling)
Chain 2: Iteration: 2800 / 9000 [ 31%]  (Sampling)
Chain 2: Iteration: 3700 / 9000 [ 41%]  (Sampling)
Chain 2: Iteration: 4600 / 9000 [ 51%]  (Sampling)
Chain 2: Iteration: 5500 / 9000 [ 61%]  (Sampling)
Chain 2: Iteration: 6400 / 9000 [ 71%]  (Sampling)
Chain 2: Iteration: 7300 / 9000 [ 81%]  (Sampling)
Chain 2: Iteration: 8200 / 9000 [ 91%]  (Sampling)
Chain 2: Iteration: 9000 / 9000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.078 seconds (Warm-up)
Chain 2:                0.594 seconds (Sampling)
Chain 2:                0.672 seconds (Total)
Chain 2: &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Different packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the &lt;code&gt;bayesplot&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Gabry and Mahr (2017)&lt;/span&gt;) to obtain graphical diagnostics and results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;bayesplot&amp;quot;)
&amp;gt; library(bayesplot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, density and trace plots can be obtained by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; mcmc_combo(as.array(basic.mod),regex_pars=&amp;quot;beta0|beta1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/STAN/basic-introduction-to-stan/2018-07-06-super-basic-introduction-to-stan_files/figure-html/diagnostic3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;This tutorial was simply a brief introduction on how simple linear regression models can be fitted using the Bayesian software &lt;code&gt;STAN&lt;/code&gt; via the &lt;code&gt;rstan&lt;/code&gt; package. Although this may seem a complex procedure compared with simply fitting a linear model under the frequentist framework, however, the real advantages of Bayesian methods become evident when the complexity of the analysis is increased (which is often the case in real applications). Indeed, the flexibility in Bayesian modelling allows to account for increasingly complex models in a relatively easy way. In addition, Bayesian methods are ideal when the interest is in taking into account the potential impact that different sources of uncertainty may have on the final results, as they allow the natural propagation of uncertainty throughout each quantity in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-gabry2017bayesplot&#34;&gt;
&lt;p&gt;Gabry, J, and T Mahr. 2017. “Bayesplot: Plotting for Bayesian Models.” &lt;em&gt;R Package Version&lt;/em&gt; 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2015stan&#34;&gt;
&lt;p&gt;Gelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” &lt;em&gt;Journal of Educational and Behavioral Statistics&lt;/em&gt; 40 (5): 530–43.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rstanpackage&#34;&gt;
&lt;p&gt;Stan Development Team. 2018. “RStan: The R Interface to Stan.” &lt;a href=&#34;http://mc-stan.org/&#34;&gt;http://mc-stan.org/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Super basic introduction to OpenBUGS</title>
      <link>/openbugs/basic-introduction-to-openbugs/super-basic-introduction-to-openbugs/</link>
      <pubDate>Tue, 02 Jul 2019 21:11:14 -0500</pubDate>
      
      <guid>/openbugs/basic-introduction-to-openbugs/super-basic-introduction-to-openbugs/</guid>
      <description>


&lt;p&gt;The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using &lt;code&gt;OpenBUGS&lt;/code&gt; via &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Prerequisites:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latest version of &lt;code&gt;R&lt;/code&gt;, which can be downloaded and installed for Windows, Mac or Linux OS from the &lt;a href=&#34;https://www.r-project.org/%7D&#34;&gt;CRAN&lt;/a&gt; website&lt;/li&gt;
&lt;li&gt;I also &lt;strong&gt;strongly&lt;/strong&gt; recommend to download and install &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;Rstudio&lt;/a&gt;, an integrated development environment which provides an “user-friendly” interaction with &lt;code&gt;R&lt;/code&gt; (e.g. many drop-down menus, tabs, customisation options)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Preliminaries&lt;/h1&gt;
&lt;div id=&#34;what-is-openbugs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is OpenBUGS?&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;OpenBUGS&lt;/code&gt; is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (&lt;span class=&#34;citation&#34;&gt;Spiegelhalter et al. (2007)&lt;/span&gt;). &lt;code&gt;OpenBUGS&lt;/code&gt; is a free software based on the &lt;strong&gt;B&lt;/strong&gt;ayesian inference &lt;strong&gt;U&lt;/strong&gt;sing &lt;strong&gt;G&lt;/strong&gt;ibbs &lt;strong&gt;S&lt;/strong&gt;ampling (informally &lt;code&gt;BUGS&lt;/code&gt;) language at the base of &lt;code&gt;WinBUGS&lt;/code&gt; but, unlike this program, is platform independent. The latest version of &lt;code&gt;OpenBUGS&lt;/code&gt; can be dowloaded from the web &lt;a href=&#34;http://www.openbugs.net/w/FrontPage&#34;&gt;repository&lt;/a&gt; and is available for different OS. There are different &lt;code&gt;R&lt;/code&gt; packages which function as frontends for &lt;code&gt;OpenBUGS&lt;/code&gt;. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the &lt;code&gt;R2OpenBUGS&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Sturtz, Ligges, and Gelman (2010)&lt;/span&gt;) and show how to fit &lt;code&gt;OpenBUGS&lt;/code&gt; models using this package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-openbugs-and-r2openbugs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing OpenBUGS and R2OpenBUGS&lt;/h2&gt;
&lt;p&gt;Install the latest version of &lt;code&gt;OpenBUGS&lt;/code&gt; for your OS. Next, install the package &lt;code&gt;R2OpenBUGS&lt;/code&gt; from within &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;Rstudio&lt;/code&gt;, via the package installer or by typing in the command line&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;R2OpenBUGS&amp;quot;, dependencies = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;dependencies = TRUE&lt;/code&gt; option will automatically install all the packages on which the functions in the &lt;code&gt;R2OpenBUGS&lt;/code&gt; package rely.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic model&lt;/h1&gt;
&lt;div id=&#34;simulate-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulate data&lt;/h2&gt;
&lt;p&gt;For an example dataset, I simulate my own data in &lt;code&gt;R&lt;/code&gt;. I create a continuous outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as a function of one predictor &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and a disturbance term &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficients, i.e.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \beta_1 x + \epsilon  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;R&lt;/code&gt; commands which I use to simulate the data are the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; n.sim=100; set.seed(123)
&amp;gt; x=rnorm(n.sim, mean = 5, sd = 2)
&amp;gt; epsilon=rnorm(n.sim, mean = 0, sd = 1)
&amp;gt; beta0=1.5
&amp;gt; beta1=1.2
&amp;gt; y=beta0 + beta1 * x + epsilon&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I define all the data for &lt;code&gt;JAGS&lt;/code&gt; in a list object&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; datalist=list(&amp;quot;y&amp;quot;,&amp;quot;x&amp;quot;,&amp;quot;n.sim&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model file&lt;/h2&gt;
&lt;p&gt;Now, I write the model for &lt;code&gt;OpenBUGS&lt;/code&gt; and save it as a text file named &lt;code&gt;&#34;basicmodbugs.txt&#34;&lt;/code&gt; in the current working directory&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod= &amp;quot;
+ model {
+ #model
+  for(i in 1:n.sim){
+   y[i] ~ dnorm(mu[i], tau)
+   mu[i] &amp;lt;- beta0 + beta1 * x[i]
+  }
+ #priors
+ beta0 ~ dnorm(0, 0.01)
+ beta1 ~ dnorm(0, 0.01)
+ tau ~ dgamma(0.01,0.01)
+ }
+ &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The part of the model inside the for loop denotes the likelihood, which is evaluated for each individual in the sample using a Normal distribution parameterised by some mean &lt;code&gt;mu&lt;/code&gt; and precision &lt;code&gt;tau&lt;/code&gt; (where, precision = 1/variance). The covariate &lt;code&gt;x&lt;/code&gt; is included at the mean level using a linear regression, which is indexed by the intercept &lt;code&gt;beta0&lt;/code&gt; and slope &lt;code&gt;beta1&lt;/code&gt; terms. The second part defines the prior distributions for all parameters of the model, namely the regression coefficients and the precision. Weakly informative priors are used since I assume that I do not have any prior knowledge about these parameters.&lt;/p&gt;
&lt;p&gt;To write and save the model as the text file “basicmodbugs.txt” in the current working directory, I use the &lt;code&gt;writeLines&lt;/code&gt; function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; writeLines(basic.mod, &amp;quot;basicmodbugs.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pre-processing&lt;/h2&gt;
&lt;p&gt;Define the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in &lt;code&gt;OpenBUGS&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params=c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;)
&amp;gt; inits=function(){list(&amp;quot;beta0&amp;quot;=rnorm(1), &amp;quot;beta1&amp;quot;=rnorm(1), &amp;quot;tau&amp;quot;=rgamma(1,1,1))}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters in the object &lt;code&gt;inits&lt;/code&gt; which is then passed to the &lt;code&gt;bugs&lt;/code&gt; function in &lt;code&gt;R2OpenBUGS&lt;/code&gt;. However, for more complex models, this may not be immediate and a lot of trial and error may be required.&lt;/p&gt;
&lt;p&gt;Before using &lt;code&gt;R2OpenBUGS&lt;/code&gt; for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(R2OpenBUGS)
&amp;gt; set.seed(123)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit the model&lt;/h2&gt;
&lt;p&gt;Now, we can fit the model in &lt;code&gt;OpenBUGS&lt;/code&gt; using the &lt;code&gt;bugs&lt;/code&gt; function in the &lt;code&gt;R2openBUGS&lt;/code&gt; package and save it in the object &lt;code&gt;basic.mod&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod.bugs=bugs(data = datalist, inits = inits, 
+   parameters.to.save = params, n.chains = 2, n.iter = 2000,
+   n.burnin = 1000, model.file = &amp;quot;basicmodbugs.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the model is running, the function prints out some information related to the Bayesian graph (corresponding to the specification used for the model) underneath &lt;code&gt;OpenBUGS&lt;/code&gt;, such as number of observed and unobserved nodes and graph size.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;post-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Post-processing&lt;/h2&gt;
&lt;p&gt;Once the model has finished running, a summary of the posteiror estimates and convergence diagnostics for all parameters specified can be seen by typing &lt;code&gt;print(basic.mod)&lt;/code&gt; or, alternatively,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; print(basic.mod.bugs$summary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          mean    sd   2.5%   25%   50%   75% 97.5% Rhat n.eff
beta0      1.5 0.293   0.99   1.3   1.5   1.7   2.1    1  1700
beta1      1.2 0.053   1.06   1.1   1.2   1.2   1.3    1  2000
deviance 278.8 2.439 276.00 277.1 278.2 280.0 285.2    1  2000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The posterior distribution of each parameter is summarised in terms of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mean, sd and some percentiles&lt;/li&gt;
&lt;li&gt;Potential scale reduction factor &lt;code&gt;Rhat&lt;/code&gt; and effective sample size &lt;code&gt;n.eff&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Gelman (2013)&lt;/span&gt;). The first is a measure to assess issues in convergence of the MCMC algorithm (typically a value below &lt;span class=&#34;math inline&#34;&gt;\(1.05\)&lt;/span&gt; for all parameters is considered ok). The second is a measure which assesses the adequacy of the posterior sample (typically values close to the total number of iterations are desirable for all parameters).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The deviance is a goodness of fit statistic and is used in the construction of the “Deviance Information Criterion” or DIC (&lt;span class=&#34;citation&#34;&gt;Spiegelhalter et al. (2014)&lt;/span&gt;), which is a &lt;em&gt;relative&lt;/em&gt; measure of model comparison. The DIC of the model can be accessed by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod.bugs$DIC
[1] 282&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;diagnostics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagnostics&lt;/h2&gt;
&lt;p&gt;More diagnostics are available when we convert the model output into an MCMC object using the command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;coda&amp;quot;)
&amp;gt; library(coda)
&amp;gt; basic.mod.mcmc.bugs=as.mcmc.list(basic.mod.bugs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Different packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the &lt;code&gt;mcmcplots&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Curtis (2015)&lt;/span&gt;) to obtain graphical diagnostics and results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;mcmcplots&amp;quot;)
&amp;gt; library(mcmcplots)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, density and trace plots can be obtained by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; denplot(basic.mod.mcmc.bugs, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/OpenBUGS/basic-introduction-to-openbugs/2018-07-23-super-basic-introduction-to-openbugs_files/figure-html/diagnostic3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(basic.mod.mcmc.bugs, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/OpenBUGS/basic-introduction-to-openbugs/2018-07-23-super-basic-introduction-to-openbugs_files/figure-html/diagnostic3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;This tutorial was simply a brief introduction on how simple linear regression models can be fitted using the Bayesian software &lt;code&gt;OpenBUGS&lt;/code&gt; via the &lt;code&gt;R2OpenBUGS&lt;/code&gt; package. Although this may seem a complex procedure compared with simply fitting a linear model under the frequentist framework, however, the real advantages of Bayesian methods become evident when the complexity of the analysis is increased (which is often the case in real applications). Indeed, the flexibility in Bayesian modelling allows to account for increasingly complex models in a relatively easy way. In addition, Bayesian methods are ideal when the interest is in taking into account the potential impact that different sources of uncertainty may have on the final results, as they allow the natural propagation of uncertainty throughout each quantity in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-curtis2015mcmcplots&#34;&gt;
&lt;p&gt;Curtis, SM. 2015. “Mcmcplots: Create Plots from Mcmc Output.” &lt;em&gt;R Package Version 0.4&lt;/em&gt; 2.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34;&gt;
&lt;p&gt;Gelman, Andrew. 2013. &lt;em&gt;Bayesian Data Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-spiegelhalter2014deviance&#34;&gt;
&lt;p&gt;Spiegelhalter, David J, Nicola G Best, Bradley P Carlin, and Angelika Van Der Linde. 2014. “The Deviance Information Criterion: 12 Years on.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 76 (3): 485–93.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-spiegelhalter2007openbugs&#34;&gt;
&lt;p&gt;Spiegelhalter, David, Andrew Thomas, Nicky Best, and Dave Lunn. 2007. “OpenBUGS User Manual, Version 3.0. 2.” &lt;em&gt;MRC Biostatistics Unit, Cambridge&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-sturtz2010r2openbugs&#34;&gt;
&lt;p&gt;Sturtz, Sibylle, Uwe Ligges, and Andrew Gelman. 2010. “R2OpenBUGS: A Package for Running Openbugs from R.” &lt;em&gt;URL Http://Cran. Rproject. Org/Web/Packages/R2OpenBUGS/Vignettes/R2OpenBUGS. Pdf&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>HESG Summer Meeting 2019</title>
      <link>/post/hesg-norwich-2019/hesg-summer-meeting-2019/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/hesg-norwich-2019/hesg-summer-meeting-2019/</guid>
      <description>&lt;p&gt;I have just come back form my first Health Economists&amp;rsquo; Study Group (&lt;a href=&#34;https://hesg.org.uk/meetings/summer-2019-university-of-east-anglia/&#34; target=&#34;_blank&#34;&gt;HESG&lt;/a&gt;) meeting, which this year was held at the &lt;a href=&#34;https://www.uea.ac.uk/&#34; target=&#34;_blank&#34;&gt;University of East Anglia&lt;/a&gt; in the beautiful city of Norwich,
south east of England, and where I presented some preliminary results from one of my on-going works. I have to say, it was a remarkable experience which
I really liked thanks to a wonderful and welcoming environment. I had the pleasure to talk to many people from different research areas involved in
health economics (both from academia and industry) and to see many different projects and works.&lt;/p&gt;

&lt;p&gt;I particularly enjoy the structure of the meeting, which requires some chair and discussant who have to present and discuss the paper of the authors,
who are only allowed to provide some clarification if needed. At first I thought this structure of the sessions was strange, but after attending many
sessions and experiencing this for my own paper, I feel that it is a very good way to encourage discussion about works from different people rather than
just focussing on your own presentation. Plus, the weather and always sunny, it felt like Italy for a few days.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/Norwich.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;The beautiful Norwich&amp;rsquo;s cathedral&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Other nice people and colleagues from HEART and other UCL department came to HESG with me, including &lt;a href=&#34;https://iris.ucl.ac.uk/iris/browse/profile?upi=CSCLA53&#34; target=&#34;_blank&#34;&gt;Caroline&lt;/a&gt; and &lt;a href=&#34;https://www.ucl.ac.uk/comprehensive-clinical-trials-unit/meet-team/health-economics/junior-health-economist-ekaterina-kuznetsova&#34; target=&#34;_blank&#34;&gt;Ekaterina&lt;/a&gt; (aka Katia),
you can see them in thumbnail of this post. I was also pleased to meet &lt;a href=&#34;https://www.lshtm.ac.uk/aboutus/people/leurent.baptiste&#34; target=&#34;_blank&#34;&gt;Baptiste&lt;/a&gt; from &lt;a href=&#34;https://www.lshtm.ac.uk/&#34; target=&#34;_blank&#34;&gt;LSHTM&lt;/a&gt;, who shares with me the interest in missing data
methods for cost-effectiveness analysis and who presented some very nice work on that. I had the chance to give some feedback to him and he did the same for me.
It felt so nice when we started discussing about some aspects of our analyses and after some minutes we simply lost track of time and everyone else disappeared.
I also had the opportunity to talk about my work with the discussant of my session, &lt;a href=&#34;https://cheme.bangor.ac.uk/CatrinPlumptonBiography.php&#34; target=&#34;_blank&#34;&gt;Catrin Plumpton&lt;/a&gt; from the &lt;a href=&#34;https://cheme.bangor.ac.uk/&#34; target=&#34;_blank&#34;&gt;Centre for Health Economics and Medicines Evaluation&lt;/a&gt;,
who gave me some nice feedback which I really appreciated, especially given her mathematical background.&lt;/p&gt;

&lt;p&gt;An important contribution to the success of the meeting was also given by the wonderful organisation of the event, including an accommodation located very closely
to the main building of the meeting, plenty of food provided during each day, a nice bus tour of the city and a wonderful conference dinner. I must thank all the people,
who organised the event who were very extremely nice to us and who were always ready to help us for whatever need we had, with a special mention for &lt;a href=&#34;https://people.uea.ac.uk/emma_mcmanus&#34; target=&#34;_blank&#34;&gt;Emma Mcmanus&lt;/a&gt; who
was amazing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/cWnICjtVkJJsgGKhyX/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In summary, everything was good. Well, almost. Going back to the works presented, as usual, the only less positive note that I would like to make
is the almost total absence of Bayesian applications. Some authors mentioned that they used some popular Bayesian program, such as &lt;a href=&#34;https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-winbugs/&#34; target=&#34;_blank&#34;&gt;WinBUGS&lt;/a&gt;, but this was
mainly related to the usual meta-analysis stuff which is pretty standardised. I hope next time I will be able to see more people going Bayesian as this is what I am.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding health economics in clinical trials</title>
      <link>/post/introduction-to-health-economics/understanding-health-economics-in-clinical-trials/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/introduction-to-health-economics/understanding-health-economics-in-clinical-trials/</guid>
      <description>&lt;p&gt;As member of the Health Economics Analysis and Research Methods Team (&lt;a href=&#34;https://hearteam.blogspot.com/&#34; target=&#34;_blank&#34;&gt;HEART&lt;/a&gt;), together with my colleagues, on Tuesday 2 July I took part in a 1-day introductory short course entitled “Understanding health economics in clinical trials”, which was designed and delivered by the team. HEART is a new group of health economists who are based in UCL’s Institute of Clinical Trials and Methodology (ICTM), led by &lt;a href=&#34;https://iris.ucl.ac.uk/iris/browse/profile?upi=RMHUN48&#34; target=&#34;_blank&#34;&gt;Rachael Hunter&lt;/a&gt;, and is involved at different levels in the economic components of clinical trials in different trial units at UCL. This short course was aimed at ICTM staff who are not health economists (e.g. trial managers, CIs/PIs, statisticians, data managers, research assistants, etc.) and was designed in response to the need we have identified over the last few years in working on trials as well as in response to colleagues across ICTM. This course was unique as it was intended specifically for non health economists working in trials, who wish to better understand the health economics in their study, and/or the health economist on their study. The course used a mix of lectures, group discussions and practical exercises to help participants consolidate their learning and see how to apply information from the sessions to real studies. No prior knowledge of health economics was assumed.&lt;/p&gt;

&lt;p&gt;I believe the course was a success both in terms of the quality/quantity of the material covered during the six sessions throughout the day, as well as in terms of the positive feedback we received from the participants (almost entirely women, with the exception of two men). Many key and typically not well understood economic topics were discussed during the day, e.g. what are and how QALYs and costs are calculated, the potential limitations and issues of an economic analysis within a trial, or the role played by the protocol and analysis plan in the economic evaluation. My session was related to reporting and interpreting health economic results and I realised that most people who do not routinely deal with health economics may find difficult to grasp certain concepts or tools used in the economic analysis (e.g. what is a cost-effectiveness acceptability curve and how it can be computed). Nevertheless, I must admit that I was surprised by how many people were very motivated to learn these concepts and these &amp;ldquo;difficult&amp;rdquo; methods, often asking questions and making good comments (despite the fact that my session was the last of the course at the end of the day).&lt;br /&gt;
We ran this course as a trial as we did not have clear ideas of what an optimal design should be or the number of topics that should be covered for this type of course. We are now confident that the course has a solid structure and that there is a clear demand to learn the basic concepts of health economics, at least among people involved in trial analyses. Following the successful delivery of the course, we are planning to replicate the experience in the future, improving certain aspects of the sessions based on the feedback we received and also considering to open the course to meet the demand of a wider audience.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/bQrVMr3CO3QaY/giphy.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I have to say that this was an extremely positive experience for me as it was the first time I was involved in this type of projects. Me and my colleagues worked hard to design and prepare the different sessions of the course over the last few months, find the best way to link the arguments across the sessions, provide interesting group activities and materials for the practicals, etc. I have to thank all my colleagues who contributed to the promotion and realisation of this project, with a special mention for &lt;a href=&#34;https://iris.ucl.ac.uk/iris/browse/profile?upi=CSCLA53&#34; target=&#34;_blank&#34;&gt;Caroline Clarke&lt;/a&gt;, who spent a lot of time and effort to organise the course and who personally contributed in giving one of the session of the course. Finally, I would also like to thank my colleague and health economist &lt;a href=&#34;https://www.ucl.ac.uk/comprehensive-clinical-trials-unit/meet-team/health-economics/junior-health-economist-ekaterina-kuznetsova&#34; target=&#34;_blank&#34;&gt;Ekaterina&lt;/a&gt;, with whom I had the pleasure to share the presentation and practical of my session in the course.&lt;/p&gt;

&lt;p&gt;Perhaps the only true negative aspect of the course was the absence of a Bayesian perspective, especially related to the interpretation of the results and the statistical methods that can be used to perform the analysis. Given the generally low familiarity of the people attending the course with statistics, I believe it was reasonable not to further confuse them with another new element into the picture. However, I truly hope that people will become more and more familiar with the importance of using tailored statistical methods in economic evaluations to avoid biased results, and from that point to justify a Bayesian approach, well, at least for me, the step is straightforward!.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Health Economics in Clinical Trials</title>
      <link>/talk/heartcourse2019/</link>
      <pubDate>Tue, 02 Jul 2019 09:00:00 +0000</pubDate>
      
      <guid>/talk/heartcourse2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Super basic introduction to JAGS</title>
      <link>/jags/basic-introduction-to-jags/super-basic-introduction-to-jags/</link>
      <pubDate>Mon, 01 Jul 2019 21:13:14 -0500</pubDate>
      
      <guid>/jags/basic-introduction-to-jags/super-basic-introduction-to-jags/</guid>
      <description>


&lt;p&gt;The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using &lt;code&gt;JAGS&lt;/code&gt; via &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Prerequisites:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latest version of &lt;code&gt;R&lt;/code&gt;, which can be downloaded and installed for Windows, Mac or Linux OS from the &lt;a href=&#34;https://www.r-project.org/%7D&#34;&gt;CRAN&lt;/a&gt; website&lt;/li&gt;
&lt;li&gt;I also &lt;strong&gt;strongly&lt;/strong&gt; recommend to download and install &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;Rstudio&lt;/a&gt;, an integrated development environment which provides an “user-friendly” interaction with &lt;code&gt;R&lt;/code&gt; (e.g. many drop-down menus, tabs, customisation options)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Preliminaries&lt;/h1&gt;
&lt;div id=&#34;what-is-jags&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is JAGS?&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;JAGS&lt;/code&gt; or &lt;strong&gt;J&lt;/strong&gt;ust &lt;strong&gt;A&lt;/strong&gt;nother &lt;strong&gt;G&lt;/strong&gt;ibbs &lt;strong&gt;S&lt;/strong&gt;ampler is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (&lt;span class=&#34;citation&#34;&gt;Plummer (2004)&lt;/span&gt;). &lt;code&gt;JAGS&lt;/code&gt; is a free software based on the &lt;strong&gt;B&lt;/strong&gt;ayesian inference &lt;strong&gt;U&lt;/strong&gt;sing &lt;strong&gt;G&lt;/strong&gt;ibbs &lt;strong&gt;S&lt;/strong&gt;ampling (informally &lt;code&gt;BUGS&lt;/code&gt;) language at the base of &lt;code&gt;WinBUGS/OpenBUGS&lt;/code&gt; but, unlike these programs, it is written in &lt;code&gt;C++&lt;/code&gt; and is platform independent. The latest version of &lt;code&gt;JAGS&lt;/code&gt; can be dowloaded from Martyn Plummer’s &lt;a href=&#34;https://sourceforge.net/projects/mcmc-jags/files/JAGS/&#34;&gt;repository&lt;/a&gt; and is available for different OS. There are different &lt;code&gt;R&lt;/code&gt; packages which function as frontends for &lt;code&gt;JAGS&lt;/code&gt;. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the &lt;code&gt;R2jags&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Su et al. (2015)&lt;/span&gt;) and show how to fit &lt;code&gt;JAGS&lt;/code&gt; models using this package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-jags-and-r2jags&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing JAGS and R2jags&lt;/h2&gt;
&lt;p&gt;Install the latest version of &lt;code&gt;JAGS&lt;/code&gt; for your OS. Next, install the package &lt;code&gt;R2jags&lt;/code&gt; from within &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;Rstudio&lt;/code&gt;, via the package installer or by typing in the command line&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;R2jags&amp;quot;, dependencies = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;dependencies = TRUE&lt;/code&gt; option will automatically install all the packages on which the functions in the &lt;code&gt;R2jags&lt;/code&gt; package rely.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic model&lt;/h1&gt;
&lt;div id=&#34;simulate-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulate data&lt;/h2&gt;
&lt;p&gt;For an example dataset, I simulate my own data in &lt;code&gt;R&lt;/code&gt;. I create a continuous outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as a function of one predictor &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and a disturbance term &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; coefficients, i.e.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \beta_1 x + \epsilon  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;R&lt;/code&gt; commands which I use to simulate the data are the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; n.sim=100; set.seed(123)
&amp;gt; x=rnorm(n.sim, mean = 5, sd = 2)
&amp;gt; epsilon=rnorm(n.sim, mean = 0, sd = 1)
&amp;gt; beta0=1.5
&amp;gt; beta1=1.2
&amp;gt; y=beta0 + beta1 * x + epsilon&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I define all the data for &lt;code&gt;JAGS&lt;/code&gt; in a list object&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; datalist=list(&amp;quot;y&amp;quot;,&amp;quot;x&amp;quot;,&amp;quot;n.sim&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model file&lt;/h2&gt;
&lt;p&gt;Now, I write the model for &lt;code&gt;JAGS&lt;/code&gt; and save it as a text file named &lt;code&gt;&#34;basic.mod.txt&#34;&lt;/code&gt; in the current working directory&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod= &amp;quot;
+ model {
+ #model
+  for(i in 1:n.sim){
+   y[i] ~ dnorm(mu[i], tau)
+   mu[i] = beta0 + beta1 * x[i]
+  }
+ #priors
+ beta0 ~ dnorm(0, 0.01)
+ beta1 ~ dnorm(0, 0.01)
+ tau ~ dgamma(0.01,0.01)
+ }
+ &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The part of the model inside the for loop denotes the likelihood, which is evaluated for each individual in the sample using a Normal distribution parameterised by some mean &lt;code&gt;mu&lt;/code&gt; and precision &lt;code&gt;tau&lt;/code&gt; (where, precision = 1/variance). The covariate &lt;code&gt;x&lt;/code&gt; is included at the mean level using a linear regression, which is indexed by the intercept &lt;code&gt;beta0&lt;/code&gt; and slope &lt;code&gt;beta1&lt;/code&gt; terms. The second part defines the prior distributions for all parameters of the model, namely the regression coefficients and the precision. Weakly informative priors are used since I assume that I do not have any prior knowledge about these parameters.&lt;/p&gt;
&lt;p&gt;To write and save the model as the text file “basic.mod.txt” in the current working directory, I use the &lt;code&gt;writeLines&lt;/code&gt; function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; writeLines(basic.mod, &amp;quot;basic.mod.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pre-processing&lt;/h2&gt;
&lt;p&gt;Define the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in &lt;code&gt;JAGS&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; params=c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;)
&amp;gt; inits=function(){list(&amp;quot;beta0&amp;quot;=rnorm(1), &amp;quot;beta1&amp;quot;=rnorm(1))}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters. However, for more complex models, this may not be immediate and a lot of trial and error may be required. However, &lt;code&gt;JAGS&lt;/code&gt; can automatically select the initial values for all parameters in an efficient way even for relatively complex models. This can be achieved by setting &lt;code&gt;inits=NULL&lt;/code&gt;, which is then passed to the &lt;code&gt;jags&lt;/code&gt; function in &lt;code&gt;R2jags&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before using &lt;code&gt;R2jags&lt;/code&gt; for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; library(R2jags)
&amp;gt; set.seed(123)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit the model&lt;/h2&gt;
&lt;p&gt;Now, we can fit the model in &lt;code&gt;JAGS&lt;/code&gt; using the &lt;code&gt;jags&lt;/code&gt; function in the &lt;code&gt;R2jags&lt;/code&gt; package and save it in the object &lt;code&gt;basic.mod&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod=jags(data = datalist, inits = inits,
+   parameters.to.save = params, n.chains = 2, n.iter = 2000, 
+   n.burnin = 1000, model.file = &amp;quot;basic.mod.txt&amp;quot;)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 100
   Unobserved stochastic nodes: 3
   Total graph size: 406

Initializing model&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the model is running, the function prints out some information related to the Bayesian graph (corresponding to the specification used for the model) underneath &lt;code&gt;JAGS&lt;/code&gt;, such as number of observed and unobserved nodes and graph size.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;post-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Post-processing&lt;/h2&gt;
&lt;p&gt;Once the model has finished running, a summary of the posteiror estimates and convergence diagnostics for all parameters specified can be seen by typing &lt;code&gt;print(basic.mod)&lt;/code&gt; or, alternatively,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; print(basic.mod$BUGSoutput$summary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          mean    sd   2.5%   25%   50%   75% 97.5% Rhat n.eff
beta0      1.5 0.294   0.95   1.3   1.5   1.7   2.1    1  2000
beta1      1.2 0.054   1.07   1.1   1.2   1.2   1.3    1  2000
deviance 278.8 2.475 276.03 277.1 278.2 279.9 285.1    1  2000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The posterior distribution of each parameter is summarised in terms of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mean, sd and some percentiles&lt;/li&gt;
&lt;li&gt;Potential scale reduction factor &lt;code&gt;Rhat&lt;/code&gt; and effective sample size &lt;code&gt;n.eff&lt;/code&gt; (&lt;span class=&#34;citation&#34;&gt;Gelman (2013)&lt;/span&gt;). The first is a measure to assess issues in convergence of the MCMC algorithm (typically a value below &lt;span class=&#34;math inline&#34;&gt;\(1.05\)&lt;/span&gt; for all parameters is considered ok). The second is a measure which assesses the adequacy of the posterior sample (typically values close to the total number of iterations are desirable for all parameters).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The deviance is a goodness of fit statistic and is used in the construction of the “Deviance Information Criterion” or DIC (&lt;span class=&#34;citation&#34;&gt;Spiegelhalter et al. (2014)&lt;/span&gt;), which is a &lt;em&gt;relative&lt;/em&gt; measure of model comparison. The DIC of the model can be accessed by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod$BUGSoutput$DIC
[1] 282&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;diagnostics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagnostics&lt;/h2&gt;
&lt;p&gt;More diagnostics are available when we convert the model output into an MCMC object using the command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; basic.mod.mcmc=as.mcmc(basic.mod)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Different packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the &lt;code&gt;mcmcplots&lt;/code&gt; package (&lt;span class=&#34;citation&#34;&gt;Curtis (2015)&lt;/span&gt;) to obtain graphical diagnostics and results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; install.packages(&amp;quot;mcmcplots&amp;quot;)
&amp;gt; library(mcmcplots)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, density and trace plots can be obtained by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; denplot(basic.mod.mcmc, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/basic-introduction-to-jags/2018-08-23-super-basic-introduction-to-jags_files/figure-html/diagnostic3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; traplot(basic.mod.mcmc, parms = c(&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/JAGS/basic-introduction-to-jags/2018-08-23-super-basic-introduction-to-jags_files/figure-html/diagnostic3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;This tutorial was simply a brief introduction on how simple linear regression models can be fitted using the Bayesian software &lt;code&gt;JAGS&lt;/code&gt; via the &lt;code&gt;R2jags&lt;/code&gt; package. Although this may seem a complex procedure compared with simply fitting a linear model under the frequentist framework, however, the real advantages of Bayesian methods become evident when the complexity of the analysis is increased (which is often the case in real applications). Indeed, the flexibility in Bayesian modelling allows to account for increasingly complex models in a relatively easy way. In addition, Bayesian methods are ideal when the interest is in taking into account the potential impact that different sources of uncertainty may have on the final results, as they allow the natural propagation of uncertainty throughout each quantity in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-curtis2015mcmcplots&#34;&gt;
&lt;p&gt;Curtis, SM. 2015. “Mcmcplots: Create Plots from Mcmc Output.” &lt;em&gt;R Package Version 0.4&lt;/em&gt; 2.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2013bayesian&#34;&gt;
&lt;p&gt;Gelman, Andrew. 2013. &lt;em&gt;Bayesian Data Analysis&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-plummer2004jags&#34;&gt;
&lt;p&gt;Plummer, Martyn. 2004. “JAGS: Just Another Gibbs Sampler.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-spiegelhalter2014deviance&#34;&gt;
&lt;p&gt;Spiegelhalter, David J, Nicola G Best, Bradley P Carlin, and Angelika Van Der Linde. 2014. “The Deviance Information Criterion: 12 Years on.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 76 (3): 485–93.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-su2015package&#34;&gt;
&lt;p&gt;Su, Yu-Sung, Masanao Yajima, Maintainer Yu-Sung Su, and JAGS SystemRequirements. 2015. “Package ‘R2jags’.” &lt;em&gt;R Package Version 0.03-08, URL Http://CRAN. R-Project. Org/Package= R2jags&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian methods for addressing missing data in health economic evaluations</title>
      <link>/talk/albacete2019/</link>
      <pubDate>Tue, 11 Jun 2019 10:00:00 +0000</pubDate>
      
      <guid>/talk/albacete2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations</title>
      <link>/publication/gabrio2019c/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2019c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Statistical Economic Evaluation Methods for Health Technology Assessment</title>
      <link>/publication/gabrio2019b/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2019b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pitfalls of adjusting for mean baseline utilities/costs in trial-based cost-effectiveness analysis with missing data</title>
      <link>/publication/gabrio2019d/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2019d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Full Bayesian Model to Handle Structural Ones and Missingness in Economic Evaluations from Individual-Level Data</title>
      <link>/publication/gabrio2019a/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2019a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Bayesian Parametric Approach to Handle Nonignorable Missingness in Economic Evaluations</title>
      <link>/talk/priment2018/</link>
      <pubDate>Fri, 01 Jun 2018 13:00:00 +0000</pubDate>
      
      <guid>/talk/priment2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Full Bayesian Model to Handle Structural Ones and Missingness in Health Economic Evaluations from Individual-Level Data</title>
      <link>/talk/hesymposium2018/</link>
      <pubDate>Mon, 05 Feb 2018 10:00:00 +0000</pubDate>
      
      <guid>/talk/hesymposium2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Handling Missing Data in Within-Trial Cost-Effectiveness Analysis: A Review with Future Recommendations</title>
      <link>/publication/gabrio2017/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/gabrio2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Missingness Methods in trial-based CEA</title>
      <link>/project/missing-data-review/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/missing-data-review/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;We performed a systematic literature review that assesses the quality of the information reported and type of methods used to handle missing outcome data in trial-based economic evaluations. The purpose of this review is to critically appraise the current literature in within-trial CEAs with respect to the quality of the information reported and the methods used to deal with missingness for both effectiveness and costs. The review complements previous work, covering 2003-2009 (88 articles) with a new systematic review, covering 2009-2015 (81 articles) and focuses on two~perspectives.&lt;/p&gt;

&lt;p&gt;First, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. We propose to address this issue by means of a &lt;em&gt;quality evaluation scheme&lt;/em&gt;, providing a structured approach that can be used to guide the collection of information, formulation of the assumptions, choice of methods, and considerations of possible limitations for the given missingness problem. Second, we review the description of the missing data, the statistical methods used to deal with them and the quality of the judgement underpinning the choice of these methods.&lt;/p&gt;

&lt;h1 id=&#34;quality-evaluation-scheme&#34;&gt;Quality Evaluation Scheme&lt;/h1&gt;

&lt;p&gt;In order to judge whether missing data in CEAs have been adequately handled, we assembled guidelines from previous review articles on how information relating to the missing data should be reported. In particular, we defined three broad components of the analysis that are related to the description of the missingness problem (Description), details of the methods used to address it (Methods) and a discussion on the uncertainty in the conclusions resulting from the missingness (Limitations). For each component, information that is considered to be vital for transparency is listed under &lt;em&gt;key considerations&lt;/em&gt;, while other details that could usefully be provided as supplementary material are suggested under &lt;em&gt;optimal considerations&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Using the list of key considerations, we determine whether null (all key considerations absent), partial (one or more key considerations absent) or full (all key considerations present) information has been provided for each component. The set of key considerations is defined to ensure a full assessment of the impact that missingness may have on the final conclusions of the analysis with respect to all three components. However, providing a certain level of information on one component (e.g.~full information on Description) typically has a different impact on the results with respect to providing the same level of information on another component (e.g.~full information on Limitations). Based on this, we suggest computing a numerical score that weights each component by the impact that it may have on the final results to summarise the overall information provided on missingness.&lt;/p&gt;

&lt;p&gt;Different score values are calculated based on whether full, partial or null information content is provided in each component and by weighting the three components in a ratio of 3:2:1 (Description: Method: Limitations). This weighting scheme has been chosen according to the impact that each component is likely to have on the final conclusions based on assumptions that we deemed to be~reasonable. Specifically, the Limitations component typically has the least importance among the three because of its limited impact on the conclusions. In the same way, the Description component has potentially a higher impact on the results than the Method component as it generally drives the choice for the initial assumptions about the missingness.&lt;/p&gt;

&lt;p&gt;Finally, the relevance of the scores in terms of decision analysis is mainly associated with a qualitative assessment of the articles. Therefore, we suggest converting the scores into ordered grades (A-E) to evaluate the studies based on the overall information reported on the handling of the missing data. Studies that are graded in the top categories should be associated with a higher degree of confidence in their results, whereas more caution should be given in the consideration of results coming from studies that are graded in the bottom categories. When qualitatively assessing the articles, the different grading assigned to each of them could be an indication of a lack in the robustness of the conclusions provided due to missingness uncertainty.
With respect to the quality assessment of the studies, the aggregation of the quality scores on the components of the analysis (Description, Method and Limitations) into ordered grades could lead to some loss of information compared with the direct use of the quality scores on each component. However, merging the scores into a fewer number of categories ensures a relatively easy comparison of the quality of the information provided across the three analysis components and provides a useful indication about the different degree of confidence to assign to the results obtained by each study.&lt;/p&gt;

&lt;p&gt;The Figure below shows a visual representation of the grade (and score) assignment in the quality evaluation scheme. Although the importance between the different components is subjective, the chosen structure represents a reasonable and relatively straightforward assessment scheme.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/diagram.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Quality Evaluation Scheme.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;The articles reviewed for the two periods are presented and compared by type of analysis performed. First, the base-case methods are considered, i.e.~those used in the main analysis. Second, any alternative methods in these analyses are discussed; when present, these assess the robustness of the results obtained in the main analysis against departures from the initial assumptions on missingness.&lt;/p&gt;

&lt;h1 id=&#34;summary-of-the-findings&#34;&gt;Summary of the findings&lt;/h1&gt;

&lt;p&gt;Our review is based on a sample of recently published studies and should therefore provide a picture of current missing data handling in within-trial CEAs. However, the quality assessment of the articles is based on the information reported in the articles. It is possible that authors had assessed the robustness of their conclusions to the missing data using alternative approaches that were not reported in the published version because of space limitations in journals. In these cases, it is important that on-line appendices and supplementary material are used to report these~alternatives.
In our literature review, information about missing data information and methods was available from $4$ and $9$ on-line supplementary materials for the period 2003-2009 and 2009-2015, respectively. Both the larger number of on-line materials and more detailed information reported about missingness handling in the analyses indicate an increased use of this tool in the later period (2009-2015) compared to the first period (2003-2009).&lt;/p&gt;

&lt;h2 id=&#34;descriptive-review&#34;&gt;Descriptive Review&lt;/h2&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/res_methods.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Missingness methods by outcome and period.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;From the comparison of the base-case methods used for the costs and effects between 2009 and 2015, the Figure above shows a marked reduction in the number of methods not clearly described for the effects, compared to those for the costs. A possible reason for this is that, while clinical effectiveness measures are often collected through self-reported questionnaires, which are naturally prone to missingness, cost measures rely more on clinical patient files which may ensure a higher completeness rate. It was not possible to confirm this interpretation in the reviewed studies due to the high proportions of articles not clearly reporting the missing rates in both 2003-2009 and 2009-2015 periods, for effects
($\approx 45\%$ and $\approx 38\%$) and costs ( $\approx 50\%$ and $\approx 62\%$). In addition, clinical outcomes are almost invariably the main objective of RCTs and are usually subject to more advanced and standardised analyses. Arguably, costs are often considered as an add-on to the standard trial: for instance, sample size calculations are almost always performed with the effectiveness measure as the only outcome of interest. Consequently, missing data methods are less frequently well thought through for the analysis of the costs. However, this situation is likely to change as cost data from different perspectives (e.g. caregivers, patients, society, etc.) are being increasingly used in trials, leading to the more frequent adoption of self-report cost data which may start to exhibit similar missingness characteristics to effect data.&lt;/p&gt;

&lt;p&gt;The review identified only a few articles using more than one alternative method. In addition, these analyses are typically conducted without any clear justification about their underlying missing data assumptions and may therefore not provide a concrete assessment of the impact of missingness uncertainty. This situation indicates a gap in the literature associated with an under-implementation of sensitivity analysis, which may significantly affect the whole decision-making process outcome, under the perspective of a body who is responsible for providing recommendations about the implementation of alternative interventions for health care matters.&lt;/p&gt;

&lt;p&gt;Limiting the assessment of missingness assumptions to a single case is unlikely to provide a reliable picture of the underlying mechanism. This, in turn, may have a significant impact on the CEA and mislead its conclusions, suggesting the implementation of non-cost-effective treatments. Robustness analyses assess the sensitivity of the results to alternative missing data methods but do not justify the choice of these methods and their underlying assumptions about missingness which may therefore be inappropriate in the specific context analysed. By contrast, sensitivity analyses, which rely on external information to explore plausible alternative methods and missingness assumptions, represent an important and more appropriate tool to provide realistic assessments of the impact of missing data uncertainty on the final conclusions.&lt;/p&gt;

&lt;h2 id=&#34;quality-assessment&#34;&gt;Quality assessment&lt;/h2&gt;

&lt;p&gt;Generally speaking, most of the reviewed papers achieved an unsatisfactory quality score under the Quality Evaluation Scheme. Indeed, the benchmark area on the top-right corner of the graphs is barely reached by less than $7\%$ of the articles, both for cost and effect data.&lt;/p&gt;

&lt;p&gt;Overall, the proportions of the studies associated with the lowest category (E) prevails in the majority of the years, with a similar pattern over time between missing costs and effects. All the articles that are associated with the top category (A) belong to the period 2013-2015, with the highest proportions of articles falling in this category being observed in 2015 for both outcomes.
The opportunity of reaching such a target might be precluded by the choice of the method adopted, which may not be able to support less restrictive assumptions about missingness, even when this would be desirable. As a result, when simple methods cannot be fully justified it is necessary to replace them with more flexible ones that can relax assumptions and incorporate more alternatives. In settings such as those involving MNAR, sensitivity analysis might represent the only possible approach to account for the uncertainty due to the missingness in a principled way. However, due to the lack of studies either performing a sensitivity analysis or providing high quality scores on the assumptions, missingness is not adequately addressed in most studies. This could have the serious consequence of imposing too restrictive assumptions about missingness and affect the outcome of decision making.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;Our review shows, over time, a significant change from more to less restrictive methods in terms of the assumptions on the missingness mechanism. This is an encouraging movement towards a more suitable and careful missing data analysis. The results from the disaggregated analysis by year of publication in the later period (2009-2015) indicates the rise of a better and more transparent approach to handle missingness in the latest years of the review, especially in 2015. In particular, compared to the previous years, the articles reviewed from 2015 are associated with a higher proportion of MI methods used in the base-case analysis, a substantial increase in the number of robustness methods implemented, and a better quality score assignment.&lt;/p&gt;

&lt;p&gt;Nevertheless, improvements are still needed as, overall, only a small number of articles provide transparent information about the missing data and almost no study performs a sensitivity~analysis. These failings are probably due to the fact that the implications of using methods that do not handle missingness in a principled way are not well-known among practitioners. In addition, the choice of the missing data methods may also be guided by their ease of implementation in standard software packages rather than methodological reasons. This is a potentially serious issue for bodies such as the NICE who use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new treatment options.&lt;/p&gt;

&lt;p&gt;The Quality Evaluation Scheme represents a valuable tool to improve missing data handling. By carefully thinking about each component in the analysis we are forced to explicitly consider all the assumptions we make about missingness and assess the impact of their variation on final conclusions. The main advantage is a more comparable formalisation of the uncertainty as well as a better indication of possible issues in assessing the cost-effectiveness of new treatments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Handling Missing Data in Within-Trial Cost-Effectiveness Analysis: a Review with Future Recommendations</title>
      <link>/talk/euhea2016/</link>
      <pubDate>Thu, 08 Sep 2016 15:00:00 +0000</pubDate>
      
      <guid>/talk/euhea2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Available Case Analysis</title>
      <link>/missmethods/available-case-analysis/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/available-case-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Methods for Health Technology Assessment</title>
      <link>/project/health-economics/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/health-economics/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;The type of data used in economic evaluations typically come from a range of sources, whose evidence is combined to inform HTA decision-making. Traditionally, relative effectiveness data are derived from &lt;em&gt;randomised controlled clinical trials&lt;/em&gt; (RCTs), while healthcare resource utilisation, costs and preference-based quality of life data may come from the same study that estimated the clinical effectiveness or not. A number of HTA agencies have developed their own methodological guidelines to support the generation of the evidence required to inform their decisions. In this context, the primary role of economic evaluation for HTA is not the estimation of the quantities of interest (e.g. the computation of point or interval estimation, or hypothesis testing), but to aid decision making. The implication of this is that the standard frequentist analyses that rely on power calculations and $P$-values to estimate  statistical and clinical significance, typically used in RCTs, are not well-suited for addressing these HTA requirements.&lt;/p&gt;

&lt;p&gt;It has been argued that, to be consistent with its intended role in HTA, economic evaluation should embrace a decision-theoretic paradigm and develop ideally within a Bayesian statistical framework to inform two decisions&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;whether the treatments under evaluation are cost-effective given the available evidence and&lt;/li&gt;
&lt;li&gt;whether the level of uncertainty surrounding the decision is acceptable (i.e. the potential benefits are worth the costs of making the wrong decision).&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This corresponds to quantify the impact of the uncertainty in the evidence on the entire decision-making process (e.g. to what extent the uncertainty in the estimation of the effectiveness of a new intervention affects the decision about whether it is paid for by the public provider).&lt;/p&gt;

&lt;h1 id=&#34;bayesian-methods-in-hta&#34;&gt;Bayesian methods in HTA&lt;/h1&gt;

&lt;p&gt;There are several reasons that make the use of Bayesian methods in economic evaluations particularly appealing. First, Bayesian modelling is naturally embedded in the wider scheme of decision theory; by taking a probabilistic approach, based on decision rules and available information, it is possible to explicitly account for relevant sources of uncertainty in the decision process and obtain an &lt;em&gt;optimal&lt;/em&gt; course of action. Second, Bayesian methods allow extreme flexibility in modelling using computational algorithms such as &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; (MCMC) methods; this allows to handle in a relatively easy way the generally sophisticated structure of the relationships and complexities that characterise effectiveness, quality of life and cost data. Third, through the use of prior distributions, the Bayesian approach naturally allows the incorporation of evidence from different sources in the analysis (e.g. expert opinion or multiple studies), which may improve the estimation of the quantities of interest; the process is generally referred to as evidence synthesis and finds its most common application in the use of meta-analytic tools. This may be extremely important when, as it often happens, there is only some partial (imperfect) information to identify the model parameters. In this case analysts are required to develop chain-of-evidence models. When required by the limitations in the evidence base, subjective prior distributions can be specified based on the synthesis and elicitation of expert opinion to identify the model, and their impact on the results can be assessed by presenting or combining the results across a range of plausible alternatives. Finally, under a Bayesian approach, it is straightforward to conduct &lt;em&gt;sensitivity analysis&lt;/em&gt; to properly account for the impact of uncertainty in all inputs of the decision process; this is a required component in the approval or reimbursement of a new intervention for many decision-making bodies, such as NICE in the UK.&lt;/p&gt;

&lt;p&gt;The general process of conducting a Bayesian analysis (with a view of using the results of the model to perform an economic evaluation) can be broken down in several steps, which are graphically summarized in the Figure below.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/HTA.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Process of health economic evaluation.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;The starting point is the identification of the decision problem, which defines the objective of the economic evaluation (e.g. the interventions being compared, the target population, the relevant time horizon). In line with the decision problem, a statistical model is constructed to describe the (by necessity, limited) knowledge of the underlying clinical pathways. This implies, for example, the definition of suitable models to describe variability in potentially observed data (e.g. the number of patients recovering from the disease because of a given treatment), as well as the epistemic uncertainty in the population parameters (e.g.~the underlying probability that a random individual in the target population is cured, if given the treatment under study).  At this point, all the relevant data are identified, collected and quantitatively sytnthesised to derive the estimates of the input parameters of interest for the model.&lt;/p&gt;

&lt;p&gt;These parameter estimates (and associated uncertainties) are then fed to the economic model, with the objective of obtaining some relevant summaries indicating the benefits and costs for each intervention under evaluation. Uncertainty analysis represents some sort of &lt;em&gt;detour&lt;/em&gt; from the straight path going from the statistical model to the decision analysis: if the output of the statistical model allowed us to know with perfect certainty the &lt;em&gt;true&lt;/em&gt; value of the model parameters, then it would be possible to simply run the decision analysis and make the decision.  Of course, even if the statistical model were the &lt;em&gt;true&lt;/em&gt; representation of the underlying data generating process (which it most certainly is not), because the data may be limited in terms of length of follow up, or sample size, the uncertainty in the value of the model parameters would still remain. This &lt;em&gt;parameter&lt;/em&gt; (and &lt;em&gt;structural&lt;/em&gt;) uncertainty is propagated throughout the whole process to evaluate its impact on the decision-making. In some cases, although there might be substantial uncertainty in the model inputs, this may not turn out to modify substantially the output of the decision analysis, i.e. the new treatment would be deemed as optimal irrespectively.  In other cases, however, even a small amount of uncertainty in the inputs could be associated with very serious consequences. In such circumstances, the decision-maker may conclude that the availbale evidence is not sufficient to decide on which intervention to select and require more information before a decision can be made.&lt;/p&gt;

&lt;p&gt;The results of the above analysis can be used to inform policy makers about two related decisions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;whether the new intervention is to be considered (on average) &lt;em&gt;value for money&lt;/em&gt;, given the evidence base available at the time of decision, and&lt;/li&gt;
&lt;li&gt;whether the consequences (in terms of net health loss) of making the wrong decision would warrant further research to reduce this &lt;em&gt;decision uncertaint&lt;/em&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the type and specification of the statistical and economic models vary with the nature of the underlying data (e.g. individual (ILD) level versus aggregated (ALD) data, the decision and uncertainty analyses have a more standardised set up.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;HTA has been slow to adopt Bayesian methods; this could be due to a reluctance to use prior opinions, unfamiliarity, mathematical complexity, lack of software, or conservatism of the healthcare establishment and, in particular, the regulatory~authorities. However, the use of Bayesian approach has been increasingly advocated as an efficient tool to integrate statistical evidence synthesis and parameter estimation with probabilistic decision analysis in an unified framework for HTA. This enables a transparent &lt;em&gt;evidence-based&lt;/em&gt; decision modelling, reflecting the uncertainty and the structural relationships in all the available~data.&lt;/p&gt;

&lt;p&gt;With respect to trial-based analyses, the flexibility and modularity of the Bayesian modelling structure are well-suited to jointly account for the typical complexities that affect ILD. In addition, prior distributions can be used as convenient means to incorporate external information into the model when the evidence from the data is limited or absent (e.g. for missing values). In the context of evidence synthesis, the Bayesian approach is particularly appealing in that it allows for all the uncertainty and correlation induced by the often heterogeneous nature of the evidence (either ALD only or both ALD and ILD) to be synthesised in a way that can be easily integrated within a decision modelling framework.&lt;/p&gt;

&lt;p&gt;The availability and spread of Bayesian software among practitioners since the late 1990s, such as &lt;code&gt;OpenBUGS&lt;/code&gt; or &lt;code&gt;JAGS&lt;/code&gt;, has greatly improved the applicability and reduced the computational costs of these models. Thus, analysts are provided with a powerful framework, which has been termed &lt;em&gt;comprehensive decision modelling&lt;/em&gt;, for simultaneously estimating posterior distributions for parameters based on specified prior knowledge and data evidence, and for translating this into the ultimate measures used in the decision analysis to inform cost-effectiveness conclusions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Modelling for Health Economic Evaluations</title>
      <link>/project/bayesian-modelling/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/bayesian-modelling/</guid>
      <description>

&lt;h1 id=&#34;modelling-framework&#34;&gt;Modelling Framework&lt;/h1&gt;

&lt;p&gt;We propose a unified Bayesian framework that jointly accounts for the typical complexities of the data (e.g. correlation, skewness, spikes at the boundaries  and missingness), and that can be implemented in a relatively easy way.&lt;/p&gt;

&lt;p&gt;Consider the usual cross-sectional bivariate outcome formed by the QALYs and total cost variables $(e_{it}, c_{it})$ calculated for the $i-$th person in group $t$ of the trial. To simplify the notation, unless necessary, we suppress the treatment indicator $t$.
We specify the joint distribution $p(e_i,c_i)$ as&lt;/p&gt;

&lt;p&gt;\[
p(e_i,c_i) = p(c_i)p(e_i\mid c_i) = p(e_i)p(c_i\mid e_i)
\]&lt;/p&gt;

&lt;p&gt;where, for example, $p(e_i)$ is the &lt;em&gt;marginal&lt;/em&gt; distribution of the QALYs and $p(c_i\mid e_i)$ is the &lt;em&gt;conditional&lt;/em&gt; distribution of the costs given the QALYs. Note that, although the two factorisations are mathematically equivalent, the choice of which to use has different practical implications. From a statistical point of view, the factorisations require the specifications of different statistical models, e.g. $p(e_i)$ or $p(e_i\mid c_i)$, which may have different approximation errors. From a clinical point of view, the two versions make different assumptions about the casual relationships between the outcomes, i.e. either $e_i$ determines $c_i$ or vice versa. We describe our analysis under the assumption that the costs are determined by the effectiveness measures and therefore we specify the joint distribution $p(e_i,c_i)$ in terms of a marginal distribution for the QALYs and a conditional distribution for the costs.&lt;/p&gt;

&lt;p&gt;For each individual we consider a marginal distribution $p(e_i \mid \boldsymbol \theta_e)$ indexed by a set of parameters $\boldsymbol \theta_e$ comprising a &lt;em&gt;location&lt;/em&gt; $\boldsymbol \phi_{ie}$ and a set of &lt;em&gt;ancillary&lt;/em&gt; parameters $\boldsymbol\psi_e$ typically including some measure of &lt;em&gt;marginal&lt;/em&gt; variance $\sigma^2_e$. We can model the location parameter using a generalised linear structure, e.g.&lt;/p&gt;

&lt;p&gt;\[
g_e(\phi_{ie})= \alpha_0 \,\,[+ \ldots]
\]&lt;/p&gt;

&lt;p&gt;where $\alpha_0$ is the intercept and the notation $[+\ldots]$ indicates that other terms (e.g. quantifying the effect of relevant covariates) may or may not be included. In the absence of covariates or assuming that a centered version $x_i^{\star} = (x_i - \bar{x})$ is used, the parameter $\mu_e = g_e^{-1}(\alpha_0)$ represents the population average QALYs. For the costs, we consider a conditional model $p(c_i\mid e_i,\boldsymbol\theta_c)$, which explicitly depends on the QALYs, as well as on a set of quantities $\boldsymbol\theta_c$, again comprising a location $\phi_{ic}$ and ancillary parameters $\boldsymbol \psi_{c}$. For example, when normal distributions are assumed for both $p(e_i \mid \boldsymbol \theta_e)$ and $p(c_i \mid e_i, \boldsymbol \theta_c)$, i.e. bivariate normal on both outcomes, the ancillary parameters $\boldsymbol\psi_c$ include a &lt;em&gt;conditional&lt;/em&gt; variance $\tau^2_c$, which can be expressed as a function of the marginal variance $\sigma^2_c$. More specifically, the conditional variance of $p(c_i \mid e_i, \boldsymbol \theta_c)$ is a function of the marginal effectiveness and cost variances and has the closed form $\tau^2_c=\sigma^2_c - \sigma^2_e \beta^2$, where $\beta=\rho \frac{\sigma_c}{\sigma_e}$ and $\rho$ is the parameter capturing the correlation between the variables.&lt;/p&gt;

&lt;p&gt;The location can be modelled as a function of the QALYs as&lt;/p&gt;

&lt;p&gt;\[
g_c(\phi_{ic}) = \beta_{0} + \beta_{1}(e_{i}-\mu_{e})\,\,[+\ldots]
\]&lt;/p&gt;

&lt;p&gt;Here, $(e_i-\mu_e)$ is the centered version of the QALYs, while $\beta_{1}$ quantifies the correlation between costs and QALYs. Assuming other covariates are either also centered or absent, $\mu_c = g_c^{-1}(\beta_{0})$ is the estimated population average cost. The Figure below shows a graphical representation of the general modelling framework.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/framework.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Modelling framework.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;The QALYs and cost distributions are represented in terms of combined &lt;em&gt;modules&lt;/em&gt;, the blue and the red boxes, in which the random quantities are linked through logical relationships. This ensures the full characterisation of the uncertainty for each variable in the model. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in either or both the modules.&lt;/p&gt;

&lt;p&gt;The proposed framework allows jointly tackling of the different complexities that affect the data in a relatively easy way by means of its modular structure and flexible choice for the distributions of the QALYs and cost variables. Using the MenSS trial as motivating example, we start from the original analysis and expand the model using alternative specifications that progressively account for an increasing number of complexities in the outcomes. We specifically focus on appropriately modelling spikes at the boundary and missingness, as they have substantial implications in terms of inferences and, crucially, cost-effectiveness results.&lt;/p&gt;

&lt;h1 id=&#34;example&#34;&gt;Example&lt;/h1&gt;

&lt;p&gt;Three model specifications are considered and applied to QALY data from a RCT case study: 1) Normal marginal for the QALYs and Normal conditional for the costs (which is identical to a Bivariate Normal distribution for the two outcomes); 2) Beta marginal for the QALYs and Gamma conditional for the costs; and 3) Hurdle Model. The following Figure shows the observed QALYs in both treatment groups (indicated with black crosses) as well as summaries of the posterior distributions for the imputed values, obtained from each model. Imputations are distinguished based on whether the corresponding baseline utility value is observed or missing (blue or red lines and dots, respectively) and are summarised in terms of posterior mean and $90\%$ HPD intervals.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/imputations.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Imputed QALYs under alternative model specifications.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;There are clear differences in the imputed values and corresponding credible intervals between the three models in both treatment groups. Neither the Bivariate Normal nor the Beta-Gamma models  produce imputed values that capture the structural one component in the data. In addition, as to be expected, the Bivariate Normal fails to respect the natural support for the observed QALYs, with many of the imputations exceeding the unit threshold bound. These unrealistic imputed values highlight the inadequacy of the Normal distribution for the data and may lead to distorted inferences. Conversely, imputations under the Hurdle Model are more realistic, as they can replicate values in the whole range of the observed data, including the structural ones. Imputed unit QALYs with no discernible interval are only observed in the intervention group due to the original data composition, i.e. individuals associated with a unit baseline utility and missing QALYs are almost exclusively present in the intervention group.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;We have presented a flexible Bayesian framework that can handle the typical complexities affecting outcome data in CEA, while also being relatively easy to implement using freely available Bayesian software.  This is a key advantage that can encourage practitioners to move away from likely biased methods and promote the use of our framework in routine analyses. In conclusion, the proposed framework can:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Jointly model costs and QALYs;&lt;/li&gt;
&lt;li&gt;Account for skewness and structural values;&lt;/li&gt;
&lt;li&gt;Assess the robustness of the results under a set of differing missingness assumptions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The original contribution of this work consists in the joint implementation of methods that account for the complexities of the data within a unique and flexible framework that is relatively easy to apply. In the next chapter we will take a step forward in the analysis and present a longitudinal model that can use all observed utility and cost data in the analysis, explore alternative nonignorable missing data assumptions, while simultaneously handling the complexities that affect the data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Complete Case Analysis</title>
      <link>/missmethods/complete-case-analysis/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/complete-case-analysis/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Complete case analysis&lt;/em&gt; (CCA), also known as &lt;em&gt;case&lt;/em&gt; or &lt;em&gt;listwise deletion&lt;/em&gt; (LD), is one of the oldest methods to handle missing data and consists in discarding any unit or case whose information is incomplete. Only the cases with observed values for all the variables under consideration are used in the analysis. For example, suppose we have a data set formed by &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; individuals and that we want to fit a linear regression on some outcome variable &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; using some other variables &lt;span class=&#34;math inline&#34;&gt;\(x_{i1},\ldots,x_{ik}\)&lt;/span&gt; as covariates. CCA uses only the subset of cases with observed values on all the variables included in the analysis (&lt;em&gt;completers&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;CCA has been a quite popular approach to deal with missingness, mainly because it is very easy to implement (used by default in many statistical programs) and it allows the comparison of different univariate statistics in a straightforward way (calculated on a common set of cases). However, there are a number of potential disadvantages which threatens the validity of this method:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Bias, when the missing data mechanism is not &lt;em&gt;missing completely at random&lt;/em&gt; (MCAR) and the completers are not a random samples of all the cases&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Loss of efficiency, due to the potential loss of information in discarding the incomplete cases.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;CCA may be justified when the loss of precision and bias are minimal, which is more likley to occur when the proportion of completers is high, although it is difficult to formulate rules that apply in general circumstances. Indeed, both the degree of loss of precision and bias depend not only on the fraction of completers and missingness patterns, but also on the extent to which complete and incomplete cases differ and the parameters of interest.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_{cc}\)&lt;/span&gt; be an estimate of a parameter of interest from the completers. One might measure the increase in variance of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_{cc}\)&lt;/span&gt; with respect to the estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt; that would be obtained in the absence of missing values. Using the notation of &lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;\[
\text{Var}(\hat{\theta}_{cc}) = \text{Var}(\hat{\theta})(1 + \Delta^{\star}_{cc}),
\]&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Delta^{\star}_{cc}\)&lt;/span&gt; is the proportional increase in variance from the loss of information. A more practical measure of the loss of inofrmation is &lt;span class=&#34;math inline&#34;&gt;\(\Delta_{cc}\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;\[
\text{Var}(\hat{\theta}_{cc}) = \text{Var}(\hat{\theta}_{eff})(1 + \Delta_{cc}),
\]&lt;/p&gt;
&lt;p&gt;and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_{eff}\)&lt;/span&gt; is an efficient estimate based on all the available data.&lt;/p&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;p&gt;Consider bivariate normal monotone data &lt;span class=&#34;math inline&#34;&gt;\(\bf y = (y_1,y_2)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(n_{cc}\)&lt;/span&gt; out of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; cases are complete and &lt;span class=&#34;math inline&#34;&gt;\(n - n_{cc}\)&lt;/span&gt; cases have observed values only on &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;. Assume for simplicity that the missingness mechanism is MCAR and that the mean of &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; is estimated by the empirical mean from the complete cases &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}^{cc}_j\)&lt;/span&gt;. Then, the loss in sample size for estimating the mean of &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;\[
\Delta^{\star}_{cc} = \Delta_{cc} = \frac{n - n_{cc}}{n_{cc}},
\]&lt;/p&gt;
&lt;p&gt;so that if half the cases are missing, the variance is doubled. For the mean of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt;, the loss of information alos depends on the squared correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}\)&lt;/span&gt; between the variables (&lt;span class=&#34;citation&#34;&gt;Little and Rubin (2019)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;\[
\Delta_{cc} \approx \frac{n - n_{cc}\rho^{2}}{n_{cc}(1 - \rho^{2}) + n_{cc}\rho^{2}}.
\]&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Delta_{cc}\)&lt;/span&gt; varies from zero (when &lt;span class=&#34;math inline&#34;&gt;\(\rho=0\)&lt;/span&gt;) to &lt;span class=&#34;math inline&#34;&gt;\(\Delta^{\star}_{cc}\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2} \rightarrow 1\)&lt;/span&gt;. However, for the regression coefficient of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; we have that &lt;span class=&#34;math inline&#34;&gt;\(\Delta\_{cc}=0\)&lt;/span&gt; since the incomplete observations of &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; provide no information for the regression of &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2&lt;/h2&gt;
&lt;p&gt;For inference about the population mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, the bias of CCA depends on the proportion of the completers &lt;span class=&#34;math inline&#34;&gt;\(\pi_{cc}\)&lt;/span&gt; and the extent to which complete and incomplete cases differ on the variables of interest. Suppose a variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is partially-observed and that we partition the data into the subset of the completers &lt;span class=&#34;math inline&#34;&gt;\(y_{cc}\)&lt;/span&gt; and incompleters &lt;span class=&#34;math inline&#34;&gt;\(y_{ic}\)&lt;/span&gt;, with associated population means &lt;span class=&#34;math inline&#34;&gt;\(\mu_{cc}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ic}\)&lt;/span&gt;, respectively. The overall mean can be written as a weighted average of the means of the two subsets&lt;/p&gt;
&lt;p&gt;\[
\mu = \pi_{cc}\mu_{cc} + (1 - \pi_{cc}\mu_{ic}).
\]&lt;/p&gt;
&lt;p&gt;The bias of CCA is then equal to the expected fraction of incomplete cases multiplied by the differences in the means for complete and incomplete cases&lt;/p&gt;
&lt;p&gt;\[
\mu_{cc} - \mu = (1 - \pi_{cc})(\mu_{cc} - \mu_{ic}).&lt;br /&gt;
\]&lt;/p&gt;
&lt;p&gt;Under MCAR, we have that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{cc} = \mu_{ic}\)&lt;/span&gt; and therefore the bias is zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 3&lt;/h2&gt;
&lt;p&gt;Consider the estimation of the regression of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(x_1,\ldots,x_k\)&lt;/span&gt; from data with potential missing values on all variables and with the regression function correctly specified. The bias of CCA for estimating the regression coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_1,\ldots,\beta_k\)&lt;/span&gt; associated with the covariates is null if the probbaility of being a completer depends on the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;s but not &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, since the analysis conditions on the values of the covariates (&lt;span class=&#34;citation&#34;&gt;Glynn and Laird (1986)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;White and Carlin (2010)&lt;/span&gt;). This class of missing data mechanisms includes &lt;em&gt;missing not at random&lt;/em&gt; (MNAR), where the probability that a covariate is missing depends on the value of that covariate. However, CCA is biased if the probability of being a completer depends on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; after conditioning on the covariates. A nice example of this particular topic and its implications for the analysis has been provided by professor &lt;a href=&#34;https://thestatsgeek.com/about-thestatsgeek-com/&#34;&gt;Bartlett&lt;/a&gt; using some nice &lt;a href=&#34;http://thestatsgeek.com/wp-content/uploads/2016/08/Jonathan-Bartlett-28-06-2013.pdf&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The main virtue of case deletion is simplicity. If a missing data problem can be resolved by discarding only a small part of the sample, then the method can be quite effective. However, even in that situation, one should explore the data (&lt;span class=&#34;citation&#34;&gt;Schafer and Graham (2002)&lt;/span&gt;). The discarded information from incomplete cases can be used to study whether the complete cases are plausibly a random subsample of the original sample, that is, whether MCAR is a reasonable assumption. A simple procedure is to compare the distribution of a particular variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; based on complete cases with the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; based on incomplete cases for which &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is recorded. Significant differences indicate that the MCAR assumption is invalid, and the complete-case analysis yields potentially biased estimates. Such tests are useful but have limited power when the sample of incomplete cases is small. Also the tests can offer no direct evidence on the validity of the &lt;em&gt;missing at random&lt;/em&gt; (MAR) assumption.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-glynn1986regression&#34;&gt;
&lt;p&gt;Glynn, RJ, and NM Laird. 1986. “Regression Estimates and Missing Data: Complete Case Analysis.” &lt;em&gt;Cambridge MA: Harvard School of Public Health, Department of Biostatistics&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-little2019statistical&#34;&gt;
&lt;p&gt;Little, Roderick JA, and Donald B Rubin. 2019. &lt;em&gt;Statistical Analysis with Missing Data&lt;/em&gt;. Vol. 793. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schafer2002missing&#34;&gt;
&lt;p&gt;Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” &lt;em&gt;Psychological Methods&lt;/em&gt; 7 (2): 147.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-white2010bias&#34;&gt;
&lt;p&gt;White, Ian R, and John B Carlin. 2010. “Bias and Efficiency of Multiple Imputation Compared with Complete-Case Analysis for Missing Covariate Values.” &lt;em&gt;Statistics in Medicine&lt;/em&gt; 29 (28): 2920–31.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Augmentation via MCMC</title>
      <link>/missmethods/data-augmentation-mcmc/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/data-augmentation-mcmc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Joint Multiple Imputation</title>
      <link>/missmethods/joint-multiple-imputation/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/joint-multiple-imputation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Last Value Carried Forward</title>
      <link>/missmethods/last-value-carried-forward/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/last-value-carried-forward/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mean Imputation</title>
      <link>/missmethods/mean-imputation/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/mean-imputation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple Imputation by Chained Equations</title>
      <link>/missmethods/multiple-imputation-by-chained-equations/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/missmethods/multiple-imputation-by-chained-equations/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Nonignorable Missingness Models in HTA</title>
      <link>/project/missing-data/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/missing-data/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Economic evaluation alongside Randomised Clinical Trials (RCTs) is an important and increasingly popular component of the process of technology appraisal. The typical analysis of individual level data involves the comparison of two interventions for which suitable measures of clinical benefits and costs are observed on each patient enrolled in the trial at different time points throughout the follow up. Individual level data from RCTs are almost invariably affected by missingness. The recorded outcome process is often incomplete due to individuals who drop out or are observed intermittently throughout the study, causing some observations to be missing. In most applications, the economic evaluation is performed on the cross-sectional variables, computed using only the data from the individuals who are observed at each time point in the trial (completers), with at most limited sensitivity analysis to missingness assumptions. This, however, is an extremely inefficient approach as the information from the responses of all partially observed subjects is completely lost and it is also likely biased unless the completers are a random sample of the subjects on each arm. The problem of missingness is often embedded within a more complex framework, which makes the modelling task in economic evaluations particularly challenging. Specifically, the effectiveness and cost data typically present a series of complexities that need to be simultaneously addressed to avoid biased results.&lt;/p&gt;

&lt;p&gt;Using a recent randomised trial as our motivating example, we present a Bayesian parametric model for conducting inference on a bivariate health economic longitudinal response. We specify our model to account for the different types of complexities affecting the data while accommodating a sensitivity analysis to explore the impact of alternative missingness assumptions on the inferences and on the decision-making process for health technology assessment.&lt;/p&gt;

&lt;h1 id=&#34;standard-approach&#34;&gt;Standard approach&lt;/h1&gt;

&lt;p&gt;To perform the economic evaluation, aggregated measures for both utilities and costs are typically derived from the longitudinal responses recorded in the study. QALYs ($e_{it}$) and total costs ($c_{it}$) measures are computed as:&lt;/p&gt;

&lt;p&gt;\[
e_{it}=\sum_{j=1}^{J}(u_{ijt}+u_{ij-1t})\frac{\delta_{j}}{2} \;\;\; \text{and} \;\;\;\  c_{it}=\sum_{j=1}^{J}c_{ijt},
\]&lt;/p&gt;

&lt;p&gt;where $t$ denotes the treatment group, while $\delta_{j}=\frac{\text{Time}_{j}-\text{Time}_{j-1}}{\text{Unit of time}}$ is the percentage of the time unit (typically one year) which is covered between time $j-1$ and $j$ in the trial. The economic evaluation is then performed by applying some parametric model $p(e_{it},c_{it}\mid \boldsymbol \theta)$, indexed by a set of parameters $\boldsymbol \theta$, to these cross-sectional quantities, typically using linear regression methods to account for the imbalance in some baseline variables between treatments. We note that the term cross-sectional here refers to analyses based on variables derived from the combination of repeated measurements collected at different times over the trial duration and not on data collected at a single point in time.
Finally, QALYs and total costs population mean values are derived from the model:&lt;/p&gt;

&lt;p&gt;\[
\mu_{et} = \text{E}\left(e_{it} \mid \boldsymbol \theta\right) \;\;\; \text{and} \;\;\; \mu_{ct} = \text{E}\left(c_{it} \mid \boldsymbol \theta \right).
\]&lt;/p&gt;

&lt;p&gt;The differences in $\mu_{et}$ and $\mu_{ct}$ between the treatment groups represent the quantities of interest in the economic evaluation and are used in assessing the relative cost-effectiveness of the interventions. This modelling approach has the limitation that $\mu_{et}$ and $\mu_{ct}$ are derived based only on the completers in the study and does not assess the robustness of the results to a range of plausible missingness assumptions. The model also fails to account for the different complexities that affect the utility and cost data in the trial: from the correlation between variables to the skewness and the presence of structural values (zero for the costs and one for the utilities) in both outcomes.&lt;/p&gt;

&lt;h1 id=&#34;longitudinal-model-to-deal-with-missingness&#34;&gt;Longitudinal model to deal with missingness&lt;/h1&gt;

&lt;p&gt;We propose an alternative approach to deal with a missing bivariate outcome in economic evaluations, while simultaneously allowing for the different complexities that typically affect utility and cost data. Our approach includes a longitudinal model that improves the current practice by taking into account the information from all observed data as well as the time dependence between the responses.&lt;/p&gt;

&lt;p&gt;Let $\boldsymbol u_i=(u_{i0},\ldots,u_{iJ})$ and $\boldsymbol c_i=(c_{i0},\ldots,c_{iJ})$ denote the vectors of utilities and costs that were supposed to be observed for subject $i$ at time $j$ in the study, with $j \in {0,1,J}$. We denote with $\boldsymbol y_{ij}=(u_{ij},c_{ij})$ the bivariate outcome for subject $i$ formed by the utility and cost pair at time $j$. We group the individuals according to the missingness patterns and denote with $\boldsymbol r_{ij}=(r^u_{ij},r^c_{ij})$ a pair of indicator variables that take value $1$ if the corresponding outcome for subject $i$ at time $j$ is observed and $0$ otherwise. We denote with $\boldsymbol r_i = (\boldsymbol r_{i0}, \ldots, \boldsymbol r_{iJ})$ the missingness pattern to which subject $i$ belongs, where each pattern is associated with different values for $\boldsymbol r_{ij}$.&lt;/p&gt;

&lt;p&gt;We then define our modelling strategy and factor the joint distribution for the response and missingness as:&lt;/p&gt;

&lt;p&gt;\[
p(\boldsymbol y, \boldsymbol r \mid \boldsymbol \omega) = p(\boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r \mid  \boldsymbol \omega)p(\boldsymbol y^{\boldsymbol r}_{mis} \mid \boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r, \boldsymbol \omega)
\]&lt;/p&gt;

&lt;p&gt;where $\boldsymbol y^{\boldsymbol r}_{obs}$ and $\boldsymbol y^{\boldsymbol r}_{mis}$ indicate the observed and missing responses within pattern $\boldsymbol r$, respectively. This is the extrapolation &lt;em&gt;factorisation&lt;/em&gt; and factors the joint into two components, of which the extrapolation &lt;em&gt;distribution&lt;/em&gt; $p(\boldsymbol y^{\boldsymbol r}_{mis} \mid \boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r, \boldsymbol \omega)$ remains unidentified by the data in the absence of unverifiable assumptions about the full data.&lt;/p&gt;

&lt;p&gt;To specify the observed data distribution $p(\boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r \mid  \boldsymbol \omega)$ we use a working model $p^{\star}$ for the joint distribution of the response and missingness. Essentially, the idea is to use the working model $p^{\star}(\boldsymbol y, \boldsymbol{r} \mid  \boldsymbol \omega)$ to draw inferences about the distribution of the observed data $p(\boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r \mid \boldsymbol \omega)$ by integrating out the missing responses:&lt;/p&gt;

&lt;p&gt;\[
p(\boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r \mid \boldsymbol \omega) = \int p^{\star}(\boldsymbol y, \boldsymbol{r} \mid  \boldsymbol \omega)d \boldsymbol y^{\boldsymbol r}_{mis}.
\]&lt;/p&gt;

&lt;p&gt;This approach avoids direct specification of the joint distribution of the observed and missing data $p(\boldsymbol y, \boldsymbol r\mid \boldsymbol \omega)$, which has the undesirable consequence of identifying the extrapolation distribution with assumptions that are difficult to check. Indeed, since we use $p^{\star}(\boldsymbol y, \boldsymbol r \mid  \boldsymbol \omega)$ only to obtain a model for $p(\boldsymbol y^{\boldsymbol r}_{obs}, \boldsymbol r \mid  \boldsymbol \omega)$ and not as a basis for inference, the extrapolation distribution is left unidentified. Any inference depending on the observed data distribution may be obtained using the working model as the true model, with the advantage that  it is often easier to specify a model for the the full data $p(\boldsymbol y,\boldsymbol r)$ compared with a model  for the observed data $p(\boldsymbol y^{\boldsymbol r}_{obs},\boldsymbol r)$.&lt;/p&gt;

&lt;p&gt;We specify $p^{\star}$ using a pattern mixture approach, factoring the joint $p(\boldsymbol y,\boldsymbol r \mid \boldsymbol \omega)$ as the product between the marginal distribution of the missingness patterns $p(\boldsymbol r\mid \boldsymbol \psi)$ and the distribution of the response conditional on the patterns $p(\boldsymbol y\mid \boldsymbol r,\boldsymbol \theta)$, respectively indexed by the distinct parameter vectors $\boldsymbol \psi$ and $\boldsymbol \theta$. If missingness is monotone it is possible to summarise the patterns by dropout time and directly model the dropout process. Unfortunately, as it often occurs in trial-based health economic data, missingness in the case study is mostly nonmonotone and the sparsity of the data in most patterns makes it infeasible to fit the response model within each pattern, with the exception of the completers ($\boldsymbol r = \boldsymbol 1$). Thus, we decided to collapse together all the non-completers patterns ($\boldsymbol r \neq \boldsymbol 1$) and fit the model separately to this aggregated pattern and to the completers. The joint distribution has three components. The first is given by the model for the patterns and the model for the completers ($\boldsymbol r = \boldsymbol 1$), where no missingness occurs. The second component is a model for the observed data in the collapsed patterns $\boldsymbol r \neq \boldsymbol 1$ that, together with the first component, form the observed data distribution. The last component is the extrapolation distribution.&lt;/p&gt;

&lt;p&gt;Because the targeted quantities of interest can be derived based on the marginal utility and cost means at each time $j$, in our analysis we do not require the full identification of $p(\boldsymbol y^{\boldsymbol r}_{mis} \mid \boldsymbol y^{\boldsymbol r}_{obs},  \boldsymbol r,\boldsymbol \xi)$. Instead, we only partially identify the extrapolation distribution using &lt;em&gt;partial identifying restrictions&lt;/em&gt;. Specifically, we only require the identification of the marginal means for the missing responses in each pattern. We identify the marginal mean of $\boldsymbol y^{\boldsymbol r}_{mis}$ using the observed values, averaged across $\boldsymbol r^\prime \neq \boldsymbol 1$, and some &lt;em&gt;sensitivity parameters&lt;/em&gt; $\boldsymbol \Delta = (\Delta_u,\Delta_c)$. Therefore, we compute the marginal means by averaging only across the observed components in pattern ${\boldsymbol r}^\prime$ and ignore the components that are missing.&lt;/p&gt;

&lt;p&gt;We start by setting a benchmark assumption with $\boldsymbol \Delta = \boldsymbol 0$, and then explore the sensitivity of the results to alternative scenarios by using different prior distributions on $\boldsymbol \Delta$, calibrated on the observed data. This provides a convenient benchmark scenario from which departures can be explored using alternative informative priors on $\boldsymbol \Delta$. Once the working model has been fitted to the observed data and the extrapolation distribution has been identified, the overall marginal mean for the response model can be computed by marginalising over $\boldsymbol r$, i.e. $\text{E}\left[\boldsymbol Y\right] = \sum_{\boldsymbol r} p(\boldsymbol r)\text{E}\left[\boldsymbol Y \mid \boldsymbol r \right]$.&lt;/p&gt;

&lt;h2 id=&#34;modelling-framework&#34;&gt;Modelling framework&lt;/h2&gt;

&lt;p&gt;The distribution of the observed responses $\boldsymbol y_{ijt}=(u_{ijv},c_{ijt})$ is specified in terms of a model for the utility and cost variables at time $j=0,1,2$, which are jointly modelled without using a multilevel approach and separately by treatment group. In particular, the joint distribution for $\boldsymbol y_{ijt}$ is specified as a series of conditional distributions that capture the dependence between utilities and costs as well as the time dependence.&lt;/p&gt;

&lt;p&gt;Following the recommendations from the published literature, we account for the skewness using Beta and Log-Normal distributions for the utilities and costs, respectively. Since the Beta distribution does not allow for negative values, we scaled the utilities on $[0,1]$ through the transformation $u^{\star}_{ij}=\frac{u_{ij}-\text{min}(\boldsymbol u_{j})}{\text{max}(\boldsymbol u_{j})-\text{min}(\boldsymbol u_{j})}$, and fit the model to these transformed variables. To account for the structural values $u_{ij} = 1$ and $c_{ij} = 0$ we use a hurdle approach by including in the model the indicator variables $d^u_{ij}:=\mathbb{I}(u_{ij}=1)$ and $d^c_{ij}:=\mathbb{I}(c_{ij}=0)$, which take value $1$ if subject $i$ is associated with a structural value at time $j$ and 0 otherwise. The probabilities of observing these values, as well as the mean of each variable, are then modelled conditionally on other variables via linear regressions defined on the logit or log scale. Specifically, at time $j=1,2$, the probability of observing a zero and the mean costs are modelled conditionally on the utilities and costs at the previous times, while the probability of observing a one and the mean utilities are modelled conditionally on the current costs (also at $j=0$) and the utilities at the previous times (only at $j=1,2$). The model is summarised by the following Figure.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;/img/missing_model.jpg&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Longitudinal model for missingness.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;We use partial identifying restrictions to link the observed data distribution $p(\boldsymbol y_{obs},\boldsymbol r)$ to the extrapolation distribution $p(\boldsymbol y_{mis} \mid \boldsymbol y_{obs},\boldsymbol r)$ and consider interpretable deviations from a benchmark scenario to assess how inferences are driven by our assumptions. Specifically, we identify the marginal mean of the missing responses in each pattern $\boldsymbol y^{\boldsymbol r}_{mis}$ by averaging across the corresponding components that are observed and add the sensitivity parameters $\boldsymbol \Delta_j$.&lt;/p&gt;

&lt;p&gt;We define $\boldsymbol \Delta_j=(\Delta_{c_{j}},\Delta_{u_{j}})$ to be time-specific location shifts at the marginal mean in each pattern and set $\boldsymbol \Delta_j = \boldsymbol 0$ as the benchmark scenario. We then explore departures from this benchmark using alternative priors on $\boldsymbol \Delta_j$, which are calibrated using the observed standard deviations for costs and utilities at each time $j$ to define the amplitude of the departures from $\boldsymbol \Delta_j=\boldsymbol 0$.&lt;/p&gt;

&lt;h1 id=&#34;conlcusions&#34;&gt;Conlcusions&lt;/h1&gt;

&lt;p&gt;Missingness represents a threat to economic evaluations as, when dealing with partially-observed data, any analysis makes assumptions about the missing values that cannot be verified from the data at hand. Trial-based analyses are typically conducted on cross-sectional quantities, e.g. QALYs and total costs, which are derived based only on the observed data from the completers in the study. This is an inefficient approach which may discard a substantial proportion of the sample, especially when there is a relatively large number of time points, where individuals are more likely to have some missing value or to drop out from the study. In addition, when there are systematic differences between the responses of the completers and non-completers, which is typically the case when dealing with self-reported outcomes in trial-based analyses, the results based only on the former may be biased and mislead the final assessment. A further concern is that routine analyses typically rely on standard models that ignore or at best fail to properly account for potentially important features in the data such as correlation, skewness, and the presence of structural values.&lt;/p&gt;

&lt;p&gt;Our framework represents a considerable step forward for the handling of missingness in economic evaluations compared with the current practice, which typically relies on methods that assume an ignorable MAR and rarely conducts sensitivity analysis to MNAR departures. Nevertheless, further improvements are certainly possible. For example, a potential area for future work is to increase the flexibility of our approach through a semi-parametric or nonparametric specification for the observed data distribution, which would allow a weakening of the model assumptions and likely further improve the fit of the model to the observed data and address sparse patterns in an automated way. As for the extrapolation distribution, alternative identifying restrictions that introduce the sensitivity parameters via the conditional mean (rather than the marginal mean) could be considered, and their impact on the conclusions assessed in a sensitivity analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Missing Data</title>
      <link>/missingdata/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/missingdata/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/research/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
