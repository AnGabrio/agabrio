<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.1">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Andrea Gabrio">

  
  
  
    
  
  <meta name="description" content="This tutorial will focus on the use of Bayesian estimation to explore differences between two populations. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:
OpenBUGS - written in component pascal.
JAGS - (Just Another Gibbs Sampler) - written in C&#43;&#43;.">

  
  <link rel="alternate" hreflang="en-us" href="/stan/comparing-two-populations-stan/comparing-two-populations-stan/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:450,700|Oswald+Sans:600,700|Roboto+Mono:550,700">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.79d76725ea956cf98045e5d9e289f2aa.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academic.d8c4ad17876b79f73e24a31116b20b9c.css">
  

  
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/stan/comparing-two-populations-stan/comparing-two-populations-stan/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Andrea Gabrio">
  <meta property="og:url" content="/stan/comparing-two-populations-stan/comparing-two-populations-stan/">
  <meta property="og:title" content="Comparing Two Populations - STAN | Andrea Gabrio">
  <meta property="og:description" content="This tutorial will focus on the use of Bayesian estimation to explore differences between two populations. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:
OpenBUGS - written in component pascal.
JAGS - (Just Another Gibbs Sampler) - written in C&#43;&#43;."><meta property="og:image" content="/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2020-02-01T21:13:14-05:00">
  
  <meta property="article:modified_time" content="2020-02-01T21:13:14-05:00">
  

  


  





  <title>Comparing Two Populations - STAN | Andrea Gabrio</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Andrea Gabrio</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/post/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/research/"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/publication/"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/talk/"><span>Talks</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Software</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/missingHE/"><span>missingHE</span></a>
            </li>
            
          </ul>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Tutorials</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/JAGS/"><span>JAGS</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/OpenBUGS/"><span>OpenBUGS</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/STAN/"><span>STAN</span></a>
            </li>
            
          </ul>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/rubric/"><span>Missing Data</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Comparing Two Populations - STAN</h1>

  

  
    



<meta content="2020-02-01 21:13:14 -0500 -0500" itemprop="datePublished">
<meta content="2020-02-01 21:13:14 -0500 -0500" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a>Andrea Gabrio</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Feb 1, 2020</time>
  </span>
  

  

  

  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    <a href="/categories/r/">R</a>, <a href="/categories/one-sample-t-test/">one sample t-test</a>, <a href="/categories/stan/">STAN</a></span>
  

  
    

  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>This tutorial will focus on the use of Bayesian estimation to explore differences between two populations. <code>BUGS</code> (Bayesian inference Using <em>Gibbs Sampling</em>) is an algorithm and supporting language (resembling <code>R</code>) dedicated to performing the Gibbs sampling implementation of <em>Markov Chain Monte Carlo</em> (MCMC) method. Dialects of the <code>BUGS</code> language are implemented within three main projects:</p>
<ol style="list-style-type: decimal">
<li><p><strong>OpenBUGS</strong> - written in component pascal.</p></li>
<li><p><strong>JAGS</strong> - (Just Another Gibbs Sampler) - written in <code>C++</code>.</p></li>
<li><p><strong>STAN</strong> - a dedicated Bayesian modelling framework written in <code>C++</code> and implementing <em>Hamiltonian</em> MCMC samplers.</p></li>
</ol>
<p>Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within <code>R</code> itself. As such there are multiple packages devoted to interfacing with the various software implementations:</p>
<ul>
<li><p><em>R2OpenBUGS</em> - interfaces with <code>OpenBUGS</code></p></li>
<li><p><em>R2jags</em> - interfaces with <code>JAGS</code></p></li>
<li><p><em>rstan</em> - interfaces with <code>STAN</code></p></li>
</ul>
<p>The <code>BUGS/JAGS/STAN</code> languages and algorithms are very powerful and flexible. However, the cost of this power and flexibility is complexity and the need for a firm understanding of the model you wish to fit as well as the priors to be used. The algorithms requires the following inputs.</p>
<ul>
<li><p>Within the model:</p>
<ol style="list-style-type: decimal">
<li><p>The likelihood function relating the response to the predictors.</p></li>
<li><p>The definition of the priors.</p></li>
</ol></li>
<li><p>Chain properties:</p>
<ol style="list-style-type: decimal">
<li><p>The number of chains.</p></li>
<li><p>The length of chains (number of iterations).</p></li>
<li><p>The burn-in length (number of initial iterations to ignore).</p></li>
<li><p>The thinning rate (number of iterations to count on before storing a sample).</p></li>
</ol></li>
<li><p>The initial estimates to start an MCMC chain. If there are multiple chains, these starting values can differ between chains.</p></li>
<li><p>The list of model parameters and derivatives to monitor (and return the posterior distributions of)</p></li>
</ul>
<p>This tutorial will demonstrate how to fit models in <code>STAN</code> (<span class="citation">Gelman, Lee, and Guo (2015)</span>) using the package <code>rstan</code> (<span class="citation">Stan Development Team (2018)</span>) as interface, which also requires to load some other packages.</p>
<div id="data-generation" class="section level1">
<h1>Data generation</h1>
<p>We will start by generating a random data set. Note, I am creating two versions of the predictor variable (a numeric version and a factorial version).</p>
<pre class="r"><code>&gt; set.seed(123)
&gt; nA &lt;- 60  #sample size from Population A
&gt; nB &lt;- 40  #sample size from Population B
&gt; muA &lt;- 105  #population mean of Population A
&gt; muB &lt;- 77.5  #population mean of Population B
&gt; sigma &lt;- 3  #standard deviation of both populations (equally varied)
&gt; yA &lt;- rnorm(nA, muA, sigma)  #Population A sample
&gt; yB &lt;- rnorm(nB, muB, sigma)  #Population B sample
&gt; y &lt;- c(yA, yB)
&gt; x &lt;- factor(rep(c(&quot;A&quot;, &quot;B&quot;), c(nA, nB)))  #categorical listing of the populations
&gt; xn &lt;- as.numeric(x)  #numerical version of the population category for means parameterization. # Should not start at 0.
&gt; data &lt;- data.frame(y, x, xn)  # dataset</code></pre>
<p>Let inspect the first few rows of the dataset using the command <code>head</code></p>
<pre class="r"><code>&gt; head(data)
         y x xn
1 103.3186 A  1
2 104.3095 A  1
3 109.6761 A  1
4 105.2115 A  1
5 105.3879 A  1
6 110.1452 A  1</code></pre>
<p>We can also perform some exploratory data analysis - in this case, a boxplot of the response for each level of the predictor.</p>
<pre class="r"><code>&gt; boxplot(y ~ x, data)</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/boxplot-1.png" width="672" /></p>
</div>
<div id="the-one-sample-t-test" class="section level1">
<h1>The One Sample t-test</h1>
<p>A <em>t-test</em> is essentially just a simple regression model in which the categorical predictor is represented by a binary variable in which one level is coded as <span class="math inline">\(0\)</span> and the other <span class="math inline">\(1\)</span>. For the model itself, the observed response <span class="math inline">\(y_i\)</span> are assumed to be drawn from a normal distribution with a given mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. The expected values are themselves determined by the linear predictor <span class="math inline">\(\mu_i=\beta_0+\beta_1x_i\)</span>, where <span class="math inline">\(\beta_0\)</span> represents the mean of the first treatment group and <span class="math inline">\(\beta_1\)</span> represents the difference between the mean of the first group and the mean of the second group (the effect of interest).</p>
<p>MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying “uninformative” priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (<span class="math inline">\(1000\)</span>) for both the intercept and the treatment effect and a wide half-cauchy (scale=<span class="math inline">\(25\)</span>) for the standard deviation (<span class="citation">Gelman and others (2006)</span>).</p>
<p><span class="math display">\[y_i \sim \text{Normal}(\mu_i, \sigma),  \]</span></p>
<p>where <span class="math inline">\(\mu_i=\beta_0+\beta_1x_i\)</span>.</p>
<p>Priors are defined as:</p>
<p><span class="math display">\[ \beta_j \sim \text{Normal}(0,1000),  \;\;\; \text{and} \;\;\; \sigma \sim \text{Cauchy}(0,25),  \]</span></p>
<p>for <span class="math inline">\(j=0,1\)</span>.</p>
<div id="fitting-the-model-in-stan" class="section level2">
<h2>Fitting the model in STAN</h2>
<p>Broadly, there are two ways of parameterising (expressing the unknown (to be estimated) components of a model) a model. Either we can estimate the means of each group (<em>Means parameterisation</em>) or we can estimate the mean of one group and the difference between this group and the other group(s) (<em>Effects parameterisation</em>). The latter is commonly used for frequentist null hypothesis testing as its parameters are more consistent with the null hypothesis of interest (that the difference between the two groups equals zero).</p>
<ol style="list-style-type: decimal">
<li><strong>Effects parameterisation</strong></li>
</ol>
<p><span class="math display">\[ y_i = \beta_0 + \beta_{j}x_i + \epsilon_i, \;\;\; \text{with} \;\;\; \epsilon_i \sim \text{Normal}(0,\sigma).  \]</span></p>
<p>Each <span class="math inline">\(y_i\)</span> is modelled by an intercept <span class="math inline">\(\beta_0\)</span> (mean of group A) plus a difference parameter <span class="math inline">\(\beta_j\)</span> (difference between mean of group A and group B) multiplied by an indicator of which group the observation came from (<span class="math inline">\(x_i\)</span>), plus a residual drawn from a normal distribution with mean <span class="math inline">\(0\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. Actually, there are as many <span class="math inline">\(\beta_j\)</span> parameters as there are groups but one of them (typically the first) is set to be equal to zero (to avoid over-parameterization). Expected values of <span class="math inline">\(y\)</span> are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of effect parameters and whose variance is defined by the degree of variability in this mean. The parameters are: <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Means parameterisation</strong></li>
</ol>
<p><span class="math display">\[ y_i = \beta_{j} + \epsilon_i, \;\;\; \text{with} \;\;\; \epsilon_i \sim \text{Normal}(0,\sigma).  \]</span></p>
<p>Each <span class="math inline">\(y_i\)</span> is modelled as the mean <span class="math inline">\(\beta_j\)</span> of each group (<span class="math inline">\(j=1,2\)</span>) plus a residual drawn from a normal distribution with a mean of zero and a standard deviation of <span class="math inline">\(\sigma\)</span>. Actually, <span class="math inline">\(\boldsymbol \beta\)</span> is a set of <span class="math inline">\(j\)</span> coefficients corresponding to the <span class="math inline">\(j\)</span> dummy coded factor levels. Expected values of <span class="math inline">\(y\)</span> are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of means parameters and whose variance is defined by the degree of variability in this mean. The parameters are: <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
<p>Whilst the <code>STAN</code> language broadly resembles <code>BUGS/JAGS</code>, there are numerous important differences. Some of these differences are to support translation to <code>c++</code> for compilation (such as declaring variables). Others reflect leveraging of vectorization to speed up run time. Here are some important notes about <code>STAN</code>:</p>
<ul>
<li><p>All variables must be declared</p></li>
<li><p>Variables declared in the parameters block will be collected</p></li>
<li><p>Anything in the transformed block will be collected as samples. Also, checks will be made every loop</p></li>
</ul>
<p>Now I will demonstrate fitting the models with <code>STAN</code>. Note, I am using the <code>refresh=0</code> option so as to suppress the larger regular output in the interest of keeping output to what is necessary for this tutorial. When running outside of a tutorial context, the regular verbose output is useful as it provides a way to gauge progress.</p>
<p><strong>Effects Parameterisation</strong></p>
<pre class="r"><code>&gt; stanString = &quot; 
+ data {
+  int n;
+  vector [n] y;
+  vector [n] x;
+  }
+  parameters {
+  real &lt;lower=0, upper=100&gt; sigma;
+  real beta0;
+  real beta;
+  }
+  transformed parameters {
+  }
+  model {
+  vector [n] mu;
+  
+  //Priors
+  beta0 ~ normal(0,1000);
+  beta ~ normal(0,1000);
+  sigma ~ cauchy(0,25);
+  
+  mu = beta0 + beta*x;
+  //Likelihood
+  y ~ normal(mu, sigma);
+  }
+  generated quantities {
+  vector [2] Group_means;
+  real CohensD;
+  //Other Derived parameters 
+  //# Group means (note, beta is a vector)
+  Group_means[1] = beta0;
+  Group_means[2] = beta0+beta;
+  
+  CohensD = beta /sigma;  
+  }
+  
+  &quot;
&gt; ## write the model to a text file
&gt; writeLines(stanString, con = &quot;ttestModel.stan&quot;)</code></pre>
<p><strong>Means Parameterisation</strong></p>
<pre class="r"><code>&gt; stanString.means = &quot;  
+  data {
+  int n;
+  int nX;
+  vector [n] y;
+  matrix [n,nX] x;
+  }
+  parameters {
+  real &lt;lower=0, upper=100&gt; sigma;
+  vector [nX] beta;
+  }
+  transformed parameters {
+  }
+  model {
+  vector [n] mu;
+  
+  //Priors
+  beta ~ normal(0,1000);
+  sigma ~ cauchy(0,25);
+  
+  mu = x*beta;
+  //Likelihood
+  y ~ normal(mu, sigma);
+  }
+  generated quantities {
+  vector [2] Group_means;
+  real CohensD;
+  
+  //Other Derived parameters 
+  Group_means[1] = beta[1];
+  Group_means[2] = beta[1]+beta[2];
+  
+  CohensD = beta[2] /sigma;  
+  }
+  
+  &quot;
&gt; ## write the model to a text file
&gt; writeLines(stanString.means, con = &quot;ttestModelMeans.stan&quot;)</code></pre>
<p>Arrange the data as a list (as required by <code>STAN</code>).</p>
<pre class="r"><code>&gt; data.list &lt;- with(data, list(y = y, x = (xn - 1), n = nrow(data)))
&gt; X &lt;- model.matrix(~x, data)
&gt; data.list.means = with(data, list(y = y, x = X, n = nrow(data), nX = ncol(X)))</code></pre>
<p>Define the initial values for the chain. Reasonable starting points can be gleaned from the data themselves.</p>
<pre class="r"><code>&gt; inits &lt;- list(beta0 = mean(data$y), beta = c(NA, diff(tapply(data$y,
+     data$x, mean))), sigma = sd(data$y/2))
&gt; inits.means &lt;- list(beta = tapply(data$y, data$x, mean), sigma = sd(data$y/2))</code></pre>
<p>Define the nodes (parameters and derivatives) to monitor.</p>
<pre class="r"><code>&gt; params &lt;- c(&quot;beta0&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;Group_means&quot;, &quot;CohensD&quot;)
&gt; params.means &lt;- c(&quot;beta&quot;, &quot;sigma&quot;, &quot;Group_means&quot;,&quot;CohensD&quot;)</code></pre>
<p>Define the chain parameters.</p>
<pre class="r"><code>&gt; burnInSteps = 500  # the number of initial samples to discard
&gt; nChains = 2  # the number of independed sampling chains to perform 
&gt; thinSteps = 1  # the thinning rate
&gt; nIter = 2000</code></pre>
<p>Start the <code>STAN</code> model (check the model, load data into the model, specify the number of chains and compile the model). Load the <code>rstan</code> package.</p>
<pre class="r"><code>&gt; library(rstan)</code></pre>
<p>When using the <code>stan</code> function (<code>rtsan</code> package), it is not necessary to provide initial values. However, if they are to be supplied, the inital values must be provided as a list of the same length as the number of chains.</p>
<p><strong>Effects Parameterisation</strong></p>
<pre class="r"><code>&gt; data.stan = stan(file = &quot;ttestModel.stan&quot;, 
+   data = data.list, 
+   pars = params,
+   iter = nIter,
+   warmup = burnInSteps, 
+   chains = nChains, 
+   thin = thinSteps, 
+   init = &quot;random&quot;, #or inits=list(inits,inits)
+   refresh = 0)
&gt; 
&gt; #print results
&gt; print(data.stan)
Inference for Stan model: ttestModel.
2 chains, each with iter=2000; warmup=500; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=3000.

                  mean se_mean   sd    2.5%     25%     50%     75%   97.5%
beta0           105.20    0.01 0.36  104.47  104.95  105.20  105.44  105.91
beta            -27.32    0.01 0.57  -28.43  -27.72  -27.33  -26.93  -26.22
sigma             2.79    0.00 0.21    2.41    2.64    2.77    2.92    3.23
Group_means[1]  105.20    0.01 0.36  104.47  104.95  105.20  105.44  105.91
Group_means[2]   77.88    0.01 0.45   77.01   77.59   77.87   78.18   78.76
CohensD          -9.85    0.02 0.75  -11.36  -10.35   -9.86   -9.35   -8.36
lp__           -150.78    0.04 1.25 -154.05 -151.31 -150.44 -149.88 -149.34
               n_eff Rhat
beta0           1802    1
beta            1731    1
sigma           2187    1
Group_means[1]  1802    1
Group_means[2]  2826    1
CohensD         2238    1
lp__            1272    1

Samples were drawn using NUTS(diag_e) at Mon Feb 10 14:10:29 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p><strong>Means Parameterisation</strong></p>
<pre class="r"><code>&gt; data.stan.means = stan(file = &quot;ttestModelMeans.stan&quot;, 
+   data = data.list.means, 
+   pars = params.means,
+   iter = nIter,
+   warmup = burnInSteps, 
+   chains = nChains, 
+   thin = thinSteps, 
+   init = &quot;random&quot;, #or inits=list(inits.means,inits.means)
+   refresh = 0)
&gt; 
&gt; #print results
&gt; print(data.stan.means)
Inference for Stan model: ttestModelMeans.
2 chains, each with iter=2000; warmup=500; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=3000.

                  mean se_mean   sd    2.5%     25%     50%     75%   97.5%
beta[1]         105.21    0.01 0.37  104.51  104.96  105.20  105.44  105.92
beta[2]         -27.33    0.01 0.58  -28.47  -27.71  -27.31  -26.93  -26.23
sigma             2.78    0.00 0.20    2.43    2.64    2.76    2.90    3.22
Group_means[1]  105.21    0.01 0.37  104.51  104.96  105.20  105.44  105.92
Group_means[2]   77.88    0.01 0.44   77.02   77.59   77.88   78.17   78.77
CohensD          -9.88    0.02 0.74  -11.35  -10.40   -9.89   -9.40   -8.40
lp__           -150.74    0.03 1.26 -153.85 -151.33 -150.42 -149.83 -149.33
               n_eff Rhat
beta[1]         1439    1
beta[2]         1654    1
sigma           1955    1
Group_means[1]  1439    1
Group_means[2]  3595    1
CohensD         2056    1
lp__            1397    1

Samples were drawn using NUTS(diag_e) at Mon Feb 10 14:11:08 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p><strong>Notes</strong></p>
<ul>
<li><p>If <code>inits="random"</code> the <code>stan</code> function will randomly generate initial values between <span class="math inline">\(-2\)</span> and <span class="math inline">\(2\)</span> on the <em>unconstrained support</em>. The optional additional parameter <code>init_r</code> can be set to some value other than <span class="math inline">\(2\)</span> to change the range of the randomly generated inits. Other available options include: set <code>inits="0"</code> to initialize all parameters to zero on the unconstrained support; set inital values by providing a list equal in length to the number of chains; set initial values by providing a function that returns a list for specifying the initial values of parameters for a chain.</p></li>
<li><p>In addition to the mean and quantiles of each of the sample nodes, the <code>stan</code> function will calculate.</p>
<ol style="list-style-type: decimal">
<li><p>The <em>effective sample size</em> for each sample - if <code>n.eff</code> for a node is substantially less than the number of iterations, then it suggests poor mixing.</p></li>
<li><p>The <em>Potential scale reduction factor</em> or <code>Rhat</code> values for each sample - these are a convergence diagnostic (values of <span class="math inline">\(1\)</span> indicate full convergence, values greater than <span class="math inline">\(1.01\)</span> are indicative of non-convergence.</p></li>
</ol></li>
</ul>
<p>The total number samples collected is <span class="math inline">\(3000\)</span>. That is, there are <span class="math inline">\(3000\)</span> samples collected from the multidimensional posterior distribution and thus, <span class="math inline">\(3000\)</span> samples collected from the posterior distributions of each parameter. The effective number of samples column indicates the number of independent samples represented in the total. It is clear that for all parameters the chains were well mixed.</p>
</div>
</div>
<div id="mcmc-diagnostics" class="section level1">
<h1>MCMC diagnostics</h1>
<p>Again, prior to examining the summaries, we should have explored the convergence diagnostics. There are numerous ways of working with <code>STAN</code> model fits (for exploring diagnostics and summarisation).</p>
<ol style="list-style-type: decimal">
<li><p>extract the mcmc samples and convert them into a mcmc.list to leverage the various <code>mcmcplots</code> routines</p></li>
<li><p>use the numerous routines that come with the <code>rstan</code> package</p></li>
<li><p>use the routines that come with the <code>bayesplot</code> package</p></li>
</ol>
<p>We will explore all of these.</p>
<ul>
<li><strong>mcmcplots</strong></li>
</ul>
<p>First, we need to convert the <code>rtsan</code> object into an <code>mcmc.list</code> object to apply the functions in the <code>mcmcplots</code> package.</p>
<pre class="r"><code>&gt; library(mcmcplots)
&gt; s = as.array(data.stan.means)
&gt; mcmc &lt;- do.call(mcmc.list, plyr:::alply(s[, , -(length(s[1, 1, ]))], 2, as.mcmc))</code></pre>
<p>Next we look at density and trace plots.</p>
<pre class="r"><code>&gt; denplot(mcmc, parms = c(&quot;Group_means&quot;, &quot;CohensD&quot;))</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv2-1.png" width="672" /></p>
<pre class="r"><code>&gt; traplot(mcmc, parms = c(&quot;Group_means&quot;, &quot;CohensD&quot;))</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv2-2.png" width="672" /></p>
<p>These plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.</p>
<ul>
<li><strong>rstan</strong></li>
</ul>
<p>MCMC diagnostic measures that can be directly applied to <code>rstan</code> objects via the <code>rstan</code> package include: traceplots, autocorrelation, effective sample size and Rhat diagnostics.</p>
<pre class="r"><code>&gt; #traceplots
&gt; stan_trace(data.stan.means, pars = c(&quot;Group_means&quot;, &quot;CohensD&quot;))</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv3-1.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; #autocorrelation
&gt; stan_ac(data.stan.means, pars = c(&quot;Group_means&quot;, &quot;CohensD&quot;))</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv3-2.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; #rhat
&gt; stan_rhat(data.stan.means, pars = c(&quot;Group_means&quot;, &quot;CohensD&quot;))</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv3-3.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; #ess
&gt; stan_ess(data.stan.means, pars = c(&quot;Group_means&quot;, &quot;CohensD&quot;))</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv3-4.png" width="672" /></p>
<p>Note:</p>
<ul>
<li><p>Rhat values are a measure of sampling efficiency/effectiveness. Ideally, all values should be less than <span class="math inline">\(1.05\)</span>. If there are values of 1.05 or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentiall slower than it could have been, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.</p></li>
<li><p>ESS indicates the number samples (or proportion of samples that the sampling algorithm) deamed effective. The sampler rejects samples on the basis of certain criterion and when it does so, the previous sample value is used. Hence while the MCMC sampling chain may contain <span class="math inline">\(1000\)</span> samples, if there are only <span class="math inline">\(10\)</span> effective samples (<span class="math inline">\(1\)</span>%), the estimated properties are not likely to be reliable.</p></li>
<li><p><strong>bayesplot</strong></p></li>
</ul>
<p>Another alternative is to use the package <code>bayesplot</code>, which provides a range of standardised diagnostic measures for assessing MCMC convergence and issues, which can be directly applied to the <code>rstan</code> object.</p>
<pre class="r"><code>&gt; library(bayesplot)
&gt; 
&gt; #density and trace plots
&gt; mcmc_combo(as.array(data.stan.means), regex_pars = &quot;Group_means|CohensD&quot;)</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_diagv4-1.png" width="672" /></p>
</div>
<div id="model-validation" class="section level1">
<h1>Model validation</h1>
<p>Residuals are not computed directly within <code>rstan</code>. However, we can calculate them manually form the posteriors.</p>
<pre class="r"><code>&gt; library(ggplot2)
&gt; mcmc = as.matrix(data.stan.means)[, c(&quot;beta[1]&quot;, &quot;beta[2]&quot;)]
&gt; # generate a model matrix
&gt; newdata = data.frame(x = data$x)
&gt; Xmat = model.matrix(~x, newdata)
&gt; ## get median parameter estimates
&gt; coefs = apply(mcmc, 2, median)
&gt; fit = as.vector(coefs %*% t(Xmat))
&gt; resid = data$y - fit
&gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit))</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_residuals-1.png" width="672" /></p>
<p>There is no evidence that the mcmc chain did not converge on a stable posterior distribution. We are now in a position to examine the summaries of the parameters.</p>
</div>
<div id="parameter-estimates" class="section level1">
<h1>Parameter estimates</h1>
<p>A quick look at posterior summaries can be obtained through the command <code>summary</code> which can be directly applied to our <code>rstan</code> object.</p>
<pre class="r"><code>&gt; summary(data.stan.means)
$summary
                      mean     se_mean        sd        2.5%        25%
beta[1]         105.205981 0.009650332 0.3660680  104.512893  104.95847
beta[2]         -27.327670 0.014175541 0.5765494  -28.471465  -27.70858
sigma             2.779295 0.004564259 0.2017951    2.425126    2.63877
Group_means[1]  105.205981 0.009650332 0.3660680  104.512893  104.95847
Group_means[2]   77.878310 0.007293288 0.4372737   77.017179   77.58974
CohensD          -9.883908 0.016318680 0.7398548  -11.345145  -10.39596
lp__           -150.744310 0.033765022 1.2622380 -153.847632 -151.32845
                       50%         75%       97.5%    n_eff      Rhat
beta[1]         105.197887  105.442341  105.923970 1438.928 1.0006369
beta[2]         -27.313058  -26.929462  -26.228003 1654.222 0.9996207
sigma             2.761057    2.904130    3.220382 1954.702 1.0008448
Group_means[1]  105.197887  105.442341  105.923970 1438.928 1.0006369
Group_means[2]   77.881198   78.173471   78.765424 3594.677 0.9997923
CohensD          -9.893648   -9.396558   -8.403284 2055.526 1.0013095
lp__           -150.420841 -149.826519 -149.327836 1397.489 1.0006469

$c_summary
, , chains = chain:1

                stats
parameter               mean        sd        2.5%         25%         50%
  beta[1]         105.194598 0.3722763  104.485138  104.943830  105.189222
  beta[2]         -27.316749 0.5909082  -28.503315  -27.700926  -27.303076
  sigma             2.787113 0.2017944    2.439039    2.649487    2.769964
  Group_means[1]  105.194598 0.3722763  104.485138  104.943830  105.189222
  Group_means[2]   77.877849 0.4452879   76.953676   77.589838   77.884335
  CohensD          -9.851471 0.7306980  -11.291742  -10.351622   -9.856804
  lp__           -150.774304 1.3031195 -154.143552 -151.358639 -150.446011
                stats
parameter                75%       97.5%
  beta[1]         105.430335  105.928130
  beta[2]         -26.900706  -26.189346
  sigma             2.905763    3.220038
  Group_means[1]  105.430335  105.928130
  Group_means[2]   78.167639   78.777570
  CohensD          -9.358039   -8.394201
  lp__           -149.844014 -149.328052

, , chains = chain:2

                stats
parameter               mean        sd        2.5%         25%         50%
  beta[1]         105.217363 0.3595164  104.544008  104.970466  105.208509
  beta[2]         -27.338592 0.5618086  -28.444722  -27.716894  -27.323423
  sigma             2.771476 0.2015598    2.417028    2.631247    2.750654
  Group_means[1]  105.217363 0.3595164  104.544008  104.970466  105.208509
  Group_means[2]   77.878771 0.4292579   77.031912   77.589743   77.878030
  CohensD          -9.916344 0.7477366  -11.431004  -10.435551   -9.924630
  lp__           -150.714316 1.2196850 -153.673281 -151.305580 -150.383196
                stats
parameter                75%       97.5%
  beta[1]         105.450568  105.916257
  beta[2]         -26.963106  -26.265061
  sigma             2.898644    3.219905
  Group_means[1]  105.450568  105.916257
  Group_means[2]   78.179664   78.753253
  CohensD          -9.430001   -8.422613
  lp__           -149.795340 -149.327597</code></pre>
<p>The Group A is typically <span class="math inline">\(27.3\)</span> units greater than Group B. The <span class="math inline">\(95\)</span>% confidence interval for the difference between Group A and B does not overlap with <span class="math inline">\(0\)</span> implying a significant difference between the two groups.</p>
</div>
<div id="graphical-summaries" class="section level1">
<h1>Graphical summaries</h1>
<p>A nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group. We do this by loading functions in the package <code>broom</code> and <code>dplyr</code>.</p>
<pre class="r"><code>&gt; library(broom)
&gt; library(dplyr)
&gt; mcmc = as.matrix(data.stan.means)
&gt; ## Calculate the fitted values
&gt; newdata = data.frame(x = levels(data$x))
&gt; Xmat = model.matrix(~x, newdata)
&gt; coefs = mcmc[, c(&quot;beta[1]&quot;, &quot;beta[2]&quot;)]
&gt; fit = coefs %*% t(Xmat)
&gt; newdata = newdata %&gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = &quot;HPDinterval&quot;))
&gt; newdata
  x  estimate std.error  conf.low conf.high
1 A 105.20598 0.3660680 104.52503 105.93588
2 B  77.87831 0.4372737  76.99792  78.74455
&gt; 
&gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_pointrange(aes(ymin = conf.low,
+     ymax = conf.high)) + scale_y_continuous(&quot;Y&quot;) + scale_x_discrete(&quot;X&quot;) +
+     theme_classic()</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_post1-1.png" width="672" /></p>
<p>If you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.</p>
<pre class="r"><code>&gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data, aes(y = y,
+     x = x), color = &quot;gray&quot;) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +
+     scale_y_continuous(&quot;Y&quot;) + scale_x_discrete(&quot;X&quot;) + theme_classic()</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_post2-1.png" width="672" /></p>
<p>A more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since <span class="math inline">\(\text{resid}=\text{obs}−\text{fitted}\)</span> and the fitted values depend only on the single predictor we are interested in.</p>
<pre class="r"><code>&gt; ## Calculate partial residuals fitted values
&gt; fdata = rdata = data
&gt; fMat = rMat = model.matrix(~x, fdata)
&gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))
&gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))
&gt; rdata = rdata %&gt;% mutate(partial.resid = resid + fit)
&gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),
+     color = &quot;gray&quot;) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +
+     scale_y_continuous(&quot;Y&quot;) + scale_x_discrete(&quot;X&quot;) + theme_classic()</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/mcmc_post3-1.png" width="672" /></p>
</div>
<div id="effect-sizes" class="section level1">
<h1>Effect sizes</h1>
<p>We can compute summaries for our effect size of interest (e.g. Cohen’s or the percentage ES) by post-processing our posterior distributions.</p>
<pre class="r"><code>&gt; mcmc = as.matrix(data.stan.means)
&gt; ## Cohen&#39;s D
&gt; cohenD = mcmc[, &quot;beta[2]&quot;]/mcmc[, &quot;sigma&quot;]
&gt; tidyMCMC(as.mcmc(cohenD), conf.int = TRUE, conf.method = &quot;HPDinterval&quot;)
# A tibble: 1 x 5
  term  estimate std.error conf.low conf.high
  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 var1     -9.88     0.740    -11.3     -8.38
&gt; 
&gt; # Percentage change (relative to Group A)
&gt; ES = 100 * mcmc[, &quot;beta[2]&quot;]/mcmc[, &quot;beta[1]&quot;]
&gt; 
&gt; # Probability that the effect is greater than 10% (a decline of &gt;10%)
&gt; sum(-1 * ES &gt; 10)/length(ES)
[1] 1</code></pre>
</div>
<div id="probability-statements" class="section level1">
<h1>Probability statements</h1>
<p>Any sort of probability statements of interest about our effect size can be computed in a relatively easy way by playing around with the posteriors.</p>
<pre class="r"><code>&gt; mcmc = as.matrix(data.stan.means)
&gt; 
&gt; # Percentage change (relative to Group A)
&gt; ES = 100 * mcmc[, &quot;beta[2]&quot;]/mcmc[, &quot;beta[1]&quot;]
&gt; hist(ES)</code></pre>
<p><img src="/STAN/comparing-two-populations-stan/2020-02-01-comparing-two-populations-stan_files/figure-html/prob_stat-1.png" width="672" /></p>
<pre class="r"><code>&gt; 
&gt; # Probability that the effect is greater than 10% (a decline of &gt;10%)
&gt; sum(-1 * ES &gt; 10)/length(ES)
[1] 1
&gt; 
&gt; # Probability that the effect is greater than 25% (a decline of &gt;25%)
&gt; sum(-1 * ES &gt; 25)/length(ES)
[1] 0.978</code></pre>
</div>
<div id="finite-population-standard-deviations" class="section level1">
<h1>Finite population standard deviations</h1>
<p>Estimates for the variability associated with between and within group differences can also be easily obtained.</p>
<pre><code># A tibble: 2 x 5
  term     estimate std.error conf.low conf.high
  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 sd.x        19.3     0.408     18.5      20.1 
2 sd.resid     2.75    0.0207     2.74      2.79
# A tibble: 2 x 5
  term     estimate std.error conf.low conf.high
  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 sd.x         87.5     0.238     87.1      87.8
2 sd.resid     12.5     0.238     12.2      12.9</code></pre>
</div>
<div id="unequally-varied-populations" class="section level1">
<h1>Unequally varied populations</h1>
<p>We can also generate data assuming two populations with different variances, e.g. between male and female subgroups.</p>
<pre class="r"><code>&gt; set.seed(123)
&gt; n1 &lt;- 60  #sample size from population 1
&gt; n2 &lt;- 40  #sample size from population 2
&gt; mu1 &lt;- 105  #population mean of population 1
&gt; mu2 &lt;- 77.5  #population mean of population 2
&gt; sigma1 &lt;- 3  #standard deviation of population 1
&gt; sigma2 &lt;- 2  #standard deviation of population 2
&gt; n &lt;- n1 + n2  #total sample size
&gt; y1 &lt;- rnorm(n1, mu1, sigma1)  #population 1 sample
&gt; y2 &lt;- rnorm(n2, mu2, sigma2)  #population 2 sample
&gt; y &lt;- c(y1, y2)
&gt; x &lt;- factor(rep(c(&quot;A&quot;, &quot;B&quot;), c(n1, n2)))  #categorical listing of the populations
&gt; xn &lt;- rep(c(0, 1), c(n1, n2))  #numerical version of the population category
&gt; data2 &lt;- data.frame(y, x, xn)  # dataset
&gt; head(data2)  #print out the first six rows of the data set
         y x xn
1 103.3186 A  0
2 104.3095 A  0
3 109.6761 A  0
4 105.2115 A  0
5 105.3879 A  0
6 110.1452 A  0</code></pre>
<p>Start by defining the model</p>
<p><span class="math display">\[ y_i = \beta_0 + \beta_1x_i + \epsilon, \]</span></p>
<p>where <span class="math inline">\(\epsilon_1 \sim \text{Normal}(0,\sigma_1)\)</span> for <span class="math inline">\(x_1=0\)</span> (females), and <span class="math inline">\(\epsilon_2 \sim \text{Normal}(0,\sigma_2)\)</span> for <span class="math inline">\(x_2=1\)</span> (males). In <code>STAN</code> code, the model becomes:</p>
<pre class="r"><code>&gt; stanStringv3 = &quot; 
+  data {
+  int n;
+  vector [n] y;
+  vector [n] x;
+  int&lt;lower=1,upper=2&gt; xn[n];
+  }
+  parameters {
+  vector &lt;lower=0, upper=100&gt;[2] sigma;
+  real beta0;
+  real beta;
+  }
+  transformed parameters {
+  }
+  model {
+  vector [n] mu;
+  //Priors
+  beta0 ~ normal(0,1000);
+  beta ~ normal(0,1000);
+  sigma ~ cauchy(0,25);
+ 
+  mu = beta0 + beta*x;
+  //Likelihood
+  for (i in 1:n) y[i] ~ normal(mu[i], sigma[xn[i]]);
+  }
+  generated quantities {
+  vector [2] Group_means;
+  real CohensD;
+  real CLES;
+ 
+  Group_means[1] = beta0;
+  Group_means[2] = beta0+beta;
+  CohensD = beta /(sum(sigma)/2);
+  CLES = normal_cdf(beta /sum(sigma),0,1);  
+  }
+  
+  &quot;
&gt; 
&gt; ## write the model to a text file 
&gt; writeLines(stanStringv3,con=&quot;ttestModelv3.stan&quot;)</code></pre>
<p>We specify priors directly on <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span> using Cauchy distributions with a scale of <span class="math inline">\(25\)</span>. Next, arrange the data as a list (as required by <code>STAN</code>) and define the MCMC parameters.</p>
<pre class="r"><code>&gt; data2.list &lt;- with(data, list(y = y, x = (xn - 1), xn = xn, n = nrow(data)))
&gt; paramsv3 &lt;- c(&quot;beta0&quot;,&quot;beta&quot;,&quot;sigma&quot;,&quot;Group_means&quot;,&quot;CohensD&quot;, &quot;CLES&quot;)
&gt; burnInSteps = 500
&gt; nChains = 2
&gt; thinSteps = 1
&gt; nIter = 2000</code></pre>
<p>Finally, fit the model in <code>STAN</code> and print the results.</p>
<pre class="r"><code>&gt; data.stanv3 = stan(file = &quot;ttestModelv3.stan&quot;, 
+   data = data2.list, 
+   pars = paramsv3,
+   iter = nIter,
+   warmup = burnInSteps, 
+   chains = nChains, 
+   thin = thinSteps, 
+   init = &quot;random&quot;, #or inits=list(inits,inits)
+   refresh = 0)
&gt; 
&gt; #print results
&gt; print(data.stanv3)
Inference for Stan model: ttestModelv3.
2 chains, each with iter=2000; warmup=500; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=3000.

                  mean se_mean   sd    2.5%     25%     50%     75%   97.5%
beta0           105.21    0.01 0.36  104.51  104.97  105.21  105.44  105.92
beta            -27.34    0.01 0.57  -28.45  -27.71  -27.35  -26.96  -26.21
sigma[1]          2.79    0.01 0.27    2.31    2.60    2.77    2.97    3.38
sigma[2]          2.88    0.01 0.34    2.31    2.63    2.84    3.07    3.65
Group_means[1]  105.21    0.01 0.36  104.51  104.97  105.21  105.44  105.92
Group_means[2]   77.86    0.01 0.44   77.00   77.57   77.86   78.15   78.75
CohensD          -9.70    0.02 0.76  -11.23  -10.23   -9.69   -9.17   -8.26
CLES              0.00    0.00 0.00    0.00    0.00    0.00    0.00    0.00
lp__           -150.30    0.04 1.42 -153.88 -151.02 -149.99 -149.25 -148.53
               n_eff Rhat
beta0           2426    1
beta            2359    1
sigma[1]        2166    1
sigma[2]        2547    1
Group_means[1]  2426    1
Group_means[2]  3478    1
CohensD         2468    1
CLES            1875    1
lp__            1277    1

Samples were drawn using NUTS(diag_e) at Mon Feb 10 14:11:55 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references hanging-indent">
<div id="ref-gelman2015stan">
<p>Gelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” <em>Journal of Educational and Behavioral Statistics</em> 40 (5): 530–43.</p>
</div>
<div id="ref-gelman2006prior">
<p>Gelman, Andrew, and others. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).” <em>Bayesian Analysis</em> 1 (3): 515–34.</p>
</div>
<div id="ref-rstanpackage">
<p>Stan Development Team. 2018. “RStan: The R Interface to Stan.” <a href="http://mc-stan.org/">http://mc-stan.org/</a>.</p>
</div>
</div>
</div>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/tutorials/">tutorials</a>
  
  <a class="badge badge-light" href="/tags/stan/">STAN</a>
  
  <a class="badge badge-light" href="/tags/population-differences/">population differences</a>
  
</div>



    
      








  
  
    
  
  





  
  
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="/authors/andrea-gabrio/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/jags/comparing-two-populations-jags/comparing-two-populations-jags/">Comparing Two Populations - JAGS</a></li>
          
          <li><a href="/stan/basic-introduction-to-stan/super-basic-introduction-to-stan/">Super basic introduction to STAN</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3394a224b26ce58ff36f44c54743e0ab.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">

  <p class="powered-by">
    &#169; Andrea Gabrio &middot;
    <code>2020</code> 
    &middot; Based on the 
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
