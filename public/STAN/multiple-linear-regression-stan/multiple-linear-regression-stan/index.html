<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.1">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Andrea Gabrio">

  
  
  
    
  
  <meta name="description" content="This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:
OpenBUGS - written in component pascal.
JAGS - (Just Another Gibbs Sampler) - written in C&#43;&#43;.">

  
  <link rel="alternate" hreflang="en-us" href="/stan/multiple-linear-regression-stan/multiple-linear-regression-stan/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:450,700|Oswald+Sans:600,700|Roboto+Mono:550,700">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.a71200610c21759266bff163bb521d95.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academic.4fe06d02a41da8ea4cf58ceef0b5213f.css">
  

  
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/stan/multiple-linear-regression-stan/multiple-linear-regression-stan/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Andrea Gabrio">
  <meta property="og:url" content="/stan/multiple-linear-regression-stan/multiple-linear-regression-stan/">
  <meta property="og:title" content="Multiple Linear Regression - STAN | Andrea Gabrio">
  <meta property="og:description" content="This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:
OpenBUGS - written in component pascal.
JAGS - (Just Another Gibbs Sampler) - written in C&#43;&#43;."><meta property="og:image" content="/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2020-02-03T21:13:14-05:00">
  
  <meta property="article:modified_time" content="2020-02-03T21:13:14-05:00">
  

  


  





  <title>Multiple Linear Regression - STAN | Andrea Gabrio</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Andrea Gabrio</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/post/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/research/"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/publication/"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/talk/"><span>Talks</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Software</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/missingHE/"><span>missingHE</span></a>
            </li>
            
          </ul>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Tutorials</span><span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/JAGS/"><span>JAGS</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/OpenBUGS/"><span>OpenBUGS</span></a>
            </li>
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/STAN/"><span>STAN</span></a>
            </li>
            
          </ul>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/missingdata/"><span>Missing Data</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Multiple Linear Regression - STAN</h1>

  

  
    



<meta content="2020-02-03 21:13:14 -0500 -0500" itemprop="datePublished">
<meta content="2020-02-03 21:13:14 -0500 -0500" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a>Andrea Gabrio</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Feb 3, 2020</time>
  </span>
  

  

  

  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    <a href="/categories/r/">R</a>, <a href="/categories/linear-regression/">linear regression</a>, <a href="/categories/stan/">STAN</a></span>
  

  
    

  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. <code>BUGS</code> (Bayesian inference Using <em>Gibbs Sampling</em>) is an algorithm and supporting language (resembling <code>R</code>) dedicated to performing the Gibbs sampling implementation of <em>Markov Chain Monte Carlo</em> (MCMC) method. Dialects of the <code>BUGS</code> language are implemented within three main projects:</p>
<ol style="list-style-type: decimal">
<li><p><strong>OpenBUGS</strong> - written in component pascal.</p></li>
<li><p><strong>JAGS</strong> - (Just Another Gibbs Sampler) - written in <code>C++</code>.</p></li>
<li><p><strong>STAN</strong> - a dedicated Bayesian modelling framework written in <code>C++</code> and implementing <em>Hamiltonian</em> MCMC samplers.</p></li>
</ol>
<p>Whilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of <code>R</code>, and thus, they are best accessed from within <code>R</code> itself. As such there are multiple packages devoted to interfacing with the various software implementations:</p>
<ul>
<li><p><em>R2OpenBUGS</em> - interfaces with <code>OpenBUGS</code></p></li>
<li><p><em>R2jags</em> - interfaces with <code>JAGS</code></p></li>
<li><p><em>rstan</em> - interfaces with <code>STAN</code></p></li>
</ul>
<p>This tutorial will demonstrate how to fit models in <code>STAN</code> (<span class="citation">Gelman, Lee, and Guo (2015)</span>) using the package <code>rstan</code> (<span class="citation">Stan Development Team (2018)</span>) as interface, which also requires to load some other packages.</p>
<div id="overview" class="section level1">
<h1>Overview</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Multiple regression is an extension of simple linear regression whereby a response variable is modelled against a linear combination of two or more simultaneously measured predictor variables. There are two main purposes of multiple linear regression:</p>
<ol style="list-style-type: decimal">
<li><p>To develop a better predictive model (equation) than is possible from models based on single independent variables.</p></li>
<li><p>To investigate the relative individual effects of each of the multiple independent variables above and beyond (standardised across) the effects of the other variables.</p></li>
</ol>
<p>Although the relationship between response variable and the additive effect of all the predictor variables is represented overall by a single multidimensional plane (surface), the individual effects of each of the predictor variables on the response variable (standardised across the other variables) can be depicted by single partial regression lines. The slope of any single partial regression line (partial regression slope) thereby represents the rate of change or effect of that specific predictor variable (holding all the other predictor variables constant to their respective mean values) on the response variable. In essence, it is the effect of one predictor variable at one specific level (the means) of all the other predictor variables (i.e. when each of the other predictors are set to their averages).</p>
<p>Multiple regression models can be constructed additively (containing only the predictor variables themselves) or in a multiplicative design (which incorporate interactions between predictor variables in addition to the predictor variables themselves). Multiplicative models are used primarily for testing inferences about the effects of various predictor variables and their interactions on the response variable. Additive models by contrast are used for generating predictive models and estimating the relative importance of individual predictor variables more so than hypothesis testing.</p>
</div>
<div id="additive-model" class="section level2">
<h2>Additive Model</h2>
<p><span class="math display">\[ y_i = \beta_0 + \beta_1x_{i1} +  \beta_2x_{i2} + \ldots + \beta_Jx_{iJ} + \epsilon_i,\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> is the population <span class="math inline">\(y\)</span>-intercept (value of <span class="math inline">\(y\)</span> when all partial slopes equal zero), <span class="math inline">\(\beta_1,\beta_2,\ldots,\beta_{J}\)</span> are the partial population slopes of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X_1,X_2,\ldots,X_J\)</span> respectively holding the other <span class="math inline">\(X\)</span> constant. <span class="math inline">\(\epsilon_i\)</span> is the random unexplained error or residual component. The additive model assumes that the effect of one predictor variable (partial slope) is independent of the levels of the other predictor variables.</p>
</div>
<div id="multiplicative-model" class="section level2">
<h2>Multiplicative Model</h2>
<p><span class="math display">\[ y_i = \beta_0 + \beta_1x_{i1} +  \beta_2x_{i2} + \beta_3x_{i1}x_{i2} + \ldots + \beta_Jx_{iJ} + \epsilon_i,\]</span></p>
<p>where <span class="math inline">\(\beta_3x_{i1}x_{i2}\)</span> is the interactive effect of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> on <span class="math inline">\(Y\)</span> and it examines the degree to which the effect of one of the predictor variables depends on the levels of the other predictor variable(s).</p>
</div>
<div id="data-generation" class="section level2">
<h2>Data generation</h2>
<p>Lets say we had set up a natural experiment in which we measured a response (<span class="math inline">\(y\)</span>) from each of <span class="math inline">\(20\)</span> sampling units (<span class="math inline">\(n=20\)</span>) across a landscape. At the same time, we also measured two other continuous covariates (<span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>) from each of the sampling units. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.</p>
<pre class="r"><code>&gt; set.seed(123)
&gt; n = 100
&gt; intercept = 5
&gt; temp = runif(n)
&gt; nitro = runif(n) + 0.8 * temp
&gt; int.eff = 2
&gt; temp.eff &lt;- 0.85
&gt; nitro.eff &lt;- 0.5
&gt; res = rnorm(n, 0, 1)
&gt; coef &lt;- c(int.eff, temp.eff, nitro.eff, int.eff)
&gt; mm &lt;- model.matrix(~temp * nitro)
&gt; 
&gt; y &lt;- t(coef %*% t(mm)) + res
&gt; data &lt;- data.frame(y, x1 = temp, x2 = nitro, cx1 = scale(temp,
+     scale = F), cx2 = scale(nitro, scale = F))
&gt; head(data)
         y        x1        x2         cx1         cx2
1 2.426468 0.2875775 0.8300510 -0.21098147 -0.08302110
2 4.927690 0.7883051 0.9634676  0.28974614  0.05039557
3 3.176118 0.4089769 0.8157946 -0.08958207 -0.09727750
4 6.166652 0.8830174 1.6608878  0.38445841  0.74781568
5 4.788890 0.9404673 1.2352762  0.44190829  0.32220415
6 2.541536 0.0455565 0.9267954 -0.45300249  0.01372335</code></pre>
<p>With these sort of data, we are primarily interested in investigating whether there is a relationship between the continuous response variable and the components linear predictor (continuous predictors). We could model the relationship via either:</p>
<ul>
<li><p>An additive model in which the effects of each predictor contribute in an additive way to the response - we do not allow for an interaction as we consider an interaction either not of great importance or likely to be absent.</p></li>
<li><p>A multiplicative model in which the effects of each predictor and their interaction contribute to the response - we allow for the impact of one predictor to vary across the range of the other predictor.</p></li>
</ul>
</div>
<div id="centering-the-data" class="section level2">
<h2>Centering the data</h2>
<p>When a linear model contains a covariate (continuous predictor variable) in addition to another predictor (continuous or categorical), it is nearly always advisable that the continuous predictor variables are centered prior to the analysis. Centering is a process by which the mean of a variable is subtracted from each of the values such that the scale of the variable is shifted so as to be centered around <span class="math inline">\(0\)</span>. Hence the mean of the new centered variable will be <span class="math inline">\(0\)</span>, yet it will retain the same variance.</p>
<p>There are multiple reasons for this:</p>
<ol style="list-style-type: decimal">
<li><p>It provides some clinical meaning to the <span class="math inline">\(y\)</span>-intercept. Recall that the <span class="math inline">\(y\)</span>-intercept is the value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X\)</span> is equal to zero. If <span class="math inline">\(X\)</span> is centered, then the <span class="math inline">\(y\)</span>-intercept represents the value of <span class="math inline">\(Y\)</span> at the mid-point of the <span class="math inline">\(X\)</span> range. The <span class="math inline">\(y\)</span>-intercept of an uncentered <span class="math inline">\(X\)</span> typically represents a unreal value of <span class="math inline">\(Y\)</span> (as an <span class="math inline">\(X\)</span> of <span class="math inline">\(0\)</span> is often beyond the reasonable range of values).</p></li>
<li><p>In multiplicative models (in which predictors and their interactions are included), main effects and interaction terms built from centered predictors will not be correlated to one another.</p></li>
<li><p>For more complex models, centering the covariates can increase the likelihood that the modelling engine converges (arrives at a numerically stable and reliable outcome).</p></li>
</ol>
<p>Note, centering will not effect the slope estimates. In <code>R</code>, centering is easily achieved with the <code>scale</code> function, which centers and scales (divides by standard deviation) the data. We only really need to center the data, so we provide the argument <code>scale=FALSE</code>. Also note that the <code>scale</code> function attaches the pre-centered mean (and standard deviation if scaling is performed) as attributes to the scaled data in order to facilitate back-scaling to the original scale. While these attributes are often convenient, they do cause issues for some of the Bayesian routines and so we will strip these attributes using the <code>as.numeric</code> function. Instead, we will create separate scalar variables to store the pre-scaled means.</p>
<pre class="r"><code>&gt; data &lt;- within(data, {
+     cx1 &lt;- as.numeric(scale(x1, scale = FALSE))
+     cx2 &lt;- as.numeric(scale(x2, scale = FALSE))
+ })
&gt; head(data)
         y        x1        x2         cx1         cx2
1 2.426468 0.2875775 0.8300510 -0.21098147 -0.08302110
2 4.927690 0.7883051 0.9634676  0.28974614  0.05039557
3 3.176118 0.4089769 0.8157946 -0.08958207 -0.09727750
4 6.166652 0.8830174 1.6608878  0.38445841  0.74781568
5 4.788890 0.9404673 1.2352762  0.44190829  0.32220415
6 2.541536 0.0455565 0.9267954 -0.45300249  0.01372335
&gt; 
&gt; mean.x1 = mean(data$x1)
&gt; mean.x2 = mean(data$x2)</code></pre>
</div>
</div>
<div id="assumptions" class="section level1">
<h1>Assumptions</h1>
<p>The assumptions of the model are:</p>
<ul>
<li><p>All of the observations are independent - this must be addressed at the design and collection stages.</p></li>
<li><p>The response variable (and thus the residuals) should be normally distributed. A boxplot of the entire variable is usually useful for diagnosing major issues with normality.</p></li>
<li><p>The response variable should be equally varied (variance should not be related to mean as these are supposed to be estimated separately). Scatterplots with linear smoothers can be useful for exploring the spread of observations around the trendline. The spread of observations around the trendline should not increase (or decrease) along its length.</p></li>
<li><p>The predictor variables should be uniformly or normally distributed. Again, boxplots can be useful.</p></li>
<li><p>The relationships between the linear predictors (right hand side of the regression formula) and the response variable should be linear. Scatterplots with smoothers can be useful for identifying possible non-linearity.</p></li>
<li><p><strong>(Multi)collinearity</strong>. The number of predictor variables must be less than the number of observations otherwise the linear model will be over-parameterized (more parameters to estimate than there are independent data from which estimates are calculated).</p></li>
</ul>
<p>(Multi)collinearity breaks the assumption that a predictor variable must not be correlated to the combination of other predictor variables (known collectively as the linear predictor). Multicollinearity has major detrimental effects on model fitting:</p>
<ul>
<li><p>Instability of the estimated partial regression slopes (small changes in the data or variable inclusion can cause dramatic changes in parameter estimates).</p></li>
<li><p>Inflated standard errors and confidence intervals of model parameters, thereby increasing the type II error rate (reducing power) of parameter hypothesis tests.</p></li>
</ul>
<p>Multicollinearity can be diagnosed with the following situatons:</p>
<ul>
<li><p>Investigate pairwise correlations between all the predictor variables either by a correlation matrix or a scatterplot matrix</p></li>
<li><p>Calculate the <strong>tolerance</strong> <span class="math inline">\((1−r^2)\)</span> of the relationship between a predictor variable and all the other predictor variables for each of the predictor variables. Tolerance is a measure of the degree of collinearity and values less than <span class="math inline">\(0.2\)</span> should be considered and values less than <span class="math inline">\(0.1\)</span> should be given serious attention. <strong>Variance inflation factor</strong> (VIF) is the inverse of tolerance and thus values greater than <span class="math inline">\(5\)</span>, or worse, <span class="math inline">\(10\)</span> indicate collinearity.</p></li>
<li><p><strong>PCA</strong> (principle components analysis) eigenvalues (from a correlation matrix for all the predictor variables) close to zero indicate collinearity and component loadings may be useful in determining which predictor variables cause collinearity.</p></li>
</ul>
<p>There are several approaches to dealing with collinearity (however the first two of these are likely to result in biased parameter estimates):</p>
<ol style="list-style-type: decimal">
<li><p>Remove the highly correlated predictor variable(s), starting with the least most clinically interesting variable(s)</p></li>
<li><p>PCA (principle components analysis) regression - regress the response variable against the principal components resulting from a correlation matrix for all the predictor variables. Each of these principal components by definition are completely independent, but the resulting parameter estimates must be back-calculated in order to have any clinical meaning.</p></li>
<li><p>Apply a regression tree - regression trees recursively partitioning (subsetting) the data in accordance to individual variables that explain the greatest remaining variance. Since at each iteration, each predictor variable is effectively evaluated in isolation, (multi)collinearity is not an issue.</p></li>
</ol>
</div>
<div id="model-fitting" class="section level1">
<h1>Model fitting</h1>
<p>Multiple linear regression models can include predictors (terms) that are incorporated additively (no interactions) or multiplicatively (with interactions). As such we will explore these separately for each modelling tool. The observed responses (<span class="math inline">\(y_i\)</span>) are assumed to be drawn from a normal distribution with a given mean (<span class="math inline">\(\mu\)</span>) and standard deviation (<span class="math inline">\(\sigma\)</span>). The expected values are themselves determined by the linear predictor. In this case, <span class="math inline">\(\beta_0\)</span> represents the <span class="math inline">\(y\)</span>-intercept (value of <span class="math inline">\(y\)</span> when all of the <span class="math inline">\(x\)</span>’s are equal to zero) and the set of <span class="math inline">\(\beta\)</span>’s represent the rates of change in y for every unit change in each <span class="math inline">\(x\)</span> (the effect) holding each other <span class="math inline">\(x\)</span> constant. Note that since we should always center all predictors (by subtracting the mean of each <span class="math inline">\(x\)</span> from the repective values of each <span class="math inline">\(x\)</span>), the <span class="math inline">\(y\)</span>-intercept represents the value of <span class="math inline">\(y\)</span> at the average value of each <span class="math inline">\(x\)</span>.</p>
<p>MCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying “uninformative” priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations (<span class="math inline">\(100\)</span>) for both the intercept and the treatment effect and a wide half-cauchy (<span class="math inline">\(\text{scale}=5\)</span>) for the standard deviation:</p>
<p><span class="math display">\[ y_i \sim \text{Normal}(\mu_i, \sigma),\]</span></p>
<p>where <span class="math inline">\(\mu_i=\beta_0 + \boldsymbol \beta \boldsymbol X_i\)</span>. Priors are specified as: <span class="math inline">\(\boldsymbol \beta \sim \text{Normal}(0,1000)\)</span> and <span class="math inline">\(\sigma \sim \text{Cauchy}(0,5)\)</span>. We will explore Bayesian modelling of multiple linear regression using <code>STAN</code>. The minimum model in <code>STAN</code> required to fit the above simple regression follows. Note the following modifications from the model defined in <code>JAGS</code>:</p>
<ul>
<li><p>The normal distribution is defined by standard deviation rather than precision</p></li>
<li><p>Rather than using a uniform prior for <span class="math inline">\(\sigma\)</span>, I am using a half-Cauchy</p></li>
</ul>
<div id="additive-model-1" class="section level2">
<h2>Additive model</h2>
<p>We now translate the likelihood for the additive model into <code>STAN</code> code.</p>
<pre class="r"><code>&gt; modelString = &quot;
+   data { 
+   int&lt;lower=1&gt; n;   // total number of observations 
+   vector[n] Y;      // response variable 
+   int&lt;lower=1&gt; nX;  // number of effects 
+   matrix[n, nX] X;   // model matrix 
+   } 
+   transformed data { 
+   matrix[n, nX - 1] Xc;  // centered version of X 
+   vector[nX - 1] means_X;  // column means of X before centering 
+   
+   for (i in 2:nX) { 
+   means_X[i - 1] = mean(X[, i]); 
+   Xc[, i - 1] = X[, i] - means_X[i - 1]; 
+   }  
+   } 
+   parameters { 
+   vector[nX-1] beta;  // population-level effects 
+   real cbeta0;  // center-scale intercept 
+   real&lt;lower=0&gt; sigma;  // residual SD 
+   } 
+   transformed parameters { 
+   } 
+   model { 
+   vector[n] mu; 
+   mu = Xc * beta + cbeta0; 
+   // prior specifications 
+   beta ~ normal(0, 100); 
+   cbeta0 ~ normal(0, 100); 
+   sigma ~ cauchy(0, 5); 
+   // likelihood contribution 
+   Y ~ normal(mu, sigma); 
+   } 
+   generated quantities {
+   real beta0;  // population-level intercept 
+   vector[n] log_lik;
+   beta0 = cbeta0 - dot_product(means_X, beta);
+   for (i in 1:n) {
+   log_lik[i] = normal_lpdf(Y[i] | Xc[i] * beta + cbeta0, sigma);
+   } 
+   }
+   
+   &quot;
&gt; ## write the model to a stan file 
&gt; writeLines(modelString, con = &quot;linregModeladd.stan&quot;)
&gt; writeLines(modelString, con = &quot;linregModelmult.stan&quot;)</code></pre>
<p>Arrange the data as a list (as required by <code>STAN</code>). As input, <code>STAN</code> will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.</p>
<pre class="r"><code>&gt; X = model.matrix(~cx1 + cx2, data = data)
&gt; data.list &lt;- with(data, list(Y = y, X = X, nX = ncol(X), n = nrow(data)))</code></pre>
<p>Define the nodes (parameters and derivatives) to monitor and chain parameters.</p>
<pre class="r"><code>&gt; params &lt;- c(&quot;beta&quot;,&quot;beta0&quot;, &quot;cbeta0&quot;, &quot;sigma&quot;, &quot;log_lik&quot;)
&gt; nChains = 2
&gt; burnInSteps = 1000
&gt; thinSteps = 1
&gt; numSavedSteps = 3000  #across all chains
&gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&gt; nIter
[1] 2500</code></pre>
<p>Now compile and run the Stan code via the <code>rstan</code> interface. Note that the first time <code>stan</code> is run after the <code>rstan</code> package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.</p>
<pre class="r"><code>&gt; library(rstan)</code></pre>
<p>During the warmup stage, the No-U-Turn sampler (NUTS) attempts to determine the optimum stepsize - the stepsize that achieves the target acceptance rate (<span class="math inline">\(0.8\)</span> or <span class="math inline">\(80\)</span>% by default) without divergence (occurs when the stepsize is too large relative to the curvature of the log posterior and results in approximations that are likely to diverge and be biased) - and without hitting the maximum treedepth (<span class="math inline">\(10\)</span>). At each iteration of the NUTS algorithm, the number of leapfrog steps doubles (as it increases the treedepth) and only terminates when either the NUTS criterion are satisfied or the tree depth reaches the maximum (<span class="math inline">\(10\)</span> by default).</p>
<pre class="r"><code>&gt; data.rstan.add &lt;- stan(data = data.list, file = &quot;linregModeladd.stan&quot;, chains = nChains, pars = params,
+     iter = nIter, warmup = burnInSteps, thin = thinSteps, save_dso = TRUE)

SAMPLING FOR MODEL &#39;linregModeladd&#39; NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2500 [  0%]  (Warmup)
Chain 1: Iteration:  250 / 2500 [ 10%]  (Warmup)
Chain 1: Iteration:  500 / 2500 [ 20%]  (Warmup)
Chain 1: Iteration:  750 / 2500 [ 30%]  (Warmup)
Chain 1: Iteration: 1000 / 2500 [ 40%]  (Warmup)
Chain 1: Iteration: 1001 / 2500 [ 40%]  (Sampling)
Chain 1: Iteration: 1250 / 2500 [ 50%]  (Sampling)
Chain 1: Iteration: 1500 / 2500 [ 60%]  (Sampling)
Chain 1: Iteration: 1750 / 2500 [ 70%]  (Sampling)
Chain 1: Iteration: 2000 / 2500 [ 80%]  (Sampling)
Chain 1: Iteration: 2250 / 2500 [ 90%]  (Sampling)
Chain 1: Iteration: 2500 / 2500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.069 seconds (Warm-up)
Chain 1:                0.095 seconds (Sampling)
Chain 1:                0.164 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL &#39;linregModeladd&#39; NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2500 [  0%]  (Warmup)
Chain 2: Iteration:  250 / 2500 [ 10%]  (Warmup)
Chain 2: Iteration:  500 / 2500 [ 20%]  (Warmup)
Chain 2: Iteration:  750 / 2500 [ 30%]  (Warmup)
Chain 2: Iteration: 1000 / 2500 [ 40%]  (Warmup)
Chain 2: Iteration: 1001 / 2500 [ 40%]  (Sampling)
Chain 2: Iteration: 1250 / 2500 [ 50%]  (Sampling)
Chain 2: Iteration: 1500 / 2500 [ 60%]  (Sampling)
Chain 2: Iteration: 1750 / 2500 [ 70%]  (Sampling)
Chain 2: Iteration: 2000 / 2500 [ 80%]  (Sampling)
Chain 2: Iteration: 2250 / 2500 [ 90%]  (Sampling)
Chain 2: Iteration: 2500 / 2500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.068 seconds (Warm-up)
Chain 2:                0.094 seconds (Sampling)
Chain 2:                0.162 seconds (Total)
Chain 2: 
&gt; 
&gt; data.rstan.add
Inference for Stan model: linregModeladd.
2 chains, each with iter=2500; warmup=1000; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=3000.

               mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
beta[1]        2.83    0.01 0.45   1.95   2.52   2.82   3.13   3.72  2562    1
beta[2]        1.58    0.01 0.38   0.84   1.33   1.58   1.85   2.32  2623    1
beta0          3.80    0.00 0.10   3.60   3.73   3.80   3.87   4.00  2672    1
cbeta0         3.80    0.00 0.10   3.60   3.73   3.80   3.87   4.00  2672    1
sigma          0.99    0.00 0.07   0.86   0.94   0.99   1.04   1.15  3017    1
log_lik[1]    -1.13    0.00 0.09  -1.33  -1.19  -1.13  -1.07  -0.96  2825    1
log_lik[2]    -0.95    0.00 0.08  -1.11  -1.00  -0.95  -0.89  -0.80  2813    1
log_lik[3]    -0.94    0.00 0.07  -1.09  -0.99  -0.94  -0.89  -0.80  2983    1
log_lik[4]    -0.94    0.00 0.09  -1.13  -1.00  -0.94  -0.88  -0.78  2666    1
log_lik[5]    -1.23    0.00 0.15  -1.56  -1.32  -1.22  -1.13  -0.98  3319    1
log_lik[6]    -0.94    0.00 0.08  -1.11  -0.99  -0.93  -0.88  -0.78  2591    1
log_lik[7]    -1.26    0.00 0.15  -1.60  -1.36  -1.25  -1.15  -1.00  2637    1
log_lik[8]    -2.00    0.00 0.28  -2.59  -2.18  -1.99  -1.81  -1.54  3642    1
log_lik[9]    -1.00    0.00 0.08  -1.16  -1.05  -0.99  -0.94  -0.86  2885    1
log_lik[10]   -1.43    0.00 0.17  -1.81  -1.53  -1.41  -1.30  -1.13  2549    1
log_lik[11]   -0.94    0.00 0.09  -1.12  -0.99  -0.94  -0.88  -0.78  2568    1
log_lik[12]   -1.14    0.00 0.10  -1.35  -1.20  -1.13  -1.07  -0.97  2419    1
log_lik[13]   -2.48    0.01 0.39  -3.32  -2.73  -2.44  -2.21  -1.82  2526    1
log_lik[14]   -0.93    0.00 0.08  -1.10  -0.98  -0.93  -0.88  -0.78  2702    1
log_lik[15]   -1.16    0.00 0.14  -1.46  -1.24  -1.14  -1.06  -0.93  2584    1
log_lik[16]   -0.95    0.00 0.09  -1.14  -1.01  -0.95  -0.89  -0.79  2460    1
log_lik[17]   -0.95    0.00 0.08  -1.11  -1.00  -0.94  -0.89  -0.80  2913    1
log_lik[18]   -1.16    0.00 0.17  -1.55  -1.26  -1.14  -1.04  -0.89  2488    1
log_lik[19]   -1.25    0.00 0.10  -1.46  -1.32  -1.25  -1.18  -1.06  2670    1
log_lik[20]   -1.34    0.00 0.17  -1.73  -1.44  -1.32  -1.21  -1.04  3156    1
log_lik[21]   -0.99    0.00 0.10  -1.20  -1.05  -0.99  -0.93  -0.82  3217    1
log_lik[22]   -1.43    0.00 0.14  -1.74  -1.52  -1.42  -1.33  -1.18  2520    1
log_lik[23]   -1.07    0.00 0.09  -1.26  -1.13  -1.06  -1.01  -0.90  2655    1
log_lik[24]   -0.97    0.00 0.10  -1.18  -1.02  -0.96  -0.90  -0.80  2490    1
log_lik[25]   -2.60    0.01 0.29  -3.21  -2.78  -2.59  -2.40  -2.08  2818    1
log_lik[26]   -1.05    0.00 0.12  -1.33  -1.12  -1.04  -0.96  -0.85  2885    1
log_lik[27]   -0.95    0.00 0.08  -1.12  -1.00  -0.95  -0.89  -0.80  2646    1
log_lik[28]   -0.93    0.00 0.08  -1.09  -0.98  -0.93  -0.87  -0.78  2601    1
log_lik[29]   -1.15    0.00 0.14  -1.46  -1.24  -1.14  -1.05  -0.92  3221    1
log_lik[30]   -0.93    0.00 0.08  -1.09  -0.97  -0.92  -0.87  -0.78  2844    1
log_lik[31]   -2.54    0.01 0.39  -3.37  -2.78  -2.52  -2.27  -1.86  3492    1
log_lik[32]   -1.34    0.00 0.21  -1.82  -1.47  -1.32  -1.19  -1.00  3489    1
log_lik[33]   -0.92    0.00 0.08  -1.08  -0.97  -0.92  -0.87  -0.78  2945    1
log_lik[34]   -0.95    0.00 0.08  -1.13  -1.01  -0.95  -0.90  -0.81  2937    1
log_lik[35]   -2.25    0.01 0.34  -2.96  -2.48  -2.23  -2.01  -1.66  3180    1
log_lik[36]   -1.55    0.00 0.13  -1.83  -1.64  -1.54  -1.46  -1.32  2570    1
log_lik[37]   -1.78    0.00 0.25  -2.32  -1.93  -1.76  -1.60  -1.35  3392    1
log_lik[38]   -1.21    0.00 0.14  -1.50  -1.30  -1.20  -1.11  -0.98  2419    1
log_lik[39]   -2.57    0.01 0.42  -3.49  -2.83  -2.54  -2.28  -1.86  2458    1
log_lik[40]   -1.70    0.00 0.18  -2.08  -1.82  -1.69  -1.58  -1.37  3038    1
log_lik[41]   -1.59    0.00 0.21  -2.06  -1.73  -1.57  -1.44  -1.23  3270    1
log_lik[42]   -0.94    0.00 0.08  -1.09  -0.99  -0.94  -0.89  -0.80  2984    1
log_lik[43]   -1.97    0.01 0.32  -2.67  -2.18  -1.94  -1.74  -1.42  2786    1
log_lik[44]   -1.86    0.00 0.24  -2.37  -2.02  -1.85  -1.70  -1.45  2785    1
log_lik[45]   -2.24    0.01 0.35  -2.98  -2.47  -2.22  -1.99  -1.65  2523    1
log_lik[46]   -0.93    0.00 0.08  -1.10  -0.98  -0.93  -0.88  -0.78  2849    1
log_lik[47]   -1.58    0.00 0.20  -2.01  -1.71  -1.56  -1.43  -1.22  3202    1
log_lik[48]   -1.22    0.00 0.15  -1.56  -1.31  -1.21  -1.11  -0.97  2573    1
log_lik[49]   -3.84    0.01 0.54  -5.01  -4.18  -3.82  -3.46  -2.87  3218    1
log_lik[50]   -1.47    0.00 0.20  -1.90  -1.59  -1.45  -1.34  -1.14  3648    1
log_lik[51]   -1.33    0.00 0.20  -1.78  -1.46  -1.31  -1.18  -1.01  2469    1
log_lik[52]   -1.23    0.00 0.09  -1.42  -1.29  -1.23  -1.17  -1.07  2508    1
log_lik[53]   -0.98    0.00 0.08  -1.15  -1.03  -0.98  -0.92  -0.82  2892    1
log_lik[54]   -1.05    0.00 0.12  -1.31  -1.12  -1.03  -0.97  -0.85  3408    1
log_lik[55]   -0.94    0.00 0.08  -1.11  -0.99  -0.93  -0.88  -0.79  2682    1
log_lik[56]   -0.92    0.00 0.08  -1.08  -0.97  -0.92  -0.87  -0.78  2941    1
log_lik[57]   -1.26    0.00 0.14  -1.57  -1.35  -1.25  -1.16  -1.03  2851    1
log_lik[58]   -1.03    0.00 0.10  -1.25  -1.09  -1.02  -0.96  -0.85  2528    1
log_lik[59]   -1.53    0.00 0.19  -1.94  -1.64  -1.51  -1.40  -1.20  3250    1
log_lik[60]   -0.95    0.00 0.08  -1.12  -1.00  -0.95  -0.89  -0.80  2944    1
log_lik[61]   -1.48    0.00 0.12  -1.75  -1.56  -1.48  -1.40  -1.26  2941    1
log_lik[62]   -1.09    0.00 0.12  -1.36  -1.16  -1.08  -1.01  -0.89  3504    1
log_lik[63]   -1.74    0.00 0.16  -2.08  -1.85  -1.73  -1.62  -1.45  2551    1
log_lik[64]   -7.01    0.02 0.96  -9.02  -7.60  -6.96  -6.33  -5.26  3101    1
log_lik[65]   -1.01    0.00 0.09  -1.22  -1.07  -1.01  -0.95  -0.85  2752    1
log_lik[66]   -0.96    0.00 0.08  -1.11  -1.01  -0.96  -0.91  -0.82  2946    1
log_lik[67]   -1.29    0.00 0.15  -1.62  -1.38  -1.27  -1.18  -1.03  3487    1
log_lik[68]   -1.09    0.00 0.12  -1.35  -1.16  -1.08  -1.01  -0.89  2517    1
log_lik[69]   -1.07    0.00 0.10  -1.27  -1.13  -1.06  -1.00  -0.89  2958    1
log_lik[70]   -1.02    0.00 0.09  -1.20  -1.07  -1.01  -0.96  -0.85  2673    1
log_lik[71]   -0.93    0.00 0.08  -1.08  -0.98  -0.93  -0.88  -0.79  2896    1
log_lik[72]   -0.92    0.00 0.08  -1.08  -0.97  -0.92  -0.87  -0.78  2738    1
log_lik[73]   -0.93    0.00 0.08  -1.10  -0.98  -0.93  -0.88  -0.78  2813    1
log_lik[74]   -3.84    0.01 0.63  -5.17  -4.23  -3.80  -3.39  -2.75  2911    1
log_lik[75]   -1.22    0.00 0.10  -1.41  -1.28  -1.21  -1.15  -1.04  2633    1
log_lik[76]   -1.42    0.00 0.15  -1.73  -1.52  -1.41  -1.31  -1.16  2747    1
log_lik[77]   -0.93    0.00 0.08  -1.08  -0.97  -0.92  -0.87  -0.78  2978    1
log_lik[78]   -0.96    0.00 0.08  -1.11  -1.01  -0.96  -0.91  -0.81  3039    1
log_lik[79]   -0.99    0.00 0.10  -1.20  -1.05  -0.98  -0.93  -0.82  2575    1
log_lik[80]   -0.94    0.00 0.08  -1.11  -1.00  -0.94  -0.89  -0.80  2971    1
log_lik[81]   -1.56    0.00 0.21  -1.99  -1.69  -1.54  -1.40  -1.21  2434    1
log_lik[82]   -1.68    0.00 0.17  -2.06  -1.78  -1.67  -1.56  -1.37  2568    1
log_lik[83]   -0.99    0.00 0.08  -1.15  -1.04  -0.99  -0.94  -0.84  2818    1
log_lik[84]   -1.36    0.00 0.16  -1.72  -1.46  -1.35  -1.25  -1.09  2593    1
log_lik[85]   -0.93    0.00 0.08  -1.08  -0.97  -0.92  -0.87  -0.78  2878    1
log_lik[86]   -0.93    0.00 0.07  -1.08  -0.98  -0.93  -0.88  -0.79  2977    1
log_lik[87]   -1.62    0.00 0.25  -2.18  -1.77  -1.59  -1.44  -1.19  2899    1
log_lik[88]   -0.96    0.00 0.09  -1.15  -1.02  -0.96  -0.90  -0.80  3100    1
log_lik[89]   -1.65    0.00 0.28  -2.28  -1.82  -1.62  -1.44  -1.18  3480    1
log_lik[90]   -1.09    0.00 0.13  -1.38  -1.17  -1.07  -1.00  -0.88  2482    1
log_lik[91]   -1.18    0.00 0.14  -1.51  -1.27  -1.17  -1.08  -0.95  3154    1
log_lik[92]   -0.99    0.00 0.08  -1.17  -1.04  -0.98  -0.93  -0.84  2766    1
log_lik[93]   -0.93    0.00 0.08  -1.10  -0.98  -0.93  -0.88  -0.78  2556    1
log_lik[94]   -1.31    0.00 0.11  -1.55  -1.38  -1.31  -1.24  -1.11  3091    1
log_lik[95]   -1.96    0.01 0.30  -2.60  -2.15  -1.94  -1.74  -1.47  2459    1
log_lik[96]   -3.52    0.01 0.47  -4.52  -3.81  -3.50  -3.19  -2.69  3235    1
log_lik[97]   -1.11    0.00 0.10  -1.32  -1.18  -1.10  -1.04  -0.93  2932    1
log_lik[98]   -1.48    0.00 0.19  -1.90  -1.61  -1.47  -1.34  -1.15  2845    1
log_lik[99]   -1.08    0.00 0.11  -1.33  -1.15  -1.07  -1.00  -0.89  2761    1
log_lik[100]  -1.66    0.00 0.13  -1.94  -1.74  -1.65  -1.56  -1.42  2616    1
lp__         -48.86    0.04 1.42 -52.37 -49.58 -48.52 -47.80 -47.06  1447    1

Samples were drawn using NUTS(diag_e) at Thu Feb 13 15:27:59 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
</div>
<div id="multiplicative-model-1" class="section level2">
<h2>Multiplicative model</h2>
<p>We now translate the likelihood for the multiplicative model into <code>STAN</code> code. Arrange the data as a list (as required by <code>STAN</code>). As input, <code>STAN</code> will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.</p>
<pre class="r"><code>&gt; X = model.matrix(~cx1 * cx2, data = data)
&gt; data.list &lt;- with(data, list(Y = y, X = X, nX = ncol(X), n = nrow(data)))</code></pre>
<p>Define the nodes (parameters and derivatives) to monitor and chain parameters.</p>
<pre class="r"><code>&gt; params &lt;- c(&quot;beta&quot;,&quot;beta0&quot;, &quot;cbeta0&quot;, &quot;sigma&quot;, &quot;log_lik&quot;)
&gt; nChains = 2
&gt; burnInSteps = 1000
&gt; thinSteps = 1
&gt; numSavedSteps = 3000  #across all chains
&gt; nIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)
&gt; nIter
[1] 2500</code></pre>
<p>Now compile and run the Stan code via the <code>rstan</code> interface.</p>
<pre class="r"><code>&gt; data.rstan.mult &lt;- stan(data = data.list, file = &quot;linregModelmult.stan&quot;, chains = nChains, pars = params,
+     iter = nIter, warmup = burnInSteps, thin = thinSteps, save_dso = TRUE)

SAMPLING FOR MODEL &#39;linregModeladd&#39; NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2500 [  0%]  (Warmup)
Chain 1: Iteration:  250 / 2500 [ 10%]  (Warmup)
Chain 1: Iteration:  500 / 2500 [ 20%]  (Warmup)
Chain 1: Iteration:  750 / 2500 [ 30%]  (Warmup)
Chain 1: Iteration: 1000 / 2500 [ 40%]  (Warmup)
Chain 1: Iteration: 1001 / 2500 [ 40%]  (Sampling)
Chain 1: Iteration: 1250 / 2500 [ 50%]  (Sampling)
Chain 1: Iteration: 1500 / 2500 [ 60%]  (Sampling)
Chain 1: Iteration: 1750 / 2500 [ 70%]  (Sampling)
Chain 1: Iteration: 2000 / 2500 [ 80%]  (Sampling)
Chain 1: Iteration: 2250 / 2500 [ 90%]  (Sampling)
Chain 1: Iteration: 2500 / 2500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.079 seconds (Warm-up)
Chain 1:                0.098 seconds (Sampling)
Chain 1:                0.177 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL &#39;linregModeladd&#39; NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2500 [  0%]  (Warmup)
Chain 2: Iteration:  250 / 2500 [ 10%]  (Warmup)
Chain 2: Iteration:  500 / 2500 [ 20%]  (Warmup)
Chain 2: Iteration:  750 / 2500 [ 30%]  (Warmup)
Chain 2: Iteration: 1000 / 2500 [ 40%]  (Warmup)
Chain 2: Iteration: 1001 / 2500 [ 40%]  (Sampling)
Chain 2: Iteration: 1250 / 2500 [ 50%]  (Sampling)
Chain 2: Iteration: 1500 / 2500 [ 60%]  (Sampling)
Chain 2: Iteration: 1750 / 2500 [ 70%]  (Sampling)
Chain 2: Iteration: 2000 / 2500 [ 80%]  (Sampling)
Chain 2: Iteration: 2250 / 2500 [ 90%]  (Sampling)
Chain 2: Iteration: 2500 / 2500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.079 seconds (Warm-up)
Chain 2:                0.096 seconds (Sampling)
Chain 2:                0.175 seconds (Total)
Chain 2: 
&gt; 
&gt; data.rstan.mult
Inference for Stan model: linregModeladd.
2 chains, each with iter=2500; warmup=1000; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=3000.

               mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
beta[1]        2.81    0.01 0.45   1.90   2.51   2.81   3.11   3.69  3050    1
beta[2]        1.50    0.01 0.38   0.77   1.24   1.50   1.77   2.26  2954    1
beta[3]        1.41    0.02 1.22  -0.98   0.58   1.44   2.24   3.76  3328    1
beta0          3.72    0.00 0.12   3.48   3.63   3.71   3.80   3.96  3353    1
cbeta0         3.80    0.00 0.10   3.61   3.73   3.80   3.87   4.00  3449    1
sigma          0.99    0.00 0.07   0.86   0.94   0.99   1.04   1.15  3271    1
log_lik[1]    -1.10    0.00 0.09  -1.29  -1.17  -1.10  -1.04  -0.93  3407    1
log_lik[2]    -0.97    0.00 0.09  -1.15  -1.02  -0.97  -0.91  -0.82  3196    1
log_lik[3]    -0.93    0.00 0.07  -1.08  -0.98  -0.93  -0.88  -0.79  3279    1
log_lik[4]    -0.98    0.00 0.11  -1.25  -1.03  -0.96  -0.90  -0.80  2510    1
log_lik[5]    -1.31    0.00 0.18  -1.72  -1.41  -1.29  -1.18  -1.01  3885    1
log_lik[6]    -0.94    0.00 0.08  -1.12  -0.99  -0.94  -0.88  -0.79  2606    1
log_lik[7]    -1.19    0.00 0.15  -1.55  -1.28  -1.17  -1.08  -0.93  3240    1
log_lik[8]    -2.17    0.01 0.34  -2.93  -2.38  -2.14  -1.93  -1.60  4441    1
log_lik[9]    -0.97    0.00 0.08  -1.13  -1.02  -0.97  -0.92  -0.82  3225    1
log_lik[10]   -1.45    0.00 0.18  -1.85  -1.56  -1.43  -1.32  -1.14  3432    1
log_lik[11]   -1.05    0.00 0.19  -1.55  -1.13  -1.01  -0.93  -0.82  2965    1
log_lik[12]   -1.17    0.00 0.10  -1.39  -1.23  -1.16  -1.09  -0.98  3567    1
log_lik[13]   -2.26    0.01 0.41  -3.15  -2.52  -2.23  -1.96  -1.54  3444    1
log_lik[14]   -0.93    0.00 0.08  -1.09  -0.98  -0.93  -0.88  -0.79  2841    1
log_lik[15]   -1.16    0.00 0.13  -1.45  -1.24  -1.14  -1.06  -0.93  3297    1
log_lik[16]   -0.99    0.00 0.11  -1.26  -1.05  -0.98  -0.91  -0.81  2781    1
log_lik[17]   -0.95    0.00 0.08  -1.10  -1.00  -0.95  -0.89  -0.80  3330    1
log_lik[18]   -1.09    0.00 0.16  -1.46  -1.18  -1.06  -0.97  -0.84  2960    1
log_lik[19]   -1.21    0.00 0.10  -1.42  -1.27  -1.20  -1.13  -1.02  3358    1
log_lik[20]   -1.39    0.00 0.19  -1.84  -1.50  -1.37  -1.25  -1.07  3724    1
log_lik[21]   -0.96    0.00 0.09  -1.16  -1.02  -0.96  -0.90  -0.80  3449    1
log_lik[22]   -1.34    0.00 0.15  -1.69  -1.44  -1.33  -1.23  -1.08  3373    1
log_lik[23]   -1.02    0.00 0.10  -1.24  -1.08  -1.02  -0.96  -0.85  3117    1
log_lik[24]   -0.96    0.00 0.09  -1.17  -1.02  -0.95  -0.90  -0.80  2550    1
log_lik[25]   -2.77    0.01 0.35  -3.52  -3.00  -2.74  -2.53  -2.13  3713    1
log_lik[26]   -1.08    0.00 0.14  -1.40  -1.16  -1.06  -0.99  -0.86  3525    1
log_lik[27]   -0.97    0.00 0.09  -1.17  -1.02  -0.97  -0.91  -0.81  2981    1
log_lik[28]   -0.94    0.00 0.08  -1.11  -0.99  -0.94  -0.88  -0.79  2570    1
log_lik[29]   -1.26    0.00 0.18  -1.67  -1.36  -1.23  -1.13  -0.97  3476    1
log_lik[30]   -0.93    0.00 0.08  -1.08  -0.98  -0.92  -0.87  -0.78  3170    1
log_lik[31]   -2.23    0.01 0.42  -3.14  -2.51  -2.19  -1.92  -1.51  4024    1
log_lik[32]   -1.17    0.00 0.22  -1.69  -1.28  -1.12  -1.00  -0.86  3923    1
log_lik[33]   -0.93    0.00 0.07  -1.08  -0.98  -0.93  -0.87  -0.79  3259    1
log_lik[34]   -0.98    0.00 0.09  -1.16  -1.03  -0.97  -0.92  -0.81  3436    1
log_lik[35]   -2.63    0.01 0.52  -3.77  -2.96  -2.60  -2.27  -1.76  3962    1
log_lik[36]   -1.67    0.00 0.18  -2.05  -1.78  -1.66  -1.54  -1.35  3477    1
log_lik[37]   -1.86    0.00 0.27  -2.44  -2.03  -1.83  -1.67  -1.41  4335    1
log_lik[38]   -1.29    0.00 0.17  -1.67  -1.40  -1.28  -1.17  -1.01  3135    1
log_lik[39]   -2.94    0.01 0.58  -4.21  -3.31  -2.90  -2.52  -1.98  3294    1
log_lik[40]   -1.78    0.00 0.20  -2.20  -1.91  -1.76  -1.63  -1.42  3848    1
log_lik[41]   -1.38    0.00 0.24  -1.96  -1.52  -1.35  -1.21  -1.00  3613    1
log_lik[42]   -0.93    0.00 0.07  -1.08  -0.98  -0.93  -0.88  -0.79  3311    1
log_lik[43]   -2.03    0.01 0.34  -2.78  -2.24  -2.00  -1.78  -1.44  3238    1
log_lik[44]   -1.92    0.00 0.25  -2.47  -2.08  -1.90  -1.73  -1.47  3460    1
log_lik[45]   -2.08    0.01 0.34  -2.82  -2.30  -2.05  -1.84  -1.50  3103    1
log_lik[46]   -1.00    0.00 0.13  -1.31  -1.06  -0.98  -0.91  -0.81  2724    1
log_lik[47]   -1.77    0.00 0.28  -2.40  -1.95  -1.74  -1.57  -1.29  3795    1
log_lik[48]   -1.24    0.00 0.16  -1.60  -1.34  -1.22  -1.13  -0.98  3253    1
log_lik[49]   -3.58    0.01 0.54  -4.72  -3.92  -3.54  -3.20  -2.60  3539    1
log_lik[50]   -1.62    0.00 0.26  -2.21  -1.78  -1.60  -1.44  -1.20  4473    1
log_lik[51]   -1.38    0.00 0.22  -1.87  -1.51  -1.35  -1.22  -1.03  3141    1
log_lik[52]   -1.29    0.00 0.11  -1.51  -1.36  -1.28  -1.22  -1.10  3466    1
log_lik[53]   -1.00    0.00 0.09  -1.19  -1.05  -0.99  -0.94  -0.83  3475    1
log_lik[54]   -1.25    0.00 0.25  -1.85  -1.39  -1.21  -1.07  -0.90  3457    1
log_lik[55]   -0.93    0.00 0.08  -1.10  -0.98  -0.93  -0.88  -0.79  2537    1
log_lik[56]   -0.93    0.00 0.08  -1.09  -0.98  -0.93  -0.88  -0.78  3031    1
log_lik[57]   -1.21    0.00 0.14  -1.51  -1.29  -1.19  -1.11  -0.98  3535    1
log_lik[58]   -0.99    0.00 0.10  -1.22  -1.05  -0.98  -0.92  -0.82  2718    1
log_lik[59]   -1.50    0.00 0.19  -1.92  -1.61  -1.48  -1.36  -1.17  3926    1
log_lik[60]   -0.96    0.00 0.08  -1.13  -1.01  -0.95  -0.90  -0.81  3188    1
log_lik[61]   -1.56    0.00 0.15  -1.88  -1.66  -1.55  -1.45  -1.29  3824    1
log_lik[62]   -1.28    0.00 0.24  -1.84  -1.42  -1.24  -1.11  -0.93  3575    1
log_lik[63]   -1.63    0.00 0.17  -1.99  -1.75  -1.62  -1.51  -1.33  3227    1
log_lik[64]   -6.83    0.02 0.94  -8.75  -7.44  -6.78  -6.16  -5.13  3520    1
log_lik[65]   -0.99    0.00 0.09  -1.20  -1.05  -0.99  -0.93  -0.83  3065    1
log_lik[66]   -0.99    0.00 0.08  -1.15  -1.04  -0.99  -0.94  -0.85  3279    1
log_lik[67]   -1.22    0.00 0.15  -1.54  -1.31  -1.21  -1.11  -0.97  4342    1
log_lik[68]   -1.04    0.00 0.12  -1.31  -1.11  -1.03  -0.96  -0.85  2870    1
log_lik[69]   -1.09    0.00 0.10  -1.32  -1.16  -1.09  -1.02  -0.91  3711    1
log_lik[70]   -1.03    0.00 0.09  -1.22  -1.09  -1.02  -0.97  -0.87  3430    1
log_lik[71]   -0.93    0.00 0.08  -1.08  -0.98  -0.93  -0.88  -0.79  3117    1
log_lik[72]   -0.93    0.00 0.08  -1.09  -0.98  -0.93  -0.88  -0.79  2675    1
log_lik[73]   -0.93    0.00 0.08  -1.09  -0.98  -0.93  -0.87  -0.79  3199    1
log_lik[74]   -3.70    0.01 0.62  -5.03  -4.11  -3.66  -3.25  -2.63  3528    1
log_lik[75]   -1.15    0.00 0.11  -1.37  -1.22  -1.14  -1.07  -0.96  3241    1
log_lik[76]   -1.40    0.00 0.14  -1.70  -1.49  -1.39  -1.30  -1.15  3445    1
log_lik[77]   -0.93    0.00 0.07  -1.08  -0.98  -0.93  -0.88  -0.79  3260    1
log_lik[78]   -0.99    0.00 0.08  -1.15  -1.04  -0.99  -0.93  -0.84  3340    1
log_lik[79]   -1.07    0.00 0.13  -1.36  -1.14  -1.05  -0.97  -0.86  3060    1
log_lik[80]   -0.97    0.00 0.09  -1.15  -1.02  -0.96  -0.91  -0.81  3250    1
log_lik[81]   -1.42    0.00 0.21  -1.88  -1.55  -1.40  -1.27  -1.07  3112    1
log_lik[82]   -1.80    0.00 0.23  -2.30  -1.95  -1.79  -1.64  -1.40  3734    1
log_lik[83]   -0.96    0.00 0.08  -1.13  -1.01  -0.96  -0.90  -0.81  3208    1
log_lik[84]   -1.29    0.00 0.17  -1.66  -1.38  -1.27  -1.17  -1.01  3325    1
log_lik[85]   -0.93    0.00 0.08  -1.08  -0.98  -0.93  -0.87  -0.78  3030    1
log_lik[86]   -0.92    0.00 0.07  -1.07  -0.97  -0.92  -0.87  -0.78  3172    1
log_lik[87]   -1.62    0.00 0.26  -2.22  -1.78  -1.60  -1.43  -1.19  3500    1
log_lik[88]   -0.95    0.00 0.08  -1.13  -1.00  -0.94  -0.89  -0.80  3265    1
log_lik[89]   -1.41    0.00 0.30  -2.11  -1.58  -1.36  -1.18  -0.96  4135    1
log_lik[90]   -1.02    0.00 0.12  -1.31  -1.10  -1.01  -0.94  -0.82  2936    1
log_lik[91]   -1.06    0.00 0.15  -1.44  -1.13  -1.03  -0.95  -0.83  3111    1
log_lik[92]   -0.96    0.00 0.08  -1.14  -1.01  -0.96  -0.91  -0.81  2933    1
log_lik[93]   -0.96    0.00 0.09  -1.17  -1.01  -0.95  -0.89  -0.80  2348    1
log_lik[94]   -1.27    0.00 0.11  -1.52  -1.34  -1.26  -1.18  -1.07  3617    1
log_lik[95]   -1.74    0.01 0.32  -2.44  -1.93  -1.70  -1.51  -1.21  3087    1
log_lik[96]   -3.34    0.01 0.46  -4.30  -3.63  -3.31  -3.02  -2.54  3551    1
log_lik[97]   -1.14    0.00 0.11  -1.39  -1.21  -1.14  -1.06  -0.95  3744    1
log_lik[98]   -1.53    0.00 0.20  -1.97  -1.65  -1.51  -1.38  -1.18  3594    1
log_lik[99]   -1.07    0.00 0.11  -1.31  -1.13  -1.06  -0.99  -0.87  3345    1
log_lik[100]  -1.56    0.00 0.15  -1.87  -1.65  -1.54  -1.45  -1.30  3334    1
lp__         -48.64    0.04 1.58 -52.44 -49.52 -48.28 -47.44 -46.52  1457    1

Samples were drawn using NUTS(diag_e) at Thu Feb 13 15:28:01 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
</div>
</div>
<div id="mcmc-diagnostics" class="section level1">
<h1>MCMC diagnostics</h1>
<p>In addition to the regular model diagnostic checks (such as residual plots), for Bayesian analyses, it is necessary to explore the characteristics of the MCMC chains and the sampler in general. Recall that the purpose of MCMC sampling is to replicate the posterior distribution of the model likelihood and priors by drawing a known number of samples from this posterior (thereby formulating a probability distribution). This is only reliable if the MCMC samples accurately reflect the posterior. Unfortunately, since we only know the posterior in the most trivial of circumstances, it is necessary to rely on indirect measures of how accurately the MCMC samples are likely to reflect the likelihood. I will briefly outline the most important diagnostics.</p>
<ul>
<li><p><em>Traceplots</em> for each parameter illustrate the MCMC sample values after each successive iteration along the chain. Bad chain mixing (characterised by any sort of pattern) suggests that the MCMC sampling chains may not have completely traversed all features of the posterior distribution and that more iterations are required to ensure the distribution has been accurately represented.</p></li>
<li><p><em>Autocorrelation</em> plot for each parameter illustrate the degree of correlation between MCMC samples separated by different lags. For example, a lag of <span class="math inline">\(0\)</span> represents the degree of correlation between each MCMC sample and itself (obviously this will be a correlation of <span class="math inline">\(1\)</span>). A lag of <span class="math inline">\(1\)</span> represents the degree of correlation between each MCMC sample and the next sample along the chain and so on. In order to be able to generate unbiased estimates of parameters, the MCMC samples should be independent (uncorrelated).</p></li>
<li><p><em>Potential scale reduction factor</em> (Rhat) statistic for each parameter provides a measure of sampling efficiency/effectiveness. Ideally, all values should be less than <span class="math inline">\(1.05\)</span>. If there are values of <span class="math inline">\(1.05\)</span> or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentially slower than it could have been but, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.</p></li>
</ul>
<p>Prior to examining the summaries, we should have explored the convergence diagnostics. We use the package <code>mcmcplots</code> to obtain density and trace plots for the effects model as an example.</p>
<pre class="r"><code>&gt; library(mcmcplots)
&gt; s = as.array(data.rstan.mult)
&gt; mcmc &lt;- do.call(mcmc.list, plyr:::alply(s[, , -(length(s[1, 1, ]))], 2, as.mcmc))
&gt; denplot(mcmc, parms = c(&quot;beta0&quot;,&quot;beta&quot;,&quot;cbeta0&quot;,&quot;sigma&quot;))</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_diag-1.png" width="672" /></p>
<pre class="r"><code>&gt; traplot(mcmc, parms = c(&quot;beta0&quot;,&quot;beta&quot;,&quot;cbeta0&quot;,&quot;sigma&quot;))</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_diag-2.png" width="672" /></p>
<p>These plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space.</p>
<pre class="r"><code>&gt; #Raftery diagnostic
&gt; raftery.diag(mcmc)
$`1`

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 

You need a sample size of at least 3746 with these values of q, r and s

$`2`

Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95 

You need a sample size of at least 3746 with these values of q, r and s</code></pre>
<p>The Raftery diagnostics for each chain estimate that we would require no more than <span class="math inline">\(5000\)</span> samples to reach the specified level of confidence in convergence. As we have <span class="math inline">\(10500\)</span> samples, we can be confidence that convergence has occurred.</p>
<pre class="r"><code>&gt; #Autocorrelation diagnostic
&gt; stan_ac(data.rstan.mult, pars = c(&quot;beta&quot;,&quot;beta0&quot;))</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_diag3-1.png" width="672" /></p>
<p>A lag of 10 appears to be sufficient to avoid autocorrelation (poor mixing).</p>
<pre class="r"><code>&gt; stan_ac(data.rstan.mult, pars = c(&quot;beta&quot;,&quot;beta0&quot;))</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_diag4-1.png" width="672" /></p>
<pre class="r"><code>&gt; stan_ess(data.rstan.mult, pars = c(&quot;beta&quot;,&quot;beta0&quot;))</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_diag4-2.png" width="672" /></p>
<p>Rhat and effective sample size. In this instance, most of the parameters have reasonably high effective samples and thus there is likely to be a good range of values from which to estimate paramter properties.</p>
</div>
<div id="model-validation" class="section level1">
<h1>Model validation</h1>
<p>Model validation involves exploring the model diagnostics and fit to ensure that the model is broadly appropriate for the data. As such, exploration of the residuals should be routine. Ideally, a good model should also be able to predict the data used to fit the model. Residuals are not computed directly within <code>rstan</code> However, we can calculate them manually form the posteriors.</p>
<pre class="r"><code>&gt; library(ggplot2)
&gt; library(dplyr)
&gt; mcmc = as.data.frame(data.rstan.mult) %&gt;% dplyr:::select(beta0, starts_with(&quot;beta&quot;),
+     sigma) %&gt;% as.matrix
&gt; # generate a model matrix
&gt; newdata = data
&gt; Xmat = model.matrix(~cx1 * cx2, newdata)
&gt; ## get median parameter estimates
&gt; coefs = apply(mcmc[, 1:4], 2, median)
&gt; fit = as.vector(coefs %*% t(Xmat))
&gt; resid = data$y - fit
&gt; ggplot() + geom_point(data = NULL, aes(y = resid, x = fit))</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_residuals-1.png" width="672" /></p>
<p>Residuals against predictors</p>
<pre class="r"><code>&gt; library(tidyr)
&gt; mcmc = as.data.frame(data.rstan.mult) %&gt;% dplyr:::select(beta0, starts_with(&quot;beta&quot;),
+     sigma) %&gt;% as.matrix
&gt; # generate a model matrix
&gt; newdata = newdata
&gt; Xmat = model.matrix(~cx1 * cx2, newdata)
&gt; ## get median parameter estimates
&gt; coefs = apply(mcmc[, 1:4], 2, median)
&gt; fit = as.vector(coefs %*% t(Xmat))
&gt; resid = data$y - fit
&gt; newdata = data %&gt;% cbind(fit, resid)
&gt; newdata.melt = newdata %&gt;% gather(key = Pred, value = Value, cx1:cx2)
&gt; ggplot(newdata.melt) + geom_point(aes(y = resid, x = Value)) + facet_wrap(~Pred)</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_residuals2-1.png" width="672" /></p>
<p>And now for studentised residuals</p>
<pre class="r"><code>&gt; mcmc = as.data.frame(data.rstan.mult) %&gt;% dplyr:::select(beta0, starts_with(&quot;beta&quot;),
+     sigma) %&gt;% as.matrix
&gt; # generate a model matrix
&gt; newdata = data
&gt; Xmat = model.matrix(~cx1 * cx2, newdata)
&gt; ## get median parameter estimates
&gt; coefs = apply(mcmc[, 1:4], 2, median)
&gt; fit = as.vector(coefs %*% t(Xmat))
&gt; resid = data$y - fit
&gt; sresid = resid/sd(resid)
&gt; ggplot() + geom_point(data = NULL, aes(y = sresid, x = fit))</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_residuals3-1.png" width="672" /></p>
<p>For this simple model, the studentized residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.</p>
<pre class="r"><code>&gt; mcmc = as.data.frame(data.rstan.mult) %&gt;% dplyr:::select(beta0,
+     starts_with(&quot;beta&quot;), sigma) %&gt;% as.matrix
&gt; # generate a model matrix
&gt; Xmat = model.matrix(~cx1 * cx2, data)
&gt; ## get median parameter estimates
&gt; coefs = mcmc[, 1:4]
&gt; fit = coefs %*% t(Xmat)
&gt; ## draw samples from this model
&gt; yRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data), fit[i,
+     ], mcmc[i, &quot;sigma&quot;]))
&gt; ggplot() + geom_density(data = NULL, aes(x = as.vector(yRep),
+     fill = &quot;Model&quot;), alpha = 0.5) + geom_density(data = data,
+     aes(x = y, fill = &quot;Obs&quot;), alpha = 0.5)</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_rep-1.png" width="672" /></p>
<p>We can also explore the posteriors of each parameter.</p>
<pre class="r"><code>&gt; library(bayesplot)
&gt; mcmc_intervals(as.matrix(data.rstan.mult), regex_pars = &quot;beta|sigma&quot;)</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_rep2-1.png" width="672" /></p>
<pre class="r"><code>&gt; mcmc_areas(as.matrix(data.rstan.mult), regex_pars = &quot;beta|sigma&quot;)</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_rep2-2.png" width="672" /></p>
</div>
<div id="parameter-estimates" class="section level1">
<h1>Parameter estimates</h1>
<p>Although all parameters in a Bayesian analysis are considered random and are considered a distribution, rarely would it be useful to present tables of all the samples from each distribution. On the other hand, plots of the posterior distributions have some use. Nevertheless, most workers prefer to present simple statistical summaries of the posteriors. Popular choices include the median (or mean) and <span class="math inline">\(95\)</span>% credibility intervals.</p>
<pre class="r"><code>&gt; mcmcpvalue &lt;- function(samp) {
+     ## elementary version that creates an empirical p-value for the
+     ## hypothesis that the columns of samp have mean zero versus a general
+     ## multivariate distribution with elliptical contours.
+ 
+     ## differences from the mean standardized by the observed
+     ## variance-covariance factor
+ 
+     ## Note, I put in the bit for single terms
+     if (length(dim(samp)) == 0) {
+         std &lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),
+             transpose = TRUE)
+         sqdist &lt;- colSums(std * std)
+         sum(sqdist[-1] &gt; sqdist[1])/length(samp)
+     } else {
+         std &lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),
+             transpose = TRUE)
+         sqdist &lt;- colSums(std * std)
+         sum(sqdist[-1] &gt; sqdist[1])/nrow(samp)
+     }
+ 
+ }</code></pre>
<p>First, we look at the results from the additive model.</p>
<pre class="r"><code>&gt; print(data.rstan.add, pars = c(&quot;beta0&quot;, &quot;beta&quot;, &quot;sigma&quot;))
Inference for Stan model: linregModeladd.
2 chains, each with iter=2500; warmup=1000; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=3000.

        mean se_mean   sd 2.5%  25%  50%  75% 97.5% n_eff Rhat
beta0   3.80    0.00 0.10 3.60 3.73 3.80 3.87  4.00  2672    1
beta[1] 2.83    0.01 0.45 1.95 2.52 2.82 3.13  3.72  2562    1
beta[2] 1.58    0.01 0.38 0.84 1.33 1.58 1.85  2.32  2623    1
sigma   0.99    0.00 0.07 0.86 0.94 0.99 1.04  1.15  3017    1

Samples were drawn using NUTS(diag_e) at Thu Feb 13 15:27:59 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
&gt; 
&gt; # OR
&gt; library(broom)
&gt; tidyMCMC(data.rstan.add, conf.int = TRUE, conf.method = &quot;HPDinterval&quot;,
+     pars = c(&quot;beta0&quot;, &quot;beta&quot;, &quot;sigma&quot;))
# A tibble: 4 x 5
  term    estimate std.error conf.low conf.high
  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 beta0      3.80     0.101     3.59       3.99
2 beta[1]    2.83     0.445     1.93       3.69
3 beta[2]    1.58     0.377     0.823      2.31
4 sigma      0.994    0.0740    0.856      1.15</code></pre>
<p><strong>Conclusions</strong></p>
<ul>
<li><p>When <code>cx2</code> is held constant, a one unit increase in <code>cx1</code> is associated with a <span class="math inline">\(2.83\)</span> change in <span class="math inline">\(y\)</span>. That is, <span class="math inline">\(y\)</span> increases at a rate of <span class="math inline">\(2.83\)</span> per unit increase in <code>cx1</code> when standardised for <code>cx2</code>.</p></li>
<li><p>When <code>cx1</code> is held constant, a one unit increase in <code>cx2</code> is associated with a <span class="math inline">\(1.58\)</span> change in <span class="math inline">\(y\)</span>. That is, <span class="math inline">\(y\)</span> increases at a rate of <span class="math inline">\(1.58\)</span> per unit increase in <code>cx2</code> when standardised for <code>cx1</code>.</p></li>
</ul>
<p>Note, as this is an additive model, the rates associated with <code>cx1</code> are assumed to be constant throughtout the range of <code>cx2</code> and vice versa. The <span class="math inline">\(95\)</span>% confidence interval for each partial slope does not overlap with <span class="math inline">\(0\)</span> implying a significant effects of <code>cx1</code> and <code>cx2</code> on <span class="math inline">\(y\)</span>. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.</p>
<pre class="r"><code>&gt; mcmcpvalue(as.matrix(data.rstan.add)[, &quot;beta[1]&quot;])
[1] 0
&gt; mcmcpvalue(as.matrix(data.rstan.add)[, &quot;beta[2]&quot;])
[1] 0</code></pre>
<p>With a p-value of essentially <span class="math inline">\(0\)</span>, we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship. Next, we look at the results from the multiplicative model.</p>
<pre class="r"><code>&gt; print(data.rstan.mult, pars = c(&quot;beta0&quot;, &quot;beta&quot;, &quot;sigma&quot;))
Inference for Stan model: linregModeladd.
2 chains, each with iter=2500; warmup=1000; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=3000.

        mean se_mean   sd  2.5%  25%  50%  75% 97.5% n_eff Rhat
beta0   3.72    0.00 0.12  3.48 3.63 3.71 3.80  3.96  3353    1
beta[1] 2.81    0.01 0.45  1.90 2.51 2.81 3.11  3.69  3050    1
beta[2] 1.50    0.01 0.38  0.77 1.24 1.50 1.77  2.26  2954    1
beta[3] 1.41    0.02 1.22 -0.98 0.58 1.44 2.24  3.76  3328    1
sigma   0.99    0.00 0.07  0.86 0.94 0.99 1.04  1.15  3271    1

Samples were drawn using NUTS(diag_e) at Thu Feb 13 15:28:01 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
&gt; 
&gt; # OR
&gt; library(broom)
&gt; tidyMCMC(data.rstan.mult, conf.int = TRUE, conf.method = &quot;HPDinterval&quot;,
+     pars = c(&quot;beta0&quot;, &quot;beta&quot;, &quot;sigma&quot;))
# A tibble: 5 x 5
  term    estimate std.error conf.low conf.high
  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 beta0      3.72     0.124     3.48       3.96
2 beta[1]    2.81     0.455     1.96       3.73
3 beta[2]    1.50     0.385     0.769      2.26
4 beta[3]    1.41     1.22     -0.985      3.76
5 sigma      0.993    0.0725    0.850      1.13</code></pre>
<p><strong>Conclusions</strong></p>
<ul>
<li><p>At the average level of <code>cx2 (=0)</code>, a one unit increase in <code>cx1</code> is associated with a <span class="math inline">\(2.80\)</span> change in y. That is, y increases at a rate of <span class="math inline">\(2.80\)</span> per unit increase in <code>cx1</code> when standardised for <code>cx2</code>.</p></li>
<li><p>At the average level of <code>cx1 (=0)</code>, a one unit increase in <code>cx2</code> is associated with a <span class="math inline">\(1.50\)</span> change in <span class="math inline">\(y\)</span>. That is, <span class="math inline">\(y\)</span> increases at a rate of <span class="math inline">\(1.50\)</span> per unit increase in <code>cx2</code> when standardised for <code>cx1</code>.</p></li>
<li><p>The degree to which the rate of change in response associated with a one unit change in <code>cx1</code> changes over the range of <code>cx2</code> (and vice versa) is <span class="math inline">\(1.45\)</span>.</p></li>
</ul>
<p>The <span class="math inline">\(95\)</span>% confidence intervals for the interaction partial slope does not overlap with <span class="math inline">\(0\)</span> implying a significant interaction between <code>cx1</code> and <code>cx2</code>. This suggests that the nature of the relationship between <span class="math inline">\(y\)</span> and <code>cx1</code> depends on the level of <code>cx2</code> (and vice versa). The estimates of the effect of <code>cx1</code> are only appropriate when <code>cx2 = 0</code> etc. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.</p>
<pre class="r"><code>&gt; ## since values are less than zero
&gt; mcmcpvalue(as.matrix(data.rstan.mult)[, &quot;beta[1]&quot;])
[1] 0
&gt; mcmcpvalue(as.matrix(data.rstan.mult)[, &quot;beta[2]&quot;])
[1] 0
&gt; mcmcpvalue(as.matrix(data.rstan.mult)[, &quot;beta[3]&quot;])
[1] 0.2476667</code></pre>
<p>With a p-value of essentially <span class="math inline">\(0\)</span>, we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship. An alternative way of quantifying the impact of an interaction is to compare models with and without the interactions. In a Bayesian context, this can be achieved by comparing the <strong>leave-one-out cross-validation</strong> statistics. Leave-one-out (LOO) cross-validation explores how well a series of models can predict withheld values <span class="citation">Vehtari, Gelman, and Gabry (2017)</span>. The LOO Information Criterion (LOOIC) is analogous to the AIC except that the LOOIC takes priors into consideration, does not assume that the posterior distribution is drawn from a multivariate normal and integrates over parameter uncertainty so as to yield a distribution of looic rather than just a point estimate. The LOOIC does however assume that all observations are equally influential (it does not matter which observations are left out). This assumption can be examined via the Pareto <span class="math inline">\(k\)</span> estimate (values greater than <span class="math inline">\(0.5\)</span> or more conservatively <span class="math inline">\(0.75\)</span> are considered overly influential). We can compute LOOIC if we store the loglikelihood from our <code>STAN</code> model, which can then be extracted to compute the information criterion using the package <code>loo</code>.</p>
<pre class="r"><code>&gt; ## since values are less than zero
&gt; library(loo)
&gt; (full = loo(extract_log_lik(data.rstan.mult)))

Computed from 3000 by 100 log-likelihood matrix

         Estimate   SE
elpd_loo   -143.3  8.5
p_loo         5.2  1.1
looic       286.5 17.0
------
Monte Carlo SE of elpd_loo is 0.1.

All Pareto k estimates are good (k &lt; 0.5).
See help(&#39;pareto-k-diagnostic&#39;) for details.
&gt; 
&gt; (reduced = loo(extract_log_lik(data.rstan.add)))

Computed from 3000 by 100 log-likelihood matrix

         Estimate   SE
elpd_loo   -143.1  8.7
p_loo         4.4  1.1
looic       286.1 17.4
------
Monte Carlo SE of elpd_loo is 0.0.

All Pareto k estimates are good (k &lt; 0.5).
See help(&#39;pareto-k-diagnostic&#39;) for details.
&gt; 
&gt; par(mfrow = 1:2, mar = c(5, 3.8, 1, 0) + 0.1, las = 3)
&gt; plot(full, label_points = TRUE)
&gt; plot(reduced, label_points = TRUE)</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_loo-1.png" width="672" /></p>
<p>The expected out-of-sample predictive accuracy is very similar (slightly lower) for the additive model compared to the multiplicative model (model containing the interaction). This might be used to suggest that the inferential evidence for an interaction is low.</p>
</div>
<div id="graphical-summaries" class="section level1">
<h1>Graphical summaries</h1>
<p>With appropriate use of model matrices and data wrangling, it is possible to produce a single prediction data set along with <code>ggplot</code> syntax to produce a multi-panel figure. First we look at the additive model.</p>
<pre class="r"><code>&gt; mcmc = as.matrix(data.rstan.add)
&gt; ## Calculate the fitted values
&gt; newdata = rbind(data.frame(cx1 = seq(min(data$cx1, na.rm = TRUE), max(data$cx1,
+     na.rm = TRUE), len = 100), cx2 = 0, Pred = 1), data.frame(cx1 = 0,
+     cx2 = seq(min(data$cx2, na.rm = TRUE), max(data$cx2, na.rm = TRUE),
+         len = 100), Pred = 2))
&gt; Xmat = model.matrix(~cx1 + cx2, newdata)
&gt; coefs = mcmc[, c(&quot;beta0&quot;, &quot;beta[1]&quot;, &quot;beta[2]&quot;)]
&gt; fit = coefs %*% t(Xmat)
&gt; newdata = newdata %&gt;% mutate(x1 = cx1 + mean.x1, x2 = cx2 + mean.x2) %&gt;%
+     cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = &quot;HPDinterval&quot;)) %&gt;%
+     mutate(x = dplyr:::recode(Pred, x1, x2))
&gt; 
&gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_line() + geom_ribbon(aes(ymin = conf.low,
+     ymax = conf.high), fill = &quot;blue&quot;, alpha = 0.3) + scale_y_continuous(&quot;Y&quot;) +
+     scale_x_continuous(&quot;X&quot;) + theme_classic() + facet_wrap(~Pred)</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_post1-1.png" width="672" /></p>
<p>We cannot simply add the raw data to this figure. The reason for this is that the trends represent the effect of one predictor holding the other variable constant. Therefore, the observations we represent on the figure must likewise be standardised. We can achieve this by adding the partial residuals to the figure. Partial residuals are the fitted values plus the residuals.</p>
<pre class="r"><code>&gt; ## Calculate partial residuals fitted values
&gt; fdata = rdata = rbind(data.frame(cx1 = data$cx1, cx2 = 0, Pred = 1), data.frame(cx1 = 0,
+     cx2 = data$cx2, Pred = 2))
&gt; fMat = rMat = model.matrix(~cx1 + cx2, fdata)
&gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))
&gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))
&gt; rdata = rdata %&gt;% mutate(partial.resid = resid + fit) %&gt;% mutate(x1 = cx1 +
+     mean.x1, x2 = cx2 + mean.x2) %&gt;% mutate(x = dplyr:::recode(Pred, x1,
+     x2))
&gt; 
&gt; ggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),
+     color = &quot;gray&quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low, ymax = conf.high),
+     fill = &quot;blue&quot;, alpha = 0.3) + scale_y_continuous(&quot;Y&quot;) + theme_classic() +
+     facet_wrap(~Pred, strip.position = &quot;bottom&quot;, labeller = label_bquote(&quot;x&quot; *
+         .(Pred))) + theme(axis.title.x = element_blank(), strip.background = element_blank(),
+     strip.placement = &quot;outside&quot;)</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_post2-1.png" width="672" /></p>
<p>However, this method (whist partially elegant) does become overly opaque if we need more extensive axes labels since the x-axes labels are actually strip labels (which must largely be defined outside of the <code>ggplot</code> structure). The alternative is to simply produce each partial plot separately before arranging them together in the one figure using the package <code>gridExtra</code>.</p>
<pre class="r"><code>&gt; library(gridExtra)
&gt; mcmc = as.matrix(data.rstan.add)
&gt; ## Calculate the fitted values
&gt; newdata = data.frame(cx1 = seq(min(data$cx1, na.rm = TRUE), max(data$cx1,
+     na.rm = TRUE), len = 100), cx2 = 0)
&gt; Xmat = model.matrix(~cx1 + cx2, newdata)
&gt; coefs = mcmc[, c(&quot;beta0&quot;, &quot;beta[1]&quot;, &quot;beta[2]&quot;)]
&gt; fit = coefs %*% t(Xmat)
&gt; newdata = newdata %&gt;% mutate(x1 = cx1 + mean.x1, x2 = cx2 + mean.x2) %&gt;%
+     cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = &quot;HPDinterval&quot;))
&gt; ## Now the partial residuals
&gt; fdata = rdata = data.frame(cx1 = data$cx1, cx2 = 0)
&gt; fMat = rMat = model.matrix(~cx1 + cx2, fdata)
&gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))
&gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))
&gt; rdata = rdata %&gt;% mutate(partial.resid = resid + fit) %&gt;% mutate(x1 = cx1 +
+     mean.x1, x2 = cx2 + mean.x2)
&gt; g1 = ggplot(newdata, aes(y = estimate, x = x1)) + geom_point(data = rdata,
+     aes(y = partial.resid), color = &quot;grey&quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low,
+     ymax = conf.high), fill = &quot;blue&quot;, alpha = 0.3) + scale_y_continuous(&quot;Y&quot;) +
+     scale_x_continuous(&quot;X1&quot;) + theme_classic()
&gt; 
&gt; newdata = data.frame(cx2 = seq(min(data$cx2, na.rm = TRUE), max(data$cx2,
+     na.rm = TRUE), len = 100), cx1 = 0)
&gt; Xmat = model.matrix(~cx1 + cx2, newdata)
&gt; coefs = mcmc[, c(&quot;beta0&quot;, &quot;beta[1]&quot;, &quot;beta[2]&quot;)]
&gt; fit = coefs %*% t(Xmat)
&gt; newdata = newdata %&gt;% mutate(x1 = cx1 + mean.x1, x2 = cx2 + mean.x2) %&gt;%
+     cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = &quot;HPDinterval&quot;))
&gt; ## Now the partial residuals
&gt; fdata = rdata = data.frame(cx1 = 0, cx2 = data$cx2)
&gt; fMat = rMat = model.matrix(~cx1 + cx2, fdata)
&gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))
&gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))
&gt; rdata = rdata %&gt;% mutate(partial.resid = resid + fit) %&gt;% mutate(x1 = cx1 +
+     mean.x1, x2 = cx2 + mean.x2)
&gt; g2 = ggplot(newdata, aes(y = estimate, x = x2)) + geom_point(data = rdata,
+     aes(y = partial.resid), color = &quot;grey&quot;) + geom_line() + geom_ribbon(aes(ymin = conf.low,
+     ymax = conf.high), fill = &quot;blue&quot;, alpha = 0.3) + scale_y_continuous(&quot;Y&quot;) +
+     scale_x_continuous(&quot;X2&quot;) + theme_classic()
&gt; 
&gt; grid.arrange(g1, g2, ncol = 2)</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_post3-1.png" width="672" /></p>
<p>For the multiplicative model, we could elect to split the trends up so as to explore the effects of one predictor at several set levels of another predictor. In this example, we will explore the effects of <span class="math inline">\(x_1\)</span> when <span class="math inline">\(x_2\)</span> is equal to its mean in the original data as well as one and two standard deviations below and above this mean.</p>
<pre class="r"><code>&gt; library(fields)
&gt; mcmc = as.matrix(data.rstan.mult)
&gt; ## Calculate the fitted values
&gt; newdata = expand.grid(cx1 = seq(min(data$cx1, na.rm = TRUE), max(data$cx1,
+     na.rm = TRUE), len = 100), cx2 = mean(data$cx2) + sd(data$cx2) %*%
+     -2:2)
&gt; Xmat = model.matrix(~cx1 * cx2, newdata)
&gt; coefs = mcmc[, c(&quot;beta0&quot;, &quot;beta[1]&quot;, &quot;beta[2]&quot;, &quot;beta[3]&quot;)]
&gt; fit = coefs %*% t(Xmat)
&gt; newdata = newdata %&gt;% mutate(x1 = cx1 + mean.x1, x2 = cx2 + mean.x2) %&gt;%
+     cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = &quot;HPDinterval&quot;)) %&gt;%
+     mutate(x2 = factor(x2, labels = paste(&quot;X2:~&quot;, c(-2, -1, 0, 1, 2), &quot;*sigma&quot;)))
&gt; ## Partial residuals
&gt; fdata = rdata = expand.grid(cx1 = data$cx1, cx2 = mean(data$cx2) + sd(data$cx2) *
+     -2:2)
&gt; fMat = rMat = model.matrix(~cx1 * cx2, fdata)
&gt; fit = as.vector(apply(coefs, 2, median) %*% t(fMat))
&gt; resid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))
&gt; rdata = rdata %&gt;% mutate(partial.resid = resid + fit) %&gt;% mutate(x1 = cx1 +
+     mean.x1, x2 = cx2 + mean.x2)
&gt; ## Partition the partial residuals such that each x1 trend only includes
&gt; ## x2 data that is within that range in the observed data
&gt; findNearest = function(x, y) {
+     ff = fields:::rdist(x, y)
+     apply(ff, 1, function(x) which(x == min(x)))
+ }
&gt; fn = findNearest(x = data[, c(&quot;x1&quot;, &quot;x2&quot;)], y = rdata[, c(&quot;x1&quot;, &quot;x2&quot;)])
&gt; rdata = rdata[fn, ] %&gt;% mutate(x2 = factor(x2, labels = paste(&quot;X2:~&quot;, c(-2,
+     -1, 0, 1, 2), &quot;*sigma&quot;)))
&gt; ggplot(newdata, aes(y = estimate, x = x1)) + geom_line() + geom_blank(aes(y = 9)) +
+     geom_point(data = rdata, aes(y = partial.resid), color = &quot;grey&quot;) +
+     geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = &quot;blue&quot;,
+         alpha = 0.3) + scale_y_continuous(&quot;Y&quot;) + scale_x_continuous(&quot;X1&quot;) +
+     facet_wrap(~x2, labeller = label_parsed, nrow = 1, scales = &quot;free_y&quot;) +
+     theme_classic() + theme(strip.background = element_blank())</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/mcmc_post4-1.png" width="672" /></p>
<p>Alternatively, we could explore the interaction by plotting a two dimensional surface as a heat map.</p>
</div>
<div id="effect-sizes" class="section level1">
<h1>Effect sizes</h1>
<p>In addition to deriving the distribution means for the slope parameter, we could make use of the Bayesian framework to derive the distribution of the effect size. In so doing, effect size could be considered as either the rate of change or alternatively, the difference between pairs of values along the predictor gradient. For the latter case, there are multiple ways of calculating an effect size, but the two most common are:</p>
<ul>
<li><p><em>Raw effect size</em>. The difference between two groups (as already calculated)</p></li>
<li><p><em>Cohen’s D</em>. The effect size standardized by division with the pooled standard deviation: <span class="math inline">\(D=\frac{(\mu_A-\mu_B)}{\sigma}\)</span></p></li>
<li><p><em>Percentage change</em>. Express the effect size as a percent of one of the pairs. That is, whether you expressing a percentage increase or a percentage decline depends on which of the pairs of values are considered a reference value. Care must be exercised to ensure no division by zeros occur.</p></li>
</ul>
<p>For simple linear models, effect size based on a rate is essentially the same as above except that it is expressed per unit of the predictor. Of course in many instances, one unit change in the predictor represents too subtle a shift in the underlying gradient to likely yield any clinically meaningful or appreciable change in response.</p>
<p>Probability that a change in <span class="math inline">\(x_1\)</span> is associated with greater than a <span class="math inline">\(50\)</span>% increase in <span class="math inline">\(y\)</span> at various levels of <span class="math inline">\(x_2\)</span>. Clearly, in order to explore this inference, we must first express the change in <span class="math inline">\(y\)</span> as a percentage. This in turn requires us to calculate start and end points from which to calculate the magnitude of the effect (amount of increase in <span class="math inline">\(y\)</span>) as well as the percentage decline. Hence, we start by predicting the distribution of <span class="math inline">\(y\)</span> at the lowest and highest values of <span class="math inline">\(x_1\)</span> at five levels of <span class="math inline">\(x_2\)</span> (representing two standard deviations below the <code>cx2</code> mean, one standard deviation below the <code>cx2</code> mean, the <code>cx2</code> mean, one standard deviation above the <code>cx2</code> mean and <span class="math inline">\(2\)</span> standard deviations above the <code>cx2</code> mean. For this exercise we will only use the multiplicative model. Needless to say, the process would be very similar for the additive model.</p>
<pre class="r"><code>&gt; mcmc = as.matrix(data.rstan.mult)
&gt; newdata = expand.grid(cx1 = c(min(data$cx1), max(data$cx1)), cx2 = (-2:2) *
+     sd(data$cx1))
&gt; Xmat = model.matrix(~cx1 * cx2, newdata)
&gt; coefs = mcmc[, c(&quot;beta0&quot;, &quot;beta[1]&quot;, &quot;beta[2]&quot;, &quot;beta[3]&quot;)]
&gt; fit = coefs %*% t(Xmat)
&gt; s1 = seq(1, 9, b = 2)
&gt; s2 = seq(2, 10, b = 2)
&gt; ## Raw effect size
&gt; (RES = tidyMCMC(as.mcmc(fit[, s2] - fit[, s1]), conf.int = TRUE, conf.method = &quot;HPDinterval&quot;))
# A tibble: 5 x 5
  term  estimate std.error conf.low conf.high
  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 2         1.99     0.846    0.278      3.59
2 4         2.39     0.583    1.29       3.58
3 6         2.79     0.452    1.94       3.71
4 8         3.19     0.554    2.11       4.21
5 10        3.59     0.806    2.09       5.18
&gt; ## Cohen&#39;s D
&gt; cohenD = (fit[, s2] - fit[, s1])/sqrt(mcmc[, &quot;sigma&quot;])
&gt; (cohenDES = tidyMCMC(as.mcmc(cohenD), conf.int = TRUE, conf.method = &quot;HPDinterval&quot;))
# A tibble: 5 x 5
  term  estimate std.error conf.low conf.high
  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 2         2.00     0.849    0.283      3.60
2 4         2.40     0.590    1.26       3.56
3 6         2.81     0.465    1.91       3.73
4 8         3.21     0.570    2.14       4.33
5 10        3.61     0.820    1.90       5.10
&gt; # Percentage change (relative to Group A)
&gt; ESp = 100 * (fit[, s2] - fit[, s1])/fit[, s1]
&gt; (PES = tidyMCMC(as.mcmc(ESp), conf.int = TRUE, conf.method = &quot;HPDinterval&quot;))
# A tibble: 5 x 5
  term  estimate std.error conf.low conf.high
  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 2         121.      80.0    -1.23      277.
2 4         119.      41.4    38.8       200.
3 6         124.      33.1    64.6       190.
4 8         132.      46.6    54.8       225.
5 10        144.      73.2    37.8       282.
&gt; # Probability that the effect is greater than 50% (an increase of &gt;50%)
&gt; (p50 = apply(ESp, 2, function(x) sum(x &gt; 50)/length(x)))
        2         4         6         8        10 
0.8586667 0.9740000 0.9983333 0.9940000 0.9793333 
&gt; ## fractional change
&gt; (FES = tidyMCMC(as.mcmc(fit[, s2]/fit[, s1]), conf.int = TRUE, conf.method = &quot;HPDinterval&quot;))
# A tibble: 5 x 5
  term  estimate std.error conf.low conf.high
  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 2         2.21     0.800    0.988      3.77
2 4         2.19     0.414    1.39       3.00
3 6         2.24     0.331    1.65       2.90
4 8         2.32     0.466    1.55       3.25
5 10        2.44     0.732    1.38       3.82</code></pre>
<p><strong>Conclusions</strong></p>
<ul>
<li><p>On average, when <span class="math inline">\(x_2\)</span> is equal to its mean, <span class="math inline">\(Y\)</span> increases by <span class="math inline">\(2.79\)</span> over the observed range of <span class="math inline">\(x_1\)</span>. We are <span class="math inline">\(95\)</span>% confident that the increase is between <span class="math inline">\(1.91\)</span> and <span class="math inline">\(3.66\)</span>.</p></li>
<li><p>The Cohen’s D associated change over the observed range of <span class="math inline">\(x_1\)</span> is <span class="math inline">\(2.80\)</span>.</p></li>
<li><p>On average, <span class="math inline">\(Y\)</span> increases by <span class="math inline">\(124\)</span>% over the observed range of <span class="math inline">\(x_1\)</span> (at average <span class="math inline">\(x_2\)</span>). We are <span class="math inline">\(95\)</span>% confident that the increase is between <span class="math inline">\(65\)</span>% and <span class="math inline">\(190\)</span>%.</p></li>
<li><p>The probability that <span class="math inline">\(Y\)</span> increases by more than <span class="math inline">\(50\)</span>% over the observed range of <span class="math inline">\(x_1\)</span> (average <span class="math inline">\(x_2\)</span>) is <span class="math inline">\(0.998\)</span>.</p></li>
<li><p>On average, <span class="math inline">\(Y\)</span> increases by a factor of <span class="math inline">\(2.24\)</span>% over the observed range of <span class="math inline">\(x_1\)</span> (average <span class="math inline">\(x_2\)</span>). We are <span class="math inline">\(95\)</span>% confident that the decline is between a factor of <span class="math inline">\(1.65\)</span>% and <span class="math inline">\(2.90\)</span>%.</p></li>
</ul>
</div>
<div id="finite-population-standard-deviations" class="section level1">
<h1>Finite population standard deviations</h1>
<p>Variance components, the amount of added variance attributed to each influence, are traditionally estimated for so called random effects. These are the effects for which the levels employed in the design are randomly selected to represent a broader range of possible levels. For such effects, effect sizes (differences between each level and a reference level) are of little value. Instead, the “importance” of the variables are measured in units of variance components. On the other hand, regular variance components for fixed factors (those whose measured levels represent the only levels of interest) are not logical - since variance components estimate variance as if the levels are randomly selected from a larger population. Nevertheless, in order to compare and contrast the scale of variability of both fixed and random factors, it is necessary to measure both on the same scale (sample or population based variance).</p>
<p>Finite-population variance components assume that the levels of all factors (fixed and random) in the design are all the possible levels available (<span class="citation">Gelman and others (2005)</span>). In other words, they are assumed to represent finite populations of levels. Sample (rather than population) statistics are then used to calculate these finite-population variances (or standard deviations). Since standard deviation (and variance) are bound at zero, standard deviation posteriors are typically non-normal. Consequently, medians and HPD intervals are more robust estimates.</p>
<pre><code># A tibble: 4 x 5
  term     estimate std.error conf.low conf.high
  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 sd.x1       0.800    0.130  0.558        1.06 
2 sd.x2       0.501    0.128  0.256        0.754
3 sd.x1x2     0.134    0.0873 0.000182     0.291
4 sd.resid    0.981    0.0125 0.966        1.01 
# A tibble: 4 x 5
  term     estimate std.error conf.low conf.high
  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 sd.x1       33.2       5.00 22.5          42.1
2 sd.x2       20.8       5.11 11.2          30.9
3 sd.x1x2      5.16      3.44  0.00805      11.5
4 sd.resid    40.5       2.13 36.7          45.0</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/effects_modelv4-1.png" width="672" /></p>
<p>Approximately <span class="math inline">\(59\)</span>% of the total finite population standard deviation is due to <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span> and their interaction.</p>
</div>
<div id="r-squared" class="section level1">
<h1>R squared</h1>
<p>In a frequentist context, the <span class="math inline">\(R^2\)</span> value is seen as a useful indicator of goodness of fit. Whilst it has long been acknowledged that this measure is not appropriate for comparing models (for such purposes information criterion such as AIC are more appropriate), it is nevertheless useful for estimating the amount (percent) of variance explained by the model. In a frequentist context, <span class="math inline">\(R^2\)</span> is calculated as the variance in predicted values divided by the variance in the observed (response) values. Unfortunately, this classical formulation does not translate simply into a Bayesian context since the equivalently calculated numerator can be larger than the an equivalently calculated denominator - thereby resulting in an <span class="math inline">\(R^2\)</span> greater than <span class="math inline">\(100\)</span>%. <span class="citation">Gelman et al. (2019)</span> proposed an alternative formulation in which the denominator comprises the sum of the explained variance and the variance of the residuals.</p>
<p>So in the standard regression model notation of:</p>
<p><span class="math display">\[ y_i \sim \text{Normal}(\boldsymbol X \boldsymbol \beta, \sigma),\]</span></p>
<p>the <span class="math inline">\(R^2\)</span> could be formulated as</p>
<p><span class="math display">\[ R^2 = \frac{\sigma^2_f}{\sigma^2_f + \sigma^2_e},\]</span></p>
<p>where <span class="math inline">\(\sigma^2_f=\text{var}(\boldsymbol X \boldsymbol \beta)\)</span>, and for normal models <span class="math inline">\(\sigma^2_e=\text{var}(y-\boldsymbol X \boldsymbol \beta)\)</span></p>
<pre class="r"><code>&gt; mcmc &lt;- as.matrix(data.rstan.mult)
&gt; Xmat = model.matrix(~cx1 * cx2, data)
&gt; coefs = mcmc[, c(&quot;beta0&quot;, &quot;beta[1]&quot;, &quot;beta[2]&quot;, &quot;beta[3]&quot;)]
&gt; fit = coefs %*% t(Xmat)
&gt; resid = sweep(fit, 2, data$y, &quot;-&quot;)
&gt; var_f = apply(fit, 1, var)
&gt; var_e = apply(resid, 1, var)
&gt; R2 = var_f/(var_f + var_e)
&gt; tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = &quot;HPDinterval&quot;)
# A tibble: 1 x 5
  term  estimate std.error conf.low conf.high
  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 var1     0.605    0.0390    0.531     0.678
&gt; 
&gt; # for comparison with frequentist
&gt; summary(lm(y ~ cx1 * cx2, data))

Call:
lm(formula = y ~ cx1 * cx2, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.8173 -0.7167 -0.1092  0.5890  3.3861 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   3.7152     0.1199  30.987  &lt; 2e-16 ***
cx1           2.8072     0.4390   6.394 5.84e-09 ***
cx2           1.4988     0.3810   3.934 0.000158 ***
cx1:cx2       1.4464     1.1934   1.212 0.228476    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.9804 on 96 degrees of freedom
Multiple R-squared:  0.6115,    Adjusted R-squared:  0.5994 
F-statistic: 50.37 on 3 and 96 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="bayesian-model-selection" class="section level1">
<h1>Bayesian model selection</h1>
<p>A statistical model is by definition a low-dimensional (over simplification) representation of what is really likely to be a very complex system. As a result, no model is right. Some models however can provide useful insights into some of the processes operating on the system. Frequentist statistics have various methods (model selection, dredging, lasso, cross validation) for selecting parsimonious models. These are models that provide a good comprimise between minimizing unexplained patterns and minimizing model complexity. The basic premise is that since no model can hope to capture the full complexity of a system with all its subtleties, only the very major patterns can be estimated. Overly complex models are likely to be representing artificial complexity present only in the specific observed data (not the general population). The Bayesian approach is to apply priors to the non-variance parameters such that parameters close to zero are further shrunk towards zero whilst priors on parameters further away from zero are less effected. The most popular form of prior for sparsity is the <em>horseshoe prior</em>, so called because the shape of a component of this prior resembles a horseshoe (with most of the mass either close to <span class="math inline">\(0\)</span> or close to <span class="math inline">\(1\)</span>).</p>
<p>Rather than apply weakly informative Gaussian priors on parameters as:</p>
<p><span class="math display">\[ \beta_j \sim N(0,\sigma^2),\]</span></p>
<p>the horseshoe prior is defined as</p>
<p><span class="math display">\[ \beta_j \sim N(0,\tau^2\lambda_j^2),\]</span></p>
<p>where <span class="math inline">\(\tau \sim \text{Cauchy}(0,1)\)</span> and <span class="math inline">\(\lambda_j \sim \text{Cauchy}(0,1)\)</span>, for <span class="math inline">\(j=1,\ldots,D\)</span>. Using this prior, <span class="math inline">\(D\)</span> is the number of (non-intercept or variance) parameters, <span class="math inline">\(\tau\)</span> represents the global scale that weights or shrinks all parameters towards zero and <span class="math inline">\(\lambda_j\)</span> are thick tailed local scales that allow some of the <span class="math inline">\(j\)</span> parameters to escape shrinkage. More recently, <span class="citation">Piironen, Vehtari, and others (2017)</span> have argued that whilst the above horseshoe priors do guarantee that strong effects (parameters) will not be over-shrunk, there is the potential for weekly identified effects (those based on relatively little data) to be misrepresented in the posteriors. As an alternative they advocated the use of regularised horseshoe priors in which the amount of shrinkage applied to the largest effects can be controlled. The prior is defined as:</p>
<p><span class="math display">\[ \beta_j \sim N(0,\tau^2 \tilde{\lambda}_j^2),\]</span></p>
<p>where <span class="math inline">\(\tilde{\lambda}_j^2 = \frac{c^2\lambda^2_j}{c^2+\tau^2 \lambda^2_j}\)</span> and <span class="math inline">\(c\)</span> is (slab width, actually variance) is a constant. For small effects (when <span class="math inline">\(\tau^2 \lambda^2_j &lt; c^2\)</span>) the prior approaches a regular prior. However, for large effects (when <span class="math inline">\(\tau^2 \lambda^2_j &gt; c^2\)</span>) the prior approaches <span class="math inline">\(N(0,c^2)\)</span>. Finally, they recommend applying a inverse-gamma prior on <span class="math inline">\(c^2\)</span>:</p>
<p><span class="math display">\[ c^2 \sim \text{Inv-Gamma}(\alpha,\beta),\]</span></p>
<p>where <span class="math inline">\(\alpha=v/2\)</span> and <span class="math inline">\(\beta=vs^2/2\)</span>, which translates to a <span class="math inline">\(\text{Student-t}_ν(0, s^2)\)</span> slab for the coefficients far from zero and is typically a good default choice for a weakly informative prior. This prior can be encoded into <code>STAN</code> using the following code.</p>
<pre class="r"><code>&gt; modelStringHP = &quot;
+                        data {
+                        int &lt; lower =0 &gt; n; // number of observations
+                        int &lt; lower =0 &gt; nX; // number of predictors
+                        vector [ n] Y; // outputs
+                        matrix [n ,nX] X; // inputs
+                        real &lt; lower =0 &gt; scale_icept ; // prior std for the intercept
+                        real &lt; lower =0 &gt; scale_global ; // scale for the half -t prior for tau
+                        real &lt; lower =1 &gt; nu_global ; // degrees of freedom for the half -t priors for tau
+                        real &lt; lower =1 &gt; nu_local ; // degrees of freedom for the half - t priors for lambdas
+                        real &lt; lower =0 &gt; slab_scale ; // slab scale for the regularized horseshoe
+                        real &lt; lower =0 &gt; slab_df ; // slab degrees of freedom for the regularized horseshoe
+                        }
+                        transformed data {
+                        matrix[n, nX - 1] Xc;  // centered version of X 
+                        vector[nX - 1] means_X;  // column means of X before centering 
+                        for (i in 2:nX) { 
+                        means_X[i - 1] = mean(X[, i]); 
+                        Xc[, i - 1] = X[, i] - means_X[i - 1]; 
+                        }  
+                        }
+                        parameters {
+                        real logsigma ;
+                        real cbeta0 ;
+                        vector [ nX-1] z;
+                        real &lt; lower =0 &gt; tau ; // global shrinkage parameter
+                        vector &lt; lower =0 &gt;[ nX-1] lambda ; // local shrinkage parameter
+                        real &lt; lower =0 &gt; caux ;
+                        }
+                        transformed parameters {
+                        real &lt; lower =0 &gt; sigma ; // noise std
+                        vector &lt; lower =0 &gt;[ nX-1] lambda_tilde ; // truncated local shrinkage parameter
+                        real &lt; lower =0 &gt; c; // slab scale
+                        vector [ nX-1] beta ; // regression coefficients
+                        vector [ n] mu; // latent function values
+                        sigma = exp ( logsigma );
+                        c = slab_scale * sqrt ( caux );
+                        lambda_tilde = sqrt ( c ^2 * square ( lambda ) ./ (c ^2 + tau ^2* square ( lambda )) );
+                        beta = z .* lambda_tilde * tau ;
+                        mu = cbeta0 + Xc* beta ;
+                        }
+                        model {
+                        // half -t priors for lambdas and tau , and inverse - gamma for c ^2
+                        z ~ normal (0 , 1);
+                        lambda ~ student_t ( nu_local , 0, 1);
+                        tau ~ student_t ( nu_global , 0 , scale_global * sigma );
+                        caux ~ inv_gamma (0.5* slab_df , 0.5* slab_df );
+                        cbeta0 ~ normal (0 , scale_icept );
+                        Y ~ normal (mu , sigma );
+                        }
+                        generated quantities { 
+                        real beta0;  // population-level intercept 
+                        vector[n] log_lik;
+                        beta0 = cbeta0 - dot_product(means_X, beta);
+                        for (i in 1:n) {
+                        log_lik[i] = normal_lpdf(Y[i] | Xc[i] * beta + cbeta0, sigma);
+                        }
+                        }
+   
+   &quot;
&gt; ## write the model to a stan file 
&gt; writeLines(modelStringHP, con = &quot;linregModelHP.stan&quot;)</code></pre>
<p>We can now try to refit the model (additive) using this new specification.</p>
<pre class="r"><code>&gt; X = model.matrix(~cx1 + cx2, data = data)
&gt; data.list &lt;- with(data, list(Y = y, X = X, nX = ncol(X), n = nrow(data),
+     scale_icept = 100, scale_global = 1, nu_global = 1, nu_local = 1, slab_scale = 2,
+     slab_df = 4))
&gt; 
&gt; data.rstan.sparsity &lt;- stan(data = data.list, file = &quot;linregModelHP.stan&quot;, pars = params,
+     chains = nChains, iter = nIter, warmup = burnInSteps, thin = thinSteps, save_dso = TRUE)

SAMPLING FOR MODEL &#39;linregModelHP&#39; NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2500 [  0%]  (Warmup)
Chain 1: Iteration:  250 / 2500 [ 10%]  (Warmup)
Chain 1: Iteration:  500 / 2500 [ 20%]  (Warmup)
Chain 1: Iteration:  750 / 2500 [ 30%]  (Warmup)
Chain 1: Iteration: 1000 / 2500 [ 40%]  (Warmup)
Chain 1: Iteration: 1001 / 2500 [ 40%]  (Sampling)
Chain 1: Iteration: 1250 / 2500 [ 50%]  (Sampling)
Chain 1: Iteration: 1500 / 2500 [ 60%]  (Sampling)
Chain 1: Iteration: 1750 / 2500 [ 70%]  (Sampling)
Chain 1: Iteration: 2000 / 2500 [ 80%]  (Sampling)
Chain 1: Iteration: 2250 / 2500 [ 90%]  (Sampling)
Chain 1: Iteration: 2500 / 2500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.364 seconds (Warm-up)
Chain 1:                0.484 seconds (Sampling)
Chain 1:                0.848 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL &#39;linregModelHP&#39; NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2500 [  0%]  (Warmup)
Chain 2: Iteration:  250 / 2500 [ 10%]  (Warmup)
Chain 2: Iteration:  500 / 2500 [ 20%]  (Warmup)
Chain 2: Iteration:  750 / 2500 [ 30%]  (Warmup)
Chain 2: Iteration: 1000 / 2500 [ 40%]  (Warmup)
Chain 2: Iteration: 1001 / 2500 [ 40%]  (Sampling)
Chain 2: Iteration: 1250 / 2500 [ 50%]  (Sampling)
Chain 2: Iteration: 1500 / 2500 [ 60%]  (Sampling)
Chain 2: Iteration: 1750 / 2500 [ 70%]  (Sampling)
Chain 2: Iteration: 2000 / 2500 [ 80%]  (Sampling)
Chain 2: Iteration: 2250 / 2500 [ 90%]  (Sampling)
Chain 2: Iteration: 2500 / 2500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.328 seconds (Warm-up)
Chain 2:                0.444 seconds (Sampling)
Chain 2:                0.772 seconds (Total)
Chain 2: 
&gt; 
&gt; tidyMCMC(data.rstan.sparsity, pars = c(&quot;beta[1]&quot;, &quot;beta[2]&quot;), conf.int = TRUE,
+     conf.type = &quot;HPDinterval&quot;, rhat = TRUE, ess = TRUE)
# A tibble: 2 x 7
  term    estimate std.error conf.low conf.high  rhat   ess
  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
1 beta[1]     2.75     0.446    1.86       3.61 0.999  3138
2 beta[2]     1.56     0.379    0.816      2.30 1.000  2782
&gt; 
&gt; mcmc_areas(as.matrix(data.rstan.sparsity), regex_par = &quot;beta&quot;)</code></pre>
<p><img src="/STAN/multiple-linear-regression-stan/2020-02-01-multiple-linear-regression-stan_files/figure-html/model_HP_fit-1.png" width="672" /></p>
<p>Obviously, these data are not really appropriate for model selection as there are only two predictors. Both predictors have substantial effects mass larger than zero.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references hanging-indent">
<div id="ref-gelman2019r">
<p>Gelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” <em>The American Statistician</em> 73 (3): 307–9.</p>
</div>
<div id="ref-gelman2015stan">
<p>Gelman, Andrew, Daniel Lee, and Jiqiang Guo. 2015. “Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization.” <em>Journal of Educational and Behavioral Statistics</em> 40 (5): 530–43.</p>
</div>
<div id="ref-gelman2005analysis">
<p>Gelman, Andrew, and others. 2005. “Analysis of Variance—Why It Is More Important Than Ever.” <em>The Annals of Statistics</em> 33 (1): 1–53.</p>
</div>
<div id="ref-piironen2017sparsity">
<p>Piironen, Juho, Aki Vehtari, and others. 2017. “Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors.” <em>Electronic Journal of Statistics</em> 11 (2): 5018–51.</p>
</div>
<div id="ref-rstanpackage">
<p>Stan Development Team. 2018. “RStan: The R Interface to Stan.” <a href="http://mc-stan.org/">http://mc-stan.org/</a>.</p>
</div>
<div id="ref-vehtari2017practical">
<p>Vehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and Waic.” <em>Statistics and Computing</em> 27 (5): 1413–32.</p>
</div>
</div>
</div>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/tutorials/">tutorials</a>
  
  <a class="badge badge-light" href="/tags/stan/">STAN</a>
  
  <a class="badge badge-light" href="/tags/linear-regression/">linear regression</a>
  
</div>



    
      








  
  
    
  
  





  
  
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="/authors/andrea-gabrio/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/stan/simple-linear-regression-stan/simple-linear-regression-stan/">Simple Linear Regression - STAN</a></li>
          
          <li><a href="/jags/multiple-linear-regression-jags/multiple-linear-regression-jags/">Multiple Linear Regression - JAGS</a></li>
          
          <li><a href="/jags/simple-linear-regression-jags/simple-linear-regression-jags/">Simple Linear Regression - JAGS</a></li>
          
          <li><a href="/stan/comparing-two-populations-stan/comparing-two-populations-stan/">Comparing Two Populations - STAN</a></li>
          
          <li><a href="/stan/basic-introduction-to-stan/super-basic-introduction-to-stan/">Super basic introduction to STAN</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3394a224b26ce58ff36f44c54743e0ab.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">

  <p class="powered-by">
    &#169; Andrea Gabrio &middot;
    <code>2020</code> 
    &middot; Based on the 
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
